Document Title,Abstract,Chosen,Authors,Author Affiliations,Publication Year,Page,DOI,Link,Content type
ICT for the Prevention of Avian Influenza,"This paper intends to point out some problem of telemetry for wild birds and future technical expectations to prevent Avian Influenza. The ITU-D(International Telecommunications Union, Development Sector ) SG2(Study Group) Q14(Question) Rapporteur's Meeting hosted by the Ministry of Internal Affairs and Communications of Japan started in Tokyo on July 3-4, 2008. The Q14 deals with application of information and telecommunication technologies in health care. Meeting participants discussed the application of the information and communications technologies to help resolve some issues related to the improvement of access to medical treatment of people living in rural and remote areas of developing countries. The participants have also raised the question about the threat of avian influenza and what could be done with the ultimate goal of preventing and containing this disease. There were considerable deliberation on this emerging disease threat. Therefore, it was recommended that the ITU has to initiate the discussion at appropriate international level on how to set up the Integrated Information and Communications Network for Avian Influenza by using advanced information and telecommunication technologies for tracking of migratory birds. The participants concur in adopting the results of their work as the Statement of Appeal.",,"Nakajima I,Androuchko L,Juzoji H,Tomioka Y,Kitano T",,2009,124–129,,,Conference Paper
"MeghaOS: A Framework for Scalable, Interoperable Cloud Based Operating System","Cloud computing is becoming relevant due to increase in speed of Internet and reduction in its access cost. Desktop computing demands expensive hardware and software suits which become obsolete too often. Ownership of Personal Computers has always remained low in developing countries mainly due to its prohibitive cost. Alternate models of Computer usage like Public-Access Kiosks and low cost Laptops have been tried with limited success. Issues like security and privacy are major concerns in pubic computing. Laptops and other mobile devices lack required hardware support to run computing intensive applications. Increased penetration of Internet and mobile phones are providing new opportunities to bring computing closer to people. Cloud based Operating Systems are an effort in this direction. The present system, named MeghaOS, provides a framework for Desktop-like Operating System OS on a Web Browser. The Cloud becomes a metaphor for Operating System services accessed though Internet. Unlike traditional Operating System, MeghaOS can be accessed on any device having just a Web Browser. Since applications developed using this framework will be cached in Client's machine, network utility will be low. Since data and applications are hosted remotely users can use them without transferring data into local device. MeghaOS provides scalable, multi-device compatible, browser accessible framework for Cloud based Operating System demonstrating next generation computing paradigm.",,"Srinivasa KG,S. HR,H. MK,Venkatesh N",,2012,53–70,10.4018/ijcac.2012010104,https://doi-org.proxy.bnl.lu/10.4018/ijcac.2012010104;http://dx.doi.org/10.4018/ijcac.2012010104,Journal Article
Observing Gender Dynamics and Disparities with Mobile Phone Metadata,"We explore the extent to which gender disparities in Pakistan are reflected in the anonymized mobile phone logs of millions of Pakistani residents. Our analysis uses data capturing the communications behavior of several million individuals, for whom we observe the gender, but no additional demographic or personally identifying information. Here, we focus on validating aggregate regional patterns, correlating metrics derived from the mobile phone logs with socioeconomic statistics collected from more traditional sources. In these preliminary results, we observe a statistically significant relationship between districts with relatively high rates of female mobile phone penetration and districts that report high levels of gender parity in traditional surveys. However, this relationship is not uniform, and less developed regions exhibit a weaker correlation. We interpret these findings as suggestive evidence that such data can provide a novel perspective on gender dynamics in developing countries.",,"Reed PJ,Khan MR,Blumenstock J",,2016,,10.1145/2909609.2909632,https://doi-org.proxy.bnl.lu/10.1145/2909609.2909632;http://dx.doi.org/10.1145/2909609.2909632,Conference Paper
Information Security Vulnerability Prediction Based on Business Process Model Using Machine Learning Approach,,,"Hariyanti E,Djunaidy A,Siahaan D",,2021,,10.1016/j.cose.2021.102422,https://doi-org.proxy.bnl.lu/10.1016/j.cose.2021.102422;http://dx.doi.org/10.1016/j.cose.2021.102422,Journal Article
Shim Shimmeny: Evaluating the Security and Privacy Contributions of Link Shimming in the Modern Web,"Link shimming (also known as URL wrapping) is a technique widely used by websites, where URLs on a site are rewritten to direct link navigations to an intermediary endpoint before redirecting to the original destination. This ""shimming"" of URL clicks can serve navigation security, privacy, and analytics purposes, and has been deployed by prominent websites (e.g., Facebook, Twitter, Microsoft, Google) for over a decade. Yet, we lack a deep understanding of its purported security and privacy contributions, particularly in today's web ecosystem, where modern browsers provide potential alternative mechanisms for protecting link navigations without link shimming's costs.In this paper, we provide a large-scale empirical evaluation of link shimming's security and privacy contributions, using Facebook's real-world deployment as a case study. Our results indicate that even in the modern web, link shimming can provide meaningful security and privacy benefits to users broadly. These benefits are most notable for the sizable populations that we observed with a high prevalence of legacy browser clients, such as in mobile-centric developing countries. We discuss the tradeoff of these gains against potential costs. Beyond link shimming, our findings also provide insights for advancing user online protection, such as on the web ecosystem's distribution of responsibility, legacy software scenarios, and user responses to website security warnings.",,Li F,,2020,,,,Conference Paper
MM&Sec '11: Proceedings of the Thirteenth ACM Multimedia Workshop on Multimedia and Security,"It is our great pleasure to welcome you to the thirteenth ACM Multimedia Security Workshop -- MM&Sec'11. The workshop's continuing objective is to explore research in areas of multimedia data security such as data protection, media forensics, covert channels and security issues in biometrics, as well as related issues in public policy and multimedia infrastructure in real world application. Since 1998, MMSEC has fostered collaboration with researchers and developers in academia, industry and government.The academic world has seen a distressing drop in conference submission across fields and disciplines, and our workshop is no exception to this unfortunate trend. The call for papers attracted 24 submissions from Asia, Europe, Africa, and the United States. We are pleased, however, to see that the quality of papers remains high, confirming our theory that whatever economic phenomenon is reducing publication output, it is not holding back the most interesting and important works of scholarly research. This year's workshop features significant contributions in forensics and a track in biometrics, along with tracks in steganography, encryption and protocols, and applications.We are also pleased to feature not one but three invited speakers, in a series of invited talks on the subject of Bridging Research and Reality. The speakers--Chet Hosmer, Chief Scientist & Sr. Vice President, Allen Corporation; Tony Rodriguez, Chief Technology Officer at Digimarc; and Walter Bruehs of the FBI Forensic Audio, Video, and Imagery Analysis Unit-are experts in the fields of Steganography/Steganalysis, Watermarking, and Multimedia Forensics and will give talks that frame the discourse for related sessions within the workshop. Presentation topics will be focused on real-world lessons learned, the challenges of bringing technology to fruition, as well as applications and experiences with MM&Sec technology applications that range from commercial to law enforcement. It is our hope that these sessions will provoke and inspire attendees.",,,,2011,,,,Book
Accelerating Public Service Delivery in India: Application of Internet of Things and Artificial Intelligence in Agriculture,"The application of Information and Communication Technologies (ICTs) in the public sector can usher performance enhancement, productivity and social equity in public service delivery mechanisms. More specifically, emerging digital technologies including Artificial Intelligence (AI) can be employed for more effective retrieval and analysis of complex, real-time data that could also be captured and shared by devices supporting Internet of Things. Literature asserts that governments worldwide must adopt solutions offered by these emerging technologies to drive innovation in public service delivery mechanisms. Appreciating these claims, this study aims to explore the current and potential use of IoT and AI. Based on the related review of literature, the study puts forth a conceptual framework for creating an open and integrated national level agriculture stack (christened as KisanOne by the authors) so that developing countries like India can effectively espouse data driven approach in its agriculture sector. ""Kisan One"" combines varied aspects of a farmers' activities including weather forecast, soil health indices, seed procurement cycle, sowing cycles, details of fertilizers availability, crop prices, etc, in a unified national stack that is accessible to all the stakeholders using application programming interfaces (APIs). Needless to say, the proposed KisanOne is a utopian implementation where existing and contemporary digital initiatives get unified on a single platform.Datasets themselves have little intrinsic value sans any ability to extract meaning from it. Intelligent data analytics could be employed on real time datasets of KisanOne both for evidence based decision making as well as for malicious intent. This paper, therefore, attempts to offer an insight into such challenges as well as suggest policy recommendations that could strengthen existing regulatory mechanisms for effective implementation of IoT and AI in existing public service delivery schemes of India. The paper is divided into four broad sections. The first section builds the Background of the paper. The next section is divided into four subsections and in this section instance of Agriculture has been detailed with reference to its current scenario and prevailing solutions. India has started using technology in Agriculture to a great extent- some of these applications such as Kisan Suvidha2 mobile app, mKisan SMS Portal, Farmer's Portal, Soil Health Card, Fertilizer Monitoring System(FMS) software, Agrimarket App have been delineated in the study. A use case on transformation of agriculture sector using IoT and AI is also presented in one of the sub-sections. A National Level Integrated Agriculture Stack is also proposed in this paper. The subsequent section presents brief picture of key challenges of implementing IoT and AI in Agriculture sector followed by recommendations and Consulive Remarks. It is an innovative and descriptive study that primarily relies on secondary data gleaned from international/national journals, reports of Ministry of Electronics and Information Technology, Government of India and other online academic sources coupled with creative out-of-box thinking to propose the application of IoT and AI in varied public sectors with special emphasis on Agriculture.",,"Malhotra C,Anand R",,2020,62–69,10.1145/3428502.3428510,https://doi-org.proxy.bnl.lu/10.1145/3428502.3428510;http://dx.doi.org/10.1145/3428502.3428510,Conference Paper
Enhanced Symmetric Based Fully Homomorphic Encryption Using Residue Number System,"Cloud Computing has offered incredible benefits to online information management. However, data protection and privacy still remain an issue. The traditional cryptosystems that have been used include Advanced Encryption Standard and Data Encryption Standard. These schemes are considered inadequate because they require the release of secret keys for data decryption before computation can take place. To preserve integrity and privacy of cloud data, Homomorphic Encryption was introduced to perform computation on encrypted data. The existing asymmetric based Fully Homomorphic Encryption (FHE) schemes are not suitable forreal-world applications due to their high computation overhead and large key size management. Similarly, symmetric based FHE schemes suffer from insecurity against known plaintext/ciphertext attacks and generate a very large ciphertext size that requires a large number of bandwidths to transfer over the network. This study presents a broad assessment of FHE research practices based on 488 articles found in the Web of Science database between2008 and 2018 using bibliometric analysis. Furthermore, a symmetric based FHE scheme built on Matrix Operation for Randomization and Encryption (MORE) and Secret Information Moduli Set (SIMS) was proposed to enhance the existing MORE scheme. Likewise, to overcome the drawbacks of N-prime Model, Residue Number System (RNS) based N-prime model was proposed. Finally, hybrid symmetric based FHE framework was built based on the combination of RNS based N-prime model and MORE-SIMS to overcome the drawbacks of existing symmetric based FHE schemes. The results of bibliometric analysis across continents with publication distribution were found to be 40.16%, 27.66%, 25.41%, 3.89%, 2.05% and0.82% for Asia, North America, Europe, Australia, Africa and South America respectively. MORE-SIMS and MORE schemes produced encryption execution time of 0.066sec and0.063sec with ? × ? and ? × ? × ? storage overhead respectively. Also, the experimental results of proposed RNS based N-prime Model and hybrid framework revealed that averagely, plaintext to ciphertext size expansion ratio were 1:2.89 and 1:3.82 respectively with fast encryption and decryption time as compared to N-prime model with 1:10.21. Bibliometric Analysis revealed that the continent of Asia, North America and Europe had significant impacts in terms of research activities in comparison to other continents. Likewise, analysis highlighted top funding agencies, authors, countries and cited articles. Despite the additional security layer in MORE-SIMS, it still has very close encryption execution time and better storage overhead compared to the MORE scheme. RNS based N-prime Model for symmetric based FHE improves the system latency and reduces the ciphertext file expansion by approximately72% as compared to the existing N-prime Model. Hybrid symmetric based FHE framework satisfied homomorphism properties with robust inbuilt security that resists known plaintext/ciphertext attack or any other forms of statistical attacks. The ciphertext size produced by the proposed hybrid framework is less than 4 times of its equivalent plaintext size with a considerable encryption execution time and fast decryption time. Thus, guaranteed to provide optimum performance and reliable solution for securing integrity and privacy of user's data in the cloud.",,"Jimoh MK,Abdulraheem H,Akanbi C,Adesina L",,2021,,,,Ph.D. Thesis
Low-Cost Communication for Rural Internet Kiosks Using Mechanical Backhaul,"Rural kiosks in developing countries provide a variety of services such as birth, marriage, and death certificates, electricity bill collection, land records, email services, and consulting on medical and agricultural problems. Fundamental to a kiosk's operation is its connection to the Internet. Network connectivity today is primarily provided by dialup telephone, although Very Small Aperture Terminals (VSAT) or long-distance wireless links are also being deployed. These solutions tend to be both expensive and failure prone. Instead, we propose the use of buses and cars as ""mechanical backhaul"" devices to carry data to and from a village and an internet gateway. Building on the pioneering lead of Daknet [15], and extending the Delay Tolerant Networking Research Group architecture [24], we describe a comprehensive solution, encompassing naming, addressing, forwarding, routing, identity management, application support, and security. We believe that this architecture not only meets the top-level goals of low cost and robustness, but also exposes fundamental architectural principles necessary for any such design. We also describe our experiences in implementing a prototype of this architecture.",,"Seth A,Kroeker D,Zaharia M,Guo S,Keshav S",,2006,334–345,10.1145/1161089.1161127,https://doi-org.proxy.bnl.lu/10.1145/1161089.1161127;http://dx.doi.org/10.1145/1161089.1161127,Conference Paper
SafeStreet: Empowering Women against Street Harassment Using a Privacy-Aware Location Based Application,"Sexual harassment of women in public places (e.g., foot-paths, buses, and shopping malls) of major cities in developing countries is a growing concern. These harassments can happen in various forms ranging from commenting, catcalling, and staring to touching and groping, to attacking and raping. Though, the most severe form of harassments such as attacking and raping get some attention from the society, NGOs and law-enforcement agencies, unfortunately, other forms of harassments that are more widespread in public places remain largely un-attended or ignored in our conservative society. However, these harassments are more common and can have various negative psychological impacts on women that include a persistent feeling of insecurity, loss of self-esteem, restricted participation in daily life activities in public places. In this paper, we propose a crowd-powered privacy-aware location based mobile application, SafeStreet, that empowers women in public places against sexual harassments. SafeStreet allows a women to privately capture and share her own experiences in the street. SafeStreet enables a women to find a safe path, i.e., the path to a destination that has less harassment hazard, at any point of time.",,"Ali ME,Rishta SB,Ansari L,Hashem T,Khan AI",,2015,,10.1145/2737856.2737870,https://doi-org.proxy.bnl.lu/10.1145/2737856.2737870;http://dx.doi.org/10.1145/2737856.2737870,Conference Paper
A View from the Other Side: Understanding Mobile Phone Characteristics in the Developing World,"Mobile devices are becoming increasingly dominant in the developing world. However, there is little insight into the characteristics of devices being used in such regions. Using a dataset of 0.5 million subscribers from one of the largest cellular operators in Pakistan, we analyze the characteristics of cell phones based on different features (e.g., CPU, memory, and cellular interface). We identify potential device-level bottlenecks for Internet access and analyze the security implications of the phones being used. To aid the analysis of cell phones, we propose abstractions (e.g., connectivity, capacity, and device security) and cluster phones based on these abstractions. Our analysis reveals interesting insights for improving mobile web performance.",,"Ahmad S,Haamid AL,Qazi ZA,Zhou Z,Benson T,Qazi IA",,2016,319–325,10.1145/2987443.2987470,https://doi-org.proxy.bnl.lu/10.1145/2987443.2987470;http://dx.doi.org/10.1145/2987443.2987470,Conference Paper
AISec '12: Proceedings of the 5th ACM Workshop on Security and Artificial Intelligence,"It is our great pleasure to welcome you to the 5th ACM Workshop on Artificial Intelligence and Security -- AISec 2012. This year's workshop serves as the premier forum for the presentation of research on the leading edge issues of security, privacy, AI and machine learning, and as a venue for developing the fundamental theory and practical applications supporting the use of machine learning for security and privacy. The needs of this burgeoning community, who are especially focused on (among other topics) learning in game-theoretic adversarial environments, privacypreserving learning, or the use of sophisticated new learning algorithms in security is not met elsewhere. The mission of this workshop is thus to share novel AI solutions that fulfill the needs of security applications, to identify the unique constraints of learning in security-sensitive environments, and to explore new directions for future research and development. AISec gives researchers and practitioners a unique opportunity to share their perspectives with others interested in the intersection of AI and security applications. We are delighted to once again be co-located with the premier ACM Computer and Communication Security (CCS 2012) conference.The call for papers attracted 24 submissions from Asia, Europe, Africa, and North America. The program committee accepted 10 papers that cover a variety of topics including approaches to malware, spam, and intrusion detection as well as automatic redaction. In addition, we are delighted that the program includes a keynote speech entitled ""Machine Learning Meets Social Networking Security -- Detecting and Analyzing Malicious Social Networks for Fun and Profit"" to be presented by Guofei Gu from Texas A&M. Together, we hope these contributions comprise a wonderful program that will bring together leading researchers with backgrounds in both Security & Privacy and AI.",,,,2012,,,,Book
Ensemble Method Based on Artificial Neural Networks to Estimate Air Pollution Health Risks,,,"Araujo LN,Belotti JT,Alves TA,Tadano YS,Siqueira H",,2020,,10.1016/j.envsoft.2019.104567,https://doi-org.proxy.bnl.lu/10.1016/j.envsoft.2019.104567;http://dx.doi.org/10.1016/j.envsoft.2019.104567,Journal Article
QoSA '13: Proceedings of the 9th International ACM Sigsoft Conference on Quality of Software Architectures,"Welcome to the 9th International ACM Sigsoft Conference on the Quality of Software Architectures -- QoSA 2013. For almost a decade, QoSA has strived to advance the state of the art of quality aspects of software architecture, focusing broadly on its quality characteristics and how these relate to the design of software architectures. Specific issues of interest are defining and modeling quality measures, evaluating and managing architecture quality, linking architecture to requirements and implementation, and preserving architecture quality throughout the system lifetime. Past themes for QoSA include Evolving Architectures (2012), Quality throughout the Software Lifecycle (2011), Research into Practice -- Reality and Gaps (2010), Architectures for Adaptive Software Systems (2009), and Models and Architecture (2008).The umbrella theme of QoSA 2013 is The System View. Contributions were solicited that explore the holistic system perspective, connect quality of software architecture to system considerations, and explore foundations for assuring the desirable quality of systems that rely on software.The call for papers attracted 42 submissions from Europe, Australia, North America, Asia, South America, and Africa. The program committee accepted 17 papers that cover topics on systems-ofsystems, adaptive systems, quality analyses, reverse engineering, evolution, models, patterns, and viewpoints. In addition, the program includes a keynote speech by Mahadev Satyanarayanan on Cloudlets: At the Leading Edge of Cloud-Mobile Convergence. Finally, the program includes an invited paper by Benjamin Klatt and Martin Küster on Improving Product Copy Consolidation by Component-Architecture-Based Difference and Variation Point Analysis, which is published as part of the CompArch'12 Young Investigator Award.QoSA is one of the federated events at CompArch, this year together with ""CBSE 2013: 16th International ACM SIGSOFT Symposium on Component Based Software Engineering,"" ""ISARCS 2013: 4th International ACM SIGSOFT Symposium on Architecting Critical Systems,"" and ""WCOP 2013: 18th International Doctoral Symposium on Components and Architecture"". We are grateful to the organizers of all these events for making CompArch a successful federated event on Component-based Software Engineering and Software Architecture.",,,,2013,,,,Book
Understanding Online Shopping Adoption in India: Unified Theory of Acceptance and Use of Technology 2 UTAUT2 With Perceived Risk Application,"The purpose of this research is to analyze perceived risk and drivers of online shopping influencing behavioral intention in India. The study empirically validates website design, cash-on-delivery COD mode of payment, and different facets of perceived risk with the unified theory of acceptance and use of technology2 UTAUT2 of Venkatesh et al. [Venkatesh V, Thong JYL, Xu X 2012 Consumer acceptance and use of information technology: Extending the unified theory of acceptance and use of technology. MIS Quart. 361:157-178]. Findings of the study revealed that perceived risk had a negative relation with behavioral intention, whereas the drivers were positively associated with behavioral intention. Besides analyzing cash-on-delivery mode of payment as a construct, it also includes website design to enhance application of UTUAT2 in Indian and other similar developing countries' context. The study will help online retailers to focus in the right direction by eliminating threats and convert nonshoppers to online shoppers.",,"Tandon U,Kiran R,Sah AN",,2016,420–437,10.1287/serv.2016.0154,https://doi-org.proxy.bnl.lu/10.1287/serv.2016.0154;http://dx.doi.org/10.1287/serv.2016.0154,Journal Article
No Smurfs: Revealing Fraud Chains in Mobile Money Transfers,"Mobile Money Transfer (MMT) services provided by mobile network operators enable funds transfers made on mobile devices of end-users, using digital equivalent of cash (electronic money) without any bank accounts involved. MMT simplifies banking relationships and facilitates financial inclusion, and, therefore, is rapidly expanding all around the world, especially in developing countries. MMT systems are subject to the same controls as those required for financial institutions, including the detection of Money Laundering (ML) - a source of concern for MMT service providers. In this paper we focus on an often practiced ML technique known as micro-structuring of funds or smurfing and introduce a new method for detection of fraud chains in MMT systems. Whereas classical detection methods are based on machine learning and data mining, this work builds on Predictive Security Analysis at Runtime (PSA@R), a model-based approach for event-driven process analysis. We provide an extension to PSA@R which allows us to identify fraudsters in an MMT service monitoring network behavior of its end-users. We evaluate our method on simulated transaction logs, containing approximately 460,000 transactions for 10,000 end-users, and compare it with classical fraud detection approaches. With 99.81% precision and 90.18% recall, we achieve better recognition performance in comparison with the state of the art.",,"Zhdanova M,Repp J,Rieke R,Gaber C,Hemery B",,2014,11–20,10.1109/ARES.2014.10,https://doi-org.proxy.bnl.lu/10.1109/ARES.2014.10;http://dx.doi.org/10.1109/ARES.2014.10,Conference Paper
Towards a Unified Trust Model for M-Commerce Systems,"M-commerce has become one of the most evolutionary fields not only in the developed countries but also in the developing countries. It facilitates transactional procedures using mobile devices that are being enhanced rapidly to become simpler and more secured. Currently users can make any monetary transaction such as booking tickets or buying goods online anywhere anytime. One of the most challenging concerns in developing m-commerce systems and applications is trustworthiness, because trust is very hard to gain and very easy to lose. Trust is a fuzzy concept as it is defined differently in each discipline even within each, it's perceived in a subjective way. A key step to incorporate trust in m-commerce is to acquire an in-depth understanding of the trustworthiness. This paper presents an attempt to develop a unified trust model for m-commerce. To achieve unification and abstraction, the proposed unified trust model for m-commerce separates changeable and endurable aspects of trustiness. In addition, the proposed model is validated through the demonstration of how existing trust models are embedded within the proposed unified model.",,"Hamed WS,Hamza HS,Saroit IA",,2011,992–997,10.1109/ITNG.2011.170,https://doi-org.proxy.bnl.lu/10.1109/ITNG.2011.170;http://dx.doi.org/10.1109/ITNG.2011.170,Conference Paper
"Essays on Globalization, Labor Market, and Productivity","This dissertation studies issues at the intersection of globalization, labor market, and productivity in developing countries. It is composed of three chapters.Chapter 1 studies how a country's trade policy affects competition in its domestic labor market. In a heterogeneous-firm model with oligopsonistic local labor markets, this chapter demonstrates that opening up to trade can affect distortions in such markets. These distortions arise because firms are large and able to exercise market power over their local workers. Using a panel dataset of Chinese manufacturing firms from 1998-2007, I measure firm-level labor market distortion, captured by the ratio between marginal revenue product of labor and wage, and examine its evolution following China's trade policy reform in 2001. The baseline measure of the overall distortion implies a 53% pass-through rate of an idiosyncratic productivity shock to wage. The component of this distortion that arises purely from labor market power accounts for almost 80% of the overall distortion. I find that China's trade policy reforms have led to a substantial net reduction in the labor market power distortion, with large effects working through the liberalization of input tariffs. These findings suggest novel effects of trade policy that deviate from the conventional trade models featuring perfect competition in the labor market.Chapter 2 investigates the impact of a large export shock on intergenerational mobility in Vietnam. We use eight rounds of Vietnam Household Living Standards Surveys (VHLSSs) spanning over almost two decades to measure intergenerational mobility based on education levels of fathers and sons within households. Exploiting the US-Vietnam Bilateral Trade Agreement (BTA) in 2001 as an export shock and a difference-in-difference research design, our analysis suggests that the BTA shock has led to substantial upward occupational mobility, accounting for one-third of overall increase in mobility in Vietnam during our sample period. We also show that this effect potentially works through improvements in educational attainment. The results further reveal that both increases in exports overall and export unit-value in particular have contributed to the upward mobility. Our results highlight that international trade as an external shock can break down the persistence of socioeconomic status across generations in Vietnam.Chapter 3 examines two novel productivity effects of foreign ownership and foreign acquisitions on Chinese high-tech manufacturing firms: the dynamic and the non-(Hicks)-neutral effects. The dynamic productivity effect of foreign ownership arises because adoption of foreign technology and management practices often takes time to fully realize. On the other hand, since advanced production technologies tend to have non-neutral productivity implications in developed countries, meaning that they could be capital- or labor-augmenting, such technology, transferred through foreign investment, can have similar effects in developing countries. We propose an econometric framework to estimate both effects. Our framework extends a recent nonparametric productivity framework developed by Gandhi, Navarro, and Rivers (2017), in which identification is achieved by firm's first-order condition and timing assumptions. We find strong evidence of both effects due to foreign ownership. These effects provide a more comprehensive perspective on the impact of foreign investment on firms' productivity in developing countries.",,"Pham HL,Flores-Lagunes A,Lee Y,Buzard K,Hamersma S",,2020,,,,Ph.D. Thesis
WiMAX: Standards and Security,"As the demand for broadband services continues to grow worldwide, traditional solutions, such as digital cable and fiber optics, are often difficult and expensive to implement, especially in rural and remote areas. The emerging WiMAX system satisfies the growing need for high data-rate applications such as voiceover IP, video conferencing, interactive gaming, and multimedia streaming. WiMAX deployments not only serve residential and enterprise users but can also be deployed as a backhaul for Wi-Fi hotspots or 3G cellular towers. By providing affordable wireless broadband access, the technology of WiMAX will revolutionize broadband communications in the developed world and bridge the digital divide in developing countries. Part of the WiMAX Handbook, this volume focuses on the standards and security issues of WiMAX. The book examines standardized versus proprietary solutions for wireless broadband access, reviews the core medium access control protocol of WiMAX systems, and presents carriers' perspectives on wireless services. It also discusses the main mobility functions of the IEEE 802.16e standard, describes how to speed up WiMAX handover procedures, presents the 802.16 mesh protocol, and surveys the testing and certification processes used for WiMAX products. In addition, the book reviews the security features of both IEEE 802.16 and WiMAX. With the revolutionary technology of WiMAX, the lives of many will undoubtedly improve, thereby leading to greater economic empowerment.",,"Ahson SA,Ilyas M",,2007,,,,Book
Women in Rural Bangladesh: Empowered by Access to Mobile Phones,"Mobile phones are seen as a means for social and economic progress in rural and remote areas of developing countries. In Bangladesh the availability and use of information and communication technology (ICT), particularly mobile phones, is thought to have accelerated the development of women in the rural population by creating the possibility of a wider connection. Using qualitative and quantitative methods for data collection, this research has investigated the impact of mobile phone use by women with particular emphasis on opportunities in health, education and livelihood. A sample of 99 women from three rural villages in Bangladesh showed that mobile phones provide easy access to health related services. Although impact on facilitating girls' education appears to be limited, mobile phones have an indirect effect in ensuring security for girls. Respondents confirmed that their overall living standards have improved due to access to information on economic and income earning opportunities. These rural women also feel independent and empowered by access to a mobile phone. It can be argued that mobile phone technology can facilitate improvements in the living standards of rural women, which contribute to their personal development. Finally, the paper suggests that wide and innovative utilization of ICT is needed to accelerate development of women in the rural population with the help of low-cost mobile phone technology.",,"Islam MK,Slack F",,2016,75–84,10.1145/2910019.2910074,https://doi-org.proxy.bnl.lu/10.1145/2910019.2910074;http://dx.doi.org/10.1145/2910019.2910074,Conference Paper
Development of SWIM Registry for Air Traffic Management with the Blockchain Support,"System Wide Information Management (SWIM) including SWIM Registry for Air Traffic Management (ATM) has been successfully developed and applied in Europe and United States. The most developing countries have just started to study the employment of SWIM concept, which its establishment is required prior to the development of SWIM Registry. In this paper, we introduce the experience of the development of SWIM Registry Brazil, which comprises the study of the architecture, components, services and data accessing. In order to encourage consumers and providers to participate in the SWIM community, we developed a prototype of SWIM Registry Demonstration for the Brazilian ATM society. We propose a model based on Blockchain for managing services currently provided by Brazilian ATM in order to certificate operations which are performed by consumers, authorities and involved stakeholders. The proposed model is expected to provide services for SWIM Registry with integrity, efficiency, security and authenticity, which are fundamental for the proper operation of Brazilian aviation system.",,"Bonomo IS,Barbosa IR,Monteiro L,Bassetto C,de Barros Barreto A,Borges VR,Weigang L",,2018,3544–3549,10.1109/ITSC.2018.8569223,https://doi-org.proxy.bnl.lu/10.1109/ITSC.2018.8569223;http://dx.doi.org/10.1109/ITSC.2018.8569223,Conference Paper
Practical Receipt Authentication for Branchless Banking,"Although branchless banking systems have spread to different parts of the developing world, methods to ensure transactional security in these systems have seen slower adoption because of a variety of operational constraints. A basic requirement from such systems is the provision of secure and reliable receipts to users during transactions, and recent attacks have demonstrated that existing systems fall short of fulfilling this requirement in practice. In this paper, we propose a simple and practical protocol to enable users to authenticate transaction receipts in branchless banking systems. Our protocol makes novel use of missed calls (sent from users to the bank) to help distinguish real receipts from spoofed ones and can be implemented on any mobile phone, without software installation. Besides preventing spoofing attacks, the protocol enjoys significant advantages of usability, efficiency and cost, which make it a more practical choice than other schemes. We also discuss ways to use missed calls to mitigate man-in-the-middle attacks on branchless banking systems.",,Panjwani S,,2013,,10.1145/2442882.2442886,https://doi-org.proxy.bnl.lu/10.1145/2442882.2442886;http://dx.doi.org/10.1145/2442882.2442886,Conference Paper
Cause of and Factors Contributing to Stillbirth in Sub-Saharan Africa,"BackgroundEvery year, an estimated 2.6 million stillbirths occur worldwide, with up to 98% occurring in low- and middle-income countries (LMIC). Most stillbirths are preventable. To develop strategies and take effective actions to end preventable stillbirths, a good understanding of the cause of death and its contributing factors is necessary. There is, however, a paucity of data from most LMIC settings. This study aimed to determine the cause of stillbirth in LMIC using three methods of assessment, and to assess quality of care delivered to mothers who had stillbirth.MethodsThe study involved 1,563 stillbirths which occurred in 12 selected secondary and tertiary hospitals in Kenya, Malawi, Sierra Leone and Zimbabwe. The cause of death was determined by: (1) consensus of healthcare providers (HCPs) through stillbirth review; (2) expert review of cases and; (3) computer algorithms. Cause of death was classified using the classification according to Relevant Condition at Death (ReCoDe) and the International Classification of Diseases for Perinatal Mortality (ICD-PM). Quality of antenatal and intrapartum care and health system factors were reviewed using a set of criteria. ResultsA total of 1,329 cases were reviewed, of which 1,267 (95.3%) stillbirths met the inclusion criteria. By country, the stillbirth rate ranged from 20.3 (Malawi) to 118.1 (Sierra Leone) per 1,000 births. The distribution of the major causes of stillbirth differed by method of assessment: asphyxia (18.5% – 37.4%), placental disorders (8.4% – 15.1%), hypertensive disorders in the mother (5.1% – 13.6%), infection (4.3% – 9.0%), cord problems (3.3% – 6.5%), and ruptured uterus due to obstructed labour (2.6% – 6.1%). Information was insufficient to assign cause of stillbirth in 17.9% - 26.0% of cases. Significant agreement was observed between cause of stillbirth assigned by the expert panel and by HCP (k=0.69; p<0.0005) but there was a weaker agreement between expert panel and when using computer algorithms (k=0.34; p<0.0005).Using ReCoDe, intrapartum events (mainly intrapartum asphyxia) contributed to most of the deaths, followed by maternal diseases (mainly hypertensive disorders and infection), placental and fetal conditions. With application of ICD-PM, 42.0% were antepartum, 50.7% were intrapartum and 7.3% could not be categorised. The major categories accounting for the death were: intrapartum hypoxia and fetal growth restriction. Major contributing maternal conditions in ICD-PM were: M1 (placental, cord and membranes) and M3 (other complications of labour and delivery). Poor quality of care during antenatal care was identified in 97.8% of cases, and only 30.7% of cases of Caesarean section were conducted within one hour of decision. For 414 (37.9%) stillbirths, the outcome could have been different with better care.Conclusion Stillbirth rate was high, with high variations between countries. HCPs should be encouraged to conduct reviews and act upon findings to improve quality of care. Data requirements of computer algorithms need to be balanced between ability to find a cause and the availability of information. The new ICD-PM could work in LMIC, but there is the need for more guidance on how to handle cases of stillbirths whose time of death cannot be determined.",,Aminu M,,2017,,,,Ph.D. Thesis
A Framework to Leverage Cloud for Modernization of Indian Agricultural Produce Marketing System,"In India, Information and Communication technology (ICT) is being leveraged as a modernization tool in almost every sector of economy such as health, education, and transportation. But when we consider the agricultural scenario in the Indian context, we realise that the ICT remains to be exploited to accrue its invaluable benefits. In recent times, the Government of India has introduced several initiatives to promote the application of ICT in agriculture sector. But when we compare the scale of ICT application in Indian agriculture sector with other developing countries like China, Brazil, etc., we find that application of ICT in Indian agriculture is yet to be applied on a significant magnitude. In this paper, we propose a cloud deployment model ""Agri-Bridge"", which provides access to agricultural market related information to farmers facing market connectivity constraints and acute capital shortage. Also, this model will operate as a bridge between the farmers and consumers within the existing agricultural produce marketing chain. This model utilizes the existing Government services, Agricultural Produce Marketing Committee (APMC) databases, retail market sources besides leveraging cloud computing, mobile phone services and Internet services to provide a solution to the problem of lack of access to real-time market information to the farmers, hence modernising the Indian agricultural produce marketing system.",,"Matharu GS,Mishra A,Chhikara P",,2014,,10.1145/2677855.2677862,https://doi-org.proxy.bnl.lu/10.1145/2677855.2677862;http://dx.doi.org/10.1145/2677855.2677862,Conference Paper
Lessons Learned Building Low-Cost DIY Tactile Graphics and Conducting a Tactile Drawing Club in Colombia During COVID-19,"Perceiving images and drawing are fundamental parts of human life, and thus access to them should be a universal right. However, there is a large breach for people with visual impairments to access diverse graphics, let alone drawing. There are several techniques of tactile graphics, such as swell paper, Braille embossing, and thermoform that help to alleviate this gap. However, in developing countries, the high cost and lack of access make them impractical. In this work, we describe our experience improving access to tactile graphics and drawing in Colombia. We created low-cost, effective and efficient, tactile graphics and drawing techniques that improve on current solutions. These techniques were created from the best practices of two projects adapting pieces from the Colombian art heritage [52, 53] for blind and visually impaired people. They were then applied to a third project: running a virtual tactile drawing club with blind and visually impaired participants in the middle of the COVID-19 pandemic. The lessons learned from these experiences are presented in this paper with the hope they can help the community democratize access to tactile graphics.",,"Zuniga-Zabala MF,Guerra-Gomez JA",,2022,,10.1145/3491101.3503559,https://doi-org.proxy.bnl.lu/10.1145/3491101.3503559;http://dx.doi.org/10.1145/3491101.3503559,Conference Paper
Integrating MDA and SOA for Improving Telemedicine Services,"Telemedicine supports the geographic distribution and continuity of medical services.Model Driven Architecture (MDA) or the separation of functional and technical needs.Service Oriented Architecture (SOA) for interoperable exchanges among applications.Combined use of MDA SOA for more scalable and sustainable services in telemedicine.Reverse engineering in the RAFT application for its modernization and improvement. Through telemedicine, the health sector has seized the opportunity offered by development of information and communications technology (ICT) such as the business or industrial sectors, but ICTs are constantly evolving. To benefit from technological progress it is necessary to adapt the computer applications to these technologies, however this operation is costly to health facilities especially in developing countries. In terms of scientific research, this observation explains the development of model-driven engineering of computer systems such as the Model Driven Architecture (MDA) approach. MDA is a computer design approach for the development of computer systems that considers separately the functional needs of technical needs of an application. MDA mainly uses the models and their transformations whose traces allow MDA to capitalize expertise in terms of technology and to ensure some rapid modernization of applications to new technologies which results in a significant productivity gain. Today there is a huge requirement worldwide in the interoperable services, in particular with regard to their valuable contribution to the collaboration ability of remote information technology systems. Service Oriented Architecture (SOA) is an interesting architectural pattern in which software components contribute to the collaboration and sharing of services. In this way, the principles of SOA are intended to ensure interoperability between heterogeneous and distributed applications. Web services are at the heart of SOA, which splits functions into different services, accessible over a computer network that enables users to associate and reuse them in the exploitation of applications. Health applications have a strong need to communicate with the remote institutions in order to provide the most relevant services to patients and to collaborate with other medical partners to solve complex tasks. For this purpose, the proposed research work shows how the paradigms of SOA and MDA can be configured to implement medical software applications on an e-health platform. The case study concerns the Telemedicine in French-speaking Africa (RAFT) project in which the joint use of MDA and SOA facilitates knowledge combination and reuse in the management of applications supporting a medical collaborative work environment.",,"Traore BB,Kamsu-Foguem B,Tangara F",,2016,733–741,10.1016/j.tele.2015.11.009,https://doi-org.proxy.bnl.lu/10.1016/j.tele.2015.11.009;http://dx.doi.org/10.1016/j.tele.2015.11.009,Journal Article
Assessing the SMEs' Competitive Strategies on the Impact of Environmental Factors: A Quantitative SWOT Analysis Application,"Strength, Weakness, Opportunity and Threat (SWOT) analysis is an established methodology for assisting the formulation of strategy. This paper proposes a new quantified SWOT analytic method incorporated with the vote-ranking method. The indices of SWOT are voted, weighted and quantified to assess the competitive strategy, from top to the bottom, meanwhile the total weighted scores method will be used to get the best strategy alternatives. The competitive strategies of the Taiwanese Small and Medium Enterprises (SMEs) in the Environmental Management Systems (EMS) are taken as a case study, where eighteen certificated ISO9000 or ISO14000 auditors (or lead auditors) are invited to establish a decision group. Under the impact of environmental factors, the results show that company's image and profitability is the most important strategy for SMEs within the global markets. Lastly, Taiwanese SMEs apperceive the significance of EMS and also recognize the importance to survive within the diversified competing market environment, whereas they need to build up its environmental management that has to suit the EMS specification and attention. The findings are also applicable for other developing countries within the global markets or barriers.",,Hai HL,,2008,1701–1710,,,Journal Article
Healthcare Providers’ Perspective about the Use of Telemedicine in Egypt: A National Survey,"Incorporation of telemedicine in general clinical practice is becoming a compelling need nowadays in the context of COVID-19 pandemic and its consequent burdens on the healthcare systems. Though telemedicine appears to be appealing and carries a lot of advantages, yet it is still faced by many challenges and barriers especially in developing countries. Our aim was to explore the impression of healthcare providers about telemedicine and its applicability in clinical practice in Egypt. A cross-sectional study was conducted among healthcare providers from different Egyptian governorates through a web-based survey. The survey gathered information about demographic, socioeconomic features of the enrolled healthcare participants; their knowledge, previous experience, impression about telemedicine, advantages of telemedicine over traditional medical services, barriers that may face telemedicine, and additional services that can be provided by telemedicine were also explored. Our study enrolled 642 healthcare providers from all over Egypt, 43.77% were females, of which 55.5% were physicians, 27.3% were nurses, 6.1% were technicians, 7.6% were administrative clerks, and 3.6% were medical directors. Sixty-four percent of participants reported that they have never used telemedicine. Smartphones were the most commonly used mean in the group who used telemedicine (65%), and smartphone applications were the favorable telemedicine service for about 50% of participants. Participants assumed that the use of telemedicine might not have a negative effect on the doctor-patient relationship but raised some concerns regarding the privacy and security of patients’ data. Despite the fact that telemedicine appears to be appealing and widely accepted by healthcare providers, yet still, its implementation is confronted by some obstacles. Precise organizational guidelines need to be developed to clearly figure out the exact role of each healthcare provider to minimize their doubtfulness about telemedicine and to facilitate its adoption.",,"Alboraie M,Abdalgaber M,Youssef N,Moaz I,Abdeen N,Abosheaishaa HM,Shokry MT,El-Raey F,Asfour SS,Abdeldayem WA,Hassan AA,Mahran EE,Tag-Adeen M,Elshaarawy O,Radwan MI,Altonbary A,Fouad Y,Tsiknakis M",,2022,,10.1155/2022/3811068,https://doi-org.proxy.bnl.lu/10.1155/2022/3811068;http://dx.doi.org/10.1155/2022/3811068,Journal Article
"Using Mobile Phones for Secure, Distributed Document Processing in the Developing World","Although paper plays an essential role in many information ecologies in the developing world, paper-based record keeping can be inefficient and inflexible. The CAM document-processing framework, so called because the phoneýs built-in digital camera plays a key role in the user interface, exploits smart mobile phonesý utility, usability, and growing ubiquity to link paper with modern information tools. The CAM interface consists of CamForm augmented documents, which users interact with; the CamBrowser mobile phone application, which interprets these documents; and the CamShell scripting language, which ties the two together. CAM is a cost effective and accessible way of providing information services to remote rural areas.",,Parikh TS,,2005,74–81,10.1109/MPRV.2005.43,https://doi-org.proxy.bnl.lu/10.1109/MPRV.2005.43;http://dx.doi.org/10.1109/MPRV.2005.43,Journal Article
Evaluation of the Implementation of a Technical Package for Cardiovascular Disease Reduction with Emphasis on Hypertension Control in Colombia Using the Consolidated Framework for Implementation Research,"Worldwide, more people die from cardiovascular diseases (CVDs) than from any other diseases. Of these deaths, 80% are due to heart attacks and strokes, and about three quarters occur in low-and-middle income countries (LMICs). In the Region of the Americas, CVDs result in 1.9 million annual deaths of which one third occur before the age of 70. Hypertension is the underlying cause of 60.1% of all ischemic heart disease and 63.7% of all strokes. The estimated prevalence of hypertension ranges between 20% to 40%. To respond to CVDs, the World Health Organization (WHO) launched the Global Hearts Initiative in 2016 for the prevention and management of CVDs through policy and health system strengthening interventions organized in a set of technical packages. The HEARTS Technical Package is a group of evidence-based interventions: Healthy-lifestyle counseling, Evidence-based treatment protocols, Access to essential medicines and technology, Risk-based management, Team-based care, and Systems for monitoring. The Pan American Health Organization (PAHO) has been guiding the HEARTS implementation in the Americas, and Colombia is part of the first cohort of HEARTS implementing countries.The main purpose of this study is to analyze the current implementation of the HEARTS Initiative in Colombia, first by describing overall implementation conceptual underpinnings and second, by mapping the current implementation strategies onto the Consolidated Implementation Research Framework (CFIR). The method used is a qualitative inquiry based on semi-structured interviews of 54 implementers in Colombia from the twelve original public primary care health centers that are implementing HEARTS and few referenced national and state level health officials. Inductive analyses of the themes from the interviews allowed for the construction of a program theory of change from the ground up. Deductive analyses that applied the CIRF to the data identified higher order factors that also shaped implementation. The results show high homogeneity in the level of information and absorption of the model across the twelve public primary health centers. The core components proposed by the HEARTS model were adapted to the context of Colombia which was still undergoing a health care reform that started in 2015, which created policies and a health care model into which the HEARTS model was being inserted. HEARTS driven data cleaning processes, development of standardized hypertension treatment protocols and redesigning of patient workflows were important milestones of implementation. Leadership at all levels, human resources stability and continuous training were determining cross-cutting factors that affect the adoption of the HEARTS model. The simplicity of the HEARTS model, the inclusion of front-line practitioners from the onset and the support of the external international organizations have positively affected implementation. Lack of coordination with financing actors, such as insurance entities, may pose one of the greatest challenges to implementation and sustainability. Mapping the implementation of HEARTS in Colombia contributes to the knowledge base on effective implementation of chronic disease management models in LMIC.",,"Giraldo Arcila GP,Karlamangla A,Kuhn R,Macinko J",,2020,,,,Ph.D. Thesis
IT Governance: An International Guide to Data Security and ISO27001/ISO27002,"The development of IT governance - which recognizes the convergence between business practice and IT management - makes it essential for managers at all levels, and in organizations of all sizes, to understand how best to deal with information security and cyber risk.The fifth edition includes chapters on: information security policy and scope; controls against malicious software; e-commerce services and PCI DSS; e-mail, internet use and social media governance; network access controls and hacking attacks; mobile security controls; business continuity management;the ISO27001 audit.The new edition has been fully updated to take account of the latest regulatory and technological developments, including the creation of the International Board for IT Governance Qualifications. IT Governance also includes new material on key markets - including the UK and the US, Australia and South Africa.",,"Calder A,Watkins S",,2012,,,,Book
UbiComp '14: Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing,"2014 has been another incredible year for Ubiquitous Computing, as body worn, mobile, sensor rich technologies pervade every aspect of our daily lives and the augment the infrastructures and environments we inhabit, and instigates changes in our societies. Such technologies both enable new practices and behaviours, but also challenge us to consider the role and impact of such technologies are having on our lives, and even to question our very rights and entitlements. This year's program reflects the full gamut of these endeavours, from fundamental techniques in sensing, activity recognition, and security on mobiles, to focused interventions in healthcare and for specialist applications. We are particularly delighted to welcome our keynote, Gaetano Borriello, who's work on Open Data Kit is leading to new Applications of Mobile Devices in the Developing World, inspiring us to think how we might transform lives using ubiquitous technologies within our own work.",,,,2014,,,,Book
UbiComp '14 Adjunct: Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct Publication,"2014 has been another incredible year for Ubiquitous Computing, as body worn, mobile, sensor rich technologies pervade every aspect of our daily lives and the augment the infrastructures and environments we inhabit, and instigates changes in our societies. Such technologies both enable new practices and behaviours, but also challenge us to consider the role and impact of such technologies are having on our lives, and even to question our very rights and entitlements. This year's program reflects the full gamut of these endeavours, from fundamental techniques in sensing, activity recognition, and security on mobiles, to focused interventions in healthcare and for specialist applications. We are particularly delighted to welcome our keynote, Gaetano Borriello, who's work on Open Data Kit is leading to new Applications of Mobile Devices in the Developing World, inspiring us to think how we might transform lives using ubiquitous technologies within our own work.",,,,2014,,,,Book
Land Records on Blockchain for Implementation of Land Titling in India,,,"Thakur V,Doja MN,Dwivedi YK,Ahmad T,Khadanga G",,2020,,10.1016/j.ijinfomgt.2019.04.013,https://doi-org.proxy.bnl.lu/10.1016/j.ijinfomgt.2019.04.013;http://dx.doi.org/10.1016/j.ijinfomgt.2019.04.013,Journal Article
Determining Antecedents of Intention to Adopt Goods and Service Tax Network,"With the changing worldwide economic scenario, there occurs a need to structure new tax reforms especially for a developing country like India. With this ideation, the Indian government introduced the goods and service tax (GST) in order to conceptualise a common tax system. However, with digitalization transforming every service from offline to online mode, the government developed a not-for-profit website, GST network (GSTN), where citizens can smoothly and securely file their returns. Since the concept is new, studying the GSTN adoption by considering technology acceptance model (TAM) and unified theory of adoption and use of technology (UTAUT) variables becomes a novel approach. The article considers perceived ease of use, perceived usefulness, perceived risk, social influence, and facilitating conditions as exogenous variables whereas intention to adopt GSTN is considered as endogenous one. A partial least square (PLS) path modelling approach is applied on a survey data in order to validate the hypothesised model.",,Guleria N,,2020,30–41,10.4018/IJEA.2020010103,https://doi-org.proxy.bnl.lu/10.4018/IJEA.2020010103;http://dx.doi.org/10.4018/IJEA.2020010103,Journal Article
A Novel PCA–Whale Optimization-Based Deep Neural Network Model for Classification of Tomato Plant Diseases Using GPU,"The human population is growing at a very rapid scale. With this progressive growth, it is extremely important to ensure that healthy food is available for the survival of the inhabitants of this planet. Also, the economy of developing countries is highly dependent on agricultural production. The overall economic balance gets affected if there is a variance in the demand and supply of food or agricultural products. Diseases in plants are a great threat to the yield of the crops thereby causing famines and economy slow down. Our present study focuses on applying machine learning model for classifying tomato disease image dataset to proactively take necessary steps to combat such agricultural crisis. In this work, the dataset is collected from publicly available plant–village dataset. The significant features are extracted from the dataset using the hybrid-principal component analysis–Whale optimization algorithm. Further the extracted data are fed into a deep neural network for classification of tomato diseases. The proposed model is then evaluated with the classical machine learning techniques to establish the superiority in terms of accuracy and loss rate metrics.",,"Gadekallu TR,Rajput DS,Reddy MP,Lakshmanna K,Bhattacharya S,Singh S,Jolfaei A,Alazab M",,2021,1383–1396,10.1007/s11554-020-00987-8,https://doi-org.proxy.bnl.lu/10.1007/s11554-020-00987-8;http://dx.doi.org/10.1007/s11554-020-00987-8,Journal Article
4G Mobile and Wireless Communications Technologies,"Mobile and wireless communications are moving towards a new era that will be characterized by the seamless collaboration of heterogeneous systems, the need for high speed communications while on the move and for advanced services with quality guarantees. Recent market research studies show that most of the traffic in the future wireless networks will be produced by mobile multimedia services which are expected to proliferate by the year 2010. On the other hand mobile and wireless communications technology is becoming more and more important in developing countries where people demand fast deployment and low cost for broadband wireless internet services. The objective of this volume is to gather research and development on topics shaping the fourth generation (4G) in mobile and wireless communications and reveal the key trends and enabling technologies for 4G. We envisage 4G wireless communication systems as IP based solution providing integrated services (voice, data, multimedia) regardless of time and end-users' location. 4G technologies will manifest the benefits of the wireless and wired technologies convergence, through enabling a wide range of innovative (both indoor and outdoor) applications. 4G applications will feature premium quality, high security and an affordable cost. The vision, though fantastic, is associated with a host of technical and technological challenges. A great deal of the later are discussed in the articles of this volume, which aims at providing insights on the research issues and solutions that are directly associated with leading edge 4G technologies and services. Taking into account recent developments in the world of wireless communications we have given emphasis to cover all these technologies and aspects that are considered as cornerstones for achieving the goals set for 4G and that will further boost research and development of next-generation mobile communications.",,"Kyriazakos S,Soldatos I,Karetsos G",,2008,,,,Book
Cybersecurity Awareness Based on Software and E-Mail Security with Statistical Analysis,"The aim of this study is to discover the impact of software security and e-mail security on overall cybersecurity among the students of Imam Abdulrahman Bin Faisal University in Dammam. Another main purpose to conduct this study is to know the level of knowledge students have in the developing countries about the cybersecurity and how much are they mindful of cyber-attacks and the level of awareness among the university students. Two important hypotheses were studied to discover their importance in awareness of cybersecurity. One is software security, and the other is e-mail security. A total of 11 relevant questions were drafted, and then these questions were distributed among the university students, and around 390 responded to the questionnaires. Statistical analysis was performed on the responses using tools. Initial tests such as validity and reliability test, feasibility test of a variable, correlation test, multicollinearity test, multiple regression, and Heteroskedasticity test were conducted using SPSS. And furthermore, multiple linear regression model and coefficient of determination, hypothesis test, ANOVA test, and partial test were conducted using ANOVA. The outcome of the analysis is software security variable (X1) that has a significant and positive effect on cybersecurity awareness (p value ≤0.001, β = 0.192). This shows that having a thorough understanding of software security can raise cybersecurity awareness up to 19.2%. E-mail security variable (X2) has a significant and positive effect on cybersecurityawareness (p-value ≤0.000). This shows that having a thorough understanding of email security can raise cybersecurity awareness up to 31.3%. Software security (X1) and e-mail security (X2) variables simultaneously have a significant effect on cybersecurity awareness (p-value ≤0.000) with a correlation coefficient of 12.1% (R2 = 0.121). This shows that the independent variable used can explain the level of cybersecurity awareness up to 12.1%. Research results show that students are aware of software or application updates. Furthermore, students’ awareness of email security is also good.",,"Alqahtani MA,Kumar V",,2022,,10.1155/2022/6775980,https://doi-org.proxy.bnl.lu/10.1155/2022/6775980;http://dx.doi.org/10.1155/2022/6775980,Journal Article
The Relationship between KM Strategies and IT Applications in SMEs,"Little is known about how small and medium sized enterprises (SMEs) utilize their information technology (IT) to support their Knowledge Management (KM) strategy. Some research has been conducted in this field but from a western cultural perspective, and mainly in the large organizations context. Research on the relationships between KM strategy and IT in SMEs in developing countries, such as Saudi Arabia, is limited. The research reported in this paper addressed this relationship. KM strategy, in this research has been classified into two main strategies: aggressive KM strategy and conservative KM strategy, based on the organizations' orientation towards eight dimensions: external knowledge, internal knowledge, tacit knowledge, explicit knowledge, exploration, exploitation, broad knowledge-base and narrow knowledge-base. A total of 143 SMEs, participated in the survey. The results indicate that the proposed classifications of KM strategies were valid, the IT applications can be classified into: Internet-based IT, IT for codification and IT for collaboration, and the association between KM strategy and IT was confirmed.",,"Azyabi N,Fisher J,Tanner K,Gao S",,2014,3645–3654,10.1109/HICSS.2014.453,https://doi-org.proxy.bnl.lu/10.1109/HICSS.2014.453;http://dx.doi.org/10.1109/HICSS.2014.453,Conference Paper
Botswana's Lab-In-A-Briefcase: A Position Paper,"Detecting and managing communicable and non-communicable diseases in rural settings of Africa raises numerous structural, syntactical and semantic issues. Further, it has been observed that both Communicable Diseases (CDs) such as TB and Non-Communicable Diseases (NCDs), such as cancer, diabetes, cardiovascular diseases and chronic respiratory disease, are on the rise in sub-Saharan African countries and estimated to account for about 25% of deaths (Bloomfield et al., 2014) [8]. In Botswana, NCDs account for more than a third of all deaths in the country (WHO NCD country profile, 2014) [9]. For cash-poor part of Africa, this additional spend-requirement in healthcare is unfortunately substantial. One approach to reducing death related to CD/NCD is early detection and control through data collection and appropriate intervention. In sub-Saharan Africa countries, most of the population lives in rural areas where access to healthcare facilities is very limited. In such circumstances, a low-cost and mobile healthcare facility along with associated Information and Communication Technologies (ICT) would be of great assistance.This paper investigates the application of the ""lab-in-a-briefcase"" technology for the management of CDs/NCDs in Botswana and other SADC countries. The ""lab-in-a-briefcase is designed to provide a portable laboratory diagnosis toolkit with rapid results (about 15 minutes) that can be used in areas where access to laboratory or healthcare facility is limited and can be used with minimal training. It contains all the necessary tools and chemicals/reagents which are packaged in a briefcase form so that they can easily be carried. In addition, the Lab-in-a-briefcase employs mini-HPLC and smart camera, microphone, credit card-sized ECG and microscope so that a variety of tests can be performed quickly and efficiently in a portable manner.This paper details the design and deployment of ""Lab-in-a-briefcase"" and associated software tools at primary care health facilities so that diagnosis can be quickly carried out and the resulting medical records are automatically generated, converted into appropriate format and securely shared with different health information systems. The deployment of this system requires adaptation of the system in the context of the linguistic, legal, security, and other policy requirements of the participating countries. As part of the demonstration of the applicability, pilot studies will be extended to all participating African countries. Our end-objective is to develop a fully optioned prototype.",,Narasimhan VL,,2019,,10.1145/3290688.3290716,https://doi-org.proxy.bnl.lu/10.1145/3290688.3290716;http://dx.doi.org/10.1145/3290688.3290716,Conference Paper
"A Systematic Review of IoT in Healthcare: Applications, Techniques, and Trends",,,"Haghi Kashani M,Madanipour M,Nikravan M,Asghari P,Mahdipour E",,2021,,10.1016/j.jnca.2021.103164,https://doi-org.proxy.bnl.lu/10.1016/j.jnca.2021.103164;http://dx.doi.org/10.1016/j.jnca.2021.103164,Journal Article
Home Edge Computing Architecture for Smart and Sustainable Agriculture and Breeding,"Challenges of today and tomorrow in developing countries to ensure sustainable food security for their populations require smart agriculture and breeding. This necessarily depends on water control, soil erosion, livestock management, and so on. At the same time, Internet of Things (IoT) represents the latest evolution of the Internet and can significantly improve the ability to collect, analyze and retrieve data that we can then transform into information, knowledge and finally knowing. In the context of ensuring smart and sustainable agriculture, the importance of IoT seems obvious. Note that, IoT has implications for bandwidth, latency and processing speeds, given the huge amount of data to collect. Edge computing and Systems are one of the emerging solutions to reduce latency and improve bandwidth utilization for real-time applications and services. Thus, to achieve these aims, we propose, in this paper, a new three-tier architecture (3-TIER) for smart agriculture. It is based on that of Home Edge Computing allowing us to achieve ultra-low latency. This architecture will also allow us to effectively solve the problems related to agriculture and livestock breeding, but also to be able to resolve considerably the conflicts between farmers and herders. This proposal will be followed by an experimental validation of HEC architecture using the EdgeCloudSim simulator.",,"Babou CS,Sane BO,Diane I,Niang I",,2019,,10.1145/3320326.3320377,https://doi-org.proxy.bnl.lu/10.1145/3320326.3320377;http://dx.doi.org/10.1145/3320326.3320377,Conference Paper
Mobile Personal Health Records: Research Agenda for Applications in Global Health,"Health threats, such as HIV/AIDS, maternal health and SARS, are global in nature, as their impact goes beyond the borders of any one nation. This has compelled a global approach to combating these threats, commonly by multinational partnerships among many different types of institutions. Although diffusion of mobile technology in the developing world has been successful, and personal health records on mobile devices (mPHRs) have shown effectiveness in certain health-related contexts, they have not been widely used to address health threats globally. The purpose of this article is to discuss six areas in which research on mPHRs can be used to address global health issues.",,"Dohan MS,Abouzahra M,Tan J",,2014,2576–2585,10.1109/HICSS.2014.325,https://doi-org.proxy.bnl.lu/10.1109/HICSS.2014.325;http://dx.doi.org/10.1109/HICSS.2014.325,Conference Paper
Low-Latency Perception in off-Road Dynamical Low Visibility Environments,,,"Ferreira Neto NA,Ruiz M,Reis M,Cajahyba T,Oliveira D,Barreto AC,Simas Filho EF,de Oliveira WL,Schnitman L,Monteiro RL",,2022,,10.1016/j.eswa.2022.117010,https://doi-org.proxy.bnl.lu/10.1016/j.eswa.2022.117010;http://dx.doi.org/10.1016/j.eswa.2022.117010,Journal Article
Mobile Software Testing,"Mobile Software Testing, the second book written by author Narayanan Palani and the first ever book on Mobile Application based software testing as well, has already turned out a best reviewed in the I.T industry. Narayanan Palani is keen in sharing the technical knowledge for those starting out a career in Software Testing or even for those with few years of testing experience. He is endorsed by Tech City UK as an exceptional talent/world leader in digital technology. His aim is to reduce the unemployment of developed countries like United Kingdom and developing countries like India by training the graduate students and jobseekers through his technical books. This book is the culmination of 5 years of research and effort in this field. It gives a pragmatic view of using Mobile Application Technology Testing Techniques in various situations. And is recommended for those aspiring to be experts or advanced users of test automation and performance tools like Experitest, Perfecto Mobile, uTest, Neotys, Soasta, Robotium, Ranorex and Eggplant. From the Reviewers ""Mobile testing will capture the market space in the future and this book is very informative for testers who want to reserve the space in the future market""-Sunil Kiran Balijepalli, Team Lead at Cornerstone on demand. Mobile testing is increasingly complex on day by day due to the range of platforms, devices and innovations. Narayanan has articulated the complex mobile testing approach in simple terms with good references. I am sure, this book will enable QA community to pick up the latest developments in mobile testing arena and the tools available to deliver secured & quality product to the end users -Ponsailapathi V, Vice President, Polaris Software Lab Limited",,Palani N,,2014,,,,Book
An Exploratory Study on Policy Transfer for SIM Card Registration in Malawi,"Majority of African countries have adopted policies for mandatory Subscriber Identifiable Module SIM card registration to mitigate security threats to citizens and society. However, there are few countries that have not yet adopted the mandatory SIM card registration policies. This study investigated the means through which SIM card registration policy may be transferred in countries without the policy. The context of Malawi was analysed which represented an ideal case of an African country without mandatory SIM card registration policy. The findings showed that the mandatory SIM card registration policy may be transferred through: a voluntary transfer to address local challenges related to mobile technologies b coercive transfer in response to meet international agreements. However, lack of national identification documents for mobile phone users and delays in implementing legal framework affected the transfer of the mandatory SIM Card registration in Malawi. It will be necessary to consider of social, economic and political factors when adopting the mandatory SIM card registration policy.",,Makoza F,,2015,33–45,10.4018/IJTD.2015010102,https://doi-org.proxy.bnl.lu/10.4018/IJTD.2015010102;http://dx.doi.org/10.4018/IJTD.2015010102,Journal Article
The Indirect Carbon Emission from Household Consumption in China between 1995–2009 and 2010–2030: A Decomposition and Prediction Analysis,,,"Xia Y,Wang H,Liu W",,2019,264–276,10.1016/j.cie.2018.12.031,https://doi-org.proxy.bnl.lu/10.1016/j.cie.2018.12.031;http://dx.doi.org/10.1016/j.cie.2018.12.031,Journal Article
Turning the Postal System into a Generic Digital Communication Mechanism,"The phenomenon that rural residents and people with low incomes lag behind in Internet access is known as the ""digital divide."" This problem is particularly acute in developing countries, where most of the world's population lives. Bridging this digital divide, especially by attempting to increase the accessibility of broadband connectivity, can be challenging. The improvement of wide-area connectivity is constrained by factors such as how quickly we can dig ditches to bury fibers in the ground; and the cost of furnishing ""last-mile"" wiring can be prohibitively high.In this paper, we explore the use of digital storage media transported by the postal system as a general digital communication mechanism. While some companies have used the postal system to deliver software and movies, none of them has turned the postal system into a truly generic digital communication medium supporting a wide variety of applications. We call such a generic system a Postmanet. Compared to traditional wide-area connectivity options, the Postmanet has several important advantages, including wide global reach, great bandwidth potential and low cost.Manually preparing mobile storage devices for shipment may appear deceptively simple, but with many applications, communicating parties and messages, manual management becomes infeasible, and systems support at several levels becomes necessary. We explore the simultaneous exploitation of the Internet and the Postmanet, so we can combine their latency and bandwidth advantages to enable sophisticated bandwidth-intensive applications.",,"Wang RY,Sobti S,Garg N,Ziskind E,Lai J,Krishnamurthy A",,2004,159–166,10.1145/1015467.1015485,https://doi-org.proxy.bnl.lu/10.1145/1015467.1015485;http://dx.doi.org/10.1145/1015467.1015485,Conference Paper
Turning the Postal System into a Generic Digital Communication Mechanism,"The phenomenon that rural residents and people with low incomes lag behind in Internet access is known as the ""digital divide."" This problem is particularly acute in developing countries, where most of the world's population lives. Bridging this digital divide, especially by attempting to increase the accessibility of broadband connectivity, can be challenging. The improvement of wide-area connectivity is constrained by factors such as how quickly we can dig ditches to bury fibers in the ground; and the cost of furnishing ""last-mile"" wiring can be prohibitively high.In this paper, we explore the use of digital storage media transported by the postal system as a general digital communication mechanism. While some companies have used the postal system to deliver software and movies, none of them has turned the postal system into a truly generic digital communication medium supporting a wide variety of applications. We call such a generic system a Postmanet. Compared to traditional wide-area connectivity options, the Postmanet has several important advantages, including wide global reach, great bandwidth potential and low cost.Manually preparing mobile storage devices for shipment may appear deceptively simple, but with many applications, communicating parties and messages, manual management becomes infeasible, and systems support at several levels becomes necessary. We explore the simultaneous exploitation of the Internet and the Postmanet, so we can combine their latency and bandwidth advantages to enable sophisticated bandwidth-intensive applications.",,"Wang RY,Sobti S,Garg N,Ziskind E,Lai J,Krishnamurthy A",,2004,159–166,10.1145/1030194.1015485,https://doi-org.proxy.bnl.lu/10.1145/1030194.1015485;http://dx.doi.org/10.1145/1030194.1015485,Journal Article
Insider Perspectives of Human-Computer Interaction for Development Research: Opportunities and Challenges,"Human-Computer Interaction (HCI) research has gained traction in Africa in recent years. Researchers and designers have exploited the opportunities created by advances in mobile technology and increasing access to internet services in the communities to develop and deploy user-centered digital solutions targeting African audience and for solving Africa's problems. However, only a few researches have investigated the potential opportunities and challenges that face the adoption of HCI projects and mobile technology among indigenous people, especially in Biafra land in Nigeria. Therefore, the goal of this paper is to present an insider perspective of our lived experiences in conducting HCI4D and mobile technology research, the methodological and practical challenges, and design opportunities for advancing social and culturally-sensitive HCI research and mobile application designs in the Global South.",,"Samuel Nkwo M,Orji R,Ugah J",,2021,131–135,10.1145/3448696.3448709,https://doi-org.proxy.bnl.lu/10.1145/3448696.3448709;http://dx.doi.org/10.1145/3448696.3448709,Conference Paper
Socio-Economic Factors in the Application of Information and Communication Technologies in Nigerian Print Media,"Information and communication technologies (ICTs) have opened up new opportunities for the Nigerian print media to improve on their products and services. This study explores the socio-economic factors associated with the adoption and use of ICTs by the media. Of a total of 54 socio-economic factors considered, exactly 50% were found to have significant influence on the adoption and success of ICT applications. The factors that have the greatest positive influence on adoption of ICTs include organizational goal, profitability, organizational image, communication in the organization, productivity, and openness of workers to change. They also constitute success factors in the use of these technologies. The factors that constrained adoption and also successful application include high rate of inflation, unfavourable exchange rate of the naira to the dollar, low wage level, huge costs, low gross national product, inadequate funding, and unstable political situation. These constraining factors are indicators of economic weakness and political uncertainty. It seems that the significance of such factors, which are completely external to a business organization, was often underestimated in studies of organizational performance in developing countries.",,Ehikhamenor FA,,2002,602–611,10.1002/asi.10044,https://doi-org.proxy.bnl.lu/10.1002/asi.10044;http://dx.doi.org/10.1002/asi.10044,Journal Article
Information Operations in Africa: An Overlooked Opportunity,"With last year's activation of us African Command (AFRICOM), the United States will now address its security concerns for that turbulent continent with operations realigned under one command instead of three. Information operations will necessarily dominate the Department of Defense's activities in Africa genocide, poverty, famine, epidemic and civil war are rarely if ever amenable to solution by direct armed intervention. Information Operations (IO) involves more than network and cyber operations, which is just one of its five core elements. IO also includes psychological operations, electronic warfare, operations security, and military deception. All of these will be important in AFRICOM, both in terms of their effectiveness and in their utility in the economy of force considerations that will be a permanent part of the security environment for the foreseeable future. This presentation considers the challenges and opportunities of IO in Africa from the point of view of culture, security, infrastructure, and level of development.",,"McKinney JR,Westphal M",,2009,698–703,,,Conference Paper
Grey Markov Model Forecast in Economic System under Incomplete Information and Its Application on Foreign Direct Investment,"Foreign direct investment (FDI) plays an extraordinary role in developing countries and its fluctuations reflect the changes of influencing factors during time, and therefore the models to simulate and forecast the trends are of great significance. Model GM (1, 1) is used in this paper to overcome the problems of small sample size and poor data. In the pre-procession of the raw data, two sequences are generated from weakening operator and from logarithm process respectively, and the simulation results show that the model constructed from the logarithm sequence has better simulation accuracy. Then Markov Chain is introduced to original model to get more accurate forecast results, which indicate the growing trend of FDI inflows in the following three years. The grey system models applied in economic areas are proved to have accurate simulation and convincing results.",,"Wang Y,Chen C",,2011,117–120,10.1109/ICIII.2011.175,https://doi-org.proxy.bnl.lu/10.1109/ICIII.2011.175;http://dx.doi.org/10.1109/ICIII.2011.175,Conference Paper
A Framework for Water Loss Management in Developing Countries under Fuzzy Environment,"A multi-criteria decision analysis method for water loss management is proposed.The method integrates AHP and TOPSIS methods under fuzzy environment.It is applied to a real water distribution system in a developing country.The prevalent strategies were highly connected to the local conditions. Facing water scarcity conditions water utilities cannot longer tolerate inefficiencies in their water systems. To guarantee sustainable water management one central task is reducing water losses from the supply systems. There are numerous challenges in managing water losses, manifested in a variety of options, their complexities, multiple evaluation criteria, inherent uncertainties and the conflicting objectives and interests of different stakeholders. This study demonstrates the effectiveness of multi criteria decision analysis (MCDA) approaches for decision support in this complex topic. The study covers identifying the key options among a set of options that have been proposed within a framework of strategies to reduce water losses in water distribution systems of developing countries. The proposed methodology was initiated by developing a hierarchical structure of the decision problem that consists of four levels: Overall objective, main criteria, evaluation criteria and options. Different stakeholders were engaged in the process of structuring and evaluating the decision problem. An integrated methodology that combines fuzzy set theory with Analytic Hierarchy Process (AHP) and Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) methods was then employed. This methodology has the potential to transform qualitative data into equivalent quantitative measures. Fuzzy AHP was used to create weights for main and evaluation criteria, while Fuzzy TOPSIS was used to aid the ranking of options in terms of their potential to meet the overall objective based on the evaluations and preferences of decision makers. The results showed that pressure management and control strategy was the most prevalent one, followed by employing advanced techniques and establishment of district metered areas. Their dominance was highly connected to the local and boundary conditions of the case study. The sensitivity analysis results showed that strongest and weakest options were less sensitive to changes in weights of evaluation criteria, which could be attributed to the strong consensus in strengthening the best option and neglecting the worst option. This study emphasized the successful application of MCDA in dealing with complicated issues in the context of water loss management. It is anticipated that, the integration of this developed framework in the planning policies of water utilities in developing countries can help in conducting better control over water losses.",,"Zyoud SH,Kaufmann LG,Shaheen H,Samhan S,Fuchs-Hanusch D",,2016,86–105,10.1016/j.eswa.2016.05.016,https://doi-org.proxy.bnl.lu/10.1016/j.eswa.2016.05.016;http://dx.doi.org/10.1016/j.eswa.2016.05.016,Journal Article
Transfer Learning from Deep Features for Remote Sensing and Poverty Mapping,"The lack of reliable data in developing countries is a major obstacle to sustainable development, food security, and disaster relief. Poverty data, for example, is typically scarce, sparse in coverage, and labor-intensive to obtain. Remote sensing data such as high-resolution satellite imagery, on the other hand, is becoming increasingly available and inexpensive. Unfortunately, such data is highly unstructured and currently no techniques exist to automatically extract useful insights to inform policy decisions and help direct humanitarian efforts. We propose a novel machine learning approach to extract large-scale socioeconomic indicators from high-resolution satellite imagery. The main challenge is that training data is very scarce, making it difficult to apply modern techniques such as Convolutional Neural Networks (CNN). We therefore propose a transfer learning approach where nighttime light intensities are used as a data-rich proxy. We train a fully convolutional CNN model to predict nighttime lights from daytime imagery, simultaneously learning features that are useful for poverty prediction. The model learns filters identifying different terrains and man-made structures, including roads, buildings, and farmlands, without any supervision beyond nighttime lights. We demonstrate that these learned features are highly informative for poverty mapping, even approaching the predictive performance of survey data collected in the field.",,"Xie M,Jean N,Burke M,Lobell D,Ermon S",,2016,3929–3935,,,Conference Paper
Energy Efficiency of Encryption Schemes Applied to Wireless Sensor Networks,"In this paper, we focus on the energy efficiency of secure communication in wireless sensor networks (WSNs). Our research considers link layer security of WSNs, investigating both the ciphers and the cryptographic implementation schemes, including aspects such as the cipher mode of operation and the establishment of initialization vectors (IVs). We evaluate the computational energy efficiency of different symmetric key ciphers considering both the algorithm characteristics and the effect of channel quality on cipher synchronization. Results show that the computational energy cost of block ciphers is less than that of stream ciphers when data are encrypted and transmitted through a noisy channel. We further investigate different factors affecting the communication energy cost of link layer cryptographic schemes, such as the size of payload, the mode of operation applied to a cipher, the distribution of the IV, and the quality of the communication channel. A comprehensive performance comparison of different cryptographic schemes is undertaken by developing an energy analysis model of secure data transmission at the link layer. This model is constructed considering various factors affecting both the computational cost and communication cost, and its appropriateness is verified by simulation results. In conclusion, we recommend using a block cipher instead of a stream cipher to encrypt data for WSN applications and using a cipher feedback scheme for the cipher operation, thereby achieving energy efficiency without compromising the security in WSNs. Copyright © 2011 John Wiley & Sons, Ltd.(This study is based on “An analysis of link layer encryption schemes in wireless sensor networks” by X. Zhang, H. M. Heys, and C. Li, which appeared in the Proceedings of IEEE International Conference on Communications (ICC 2010), Cape Town, South Africa, May 2010, and “Energy efficiency of symmetric key cryptographic algorithms in wireless sensor networks” by X. Zhang, H. M. Heys, and C. Li, which appeared in the Proceedings of Biennial Symposium on Communications (QBSC 2010), Kingston, Canada, May 2010. ©2010 IEEE.)",,"Zhang X,Heys HM,Li C",,2012,789–808,10.1002/sec.375,https://doi-org.proxy.bnl.lu/10.1002/sec.375;http://dx.doi.org/10.1002/sec.375,Journal Article
Measuring the Effects of Risk and Cultural Dimensions on the Adoption of Online Stock Trading: A Developing Country Perspective,"Online stock trading OST is a growing phenomenon across countries, yet there is a sparse literature focusing on the negative utilities risks that causing the low adoption. Drawing from perceived risk theory, this article attempts to fill the gap by identifying the influential risk factors that impede the acceptance of OST in a developing country, Pakistan. The study also applies the Hofstede cultural theory to ascertain the effects of cultural moderators on investors' usage behavior UB. Based on structured questionnaire, 443 valid responses were received from current and potential investors. The model was tested using structural equation modeling through Smart-PLS. The results validate a negative and significant relationship between risk dimensions and investors' behavioral intentions BI to use OST. Especially time, financial, performance, privacy and opportunity cost risks are found having a negative impact on investors' BI. Moreover, the study finds that cultural dimensions, collectivism, and uncertainty avoidance, moderate the relationship between BI and UB.",,"Khan SU,Liu X,Khan IU,Liu C,Hameed Z",,2018,106–127,10.4018/IJEIS.2018070106,https://doi-org.proxy.bnl.lu/10.4018/IJEIS.2018070106;http://dx.doi.org/10.4018/IJEIS.2018070106,Journal Article
Exploring Entrepreneurial Activities in Marginalized Widows: A Case from Rural Sri Lanka,"In some developing countries, widows are looked down upon and are often considered inauspicious especially in rural regions. Some societies even consider them and their issues invisible. This paper presents findings from a qualitative study focused on understanding how technology could facilitate entrepreneurial and DIY activities of widows from rural Sri Lanka. We conducted semi-structured interviews and field observations with thirteen widows from low socio-economic backgrounds, who were involved in various small-scale entrepreneurial activities. Our findings showed three central aspects associated with their entrepreneurial activities which can be supported through technology: initial stages of entrepreneurship, balancing work with life, and dealing with exploitations. This paper explores how gender inequality in a social context affects marginalized women in rural Sri Lanka in conducting their entrepreneurial efforts. In particular, we highlight resilient practices that the participants apply to support their entrepreneurial activities. With an ""assets-based approach"" we conclude by providing implications for policymakers, media, and HCI practitioners to support this inbuilt resilience by leveraging their current assets.",,"Rathnayake UA,Halloluwa T,Bandara P,Narasinghe M,Vyas D",,2021,,10.1145/3449216,https://doi-org.proxy.bnl.lu/10.1145/3449216;http://dx.doi.org/10.1145/3449216,Journal Article
"Respeak: A Voice-Based, Crowd-Powered Speech Transcription System","Speech transcription is an expensive service with high turnaround time for audio files containing languages spoken in developing countries and regional accents of well-represented languages. We present Respeak - a voice-based, crowd-powered system that capitalizes on the strengths of crowdsourcing and automatic speech recognition (instead of typing) to transcribe such audio files. We created Respeak and optimized its design through a series of cognitive experiments. We deployed it with 25 university students in India who completed 5464 micro-transcription tasks, transcribing 55 minutes of widely-varied audio content, and collectively earning USD 46 as mobile airtime. The Respeak engine aligned the transcript generated by five randomly selected users to transcribe Hindi and Indian English audio files with a word error rate (WER) of 8.6% and 15.2%, respectively. The cost of speech transcription was USD 0.83 per minute with a turnaround time of 39.8 hours, substantially less than industry standards. Using a mixed-methods analysis of cognitive experiments, system performance and qualitative interviews, we evaluate Respeak's design, user experience, strengths, and weaknesses. Our findings suggest that Respeak improves the quality of speech transcription while enhancing the earning potential of low-income populations in resource-constrained settings.",,"Vashistha A,Sethi P,Anderson R",,2017,1855–1866,10.1145/3025453.3025640,https://doi-org.proxy.bnl.lu/10.1145/3025453.3025640;http://dx.doi.org/10.1145/3025453.3025640,Conference Paper
SFCS '14: Proceedings of the 2nd International Workshop on Security and Forensics in Communication Systems,"It is our great pleasure to welcome you to the Second International Workshop on Security and Forensics in Communication Systems (ASIACCS-SFCS'2014). This year workshop continues the objectives of the first edition of the workshop aiming at discussing and building emerging security and digital forensic engineering, as disciplined sciences in charge of developing novel scientific and theoretical methods, techniques, and approaches to collect, process, and analyze information retrieved from systems affected by security incidents, in order to generate conclusive descriptions and propose decisions or proofs.This year's workshop attempts to set up a tradition of being among the fora for presentation of research results and experience reports on leading edge issues of access control, including models, systems, applications, and theory. The mission of the workshop is to propose a platform for sharing novel security control solutions and investigation methods that fulfill the needs of networked applications and environments. It also aims at identifying new directions for future research and development in communication security and forensic science, and giving security researchers and practitioners the opportunity to share their perspectives with scientists and engineers interested in the various aspects of security.The call for papers attracted multiple submissions from Asia, Europe, Africa, and North America. The program committee accepted only 6 papers that cover a variety of topics, including: security of cloud computing, investigation of data leakage, analysis of tracing systems, privacy preserving in cloud storage systems and the detection of explicit images.We kindly encourage attendees to attend the invited talk presentations and the six talks planned for the workshop. These valuable and insightful talks can present an interesting forum for the exchange of ideas in innovative disciplines related to network security and forensic investigation. In particular, the invited talk ""Security Investigation in Wireless Networks"", presented by Dr. Slim Rekhis, from the University of Carthage, Tunisia will discuss the challenges and recent developments in the digital investigation in wireless networks, while highlighting the importance of evidences collection, system visibility and anti-forensic detection.",,,,2014,,,,Book
Forecasting Spot Prices of Agricultural Commodities in India: Application of Deep‐Learning Models,"Food price fluctuations can impact both producers and consumers. Forecasting the prices of the agricultural commodities is of prime concern not only to the government but also to farmers and agribusiness firms. In developing countries like India, management of food security needs competent and efficient forecasting of food prices. With the availability of data, recent innovation in deep‐learning models provides a feasible solution to accurately forecast the prices. In this study, we examine the superiority of these models using the daily spot prices of five major commodities traded on the National Commodity and Derivatives Exchange: cotton seed, castor seed, rape mustard seed, soybean seed, and guar seed. The results were obtained from the application of the traditional univariate autoregressive integrated moving average model and deep‐learning techniques like the time‐delay neural network (TDNN) and long short‐term memory (LSTM) network. The empirical results indicate that the LSTM model is indeed suitable for the financial domain and captures the directional movement of the spot price changes with high accuracy compared with the TDNN and other linear models. Accuracy of the performance of these models has been compared using out‐of‐sample performance measure. The overall objective of this paper is to demonstrate the utility of spot price forecasting for farmers and traders in offering them the best predictions of the price movements. Our results provide a possibility of developing pricing models that can help in fairly regulating agricultural commodity prices.",,"R L M,Mishra AK",,2021,72–83,10.1002/isaf.1487,https://doi-org.proxy.bnl.lu/10.1002/isaf.1487;http://dx.doi.org/10.1002/isaf.1487,Journal Article
BADS '09: Proceedings of the 2009 Workshop on Bio-Inspired Algorithms for Distributed Systems,"It is our great pleasure to welcome you to BADS 2009, the Workshop on Bio-Inspired Algorithms for Distributed Systems. The aim of the workshop is to provide a forum to explore the applications of bio-inspired algorithms and protocols in different kinds of distributed computing systems, and discuss the main trends and challenges for the next few years.Computer systems are characterized by an ever growing complexity and a pronounced distributed nature. While the use of centralized or hierarchical architectures and algorithms has been dominant so far, they are now becoming impractical because they have poor scalability and fault-tolerance characteristics. Decentralized architectures and algorithms, for example P2P and Grid systems, are increasingly popular, but to be efficiently managed they need new types of algorithms.Bio-inspired algorithms are proving effective in this context, as they can be used to solve hard parallel and distributed computational problems by imitating the autonomic nature of distributed systems, in this way achieving fault-tolerant and self-adaptive behavior. These algorithms can be inspired by a number of biological systems, including ant colonies, bird flocking, honey bees, bacteria, and many more. In other cases the adopted techniques are ""evolutionary"", as they exploit genetic rules for the selection and the recombination of candidate solutions.The success of this workshop confirms that distributed computing systems are a very natural application field for bio-inspired algorithms and protocols. The BADS workshop includes one invited paper and 11 research papers selected through an accurate review process, which allowed us to select among 20 manuscripts submitted from Africa, Asia, Brazil, United States and many European Countries.The workshop is organized in four sessions, with the papers grouped according to the application area on which the bio-inspired algorithms are exploited. The four sessions are dedicated, respectively, to Grid computing, distributed systems and P2P, sensor and wireless environments, and parallel architectures.Grid Computing. Four papers are specifically dedicated to Grid computing. The paper by González et al. analyzes the behavior of Parallel Genetic Programming (PGP) applications executing in distributed platforms with high failure rates, with the goal of characterizing the inherent fault tolerance capabilities of the PGP paradigm. Two well-known GP problems, ""even parity 5"" and ""11-multiplexer"", are examined via simulation by using trace data collected from real-world Desktop Grid platforms. Experiments show that the PGP paradigm exhibits satisfying fault tolerance properties even in very dynamic environments.The paper by Brocco et al. presents and evaluates an ant-inspired algorithm that aims to reduce the overall network traffic in a Grid. Mobile agents move across the Grid and add or remove logical links exploiting local information, in order to optimize the network topology. The paper also introduces an effective resource discovery algorithm that executes over the obtained topology and uses cache information exchanged among the nodes via a gossiping mechanism.The paper by Brun and Medvidovic shows how the natural process of crystal growth can inspire computational mechanisms that are easily susceptible to large-scale parallelization, fault tolerance, and uniform representation of data and computation, which favors data management and mobility. After summarizing the crystal-inspired solutions to some complex problems, for example the SubsetSum NP-complete problem, the authors describe how these techniques can also be devised for distributed systems and specifically computational Grids. The proposed approach ensures scalability, privacy of data, and tolerance of a wide range of faults and malicious attacks.De Falco et al. propose the use of a multi-objective Differential Evolution (DE) algorithm that finds near-optimal solutions to map the communicating tasks of a computational intensive application on multisite Grid nodes. The authors extend the classical DE paradigm to cope with this problem, also by introducing a novel operator that allows a subset of similar tasks to be allocated on the nodes of a cluster. The algorithm is defined, implemented and evaluated in many simulated scenarios by varying the load and reliability of the nodes, so reproducing a real world environment.Distributed Systems and P2P. Three papers focus on general purpose distributed systems and P2P networks. The paper by Korosec and Silic proposes a distributed ant-colony optimization technique to solve black-box global optimization problems. Prior to the application of the distributed ant algorithm, the problem space is discretized and a search graph of the problem is built. The algorithm exhibits good scalability properties in the scenarios where the black-box execution time dominates the time taken by the ants to search the global optimum. Scalability can be further improved by using a larger number of ants.Ohnishi et al. use an evolutionary-based approach to adaptively change the topology of a dynamic P2P network and improve the reliability of search procedures. The topology of the network is conveniently represented as an individual of the evolutionary algorithm, and crossover and mutation operator are used to change the network topology. The algorithm is evaluated on two different scenarios, with and without replicating the resources: the evolutionary approach proves to be particularly effective without replication.The paper by Bicocchi et al. aims to handle dynamics in diffusive aggregation schemas using a technique based on a classical gossiping technique and on a novel method named ""evaporation"". The latter is based on the search behavior of ants following pheromone trails, and tends to promote up-to-date values while making old values evaporate. The method avoids the need for periodic restarts present in standard gossiping techniques. Experimental results assess the efficiency of the method especially for large-scale networks.Sensor and Wireless Networks. This session collects three papers that focus on sensor and wireless networks and, more specifically, deal with the important issue of energy saving. The invited paper by Hernández and Blum discusses and analyzes a self-organizing mechanism that exploits the energy harvesting capabilities of a network of mobile sensors. The mechanism is inspired by the self-synchronized sleeping patterns observed in ant colonies. Correspondingly, each sensor switches between the active and the inactive state on the basis of its state parameters, the amount of energy harvested with its solar cells, and the information received by neighbor sensors. The approach is evaluated in conditions that are easily encountered in real world applications, such as de-synchronized sensor clocks, random direction mobility of sensors, and varying weather conditions influencing the amount of solar energy collected by the sensors.Garbinato et al. present a novel approach to disseminating information in a sensor network. This approach combines the decentralized and stateless properties of gossiping protocols, which assure low resource consumption, with the high reliability that is typical of structured protocols. Reliability is obtained by modulating the transmission range of sensors in accordance with a power law statistical distribution. If compared with classical gossiping protocols, the small-world and scale-free nature of the presented approach allows sensors to obtain equivalent delivery ratios with lower energy consumption.De Rango and Tropea present a routing algorithm for mobile ad hoc networks. This algorithm utilizes swarm intelligence techniques, in particular an ant colony optimization algorithm, to discover minimum drain rate paths, so combining traffic load balancing and energy saving. The main novelty of the approach consists in using quantitative metrics for energy saving and traffic load distribution in the pheromone update phase. The algorithm efficiency is proved through comparison, via NS2 simulation, with two currently adopted routing algorithms.Parallel Architectures. The session on algorithms for parallel architectures comprises two papers that exploit innovative techniques: algorithms for GPU-based architectures and DNA computing algorithms. Robilliard et al. analyze the possibilities offered by modern Graphics Processing Units (GPU) for implementing genetic programming. They evaluate and compare the performance of GP obtained by using two parallelization schemas, namely BlockGP and ThreadGP. The paper draws some important lessons for optimizing evolutionary algorithms on GPU-based architectures.In their paper, Iaccarino and Mazza propose a performance improvement of a recent DNA computing algorithm. This method is applied to an intractable problem in the field of cryptography, the ""elliptic curve discrete logarithm"" problem. In particular, the authors build the model of a parallel multiplier and adder, which outperforms the existing methods in terms of the worst case time complexity.The categorization of the workshop papers in accordance with the corresponding application domains clearly confirms the wide range of practical applications that may benefit from the adoption of bio-inspired algorithms, and the inherent capacity of these algorithms to be exploited in emerging domains such as sensor networks, Grids, P2P networks, and multi-core processors.",,,,2009,,,,Book
Pest Management In Cotton Farms: An AI-System Case Study from the Global South,"Nearly 100 million families across the world rely on cotton farming for their livelihood. Cotton is particularly vulnerable to pest attacks, leading to overuse of pesticides, lost income for farmers, and in some cases farmer suicides. We address this problem by presenting a new solution for pesticide management that uses deep learning, smartphone cameras, inexpensive pest traps, existing digital pipelines, and agricultural extension-worker programs. Although generic, the platform is specifically designed to assist smallholder farmers in the developing world. In addition to outlining the solution, we consider the set of unique constraints this context places on it: data diversity, annotation challenges, shortcomings with traditional evaluation metrics, computing on low-resource devices, and deployment through intermediaries. This paper summarizes key lessons learned while developing and deploying the proposed solution. Such lessons may be applicable to other teams interested in building AI solutions for global development.",,"Dalmia A,White J,Chaurasia A,Agarwal V,Jain R,Vora D,Dhame B,Dharmaraju R,Panicker R",,2020,3119–3127,10.1145/3394486.3403363,https://doi-org.proxy.bnl.lu/10.1145/3394486.3403363;http://dx.doi.org/10.1145/3394486.3403363,Conference Paper
Blood Pressure Concerns: Findings from a Usability Study of Culturally Infused MHealth Design,"High blood pressure BP (i.e., hypertension) is a chronic condition and risk factor for cardiovascular disease, stroke, and heart failure, occurring in populations across the globe. Currently, smart phones and applications are developing rapidly, and mobile health applications are being used to manage hypertension. The goal of this study was to understand the usability findings from an iterative cross-cultural mHealth application to identify the perceived usefulness among African migrant adopters in Maryland, United States. Qualitative and quantitative statistical method were used to collect participants’ data. Usability findings reported that the behavioral intention of using the recommended features was influenced by the perceived usefulness of the AfriBP. The cultural dimensions were rated as the most preferred recommended features, followed by the health management feature. The perceived usefulness had a strong significant effect on attitude in adopting the AfriBP. Female participants adopted the AfriBP more than the male participants. The results regarding ethnicity found that the Nigerian participants considered the perceived usefulness of the AfriBP more than the Ghanaian participants. Few participants owned and or publicly used BP machines to monitor their BP. Few number of participants were less likely to use a smartphone health application to monitor their BP for health. The health status of the participants for BP readings and body mass index (BMI) was of great concern which supported prior research on Africans ancestry having the highest concern for BP.",,"Oladapo H,Chakraborty J",,2022,296–305,10.1007/978-3-031-05028-2_20,https://doi-org.proxy.bnl.lu/10.1007/978-3-031-05028-2_20;http://dx.doi.org/10.1007/978-3-031-05028-2_20,Conference Paper
Can AI Be for Good in the Midst of Cyber Attacks and Privacy Violations? A Position Paper,"Artificial Intelligence (AI) is affecting every aspect of our lives from healthcare to finance to driving to managing the home. Sophisticated machine learning techniques with a focus on deep learning are being applied successfully to detect cancer, to make the best choices for investments, to determine the most suitable routes for driving as well as to efficiently manage the electricity in our homes. We expect AI to have even more influence as advances are made with technology as well as in learning, planning, reasoning and explainable systems. While these advances will greatly advance humanity, organizations such as the United Nations have embarked on initiatives such as ""AI for Good"" and we can expect to see more emphasis on applying AI for the good of humanity especially in developing countries. However, the question that needs to be answered is Can AI be for Good when when the AI techniques can be attacked and the AI techniques themselves can cause privacy violations? This position paper will provide an overview of this topic with protecting children and children's rights as an example.",,Thuraisingham BM,,2020,1–4,10.1145/3374664.3379334,https://doi-org.proxy.bnl.lu/10.1145/3374664.3379334;http://dx.doi.org/10.1145/3374664.3379334,Conference Paper
Application of Remote Sensing in Estimating Maize Grain Yield in Heterogeneous African Agricultural Landscapes: A Review,"Maize Zea mays L. is the second most commonly grown crop worldwide and number one staple food in Africa where it accounts for more than 50% of the energy requirements. However, despite its widespread cultivation and the significance of maize information in Africa, maize crop maps and yield forecasts are hardly available. Yet, systematic area, spatial distribution, and maize yield estimates are important in understanding and addressing food security in Africa. Objective monitoring of maize yield statisics in a systematic way is possible with remotely sensed data. However, absence of maize yield forecasts using remote sensing in Africa has been attributed to the cost of acquiring satellite imagery and the heterogeneity of agricultural landscapes. The recent advances in sensors technology and availability of free high-resolution spatial and temporal multispectral satellite images afford an opportunity to forecast maize yield as well as mapping its spatial distribution in near real-time basis. This review gives an overview of maize yield estimation using remotely sensed information and its potential application in a fragmented and highly granular agricultural landscapes in Africa, including inherent challenges and research needs. The review was motivated by challenges faced by researchers and national agricultural statistical services agents when forecasting maize yield using conventional ground-based survey methods. These problems include, but are not limited to, restricted accuracy, and cost and time spent resulting in missed opportunities in food security early warning systems and proper developmental interventions. We conclude that by picking multispectral sensors with high spatial, temporal, and spectral resolution, as well as appropriate classification techniques and accurate ground-truthing data, remote sensing can be a practical option for estimating maize grain yield and its spatio-temporal dynamics in heterogeneous African agricultural landscapes for designing appropriate developmental interventions and technological out scaling.",,"Chivasa W,Mutanga O,Biradar C",,2017,6816–6845,,,Journal Article
SPSM '12: Proceedings of the Second ACM Workshop on Security and Privacy in Smartphones and Mobile Devices,"It is our great pleasure to welcome you to the Second ACM Workshop on Security and Privacy in Smartphones and Mobile Devices -- SPSM'12, held in association with the 19th ACM Conference on Computer and Communications Security, October 19th, 2012, in Raleigh, NC (USA).The workshop was created last year to organize and foster discussion of security in the emerging area of smartphone and mobile device computing. As organizers of top security venues, we've observed an increasing number of submissions describing novel approaches to solving the challenges of this area. We wanted to provide a dedicated venue to discuss these challenges and promising approaches for future research directions. SPSM'11 was a great success, with an excellent turnout of 80 registered attendees and in-depth discussion. This year, we will continue the 15 minute back-to-back talks followed by 45 minutes of discussion and hope to meet and exceed the high bar that was set.The call for papers attracted 30 submissions from Canada, China, Germany, Greece, India, Iran, Italy, Japan, Lebanon, Nigeria, South Africa, and the United States. The program committee accepted 11 papers that cover a variety of topics, including permission models, user studies, attacks on smartphones, and methods of defense. We are especially pleased to have a keynote speech by Geir Olsen, a Principle Program Manager in the operating systems group on the Windows Phone team, on Windows Phone 8 Security. We hope that these proceedings will serve as a valuable reference for security researchers and developers.",,,,2012,,,,Book
Development of Hepatitis Disease Detection System by Exploiting Sparsity in Linear Support Vector Machine to Improve Strength of AdaBoost Ensemble Model,"Hepatitis disease is a deadliest disease. The management and diagnosis of hepatitis disease is expensive and requires high level of human expertise which poses challenges for the health care system in underdeveloped and developing countries. Hence, development of automated methods for accurate prediction of hepatitis disease is inevitable. In this paper, we develop a diagnostic system which hybridizes a linear support vector machine (SVM) model with adaptive boosting (AdaBoost) model. We exploit sparsity in linear SVM that is caused by L1 regularization. The sparse L1-regularized SVM is capable of eliminating redundant or irrelevant features from feature space. After filtering features through the sparse linear SVM, the output of the SVM is applied to the AdaBoost ensemble model which is used for classification purposes. Two types of numerical experiments are performed on the clinical features of hepatitis disease collected from UCI machine learning repository. In the first experiment, only conventional AdaBoost model is used, while in the second experiment, a feature vector is applied to the sparse linear SVM before its application to the AdaBoost model. Simulation results demonstrate that the strength of a conventional AdaBoost model is enhanced by 6.39% by the proposed method, and its time complexity is also reduced. In addition, the proposed method shows better performance than many previously developed methods for hepatitis disease prediction.",,"Akbar W,Wu WP,Saleem S,Farhan M,Saleem MA,Javeed A,Ali L,Bashir AK",,2020,,10.1155/2020/8870240,https://doi-org.proxy.bnl.lu/10.1155/2020/8870240;http://dx.doi.org/10.1155/2020/8870240,Journal Article
A Retrofit Design Science Methodology for Smart Metering Design in Developing Countries,"Traditional meters present both the users and utilities providers with various challenges in developing countries. For instance utility providers must get access to their users' premises to be able to read these meters or check fraudulent connections. Users on the other hand have to wait for months or more before knowing their utility consumptions or join long queues to purchase credit for the post-payment and pre-payment meters respectively. This paper seeks to propose a design that retrofits traditional meters on site into smart meters by adding embedded units for communication and control. The study was conducted using a modified Design Science Methodology referred to as Retrofit Design Science Research Methodology (RDSRM). RDSRM argues that, the artifact in context (Smart Meter) has undergone evaluation and acceptance as critical for solving a defined problem and that there are enough literature to support its relevance. The traditional meters were studied and the proposed system was designed using General Packet Radio Service (GPRS) technology for communication, and Peripheral Interface Controller (PIC). Network based applications were developed to present both the consumers and the utilities with the ability to interact with the meters remotely. An attempt was made to address identified security issues of smart metering by using Message Digest 5 (MD5) Algorithm in the design. The system was evaluated based on smart metering requirements. The GPRS Retrofitted Smart Metering System (GRSMS) will reduce the cost of deploying smart meters.",,"Azasoo JQ,Boateng KO",,2015,1–7,10.1109/ICCSA.2015.23,https://doi-org.proxy.bnl.lu/10.1109/ICCSA.2015.23;http://dx.doi.org/10.1109/ICCSA.2015.23,Conference Paper
The Paper Slip Should Be There! Perceptions of Transaction Receipts in Branchless Banking,"Mobile-based branchless banking has become a key mechanism for enabling financial inclusion in the developing world. A key component of all branchless banking systems is a mechanism to provide receipts to users after each transaction as evidence for successful transaction completion. In this paper, we present results from a field study that explores user perceptions of different receipt delivery mechanisms in the context of a branchless banking system in India. Our study shows that users have an affinity for paper receipts: despite the provision of an SMS receipt functionality by the system developers and their discouragement of the use of paper, users have pro-actively initiated a practice of issuing and accepting paper receipts. Several users are aware of the security limitations of paper receipts but continue to use them because of their usability benefits. We conclude with design recommendations for receipt delivery systems in branchless banking.",,"Panjwani S,Ghosh M,Kumaraguru P,Singh SV",,2013,328–331,10.1145/2493190.2493236,https://doi-org.proxy.bnl.lu/10.1145/2493190.2493236;http://dx.doi.org/10.1145/2493190.2493236,Conference Paper
How Advances in Semiconductor Technologies Have Adversely Affected Electrical and Electronics Installations in Africa,"This paper presents findings obtained in the Zambian national instrumentation surveys and highlights the little understood adverse effects that advances in semiconductor technology have on electrical and electronic instrumentation applied in all sectors in most African countries in particular Zambia. These effects have been occasioned by the lack of compliance to revised international installation and protection standards, poor power quality and poor planning compounded in many cases by severe weather conditions. These adverse impacts have been mainly caused by very high transistor integration in semiconductor (ICs). This has meant that electrical environment of yester year now requires vast improvements for the newer low dielectric strength component based equipment to operate reliably and safely. This paper finally describes good installation practices in power conditioning, earthing, bonding, surge and lightning protection solutions.",,"Namukolo S,Musonda E",,2018,86–94,10.4108/eai.20-6-2017.2270803,https://doi-org.proxy.bnl.lu/10.4108/eai.20-6-2017.2270803;http://dx.doi.org/10.4108/eai.20-6-2017.2270803,Conference Paper
Enhancing Intraday Stock Price Manipulation Detection by Leveraging Recurrent Neural Networks with Ensemble Learning,,,"Wang Q,Xu W,Huang X,Yang K",,2019,46–58,10.1016/j.neucom.2019.03.006,https://doi-org.proxy.bnl.lu/10.1016/j.neucom.2019.03.006;http://dx.doi.org/10.1016/j.neucom.2019.03.006,Journal Article
Landslide Detection Based on Contour-Based Deep Learning Framework in Case of National Scale of Nepal in 2015,,,"Yu B,Chen F,Xu C",,2020,,10.1016/j.cageo.2019.104388,https://doi-org.proxy.bnl.lu/10.1016/j.cageo.2019.104388;http://dx.doi.org/10.1016/j.cageo.2019.104388,Journal Article
CODASPY '18: Proceedings of the Eighth ACM Conference on Data and Application Security and Privacy,"It is our great pleasure to welcome you to the eighth edition of the ACM Conference on Data and Application Security and Privacy (CODASPY 2018), which follows the successful seven editions held in February/March 2011-2017. This conference series has been founded to foster novel and exciting research in this arena and to help generate new directions for further research and development. The initial concept was established by the two co-founders, Elisa Bertino and Ravi Sandhu, and sharpened by subsequent discussions with a number of fellow cyber security researchers. Their enthusiastic encouragement persuaded the co-founders to move ahead with the always daunting task of creating a high-quality conference.Data and applications that manipulate data are crucial assets in today's information age. With the increasing drive towards availability of data and services anytime and anywhere, security and privacy risks have increased. Vast amounts of privacy-sensitive data are being collected today by organizations for a variety of reasons. Unauthorized disclosure, modification, usage or denial of access to these data and corresponding services may result in high human and financial costs. New applications such as social networking and social computing provide value by aggregating input from numerous individual users and the mobile devices they carry. The emerging area of Internet of Things also poses serious privacy and security challenges. To achieve efficiency and effectiveness in traditional domains such as healthcare, there is a drive to make these records electronic and highly available. The need for organizations to share information effectively is underscored by rapid innovations in the business world that require close collaboration across traditional boundaries. Security and privacy in these and other arenas can be meaningfully achieved only in context of the application domain. Data and applications security and privacy has rapidly expanded as a research field with many important challenges to be addressed.In response to the call for papers of CODASPY 2018, 110 papers were submitted from Africa, Asia, Australia, Europe and North America. The program committee selected 23 full-length research papers (20.9% acceptance rate). These papers cover a variety of topics, including security issues in web, cloud, IoT, and mobile devices, privacy, access control, authentication, malware, code analysis, and hardware and system security. The program committee also selected 12 short papers for presentation. The program includes a poster paper session presenting exciting work in progress. The program is complemented by three keynote speeches by Christian Collberg, Ninghui Li and Brad Wardman. This year's edition also features three workshops: the International Workshop on Security and Privacy Analytics, the ACM International Workshop on Security in Software Defined Networks & Network Function Virtualization, and the ACM Workshop on Attribute-Based Access Control.",,,,2018,,,,Book
CODASPY '19: Proceedings of the Ninth ACM Conference on Data and Application Security and Privacy,"It is our great pleasure to welcome you to the ninth edition of the ACM Conference on Data and Application Security and Privacy (CODASPY 2019), which follows the successful eight editions held in February/March 2011-2018. This conference series has been founded to foster novel and exciting research in this arena and to help generate new directions for further research and development. The initial concept was established by the two co-founders, Elisa Bertino and Ravi Sandhu, and sharpened by subsequent discussions with a number of fellow data security and privacy researchers. Their enthusiastic encouragement persuaded the co-founders to move ahead with the always daunting task of creating a high-quality conference.Data and applications that manipulate data are crucial assets in today's information age. With the increasing drive towards availability of data and services anytime and anywhere, security and privacy risks have increased. Vast amounts of privacy-sensitive data are being collected today by organizations for a variety of reasons. Unauthorized disclosure, modification, usage or denial of access to these data and corresponding services may result in high human and financial costs. Important applications such as homeland security, social networking and social computing provide value by aggregating input from numerous individual users, and the mobile devices they carry. The emerging area of Internet of Things also poses serious privacy and security challenges. To achieve efficiency and effectiveness in traditional domains such as healthcare, there is a drive to make these records electronic and highly available. The need for organizations to share information effectively is underscored by rapid innovations in the business world that require close collaboration across traditional boundaries. Security and privacy in these and other arenas can be meaningfully achieved only in context of the application domain. Data and applications security and privacy has rapidly expanded as a research field with many important challenges to be addressed.In response to the call for papers of CODASPY 2019, 119 papers were submitted from Africa, Asia, Europe, South America and North America. The program committee selected 28 full-length research papers (23.5% acceptance rate). These papers cover a variety of topics, including security issues in web, cloud, IoT, and mobile devices, privacy, access control, authentication, malware, code analysis, and hardware and system security. The program includes a poster paper session presenting exciting work in progress. The program is complemented by two keynote speeches by Anupam Joshi and Engin Kirda. This year's edition also features three workshops: the ACM International Workshop on Security and Privacy Analytics, the ACM International Workshop on Security in Software Defined Networks & Network Function Virtualization, and the ACM Workshop on Automotive Cybersecurity, and a panel on the research challenges at the intersection on AI, Big Data and CyberSecurity.",,,,2019,,,,Book
Exploring and Analysing the African Web Ecosystem,"It is well known that internet infrastructure deployment is progressing at a rapid pace in the African continent. A flurry of recent research has quantified this, highlighting the expansion of its underlying connectivity network. However, improving the infrastructure is not useful without appropriately provisioned services to exploit it. This article measures the availability and utilisation of web infrastructure in Africa. Whereas others have explored web infrastructure in developed regions, we shed light on practices in developing regions. To achieve this, we apply a comprehensive measurement methodology to collect data from a variety of sources. We first focus on Google to reveal that its content infrastructure in Africa is, indeed, expanding. That said, we find that much of its web content is still served from the US and Europe, despite being the most popular website in many African countries. We repeat the same analysis across a number of other regionally popular websites to find that even top African websites prefer to host their content abroad. To explore the reasons for this, we evaluate some of the major bottlenecks facing content delivery networks (CDNs) in Africa. Amongst other factors, we find a lack of peering between the networks hosting our probes, preventing the sharing of CDN servers, as well as poorly configured DNS resolvers. Finally, our mapping of middleboxes in the region reveals that there is a greater presence of transparent proxies in Africa than in Europe or the US. We conclude the work with a number of suggestions for alleviating the issues observed.",,"Fanou R,Tyson G,Fernandes EL,Francois P,Valera F,Sathiaseelan A",,2018,,10.1145/3213897,https://doi-org.proxy.bnl.lu/10.1145/3213897;http://dx.doi.org/10.1145/3213897,Journal Article
Information Systems and Technologies to Support Learning: Proceedings of EMENA-ISTL 2018,"This book features a selection of articles from the second edition of the conference Europe Middle East & North Africa Information Systems and Technologies to Support Learning 2018 (EMENA-ISTL'18), held in Fez, Morocco between 25th and 27th October 2018. EMENA-ISTL18 was a global forum for researchers and practitioners to present and discuss recent findings and innovations, current trends, professional experiences and challenges in information systems & technologies to support learning. The main topics covered are: A) information systems technologies to support education; B) education in science, technology, engineering and Mathematics; C) emerging technologies in education learning innovation in the digital age; D) software systems, architectures, applications and tools; E) multimedia systems and applications; F) computer communications and networks; G) IOT, smart cities and people, wireless, sensor and ad-hoc networks; H) organizational models and information systems and technologies; I) humancomputer Interaction; J) computers & security, ethics and data-forensic; K) health informatics, and medical informatics security; l) information and knowledge management; m) big data analytics and applications, intelligent data systems, and machine learning; n) artificial intelligence, high performance computing; o) mobile, embedded and ubiquitous systems; p) language and image processing, computer graphics and vision; and q) the interdisciplinary field of fuzzy logic and data mining.",,"Rocha A,Serrhini M",,2018,,,,Book
Have You Locked the Castle Gate? Home and Small Business Computer Security,"From the Book: Installing Locks in the Global Village Securing Your Home or Small Business Network Introduction As I wrote this introduction, word of an e-mail virus was breaking in the news. As I sat to edit it, yet another virus had been found and was being fought. These viruses can take down major e-mail systems, disrupt communications, and destroy data. Worst of all, the viruses spread fast and easily through our networks, yet this is nothing new. Several e-mail viruses have surfaced prior to these, and many more are sure to follow. So how can they still be a threat Why hasnt someone done something to stop them The main reason is easy to see: most people arent prepared to defend their computer systems from these attacks and arent aware of the types of threats waiting for them in the electronic frontiers of the Internet. In fact, most people are so unprepared that they dont see any threat resulting from connecting their computers to the world. For this reason, these virus attacks are successful. Many people connected to the Internet are not protecting themselves in any way from such threats; in fact, most are not protecting themselves at all. I dont have statistics to back me up, but Id guess that most home users and small businesses have no effective security on their Internet-exposed networks or computers. Because we all share the same network (the Internet), we each need to place some security around our part of it to provide some protection for our data. Otherwise, we are providing an opportunity for someone to come along and exploit our computers. With so many computers on the Net, you might be lucky enough to remain safe for months or even yearswithout security because no one has looked your way yet. But this can work against you, too, by giving you a false sense of security when indeed you are compromised or under attack and you just dont know it. Dont be fooled into thinking that because you are one of many, you wont be a victim. Probably every gazelle and water buffalo in Africa thinks that, too, but the lions still eat. After hearing all of this, you might ask why not just move to the woods of the Rocky Mountains and hide Or perhaps you should simply not connect to the Internet. Both of those are options, but Im not trying to scare you away from the Internet and its great possibilities for information research, entertainment, and commerce. Rather, I mean to encourage you to use this tool wisely and securely. I hope to teach you the basics of information security so you can make decisions about the risks and benefits of doing or not doing certain things online and so you can do them as securely as possible. I dont promise to make you an expert but to show you how to get your foot in the door and where to look for expert information. Who Needs to Read This Book This book is primarily designed for home users and focuses on security issues that face these users. Home users arent the only ones who could benefit from this book, however. Small and medium-sized businesses with Internet connections could use this information, as well. The techniques discussed will transfer directly to such businesses, but the scale for a business is a bit larger. Additionally, anyone who wants to learn about information security and network security but doesnt have a strong computer background can use this book as an entry point into the concepts and techniques of information security.The content of the book ranges in nature from nontechnical examples through technical details that some readers might find hard or strange. Thats okay&38212not every reader will understand every item in this book. Because the book can help you put some basic security in place, some parts are rather technical. If you have to skip sections or come back later, thats fine. My goal is to present the material in a technically accurate way while trying to make it understandable for nontechnical readers. That is a broad range to cover, and Im sure some people will feel some areas are too technical or not technical enough. For readers who want more technical information, Ive included links and resources that can cover nearly all topics in this book to a far greater depth. On the other hand, if you find something that is too technical for you, feel free to skip ahead a bit. As you become familiar with the topics and discussions, you can go back and read again later. Although users of non-Windows operating systems such as Linux, Macintosh, or BeOS will find the conceptual parts of this book useful, the main focus is on the Windows family of operating systems most often found in homes and small businesses. Additionally, users seeking advanced technical discussions of security or in-depth scripting and coding analysis of tools will not find them in this book. Those areas of discussion are outside the scope of this book. I will, however, provide links and references to those subjects as appropriate throughout the text of the book. Why the Homestead Example Every chapter starts with an example. I chose the homestead example for a variety of reasons. First, it is an easy analogy that captures security concepts simply and in a way that most people can relate to. By introducing the concepts without their technical aspects, I hope to make them easier to understand. Then, as the chapter progresses, I introduce the technology to you slowly, carrying the concepts from a familiar example into a potentially unfamiliar one. If you find that the example is not working for you, simply skip ahead a bit in each chapter. Concepts are introduced twice in each chapter, once in the example and once more in the technical sections. I would encourage you, though, to at least read the example and be familiar with it as the book progresses, so you can refer to it as needed. Is the Example Important So really, why should you read the example I hope because it is a good illustration of security concepts in a nontechnical setting. Even people who know computers reasonably well are usually not familiar with security issues, let alone trained in them. The example takes away any preconceived notions about technology and computers and lets you concentrate on the concepts. Then when the technology is reintroduced, I hope you will see the application of the concepts more easily. But keep a few things in mind as you progress through the example. First, it does not include any factual information about real places or village growth. If you are an anthropology or sociology person, please be forgiving about any assumptions or errors in those fields. The homestead is merely an illustrative tool for this book. Second, I have tried to make the sections about our homestead and village enjoyable reading, but they are there just to provide examples. Dont worry if you dont see the security issues right away in the example; the text of the chapter will help bring out the points I am making. Introduction to the Homestead To help put the security discussions in a context that most users can understand, I have used an analogy of a homestead to demonstrate certain points and introduce concepts in the book. The homestead was started by the Smith family and grew into a village over time. Using this example, I introduce each chapters security concepts in a noncomputer-related way so you can focus on the security points before grappling with the computer terms or concepts. Then I revisit each point to reinforce the learning and provide a computer-specific application to take you from concept to practice. And that brings us to the homestead itself. On a small hill, near a river, was a fine patch of land with plenty of room for farming on the gentle slopes of the hill. The winters were not too harsh here nor the summers too dry. It was the perfect place for small animals and a small patch of grain and vegetables. And so they came. Well call them the Smiths: John, Katie, Jennifer, and Carl. They packed up everything they owned, spent nearly all their money on livestock and supplies, and headed out here for the chance at something better. Owning our own home and farm has to be better than working on someone elses, they thought. They spent several days building a small log cabin&38212just enough space for the four of them&38212and a pen for the animals. The pen was as much to keep the animals in as to keep other things out, but&38212as Johns father always told him&38212it never hurts to have some protection. They then began clearing a plot of land for the garden. Soon things settled into a daily routine of farming and tending the livestock. John Smith was no fool. He wasnt expecting trouble, but he came prepared for it. He had heard of foxes that might try for the chickens, wolves that hunted sheep, and bears that might go after a cow or even the family. He kept his shotgun handy, cleaned it nightly, and reloaded it before going to bed. Out this far, a loss of an animal could make the difference between getting through the winter or not. As John drifted to sleep each night listening to the wolves howling in the distance, he wondered how many were even closer than the ones he could hear. John and Katie Smith came to their new home knowing little about it. They had heard about foxes, wolves, and bears being around but had not seen any yet. The Smiths had built their new home and so far had been safe from intruding animals, but John and Katie were also cautious. Living this far from help and with winter coming on, they could not afford to lose an animal, have eggs stolen from the chickens by a weasel, or see their crops eaten by deer and elk. John built a fence around the property to help keep animals out and to show where the boundaries were. The loose-log fence was not the most effective at keeping out small animals, but it was good for the larger ones. John and his son Carl then built a stone wall around most of the close property, including the house, barn, and vegetable garden. This was a much better structure for keeping out the smaller animals. Katie and daughter Jennifer used this time to make winter clothing and blankets from the wool they sheared in the spring, and they built a small chicken coop near the house. The Smiths did have a lock on the door but not on the gates; locks werent needed this far out. John did, however, teach everyone in the family how to use the shotgun, just in case. John checked the stone wall every day and rode the horse out to the wood fence at least once a week, watching for animal tracks or signs of something trying to get across the fence. Normally there was nothing, and he then went about the tasks of maintaining the crops and livestock. Some days he was even able to relax. Katie spent her days cooking and sewing the necessary items for the family to continue living out here. She tended the garden, fed the livestock, and kept the house clean. The children helped where they could. They drew water from the well and assisted their mom and dad with the other chores. They also played in the fields and woods around the house. It was a good summer. One day, however, John found fox tracks near the stone fence. When he looked closer, he saw that the tracks came near the chicken coop, but he couldnt see any way for the fox to get into the coop. John spent the rest of the day inspecting and repairing the chicken coop to prevent any small holes from giving the fox an entrance to it. The rest of the summer passed uneventfully, but John didnt let his guard down. Many days he found deer tracks in the crops, and once he even found bear tracks just outside the wooden fence. Certainly there were many threats out here, but so far the Smiths preparations had paid off. Is Your House Locked at Night Odds are you are reading this in your home or office, located in a town or village or maybe even a big city. The idea of a community isnt strange to us. Many of us know our neighbors, wave to them as they walk their dog, and feel safe in our homes at night. Even so, you probably lock your doors when you go to sleep. Why Do you need to do that if youre safe and among friends The truth is that most people are trustworthy and would never break into your home, but you know that not everyone is that nice. Some people, given the chance, will come in and take things from your home, or worse. You probably dont think twice about locking your doors at night or when you plan to be away from home for any length of time. You might even have a fence or wall around your yard to keep people from getting in there. Most of us like our private spaces and will take some measures to protect them. Why, then, do most of us connect to the Internet and not provide any protection for our computers For a large number of us, our personal lives are becoming very closely tied to computers. By exposing your computer to the Internet, you are indeed living a life without locks or gates. On the surface, that sounds fine&38212maybe even a bit desirable. But lets take a closer look at what that means. How many of you have online banking or pay your bills online How many of you use e-mail to talk about personal issues with friends and family How many use software to file taxes or do other activities related to a home business Leaving your computer unprotected with your personal and financial information on it is like carrying your medical records and checkbook to a park and spreading them out on the grass to review them. It might even be worse, because in the park you probably would notice if someone began to look over your shoulder. Most people, however, will never notice the person watching in the computer world. Providing security for your home computer is like locking your door at night or looking over your shoulder in the park. It isnt all you need to do, but without it, you are an easy target. Whats Important Here Before you go on, here are some suggestions for getting the most out of the chapters. The example is a good place to start in each chapter. Read the example through completely, and then read the rest of the chapter. You might even want to read the example once more after you read the chapter to see the concepts in action after getting them in the security context. This book was designed around teaching information security concepts and principles as well as applying those concepts to the Windows family of operating systems. If you use another operating system, I will assume you understand the differences well enough that you wont be confused by them. Only apply what you feel you need. Security is a strange subject, because you can always have more. Some level of security will probably meet your needs without being all you could possibly do. After you read this book, I hope you wont feel you need a full-blown firewall system and packet filtering router just to protect your kids game machine. Please read and understand Chapter 1, Assessing Risk, before jumping into securing your home system. Dont be afraid to experiment, but make backups just in case. As with anything in computers, feel free to learn by doing. But I also encourage you to go through the steps slowly so you can assess the impacts of the changes on your system. Making regular backups of data is always high recommended, but you should certainly make a backup before changing security settings on your system. Ill tell you how to undo certain actions where appropriate, and Ill let you know when you would not be able to undo something easily. A checklist appears at the end of most chapters. You can use these checklists to track any changes you make to your system and what the settings used to be. They also include some questions designed to help you understand the security needs of your system. I encourage you to use the checklists, but dont feel obligated to do every step. Simply use the checklists as a way to track what you did and didnt do. Starting Out Everyone who knows anything about security had to learn it somewhere. No one is born with this information. It is okay to have questions and to not understand a few things. Security is a complex field. I have tried wherever possible to make it easier for you and to provide examples to help clarify. Even so, you will probably find times through the course of this book when something will not make sense immediately. This is especially true if you are less familiar with the technology side of things. So what should you do when you dont understand My first suggestion is to continue to read. Some concepts are addressed multiple times through each chapter, with some additional information each time. Also, the chapter might help clear up concepts as it progresses. Second, mark the place where you have a question and go to the Web to search for more information. The chapter on additional resources contains links and information for getting security information on the Web, and you can check there. Finally, try reading example again if you have a conceptual question, or refer to the Windows Help system if your question is specific to the computer. By trying all these things, you should be able to get the information you need to answer your question. Important Assumptions While writing this book, I have made some assumptions that I will mention here so you can understand them. Not all of these assumptions will be true for everyone, but I want you to understand where Im coming from. First, I assume that you, the reader, are an average computer user, with no special skill or knowledge of computers. I explain concepts through the course of each chapter and present information in a way that I feel can best be understood by the average person. However, I do expect you to know what tasks you do on your computer and how important each task is to you. Second, I assume that most home users are on a Windows platform. Although most of the concepts presented in this book apply to any platform, the details and checklists are tailored to Windows-based systems. Security is needed on any operating system, but I chose to focus on the systems most people are probably using. If you use another operating system, you can use the book for concept learning and even use the checklists and examples, but you will need to know enough to translate the Windows-based information to your operating system. Its Your Data Throughout this book you will find many suggestions for securing your computer. More than likely, you will not implement every one of them on your system. You might not need some settings; others might not even apply to your computer. If you feel uncomfortable or unsure about a setting, you might choose not to implement it. In rare cases, some settings might, in fact, cause problems on your computer. Think of your computers security as a continuum, with usability on one end and security on the other. A completely secure computer might be unusable, and an extremely usable computer might be completely unsecured. You must feel comfortable with where your computer fits on this continuum. Investigate each setting to ensure that it does not have a negative impact on your computer. You should always maintain backups of data stored on your computer, but I strongly encourage you to back up data before making serious security changes to your system. That way you will always have a recent backup from which you can restore your system if the unpredictable happens. Chapters 3, Securing Your Computer, and 4, Securing Your Servers, offer detailed steps for securing your Windows system, and Appendix A is a large collection of links for more information about security. Note that although hackers and crackers can damage data, they are not a threat to your hardware. You might want to buy backup drives and other devices to be more secure, but youll never need to replace hardware as the result of an attack. Where to Look First Where do you start Assessing security for your computer can seem confusing at first, but a simple method will help keep things under control. Start by asking yourself the following questions: What are you using your computer for Buying things online Electronic banking Electronic trading E-mail Do you know how secure these services are What would it mean to you if your access to these functions was compromised Keep in mind that not all the risk is monetary. By impersonating your identity, a hacker can also damage your reputation. What are you connecting your computer to Most people connect their computers to the Internet, but some connect to private networks such as corporate remote access for their company. How are you connecting Is it a full-time connection, or do you control your computers connection (and disconnection) Connecting via an analog modem has been the only method available to most users, but newer technologies such as DSL and cable modem are enabling many people to connect at much higher speeds. Using these new technologies carries certain security considerations, so you need to know your connection type. Who has physical access to your computer Do you authorize these people to use your computer Do you want to control the access these people have to your computer or local network Who do you trust Do you open an e-mail attachment from a friend From someone you dont know How do you choose secure Web sites for online shopping What operating system are you using Some operating systems are inherently more secure than others. Answering these questions will move you down the path toward securing your system. Once you have an assessment of your computer, you can weigh the risks you are open to versus the usability you require. If you dont know the answers to any of these questions, dont worry. I will help you through them as you read this book. How Secure Is Your System Out of the Box When you purchase a computer, it typically arrives with a default configuration. The company from whom purchased the computer sets this configuration, usually by installing the operating system and choosing all the default settings the operating system offers at installation. This company is usually more focused on selling computers than on your computer security, and they make some assumptions about what the average user will be doing and needing from a security and usability perspective. You can change the default settings to harden (make more secure) or relax (make less secure) your computers security settings. Additionally, you might want to use some third-party programs that can extend the functionality and security of your operating system. The makers of most computers leave that all up to you. They have to do that because most users prefer usability to security. Why Because they dont know any better or dont think they are a target. The goal of this book is to show you why you need security and then to help you get the information you need to achieve that security.",,Shea B,,2002,,,,Book
Applications of Bayesian Belief Networks in Water Resource Management,"Bayesian belief networks (BBNs) are probabilistic graphical models that can capture and integrate both quantitative and qualitative data, thus accommodating data-limited conditions. This paper systematically reviews applications of BBNs with respect to spatial factors, water domains, and the consideration of climate change impacts. The methods used for constructing and validating BBN models, and their applications in different forms of decision-making support are examined. Most reviewed publications originate from developed countries (70%), in temperate climate zones (42%), and focus mainly on water quality (42%). In 60% of the reviewed applications model validation was based on the expert or stakeholder evaluation and sensitivity analysis, and whilst in 27% model performance was not discussed. Most reviewed articles applied BBNs in strategic decision-making contexts (52%). Integrated modelling tools for addressing challenges of dynamically complex systems were also reviewed by analysing the strengths and weaknesses of BBNs, and integration of BBNs with other modelling tools. The application of BBNs to water resource management was rarely applied in developing countries and in tropical regions.Only 8% reviewed papers explored potential impacts of climate change on water resources.Only 11% and 6% of reviewed articles applied influence diagrams and Object-Oriented Bayesian Networks respectively.Most reviewed articles applied BBNs in strategic decision-making contexts (52%) for water resource management.Results from BBN models were rarely compared or tested against other modelling approaches to validate their performance.",,"Phan TD,Smart JC,Capon SJ,Hadwen WL,Sahin O",,2016,98–111,10.1016/j.envsoft.2016.08.006,https://doi-org.proxy.bnl.lu/10.1016/j.envsoft.2016.08.006;http://dx.doi.org/10.1016/j.envsoft.2016.08.006,Journal Article
CODASPY '20: Proceedings of the Tenth ACM Conference on Data and Application Security and Privacy,"It is our great pleasure to welcome you to the tenth edition of the ACM Conference on Data and Application Security and Privacy (CODASPY 2020), which follows the successful ninth editions held in February/March 2011-2019. This conference series has been founded to foster novel and exciting research in this arena and to help generate new directions for further research and development. The initial concept was established by the two co-founders, Elisa Bertino and Ravi Sandhu, and sharpened by subsequent discussions with a number of fellow data security and privacy researchers. Their enthusiastic encouragement persuaded the co-founders to move ahead with the always daunting task of creating a high-quality conference.Data and applications that manipulate data are crucial assets in today's information age. With the increasing drive towards availability of data and services anytime and anywhere, security and privacy risks have increased. Vast amounts of privacy-sensitive data are being collected today by organizations for a variety of reasons. Unauthorized disclosure, modification, usage or denial of access to these data and corresponding services may result in high human and financial costs. Important applications such as homeland security, social networking and social computing provide value by aggregating input from numerous individual users, and the mobile devices they carry. The emerging area of Internet of Things also poses serious privacy and security challenges. To achieve efficiency and effectiveness in traditional domains such as healthcare, there is a drive to make these records electronic and highly available. The need for organizations to share information effectively is underscored by rapid innovations in the business world that require close collaboration across traditional boundaries. Security and privacy in these and other arenas can be meaningfully achieved only in context of the application domain. Data and applications security and privacy has rapidly expanded as a research field with many important challenges to be addressed.In response to the call for papers of CODASPY 2020, 154 papers were submitted from Africa, Asia, Europe, South America and North America. The program committee selected 28 full-length research papers and 2 dataset papers (19.4% acceptance rate). These papers cover a variety of topics, including security issues in web, cloud, IoT, and mobile devices, privacy, access control, authentication, malware and system security. The program includes a poster paper session presenting exciting work in progress. The program is complemented by two keynote speeches by James Joshi and Somesh Jha and a panel on challenges on winning the cybersecurity arms race. This year's edition also features three workshops: ACM Workshop for Women in Cybersecurity Research, ACM International Workshop on Security and Privacy Analytics and the 2nd ACM Workshop on Automotive Cybersecurity.The organization of a conference like CODASPY requires the collaboration of many individuals. First of all, we would like to thank the authors for submitting to the conference and the keynote speakers for graciously accepting our invitation. We express our gratitude to the program committee members and external reviewers for their efforts in reviewing the papers, engaging in active online discussion during the selection process and providing valuable feedback to authors.",,,,2020,,,,Book
Analysing the Factors Affecting the Selection of ERP Package: A Fuzzy AHP Approach,,,"Bhatt N,Guru S,Thanki S,Sood G",,2021,641–682,10.1007/s10257-021-00521-8,https://doi-org.proxy.bnl.lu/10.1007/s10257-021-00521-8;http://dx.doi.org/10.1007/s10257-021-00521-8,Journal Article
"""Out from Behind This Mask"": Persona in African American Poetry, 1830-1930","""Out from Behind this Mask"": Persona in African American Poetry, 1830-1930 investigates the processes of poetry production and the politics of black identity from the nineteenth century through modernism, offering readings of significant work by George Moses Horton, Adah Isaacs Menken, Paul Laurence Dunbar, and Jean Toomer. These readings are grounded on a re-theorization of the function of the poetic speaker, or persona, which gives a poem its central identity and voice. I claim that persona's lineage as a masking device—in ancient Greek and Roman drama, a change of masks delineated changes in character—represents an enduring problem in lyric theory that conflates the identity of the poet with his/her constructed personae. This conflation has often resulted in an ambiguous binary of poetic identity as ""mimetic"" (the fictional persona) or ""authentic"" (the poet as him/herself). I argue that leveraging and contesting this binary in the lyric tradition can explain how critics have overlooked the formal innovations of black poetic identity by conceiving of masking simply as disguise rather than as a complicated series of transitions among a broad range of poetic identities. While questions of authenticity are applicable to all poets, I show that they have especially significant resonances for African American poets in the slave era and Jim Crow era. After all, these writers not only produced their work in contexts of racial hierarchy, such as the threat of slavery or the popularity of blackface minstrelsy, but they were also limited by racial-literary standards that determined and restricted the creation, dissemination, and interpretation of their poetry. Through original archival work, research on historical context, and close formal analysis, I show how each poet challenged the binary limitations placed around their verse production and reception: enslaved versus free in Horton's work; theatrical artifice versus private exposé in Menken's; black dialect versus formal English in Dunbar's; lyrical sensuousness versus didactic utility in Toomer's. While Horton, Menken, Dunbar, and Toomer attempted to turn mischaracterizations and/or incomplete assessments of their verses into more complex forms of masking (as movements between a diverse range of poetic identities), their efforts have ironically continued to be neglected in criticism and pedagogy even today. Chapter 1 examines George Moses Horton, who is considered the first African American poet to articulate an antislavery poetics while still enslaved, and more significantly, to protest his own bondage in verse. Horton's personae of positionality, as I call it—his constant manipulation of stance due to his status as a slave (and then former slave)—is first contextualized against his valentine acrostics and antebellum cultures of literacy, and then against his first volume, The Hope of Liberty (1829), which was funded by the American Colonization Society on the condition that Horton's freedom would be granted if he emigrate to Liberia. Lastly I look at Horton's replacement of antislavery personae with love verses in The Poetical Works (1845), and at his newly garnered freedom and experiences traveling with the Ninth Michigan Cavalry Volunteers during the Civil War in Naked Genius (1865). I contend that Horton's distinct move was in cultivating an intertextual poetics that reflected the discordant nature of abolitionism. From the American Colonization Society to the divided factions of gradualists and immediatists, which produced multiple positions on the state of the American racial system, slave identity, and emancipation, Horton's verses address the rightful place of African Americans as they imagined moving, and then actually moved, from enslavement to freedom. Chapter 2 explores Adah Isaacs Menken's forgotten work as the first poet besides Walt Whitman and the only female poet before the twentieth century to write an entire volume—titled Infelicia (1868)—in the form of free verse, following the revolution of prosody in Leaves of Grass (1855). I read Menken's poetry as informed and undermined by her performative role in Mazeppa, which brought her international celebrity in the risqué exposure of her body, as well as by her unconventional womanhood and alleged racial ambiguity, which sensationalized her private life. Menken found in Whitman, whom she met through the bohemian literati at Pfaff's in New York, a model for a new poetics and a shared belief in the mutable nature of identity. Not only did she champion his project publicly in one of the first endorsements of Leaves of Grass but her only poetry collection, Infelicia, was also written almost entirely in free verse during the period in which she was in direct contact with Whitman. I argue that Menken adapted a key feature of Whitman's form and theory of language for her own poetry: the democratization of voice and the license to represent others. Yet unlike Whitman, who leveraged lyric universalism to unify Americans fractured by class and racial conflict, Menken's personae, in light of her own experiences as a professional actress and radical woman, posit that the idea of universal lyric expression is irrevocably fraught. Chapter 3 considers how Paul Laurence Dunbar's metapoetic standard English poems in Oak and Ivy (1893), Majors and Minors (1895), Lyrics of Lowly Life (1896) and Lyrics of the Hearthside (1899) responded to the popularity of black dialect poetry during this era and to Dunbar's role as the first African American commercial writer. I propose that his metapoetic standard English poems, in which the speakers act as poets performing or reflecting on failed literary work, are stagings of poetic process that protest the constraints of black dialect verse as opposed to the lyric spontaneity available to his white Romantic counterparts. The perceived notion that Dunbar's ""pure"" blackness imbued his black dialect verse with more racial authenticity than white writers (and other black writers) of dialect brought him notoriety but also resulted in a poetic and financial crisis that only further contributed to the popularity of his dialect work over his standard English personae. By uncovering programs for and descriptions of Dunbar's recitals, for instance, I find that Dunbar attempted to counter many of these problems by beginning his readings with a standard English poem. Looking at his romantic poems can show us how Dunbar debunked the deeply problematic myths about the imaginative possibilities of black writers and bridged the gap between Post-Reconstruction and Modernism. Chapter 4 situates Jean Toomer's later unpublished verses from the 1930s to the 1960s against his first published work, Cane (1923), considered to be a masterpiece of African American modernist lyricism, steeped in sensuous imagery of the slave past and urban black life. Toomer spent most of his life after Cane distancing himself from the book and his African American identity, in turn influencing the critical consensus that his later didactic writings are both steeped in racial denial and aesthetically flawed. I claim, however, that his nearly four decades of writing illuminate a striving for the kind of multiracial identification that he consistently recognized himself and his work to possess. From Toomer's seeming abandonment of his racial identity and modernist lyric for the Eastern mystic George Gurdjieff in the 1930s, to his long association with the Society of Friends (Quakers) until his death, I explore how Toomer's poetry is marked by the desire for functionality. Toomer believed personae not only represented but also resolved the struggle between the ""outer shell"" of social conditioning, and the inner, ""not manifested"" state of self-consciousness and unity. He therefore envisioned lyric as an expansive genre with uniquely serviceable aims that could readjust individual and collective relations to racial and literary identity, historical violence, and psychological trauma.",,Licato AM,,2018,,,,Ph.D. Thesis
A Decision Support System for Tuberculosis Prevalence in South Africa,"Tuberculosis is one of the most prevalent diseases, which is a threat to the lives of many South Africans. The disease has been spreading at a high rate in the past years. World health organization reported that South Africa is amongst 22 countries most burdened by the disease that has around 80% of the total global Tuberculosis cases. Despite the fact that the South African government is undertaking Tuberculosis campaigns, there is still a challenge in the control of the spread of Tuberculosis. The lack of awareness and access to information further contributes to the high rate at which Tuberculosis is spreading. Therefore, we developed a decision support system for Tuberculosis prevalence in South Africa. We used Bayesian network to aggregate, analyse and mine data received from health department of Mpumalanga province. The data is stored in MySQL database. We used WampServer environment, which enabled us to develop a web application. The proposed model educates, informs, and prescribes measures to take when visiting a high prevalence location. We then tested the system developed with data from Mpumalanga provincial health department showing that males are more at risk with prevalence rate of 53%, 0–35 age group are the most affected at 50%, Ehlanzeni location in Mpumalanga province is the most affected at 47%, pulmonary Tuberculosis is the most prevalence at 90% and the survival rate is at 87%. We evaluated the model achieving a utility of 88.2%. We believe this utility can be improved with more training and accurate data.",,"Razwiedani M,Kogeda OP",,2021,270–284,10.1007/978-3-030-86973-1_19,https://doi-org.proxy.bnl.lu/10.1007/978-3-030-86973-1_19;http://dx.doi.org/10.1007/978-3-030-86973-1_19,Conference Paper
Annual Editions: Computers in Society 08/09,"This Fourteenth Edition of ANNUAL EDITIONS: COMPUTERS IN SOCIETY provides convenient, inexpensive access to current articles selected from the best of the public press. Organizational features include: an annotated listing of selected World Wide Web sites; an annotated table of contents; a topic guide; a general introduction; brief overviews for each section; a topical index; and an instructor’s resource guide with testing materials. USING ANNUAL EDITIONS IN THE CLASSROOM is offered as a practical guide for instructors. ANNUAL EDITIONS titles are supported by our student website, . Table of contents UNIT 1. Introduction 1. 34991 Five Things We Need to Know About Technological Change, Neil Postman, Address to New Tech ’98 conference, March 27, 1998 Neil Postman, a well-known cultural critic, suggests that computer technology is too important to be left entirely to the technologists. “Embedded in every technology,” he says, “is a powerful idea….” 2. 46656 Slouching Toward the Ordinary, Susan C. Herring, New Media & Society, February 2004 Contrary to what we read, changes in the ecology of the computing “will continue to make the internet a simpler, safer and—for better or worse—less fascinating communication environment.” 3. 41735 On the Nature of Computing, Jon Crowcroft, Communications of the ACM, February 2005 The author states, “ Occupying a third place in human intellectual culture, computing is not bound by the need to describe what does exist (as in natural science) or what can be built in the real world (as in engineering).” UNIT 2. The Economy 4. 46657 The Subprime Loan Machine, Lynnley Browning, The New York Times, March 23, 2007 “The rise and fall of the subprime market has been told as a story of a flood of Wall Street money and the desire of Americans desperate to be part of the housing boom,” says Lynnley Browning. Yet, the boom was made possible “by a little-noticed tool of automatic underwriting software.” 5. 46658 Click Fraud, Brian Grow and Ben Elgin, Business Week, October 2, 2006 Internet advertisers think they pay only when an interested customer clicks on their ads. Martin Fleischman, an Atlanta businessman, “noticed a growing number of puzzling clicks coming from such places as Botswana, Mongolia, and Syria.” 6. 41736 The Big Band Era, Christopher Swope, Governing, January 2005 Even as cities like Philadelphia are working to transform the entire city into a wireless hot spot—with government as the internet service provider of last resort—communications companies are fighting to keep local governments out of the broadband business. 7. 45257 The Beauty of Simplicity, Linda Tischler, Fast Company, November 2005 A simple tale about simplicity. One company hired an editor from People Magazine to translate accounting lingo into everyday language, “pared back 125 setup screens to three,” and “sold 100,000 units in its first year on the market.” 8. 41740 The Software Wars, Paul De Palma, American Scholar, Winter 2005 The article argues that software development is like military procurement, and suffers many of the same woes, including excessive complexity and cost overruns. 9. 46659 Scan This Book!, Kevin Kelly, The New York Times Magazine, May 14, 2006 What will happen to libraries, books on paper, and copyright protections if Google’s plans to scan the books of five major research libraries succeeds UNIT 3. Work and the Workplace 10. 46660 National ID, Ryan Singel, Wired, May 15, 2007 Immigration is in the news again. One proposal before Congress is to issue American workers tamper-proof biometric Social Security cards. These would replace the text-only design that’s been issued to Americans almost without change for more than 70 years. 11. 34959 Brain Circulation, AnnaLee Saxenian, Brookings Review, Winter 2002 Do immigrants displace native workers Is the United States siphoning off talent from countries that can ill afford to lose it This Berkeley professor argues that high-skill immigration is more complex than that. 12. 41774 The New Face of the Silicon Age, Daniel H. Pink, Wired, February 12, 2004 This piece on Indian programmers should be enough to keep chairs of American computer science departments awake at night. 13. 46661 Computer Software Engineers, Occupational Outlook Handbook, 200607 Edition Here is one official source that acknowledges the effect of shipping high tech jobs abroad, but still predicts that “ software engineers are projected to be one of the fastest-growing occupations from 2004 to 2014.” 14. 41753 The Computer Evolution, Rob Valletta and Geoffrey MacDonald, FRBSF Economic Letter, July 23, 2004 This article uses data from several surveys “to examine two key aspects of the computer evolution: the spread of PCs at work and the evolving wage differentials between individuals who use them and those who do not.” 15. 41754 Making Yourself Understood, Stuart Crainer and Des Dearlove, Across the Board, MayJune 2004 In a business environment where half of surveyed managers report spending more than two hours each day answering e-mail, “it’s never been so easy to be misunderstood.” 16. 46664 Privacy, Legislation, and Surveillance Software, G. Daryl Nord, Tipton F. McCubbins, and Jeretta Horn Nord, Communications of the ACM, August 2006 The authors tell us that the assumption of employee privacy in the workplace “may be naïve.” Constitutional protections against unreasonable search and seizure “usually apply only to state actions.” UNIT 4. Computers, People, and Social Participation 17. 40654 Romance in the Information Age, Christine Rosen, The New Atlantis, Winter 2004 According to Christine Rosen, “our technologies enable and often promote two detrimental forces in modern relationships: the demand for total transparency and a bias toward the over sharing of personal information.” 18. 46665 How Do I Love Thee , Lori Gottlieb, The Atlantic, March 2006 Some Internet dating sites now use social scientists to “develop a new science of attraction.” Says the author, “My matches included a film editor wearing a kilt—and not in an ironic way. Was this the best science could do ” 19. 46666 The Perfect Mark, Mitchell Zuckoff, The New Yorker, May 15, 2006 A cautionary tale about an African scam and two years in prison for bank fraud and money laundering. 20. 41755 Back-to-School Blogging, Brock Read, The Chronicle of Higher Education, September 3, 2004 It should surprise no one that entering freshmen, who grew up using the Internet, should turn to university-sponsored blogs to ease the transition to college life. 21. 46667 E-Mail Is for Old People, Dan Carnevale, The Chronicle of Higher Education, October 6, 2006 Reaching students through email has become more difficult as students turn to text-messaging and social networking sites. UNIT 5. Societal Institutions: Law, Politics, Education, and the Military 22. 37230 The Copyright Paradox, Jonathan Band, Brookings Review, Winter 2001 According to the author, “the problem with piracy is not the inadequacy of existing laws, but the high cost of enforcing any law against the large universe of infringers.” 23. 46668 Piracy, Computer Crime, and IS Misuse at the University, Timothy Paul Cronan, C. Bryan Foltz, and Thomas W. Jones, Communications of the ACM, June 2006 Who are the students who “openly admit to illegally installing software on home computers or otherwise misusing computer information systems ” This article provides some clues. 24. 41764 Facing Down the E-Maelstrom, Jeffrey Selingo, The Chronicle of Higher Education, April 29, 2005 Never an easy job, leading a college in the age of the Internet requires sifting through e-mail, reading blogs, and fending off criticism, the volume of which would be inconceivable without networked computers. 25. 46669 Can Blogs Revolutionize Progressive Politics , Lakshmi Chaudhry, In These Times, February 2006 Liberals have been envious ever since Richard Viguerie’s computer-generated mailing lists contributed to Ronald Reagan’s victory in 1980. At a time when even Senate Majority Leader Harry Reid has a blog, some Democrats hope that the computer is finally on their side. 26. 46670 Center Stage, Carl Sessions Stepp, American Journalism Review, AprilMay 2006 How do a newspaper’s web and print versions differ Unlike the print version of a newspaper, the Web version receives little editing. 27. 46671 The Coming Robot Army, Steve Featherstone, Harper’s Magazine, February 2007 “Within our lifetime,” says Featherstone, “robots will give us the ability to wage war without committing ourselves to the human cost of actually fighting a war.” Sgt. Jason Mero concurs: “These things are amazing…. They don’t complain…. They don’t cry. They’re not scared. This robot here has no fear.” UNIT 6. Risk and Avoiding Risk 28. 41768 Why Spyware Poses Multiple Threats to Security, Roger Thompson, Communications of the ACM, August 2005 Harm caused by spyware ranges from gobbling up computer speed on your PC to enlisting your machine in attacks that can disrupt major businesses or the government. 29. 41769 Terror’s Server, David Talbot, Technology Review, February 2005 “Most experts agree,” says the author, “that the Internet is not just a tool of terrorist organizations, but is central to their operations.” 30. 37238 The Virus Underground, Clive Thompson, The New York Times Magazine, February 8, 2004 Clive Thompson states, “when Mario is bored…he likes to sit at his laptop and create computer viruses and worms.” 31. 46672 Secrets of the Digital Detectives, The Economist, September 23, 2006 It’s nice to learn that the good guys have some tricks of their own. 32. 46673 Data on the Elderly, Marketed to Thieves, Charles Duhigg, The New York Times, May 20, 2007 Thieves purchase lists of the elderly from consumer databases, then pose as government workers trying to update their files on World War II veterans and retired school teachers. Some seniors find themselves with empty bank accounts. 33. 41770 The Fading Memory of the State, David Talbot, Technology Review, July 2005 Government documents, from the 38 million emails generated by the Clinton administration to electronic records of the 1989 invasion of Panama, are on disintegrating electronic media, stored using now-obsolete formats. 34. 41771 False Reporting on the Internet and the Spread of Rumors, Paul Hitlin, Gnovis, April 26, 2004 Internet news sources can sometimes be unreliable. Paul Hitlin examines Internet coverage of the Vince Foster suicide along with other stories to understand why this is so. UNIT 7. International Perspectives and Issues 35. 46675 China’s Tech Generation Finds a New Chairman to Venerate, Kevin Holden, Wired, May 24, 2007 The new China is not a place that would have made Chairman Mao comfortable. One indication is the popularity of Bill Gates. 36. 46677 Is the Crouching Tiger a Threat , Robert L. Glass, Communications of the ACM, March 2006 All indications suggest that the U.S. domination of computing is about to be eclipsed. Here is one commentator who is not quite convinced. 37. 41776 Restoring the Popularity of Computer Science, David A. Patterson, Communications of the ACM, September 2005 While India turns out more and more programmers willing to work for a fraction of their American counterparts, enrollment in computer science classes across the United States is dropping. The author believes that “inaccurate impressions of opportunities” are behind the decline. 38. 41773 China’s Computer Wasteland, Benjamin Joffe-Walt, The Progressive, January 30, 2005 What to do with the detritus of the digital age is a growing problem. Shipping it to China seems to be one solution. 39. 46678 Cat and Mouse, on the Web, The Economist, December 2, 2006 This article examines censorship on the Internet and the extraordinary steps taken by the anti-censorship community to thwart the efforts of censors. 40. 46681 In Search of a PC for the People, Bruce Einhorn, Business Week, June 12, 2006 What features get included in a $200.00 PC marketed to developing nations and “I think of digital access for kids as a human right,” says Nicholas Negroponte of MIT are two issues explored in this article. UNIT 8. The Frontier of Computing 41. 46682 A Nascent Robotics Culture, Sherry Turkle, AAAI Technical Report, July 2006 “What is a robot kind of love ” and “What will we be like, what kind of people are we becoming as we develop increasingly intimate relationships with machines ” MIT’s pioneering sociologist tries to answer both questions 42. 46683 March of the Robolawyers, The Economist, March 11, 2006 Australian researchers have developed a program that helps divorcing couples divide their property. 43. 46684 Best-Kept Secrets, Gary Stix, Scientific American, January 2005 Public-key cryptography keeps e-commerce secure for now. Quantum cryptography might take its place. 44. 46685 Toward Nature-Inspired Computing, Jiming Liu and K.C. Tsui, Communications of the ACM, October 2006 Computer scientists are turning to biology as a source of inspiration for models of complex systems. These biological models change the rules governing systems behavior. 45. 41778 The Intelligent Internet, William E. Halal, The Futurist, MarchApril 2004 The author claims the Internet will be the “main method used in 30% of courses” by 2014. As with all predictions, enjoy, but read critically. 46. 41780 Mind Control, Richard Martin, Wired, March 2005 What does a quadriplegic young man who plays pong have in common with a monkey mentally moving a joy stick and “soldier-controlled killer robots ” The answer: Brain Computer Interface or BCI.",,De Palma P,,2007,,,,Book
Macroscopic Traffic Stream Variables Prediction with Weather Impact Using Hybrid CNN-LSTM Model,"Accurate prediction of the macroscopic traffic stream variables such as speed and flow is important for traffic operation and management in an intelligent transportation system. Adverse weather conditions like fog, snow, and rainfall affect the driver’s visibility, road capacity, and mobility. The accurate prediction of the traffic stream variables in adverse weather conditions is challenging because of the non-linear and complex characteristics of the traffic stream and spatiotemporal correlation between traffic and weather variables. Prolonged heavy rain causes massive waterlogging in developing countries due to weak drainage systems, narrow streets, and encroachment, further affecting these traffic stream variables. Snow reduces the road capacity as much as waterlogging does. Prolonged snowfall creates a thick layer on the road, which affects the traffic stream variables. Traffic data has a high spatial and temporal resolution compared to weather data, which makes the problem more challenging. In this paper, we define a soft temporal threshold to capture the prolonged impact of weather variables. To capture the traffic and weather data’s spatiotemporal and temporal features, we propose a hybrid CNN-LSTM model. To validate model performance, data from San Diego and Minneapolis Minnesota Twin city are used. The test experiments show that the hybrid CNN-LSTM model learns spatiotemporal and temporal features accurately compared to other deep learning models.",,"Nigam A,Srivastava S",,2021,1–6,10.1145/3427477.3429780,https://doi-org.proxy.bnl.lu/10.1145/3427477.3429780;http://dx.doi.org/10.1145/3427477.3429780,Conference Paper
An Assessment of Comprehensive Lifestyle Modifications for African American Adult Patients with Hypertension,"Uncontrolled hypertension is a significant risk factor for conditions such as myocardial infarction, heart failure, stroke, chronic kidney disease, and increased risk for dementia. The contributing factors to poor adherence among African Americans to recommended lifestyle modifications include racial disparities, educational level, and social-economic status. Adherence to positive lifestyle measures such as increased exercise, healthy nutrition, and medication adherence was identified as an essential component of hypertension management. Use of mobile app technology may facilitate these healthy lifestyle changes. The mobile app permits users to set reminders for their medications, blood pressure measurements, and physical activities. The mobile app may help remind individuals to take their medications and engage in physical activity. This mobile app may also allow people with hypertension to view areas of their health that need improvement. Study results indicated that educational sessions on the use of sustained lifestyle modification improved systolic blood pressure (SBP). There was a statistically significant difference in mean systolic blood pressure after lifestyle modification. After eight weeks, the mean SBP pre-intervention changed from Mbefore = 156.93 (SD = 5.19) and Mafter = 152.48 (SD = 4.89), t (19) = 3.47, p < .01; after 12 weeks, SBP changed from Mbefore = 159.93 (SDbefore = 5.19) and Mafter = 146.28 (SDafter = 4.61), t (19) = 11.53, p < .01.",,"Dosunmu S,Webb B,Eldridge C",,2020,,,,Ph.D. Thesis
Fire Emergency Evacuation Simulation Based on Integrated Fire-Evacuation Model with Discrete Design Method,"Emergency evacuation under fire condition in a mass transit station is a great concern especially in developing countries. The interaction between fire and human is very important in the analysis of emergency evacuation under fire condition. An integrated fire-human model, FDS+Evac, is widely used to solve numerically the simultaneous fire and evacuation processes. However, when the simulation runs increase, the simulation time and cost will increase dramatically. The use of discrete design method (DDM) to reduce the simulation time and cost in fire emergency evacuation simulations is proposed. The method is applied to an underground subway station to study the influence of different factors on fire emergency evacuation. The grid resolution is analyzed to determine an appropriate grid size that will optimize the solution accuracy and time. Different fire locations, heat release rates, occupant loadings, ventilation conditions and material properties are considered under fire condition in the underground subway station. It shows that the heat release rate has a weak influence on fire emergency evacuation, but the fire location, occupant loading, ventilation condition and material property have a great influence on fire emergency evacuation. Furthermore, the five parameters have a coupled function on fire emergency evacuation.",,"Yang P,Li C,Chen D",,2013,101–111,10.1016/j.advengsoft.2013.06.007,https://doi-org.proxy.bnl.lu/10.1016/j.advengsoft.2013.06.007;http://dx.doi.org/10.1016/j.advengsoft.2013.06.007,Journal Article
CODASPY '16: Proceedings of the Sixth ACM Conference on Data and Application Security and Privacy,"It is our great pleasure to welcome you to the sixth edition of the ACM Conference on Data and Application Security and Privacy (CODASPY 2016), which follows the successful five editions held in February/March 2011-2015. This conference series has been founded to foster novel and exciting research in this arena and to help generate new directions for further research and development. The initial concept was established by the two cofounders, Elisa Bertino and Ravi Sandhu, and sharpened by subsequent discussions with a number of fellow cyber security researchers. Their enthusiastic encouragement persuaded the co-founders to move ahead with the always daunting task of creating a high-quality conference.Data and applications that manipulate data are crucial assets in today's information age. With the increasing drive towards availability of data and services anytime and anywhere, security and privacy risks have increased. Vast amounts of privacy-sensitive data are being collected today by organizations for a variety of reasons. Unauthorized disclosure, modification, usage or denial of access to these data and corresponding services may result in high human and financial costs. New applications such as social networking and social computing provide value by aggregating input from numerous individual users and the mobile devices they carry and computing new information of benefit to society and individuals. To achieve efficiency and effectiveness in traditional domains such as healthcare there is a drive to make these records electronic and highly available. The need for organizations to share information effectively is underscored by rapid innovations in the business world that require close collaboration across traditional boundaries. Security and privacy in these and other arenas can be meaningfully achieved only in context of the application domain. Data and applications security and privacy has rapidly expanded as a research field with many important challenges to be addressed.In response to the call for papers of CODASPY 2016 a total of 115 papers were submitted from Africa, Asia, Australia, Europe, North America, and South America. The program committee selected 22 full-length research papers (19% acceptance rate). These papers cover a variety of topics, including privacy of outsourced data, novel privacy techniques, forensics, applications, and access control and security of smart appliances and mobile devices. The program committee also selected 5 short papers for presentation. The program includes a poster paper session presenting exciting work in progress, as well as a panel session led by Murat Kantarcioglu on ""Data Security in the Cloud: Post-Snowden Era."" The program is complemented by keynote speeches by John Knight, Anita Nikolich, and Jeffrey Voas. This year's edition also features three workshops: the International Workshop on Security and Privacy Analytics, the International Workshop on Security in Software Defined Networks&Network Function Virtualization, and the ACM workshop on Attribute-Based Access Control.",,,,2016,,,,Book
CODASPY '17: Proceedings of the Seventh ACM on Conference on Data and Application Security and Privacy,"It is our great pleasure to welcome you to the seventh edition of the ACM Conference on Data and Application Security and Privacy (CODASPY 2017), which follows the successful six editions held in February/March 2011-2016. This conference series has been founded to foster novel and exciting research in this arena and to help generate new directions for further research and development. The initial concept was established by the two co-founders, Elisa Bertino and Ravi Sandhu, and sharpened by subsequent discussions with a number of fellow cyber security researchers. Their enthusiastic encouragement persuaded the co-founders to move ahead with the always daunting task of creating a high-quality conference.Data and applications that manipulate data are crucial assets in today's information age. With the increasing drive towards availability of data and services anytime and anywhere, security and privacy risks have increased. Vast amounts of privacy-sensitive data are being collected today by organizations for a variety of reasons. Unauthorized disclosure, modification, usage or denial of access to these data and corresponding services may result in high human and financial costs. New applications such as social networking and social computing provide value by aggregating input from numerous individual users and the mobile devices they carry. The emerging area of Internet of Things also poses serious privacy and security challenges. To achieve efficiency and effectiveness in traditional domains such as healthcare, there is a drive to make these records electronic and highly available. The need for organizations to share information effectively is underscored by rapid innovations in the business world that require close collaboration across traditional boundaries. Security and privacy in these and other arenas can be meaningfully achieved only in context of the application domain. Data and applications security and privacy has rapidly expanded as a research field with many important challenges to be addressed.In response to the call for papers of CODASPY 2017 a total of 134 papers were submitted from Africa, Asia, Australia, Europe, North America, and South America. The program committee selected 21 full-length research papers (16% acceptance rate). These papers cover a variety of topics, including data privacy in several distinct settings (cloud data, multi-party data aggregation), forensics, applications, and access control and security of smart appliances and mobile devices. The program committee also selected 10 short papers for presentation. The program includes a poster paper session presenting exciting work in progress, as well as a panel session led by Adam Doupé on ""Trustworthy Data Science"". The program is complemented by keynote speeches by Kang G. Shin and S. Raj Rajagopalan. This year's edition also features three workshops: the International Workshop on Security and Privacy Analytics, the ACM International Workshop on Security in Software Defined Networks & Network Function Virtualization, and the ACM Workshop on Attribute-Based Access Control.",,,,2017,,,,Book
Milking the Quality Test: Improving the Milk Supply Chain Under Competing Collection Intermediaries,"We examine operational and incentive issues that conspire to reduce the quality of milk—via deliberate adulteration by milk farmers—acquired by competing collection intermediaries in developing countries. Broadly speaking, three main forces in the milk supply chain lead to the low quality of milk: high testing costs, harmful competition among stations, and free-riding among farmers. The goal of this study is to provide recommendations that address the quality problem with minimal testing. Interestingly, some intuitive interventions—such as providing stations with better infrastructure (e.g., storage and refrigeration facilities) or subsidizing testing costs—could hurt the quality of milk in the presence of competition. To save testing costs we utilize mixed testing, where the milk combined from multiple farmers is tested once. However, mixed testing makes the system vulnerable to free-riding among farmers. We counter free-riding by applying a credible threat of individual testing (although not its actual use in equilibrium). We then propose two interventions to combat the harmful competition among stations. The novelty of our proposals lies in utilizing the force of competition to solve a problem created by competition. The incentives in our proposals provide a new tool for the stations to compete and convert the harmful effect of competition (quality reduction) into a beneficial one (quality improvement), resulting in a socially desirable equilibrium outcome: all the farmers provide high-quality milk and each competing station conducts only one mixed test and no further testing.This paper was accepted by Serguei Netessine, operations management.",,"Mu L,Dawande M,Geng X,Mookerjee V",,2016,1259–1277,10.1287/mnsc.2015.2171,https://doi-org.proxy.bnl.lu/10.1287/mnsc.2015.2171;http://dx.doi.org/10.1287/mnsc.2015.2171,Journal Article
IoT Based Air Quality Monitoring System with Power Consumption Optimization and Air Quality Parameters Prediction Using Deep Learning,"With the fast developing economy, industrial park construction and production processes, there is increase in the probable issues related to environmental pollution, especially air pollution accidents. Air pollution may be explained as the contamination of the atmosphere by gaseous, liquid, or solid wastes or by-products that can danger human health and welfare of plants and animals or produce undesirable odors. The air pollution monitoring has been emerged as a critical issue in developing countries such as India. Recently, in Delhi, AQI has crossed the bar of 1000, which may cause enormous breathing problems, cancer-like diseases and chronic respiratory conditions to the citizens of the country. Therefore, availing air pollutant concentration information in real time (monitoring) to citizens with handy tools like web and mobile interface help citizens to avoid any health hazards. Along with monitoring air pollutant concentration forecasting is an effective method of protecting public health by providing an early warning against harmful air pollutants. IoT has the potential to monitor air pollution by providing real-time updates related to sudden changes in the air quality. The main motive of this work is to develop complete air quality monitoring system for real time reporting of air pollutants using IoT, along with efficient method for prediction of such air pollutants from the historical data.Many efforts have been made to monitor air quality using IoT based technologies, still, the air quality monitoring system using IoT is an open research area because of the challenges of IoT discipline such as complex architecture, no standardization, less memory, power consumption, interfacing of sensors, reliable delivery and security. The proposed research work makes the use of light-weight protocol and compatible devices to transmit air quality parameters to the remote server without building or setting up complex network. There are various kinds of ""Things"" used in the proposed monitoring system that includes NodeMCU (controller), HiveMQ (cloud broker), Sensors, Python Script (Paho MQTT subscriber) and Android. These things coordinate with each other for the purpose of air quality parameter (temperature, humidity, Carbon Monoxide, PM 2.5 and PM 10) collection, transmission, storage and retrieval individually towards the implementation of the complete air quality monitoring system. The proposed research work represents the implementation of power consumption reduction scheme during sensing (reading) phase. Also we have proposed event based transmission method to reduce number of transmissions (messages) which further reduce power consumption. Proposed system is implemented and tested with a variety of Quality of Service levels to confirm the reliability of the system under employed architecture. A customized web interface and mobile application are designed to represent updates of pollutants at different indoor and outdoor sites. The system also avails data logging in the data base for further analytics and prediction purpose. In this work, deep learning based framework is also proposed to predict air quality parameters such as particulate matters (PM 2.5 and PM 10) and carbon monoxide (CO). Long short term memory (LSTM) neural network based model that processes sequences in both forward and backward direction to consider influence of time steps (observations) in both directions is employed. For further learning and to improve the prediction performance, the stacking of unidirectional layers is implemented. The performance of the model is optimized by fine-tuning of various hyperparameters like epochs, regularization techniques for overfitting resolution, and various merging options for the bidirectional input layer. The proposed model achieves good optimization and performs better than a simple LSTM and RNN based model. Moreover, attention-based mechanism is adopted to focus on timesteps that are more significant for prediction purpose. The addition of self-attention mechanism improves the performance further and works well for longer sequences and extended time horizons also. Experiments are conducted using the recently collected real-world data and results are evaluated using mean square error (MSE) loss function metric.",,Kantilal BV,,2022,,,,Ph.D. Thesis
Q2SWinet '07: Proceedings of the 3rd ACM Workshop on QoS and Security for Wireless and Mobile Networks,"It is our great pleasure to welcome you to the 3rd ACM International Workshop on QoS and Security for Wireless Mobile Networks -- Q2SWinet 2007. Continuing the tradition of the two previous Q2SWinet events held in Montreal (Canada) and Torremolinos (Spain), this year's workshop reaffirms the relevance and interest of research on QoS and Security topics in wireless systems. This workshop has become a wonderful forum for discussion and presentation of original ideas, recent results and achievements on various issues and challenges related to QoS and Security on different layers and different technical viewpoints. The objective of the workshop is to give researchers and practitioners a unique opportunity to share and enlarge their perspectives and ideas with each other through a stimulating program.The call for papers of this workshop has attracted 54 quality submissions from Asia, Canada, Europe, Africa, and the United States. The program committee accepted 18 papers, which represent a 33% acceptance rate and cover a variety of topics, including QoS management in wireless/mobile systems, scheduling, routing, multicast, mobility, and security management. In addition, the program includes a poster session that hosts ten relevant paper contributions on the same topics. The accepted papers come from 16 countries, reflecting the fact that this workshop is very international. We hope that the proceedings will serve as a valuable reference for the worldwide researchers and developers working on wireless systems, mobile applications and services.Based upon the TPC recommendation, the following three papers have been selected as candidates for the Q2SWinet 2007 Best Paper Award: ""A Multi-channel Defense Against Jamming Attacks in Wireless Sensor Networks"", Ghada Alnifie and Robert Simon (George Mason University, USA)""A Dynamic Bandwidth Allocation Algorithm for Supporting Real-time Applications in 802.15.3 WPANs"", G. Boggia, P. Camarda, L. A. Grieco, G. Tomasicchio (DEE - Politecnico di Bari, Italy)""Decentralized Group Key Management for Dynamic Networks Using Proxy Cryptography"", Junbeom Hur, Youngjoo Shin, Hyunsoo Yoon (KAIST, Korea).",,,,2007,,,,Book
Named Entity Recognition in a South African Context,"The feasibility of a probabilistic Named Entity Recognition system in a South African context was tested. The intended use of the system is in a cyber forensic domain. At the core of the system is a dynamic Bayesian Network, which takes into account the probabilistic relationship between variables as well as contextual information. We illustrate the performance of such a system using different probability thresholds for classification purposes and compare the performance with and without a name gazetteer. Our system compares competently with similar existing systems in the information extraction domain. Future work will involve the application of the system in the cyber forensic environment, which poses new challenges such as diverse text types.",,"Louis A,De Waal A,Venter C",,2006,170–179,10.1145/1216262.1216281,https://doi-org.proxy.bnl.lu/10.1145/1216262.1216281;http://dx.doi.org/10.1145/1216262.1216281,Conference Paper
Empirical Study of Barriers to Electronic Commerce Adoption by Small and Medium Scale Businesses in Nigeria,"Electronic commerce (E-commerce) is a technological innovation that enables small to medium enterprises (SMEs) to compete on the same level with their larger counterparts. And it has the potential to improve efficiency and productivity in many areas and, therefore, has received significant attention in many countries of the world. A thorough analysis of the impact of the internet and e-commerce across firms, industries and economies is necessary to separate hype from reality. However, several researchers have called for the investigation of the association between the perceptions of e-commerce and the barriers to its adoption in developing countries. It is however on record that SMEs the world over are faced with significant challenges that compromise their ability to function and to contribute optimally to the respective economies where they operate. This study was conducted in three states of Nigeria (Lagos, Abuja and Enugu states) with the use of interviews to gather relevant data; the aim of which was to understand the challenges which serve as barriers to E-Commerce adoption by small and medium scale enterprises in the Nigerian context. Findings indicates that small and medium scale online present is at best unknown. The most common e-Commerce applications used by most SMEs include but not limited to the use of e-mails for communication purposes and a simple website for basic product information — information contained are usually outdated as most of these websites are hardly updated. Findings revealed, among others, that lack of and total absence of a regulatory framework on e-Commerce security, as well as technical skills, and basic infrastructures are some of the barriers to electronic commerce adoption. The findings however, provide a constructive insight to financial practitioners, governments as well as other stakeholders on the need to give e-commerce a place in all aspects of e-commerce activities.",,"Agwu EM,Murray PJ",,2015,1–19,,,Journal Article
Empirical Study of Barriers to Electronic Commerce Uptake by SMEs in Developing Economies,"Electronic commerce E-commerce is a technological innovation that enables small to medium enterprises SMEs to compete on the same level with their larger counterparts. And it has the potential to improve efficiency and productivity in many areas and, therefore, has received significant attention in many countries of the world. A thorough analysis of the impact of the internet and e-commerce across firms, industries and economies is necessary to separate hype from reality. However, several researchers have called for the investigation of the association between the perceptions of e-commerce and the barriers to its adoption in developing countries. It is however on record that SMEs the world over are faced with significant challenges that compromise their ability to function and to contribute optimally to the respective economies where they operate. This study was conducted in three states of Nigeria Lagos, Abuja and Enugu states with the use of interviews to gather relevant data; the aim of which was to understand the challenges which serve as barriers to E-Commerce adoption by small and medium scale enterprises in the Nigerian context. Findings indicates that small and medium scale online present is at best unknown. The most common e-Commerce applications used by most SMEs include but not limited to the use of e-mails for communication purposes and a simple website for basic product information-information contained are usually outdated as most of these websites are hardly updated. Findings revealed, among others, that lack of and total absence of a regulatory framework on e-Commerce security, as well as technical skills, and basic infrastructures are some of the barriers to electronic commerce adoption. The findings however, provide a constructive insight to financial practitioners, governments as well as other stakeholders on the need to give e-commerce a place in all aspects of e-commerce activities.",,"Agwu EM,Murray PJ",,2015,1–19,,,Journal Article
Leveraging Deep Learning and SNA Approaches for Smart City Policing in the Developing World,,,"Hassan SU,Shabbir M,Iqbal S,Said A,Kamiran F,Nawaz R,Saif U",,2021,,10.1016/j.ijinfomgt.2019.102045,https://doi-org.proxy.bnl.lu/10.1016/j.ijinfomgt.2019.102045;http://dx.doi.org/10.1016/j.ijinfomgt.2019.102045,Journal Article
Beyond the Baseline: Establishing the Value in Mobile Phone Based Poverty Estimates,"Within the remit of `Data for Development' there have been a number of promising recent works that investigate the use of mobile phone Call Detail Records (CDRs) to estimate the spatial distribution of poverty or socio-economic status. The methods being developed have the potential to offer immense value to organisations and agencies who currently struggle to identify the poorest parts of a country, due to the lack of reliable and up to date survey data in certain parts of the world. However, the results of this research have thus far only been presented in isolation rather than in comparison to any alternative approach or benchmark. Consequently, the true practical value of these methods remains unknown. Here, we seek to allay this shortcoming, by proposing two baseline poverty estimators grounded on concrete usage scenarios: one that exploits correlation with population density only, to be used when no poverty data exists at all; and one that also exploits spatial autocorrelation, to be used when poverty data has been collected for a few regions within a country. We then compare the predictive performance of these baseline models with models that also include features derived from CDRs, so to establish their real added value. We present extensive analysis of the performance of all these models on data acquired for two developing countries -- Senegal and Ivory Coast. Our results reveal that CDR-based models do provide more accurate estimates in most cases; however, the improvement is modest and more significant when estimating (extreme) poverty intensity rates rather than mean wealth.",,"Smith-Clarke C,Capra L",,2016,425–434,10.1145/2872427.2883076,https://doi-org.proxy.bnl.lu/10.1145/2872427.2883076;http://dx.doi.org/10.1145/2872427.2883076,Conference Paper
Ubuntu Unleashed 2012 Edition: Covering 11.10 and 12.04,"Ubuntu Unleashed is filled with unique and advanced information for everyone who wants to make the most of the Ubuntu Linux operating system. This new edition has been thoroughly revised and updated by a long-time Ubuntu community leader to reflect the exciting new Ubuntu 11.10 (Oneiric Ocelot) and the forthcoming Ubuntu 12.04. Former Ubuntu Forum administrator Matthew Helmke covers all you need to know about Ubuntu 11.10/12.04 installation, configuration, productivity, multimedia, development, system administration, server operations, networking, virtualization, security, DevOps, and moreincluding intermediate-to-advanced techniques you wont find in any other book. Helmke presents up-to-the-minute introductions to Ubuntus key productivity and Web development tools, programming languages, hardware support, and more. Youll find brand-new coverage of the new Unity desktop, new NoSQL database support and Android mobile development tools, and many other Ubuntu 11.10/12.04 innovations. Whether youre new to Ubuntu or already a power user, youll turn to this book constantly: for new techniques, new solutions, and new ways to do even more with Ubuntu! Matthew Helmke served from 2006 to 2011 on the Ubuntu Forum Council, providing leadership and oversight of the Ubuntu Forums, and spent two years on the Ubuntu regional membership approval board for Europe, the Middle East, and Africa. He has written about Ubuntu for several magazines and websites, is a lead author of The Official Ubuntu Book. He works for The iPlant Collaborative, which is funded by the National Science Foundation and is building cyberinfrastructure for the biological sciences to support the growing use of massive amounts of data and computationally intensive forms of research. Quickly install Ubuntu, configure it, and get your hardware running right Configure and customize the new Unity desktop (or alternatives such as GNOME) Get started with multimedia and productivity applications, including LibreOffice Manage Linux services, users, and software packages Administer and use Ubuntu from the command line Automate tasks and use shell scripting Provide secure remote access Manage kernels and modules Administer file, print, email, proxy, LDAP, and database services (both SQL and NoSQL) Use both Apache and alternative HTTP servers Support and use virtualization Use Ubuntu in cloud environments Learn the basics about popular programming languages including Python, PHP, and Perl, and how to use Ubuntu to develop in them Learn how to get started developing Android mobile devices Ubuntu 11.10 on DVD DVD includes the full Ubuntu 11.10 distribution for Intel x86 computers as well as the complete LibreOffice office suite and hundreds of additional programs and utilities. Free Upgrade! Purchase this book anytime in 2012 and receive a free Ubuntu 12.04 Upgrade Kit by mail (U.S. or Canada only) after Ubuntu 12.04 is released. See inside back cover for details.",,Helmke M,,2012,,,,Book
Perceptions and Expectations of IT Service Delivery Post Migration to a Microsoft Platform at a University of Technology in South Africa,"The implementation of Microsoft (MS) technologies/solutions as organisational infrastructure has become popular in the South African and international higher education (HE) arena. With benefits such as reduced costs, improved productivity and improved service delivery, MS technologies/solutions seem to be the preferred choice for many institutions worldwide. This study presents work in progress on identifying the quality of service using this new organisational infrastructure within the Durban University of Technology (DUT). DUT is the first University of Technology (UoT) in South Africa to implement nine MS technologies simultaneously. Through this implementation, the DUT has attempted to greatly simplify the collaboration and communication of its employees and ensure the security and protection of the organization's information and assets. Since migration to the MS platform, staff have raised concerns regarding the new solutions, pertaining to aspects such as security (with specific reference to login credentials and data), accessibility (with specific reference to mobile devices and wireless connectivity) and reluctance to use self- help tools to improve their IT experience. Migrating from a non-integrated system to an integrated one has also resulted in IT service delivery becoming a contentious issue regarding the support received from the UoT's IT staff. This paper discusses the types of MS technologies implemented at the university and the impact that these solutions have had on the full time administrative and academic staff at the university regarding their perceptions and expectations of current IT service delivery. The quality of this service was measured using the SERVQUAL instrument. As a result of the research, a service management framework appropriate for the predominantly MS (Microsoft) environment will be proposed to the DUT's Information Technology Support Services (ITSS) management.",,"Reddy N,Singh P,Petkov D",,2013,85–89,10.1145/2513456.2513457,https://doi-org.proxy.bnl.lu/10.1145/2513456.2513457;http://dx.doi.org/10.1145/2513456.2513457,Conference Paper
Comparing Type 2 Diabetes Self-Management Apps Against the Needs of Low-Income Minority Patients: Is There An Implicit Functionality Bias?,"Background: Diabetes Mellitus is a chronic disease affecting 30 million in the US. It is a leading cause of death and a major risk factor for severe COVID-19. More than 90% of cases are Type 2 (T2DM), which has adult onset and has risk factors that are behavioral (e.g., smoking) or environmental (e.g., poor nutrition, decreased physical activity). Self-management is critical to long-term treatment of T2DM. It includes adherence to medication regimens, constant nutritional and physical activity management, blood glucose monitoring, and behavioral changes (e.g., smoking cessation). Many mobile computing health (mHealth) apps have been developed to support TM self-management.Problem: US T2DM rates among non-Hispanic whites and the well-educated have leveled off, but diagnoses continue to increase disproportionately among low-income populations, particularly African-American, Latino, and Native American minorities. This has created a growing health disparity associated with social and economic factors that include differential access to healthcare, healthy food, occupational opportunities and physical activity options. (termed Social Determinants of Health or SDOH [1]. Recent public health research [2,3] has begun to identify unique SDOH challenges faced by one such population, low-income African Americans. This poster examines the degree to which the existing T2DM mHealth apps are able to address the self-management needs exposed in this emerging research, versus the more widely studied needs and issues associated with more affluent and largely white population of persons with T2DM.Methods: Seventeen positively assessed T2DM apps were selected from recent review articles. Separately, two sets of functional features were compiled. First, from the T2DM literature, a set of 23 categories and sub-categories was compiled of general features that were identified as desirable to support the T2DM self-management process. Second, a set of eleven functional features and sub-features was developed from the research on the SDOH challenges of low income African American persons with T2DM. The T2DM apps were then compared in a two-stage process using the two sets of criteria. Because many of the criteria in the second set involved social support, only those apps that have some form of social functionality were included in the second stage comparison.Results. The results of the two comparisons are presented as two matrices comparing each app with each criterion and sub-criterion. None of the apps in stage one contained all the general functions suggested in the literature, though several come close. In stage two, most apps had few or none of the focused forms of social support for self-management capabilities of interest.Conclusions. Social capabilities of existing T2DM apps seemed based on the unconstrained social network models used in general social network media (e.g., Facebook, Twitter, Instagram). However, the needs expressed from the low-income communities focused on first order geospatially-local networks that could provide pragmatic help in self-management activities. Additionally, existing apps relied on Premium versions and in-app sales for revenue models, but such features are not accessible to low-income users. Such design decisions suggest an implicit design bias toward more affluent user populations, which also sociologically tend to be more White. Participatory design is recommended as a method that could help avoid such implicit design biases.",,"Zachary W,Gupta H",,2020,,10.1145/3388440.3414913,https://doi-org.proxy.bnl.lu/10.1145/3388440.3414913;http://dx.doi.org/10.1145/3388440.3414913,Conference Paper
Wireless Communication as a Reshaping Tool for Internet of Things (IoT) and Internet of Underwater Things (IoUT) Business in Pakistan: A Technical and Financial Review,"Pakistan is one of the growing nations, specifically in the field of Information and Communication Technology (ICT). During the last decade, an intense rise in the adaptation of ICT has been observed in all the major cities of Pakistan. This includes, but not limited to, e-commerce, mobile technology, computer communication networks, embedded systems, software engineering, etc. Due to the resource constraints, Pakistan is not the producer of any technology; however, it is a potential consumer of numerous technologies and their products. It therefore, attracts most of the producers around the globe to invest in the technology business in Pakistan. According to the Board of Investment (BOI) Pakistan, the country has received more than US$5.7 billion during the last decade as the foreign investment in IT and Telecommunication sectors only. Moreover, it has more than 140 million cellular subscribers, around 45 million 3G/4G subscribers, more than 3 million fixed local line subscribers and approximately 48 million broadband subscribers [1]. Likewise, Pakistan is also one of the biggest buyers of Consumer Electronics (CE). Very few of the local companies are producing CE products, however, a major share of the CE market has been captured by the international brands of China, Japan, Korea, USA, Germany, etc. In the light of the facts, it can be inferred that the application of ICT such as the Internet of Things (IoT) and Internet of Underwater Things (IoUT) in consumer electronics has the strong potential in shaping a new dimension of CE business in Pakistan. Moreover, the recent literature has strongly advocated for the scope of 5G IoT/IoUT. This is due to the fact that existing communication infrastructure will not be sufficient to handle modern day IoT/IoUT need. In this article, a comprehensive study on the scope of IoT/IoUT enabled consumer electronics business is presented. In addition, the rationale of 5G IoT/IoUT integration in the developing countries like Pakistan is discussed. Moreover, the threats and opportunities in the business of IoT/IoUT enabled CE devices are also been presented. Finally, this study submits the recommendations to establish IoT/IoUT enabled CE business in Pakistan.",,"Rizvi SS,Zubair M,Ahmad J,Hashmani M,Khan MW",,2021,1087–1105,10.1007/s11277-019-06937-3,https://doi-org.proxy.bnl.lu/10.1007/s11277-019-06937-3;http://dx.doi.org/10.1007/s11277-019-06937-3,Journal Article
Citizen Relationship and Grievance Management System CiR&GMS through Multi-Channel Access for e-Government Services: A Case from India,"Citizens are demanding greater access to interaction with government through their preferred channels or devices. The private sector uses different channels for their services, citizens except same level of services from the public sector. Therefore public sector needs to focus on creating multiple delivery channels Traditional such as face to face, Telephone and Modern channels such as Website, E-mail, SMS, so that citizens can have 'channels of choice', depending on specific needs, demands and preferences in order to increase citizens' participation and satisfaction. For this reason, the paper's purpose is 1 To understand multi-channel architecture, Integration, Management and its Strengths & Weakness 2 To develop a frame work for Citizen Relationship and Grievance Management System CiR&GMS for a single view 3 By applying proposed framework, To identify what types of channels are providing to access public services at National, State and Local level governments in India as a case study 4 To find out challenges and issues in implementation of multi-channel service delivery. The key findings of the case study are: a There is no declining in providing traditional channels after introducing modern channels b Many departments are offering mixed channels c Usage of Mobile/SMS, Social media and Wi-Fi hotspots based channels are in initial stage d t-Government channel is not yet initiated in any department e Multi-channel integration and management is not yet initiated by many departments, these departments are managed channels as separate silos. The proposed framework may provide some guidance to the decision and policy makers in the public sector. However, such initiatives have many challenges to the developing countries like India.",,Rao VR,,2015,43–67,10.4018/ijesma.2015040103,https://doi-org.proxy.bnl.lu/10.4018/ijesma.2015040103;http://dx.doi.org/10.4018/ijesma.2015040103,Journal Article
How to Define Value on Data under Blockchain Driven Open Data System for E-Government,"Many interesting news and activities are going on in the field of blockchain (sometimes called, Distributed Ledger Technology) related technologies like cryptocurrency, Initial Coin Offering (ICO), Internet of Things (IoT), Big Data, Artificial Intelligence (AI, mostly Deep Learning). Not only developing countries but also developed countries need to be concerned about applying blockchain technology to their e-government system. Blockchain technology has brought a very deep innovation over conventional Client Server Trusted Third Party intermediary system to pure peer-to-peer (P2P) based smart contract system. This smart contract based peer-to-peer system allow participants to share data which was not possible under Client Server Trusted Third Party system due to architectural monopoly nature of Client Server. Smart contract concept brought logical conditions to share data among participants but it does not provide any definition about data value. This peer-to-peer based data sharing can be practically possible only under definition of value on data shared. If this definition is vague, then it will be very difficult to share data among participants. Also if this definition is clear, data sharing participants can get much benefit from this data sharing. This data sharing will be only effective from blockchain based architecture.",,"Park JS,Kim YS,Choi CH,Shim J",,2018,670–672,10.1145/3209415.3209436,https://doi-org.proxy.bnl.lu/10.1145/3209415.3209436;http://dx.doi.org/10.1145/3209415.3209436,Conference Paper
User Awareness and Tolerance of Privacy Abuse on Mobile Internet,"User awareness regarding privacy abuse.Mobile market development.Influential factors on user awareness regarding internet content blocking.Relationship between country level development and the user tolerance regarding privacy abuse. The paper presents the results of an exploratory study about the level of privacy abuse and the awareness level of users when communicating and using mobile Internet. The study looks into the relationships and associations between the telecommunications market developmental level, the wealth of a country, users' skills, the affordability of mobile technologies, the level of user tolerance of state-mandated content censorship, and related privacy threats. The results and findings are drawn from a collection of data gathered from ten countries which have a low reputation for respecting human rights. These countries are primarily Asian or African states. Differences within the user community tolerance levels are discussed from the perspective of the key parameters which define the level of development of the information society and also the user skill levels. For a better understanding of the issue, a brief introduction explains the capacity of smartphones to ensure user privacy, and availability of the circumvention tools for smartphones.",,"Callanan C,Jerman-Blažič B,Blažič AJ",,2016,109–128,10.1016/j.tele.2015.04.009,https://doi-org.proxy.bnl.lu/10.1016/j.tele.2015.04.009;http://dx.doi.org/10.1016/j.tele.2015.04.009,Journal Article
"A Sub-Regional Information System for Monitoring and Managing PLHIV in Cross-Border Areas between Gambia, Senegal and Guinea Bissau: Information System for Managing PLHIV in Cross-Border Areas","Since aids appeared, it is of common knowledge that geographical spread of HIV is linked to human mobility [1]. However, this relationship between mobility and Aids is both complex and relatively unknown. The FEVE project (Frontiers and Vulnerability to HIV in West Africa), is implemented by ENDA to underline the causal process in order to achieve the UNAIDS 90-90-90 target [2]: by 2020, 90% of the people living with HIV know their HIV status, 90% of the people who know their HIV-positive status are accessing antiretroviral therapy and 90% of the people receiving antiretroviral therapy will have suppressed viral loads. However, as in most African countries, in Senegal health actors generate a large amount of information every day, such as consultation, hospitalization, monitoring infectious diseases, deaths, etc that is recorded in registers. That make difficult their exploitation. This difficulty is compounded when we interested in data related to infectious diseases such as HIV in cross-border areas. The high mobility of the population in these areas poses a great deal of problems in terms of treatment adherence as well as the search for those lost to follow-up. To overcome this problem, we propose a transboundary platform for the monitoring of People Living with HIV (PVVIH). This platform is a web and application which offers not only a sub-regional system of PLHIV management but also a system of communication and capacity building between actors.",,"Dieng Y,Diop I,Faye Y,Malack CA",,2019,,10.1145/3361570.3361576,https://doi-org.proxy.bnl.lu/10.1145/3361570.3361576;http://dx.doi.org/10.1145/3361570.3361576,Conference Paper
Capturing Smoker in Non-Smoking Room,"In developing countries legal actions regarding violations of smokers in non-smoking areas are still weak. This will be very disturbing especially those who care for their own health. Eastern cultures are generally reluctant to treat smokers who violate the rules directly. Therefore, other methods are needed, such as creating a set of cigarette smoke detection systems that can warn smokers not to do it in a place that is forbidden to smoke. This device is expected to detect cigarette smoke, then at the same time take photos of suspected smokers as evidence of violations. Warnings will be carried out by authorized persons, if the alarm signal of cigarette smoke is not also heeded. The device consists of NodeMCU, MQ-2, piezo speakers, and ArduCAM OV2640. When the device detects cigarette smoke, it will push a beep sound, capture images of the room, and immediately send images to the Cloud database. A few moments later the mobile phone supervisor of the room will get a proof of the incident and immediately take an action to reprimand and arrest the perpetrators. The testing of cigarette smoke detection devices was carried out in a cardboard room with a volume of 50cm x 50cm x 75cm. Detection was carried out at 4 different points from the source of cigarette smoke each of 6 x experiments or 24 x experiments. It was found, that the speed of detection of cigarette smoke was achieved on average 14.4 seconds with the device located above the source of cigarette smoke (75cm).",,"Hareva DH,Teguh S",,2020,102–106,10.1145/3369114.3369159,https://doi-org.proxy.bnl.lu/10.1145/3369114.3369159;http://dx.doi.org/10.1145/3369114.3369159,Conference Paper
Design of Enterprise Employee Pension Platform Based on Complex Embedded System,,,Jiang L,,2021,,10.1016/j.micpro.2020.103783,https://doi-org.proxy.bnl.lu/10.1016/j.micpro.2020.103783;http://dx.doi.org/10.1016/j.micpro.2020.103783,Journal Article
Prediction of Wheat Production Using Machine Learning Algorithms in Northern Areas of Pakistan,,,"Ahmed MU,Hussain I",,2022,,10.1016/j.telpol.2022.102370,https://doi-org.proxy.bnl.lu/10.1016/j.telpol.2022.102370;http://dx.doi.org/10.1016/j.telpol.2022.102370,Journal Article
Investigating Alternative Delivery Systems for Self-Amplifying Rna Vaccines,"The rabies virus is an enveloped, single stranded, negative-sense RNA virus of the Lyssavirus genus, zoonotic pathogens within the family Rhabdoviridae. Although extensive effort has been made in the last decades to develop efficacious vaccines to prevent rabies spread, the virus is still responsible for the mortality of about 24,000 to 90,000 people per year especially in developing countries and it has been classified as one of the major causes of death from infectious diseases in humans. Commercially available rabies vaccines for humans are considered effective, however the production costs are very high and multiple injections are required to achieve protection. Therefore, the development of new vaccines to reduce the toll of rabies disease in the developing world would be highly desirable. Within this project a nucleic acid based vaccine strategy - in particular self-amplifying RNA vaccine (SAM) - has been investigated since this platform was previously reported to elicit protective immune responses, particularly in the case of cell-mediated responses in a safe manner and for a variety of virus disease. To enhance biological stability and cell internalisation, SAM was combined with four cationic delivery systems. Oil-in-water cationic nano emulsions (CNE), polymeric nanoparticles (NPs), lipid nanoparticles (SLNs) and liposomes were formulated in the absence of or in combination with a specific SAM vaccine. Despite the differences in formulation composition, all samples contained the same concentration of cationic lipid - 1, 2-dioleoyl-3-trimethylammonium-propane (DOTAP), or dimethyldioctadecylammonium (DDA) - known as immunostimulants. In the preliminary studies, two different manufacturing processes such as Microfluidics and Microfluidisation were applied. As a proof of concept, anionic liposomes and solid lipid nanoparticles were formulated and ovalbumin was encapsulated within the delivery systems as model protein antigen. Resulting carriers were compared in terms of their physico-chemical properties. The purpose was to obtain homogeneous formulations with a diameter in the nanometres range with a given manufacturing method. Furthermore, dialysis, tangential flow filtration (TFF) and size exclusion chromatography (SEC) have been tested as purification methods and compared in terms of the ability to remove both residual organic solvent and unloaded protein from samples without altering physico-chemical attributes. These process parameters and purification method optimisations were then applied to produce cationic CNE, NPs, SLNs and liposomes in combination with a specific SAM vaccine. In the preliminary studies and during formulations development optimisation, SAM encoding for green fluorescent protein (SAM-GFP) was used as a model SAM with a reporter function,given the ease of detection in in-vitro cell cultures. However, SAM encoding for rabies glycoprotein (SAM-Rabies) represented the actual antigen of interest, employed in this project for further in vivo analysis. Cationic SLNs, NPs and liposomes were produced using microfluidics, since this method required smaller volumes compared to the Microfluidisation, thus avoiding waste of reagents. However, the Microfluidizer was used to reduce CNE size,due to incompatibility between CNE component and microfluidics chip. Moreover, particles were formulated with SAM encoding the antigen of interest and loaded into or adsorbed onto cationic carriers. All delivery systems were evaluated according to their physico-chemical properties: hydrodynamic radius, sample homogeneity (polydispersity index - PDI) and surface charge. Furthermore, in vitro activity was investigated using three different cell lines:bone marrow derived macrophages (BMDM), bone marrow derived dendritic cells (BMDC) and baby hamster kidney cells (BHK). SAM uptake and antigen expression from each formulation in each cell line were used to discriminate and down-select formulations for invivo studies. In the in vivo studies, biodistribution of carriers alone or in combination with SAM were performed. Briefly the selected SAM-carriers were administered intramuscularly (i.m.) to BALB/c mice and their movement in the animal body was tracked using a radiolabelling technique thereby allowing measurement of formulations at chosen time-points and in specific organs. The aim of the study was to understand the pharmacokinetic profile of formulations in a mouse model and assess whether biodistribution might correlate with subsequent immunogenicity studies. The initial attempt of these studies was to (i) find the antigen dose to induce high antibody and cellular responses in vivo and (ii) to compare the adjuvant properties of selected cationic candidates (i.e. SAM encapsulating DOTAP NPs, DOTAP liposomes and DDA liposomes) after i.m. injection. Formulations were selected according to the potency of inducing antigen expression in vitro. The commercial vaccine Rabipur, which is an inactivated virus rabies vaccine, was used as comparator. The aim was to find a valid and more cost-effective alternative formulation which induced an immune response comparable or superior to the commercial vaccine. Data showed that DOTAP NPs were the most potent in triggering IgG titers among candidates and the antibody levels were equivalent to the ones induced by the commercial vaccine after a single dose. Interestingly, the GMT was well above the protective threshold despite the antigen dose used, thus meaning that elicited antibodies were functional against rabies glycoprotein G. In terms of cellular response all candidates were able to activate both CD4+ and CD8+ T cells in a comparable manner to the vaccine on the market. Moreover, to evaluate if changing the route of administration might affect carriers' potency,SAM encapsulating candidates were also administered intradermally (i.d.) and intranasally (i.n.), and formulations immunogenicity was evaluated according to IgG titres and cellular response. To do so, DOTAP NPs and DOTAP SLNs were selected; NPs were tested considering the promising outcome from the first in vivo study, whereas SLNs were introduced although poor in vitro antigen expression. The aim was to understand the power of in vitro models to predict in vivo antigen immunogenicity. Results highlighted that SLNs injected i.m.showed increased immunogenicity compared to both NPs and the licenced vaccine after a single dose. Moreover, the potency of SLNs was also seen after intradermal administration,where SLNs were as potent as Rabipur to elicit IgG titer in mice after two vaccinations, inducing comparable innate and adaptive immunity to the vaccine on the market. Herein it was also reported that two doses of SAM SLNs injected i.n. induced a humoral immunity which was higher than the one elicited by Rabipur. Interestingly, intranasal administration of SLNs led to a higher percentage of IL-2 producing antigen specific CD4+ T cells compared to the licenced in both spleens and lungs. Although a significant difference was observed among formulations in the ability to enhance antigen-specific IgG titres, immunogenicity did not directly correlate with biodistribution, where carriers' pharmacokinetics were indeed similar. All together, these findings are encouraging and demonstrate that coformulation of SAM vaccine and solid lipid nanoparticles might be a valid and more advantageous alternative to produce rabies vaccines, with augmented patient' safety and compliance.",,Anderluzzi G,,2019,,,,Ph.D. Thesis
Development and Characteristics of African Satellite Augmentation System (ASAS) Network,"This paper reports on an African Satellite Augmentation System (ASAS) Space and Ground Segments as an integration part of Global Satellite Augmentation System (GSAS) for enhanced Traffic Control and Management (TCM) globally at sea, on the ground (road and railway vehicles) and in the air. The ASAS network can be used as solely systems for covering and providing TCM and Safety and Security service for entire African Continent and Middle East region, according to the International Maritime Organization (IMO), its Global Maritime Distress and Safety System (GMDSS) and International Civil Aviation Organization (ICAO) recommendations and requirements. Since 1995 few commercial Regional Satellite Augmentation System (RSAS) networks have been projected and developed to utilize Communication, Navigation and Surveillance (CNS) service for Maritime Traffic Control (MTC), Land Traffic Control (LTC) and Air Traffic Control (ATC), including for improved Safety and Security in all transportation systems. The proposed Space Segment of Geostationary Earth Orbit (GEO) constellation and Ground Segment of ASAS network are discussed, and areas examined where further investigations are needed. Specific issues related to these challenges are concluded and a set of solutions is proposed to maximize the availability of ASAS network capacity to the user applications.",,Ilcev DS,,2013,121–137,10.1007/s11235-011-9464-x,https://doi-org.proxy.bnl.lu/10.1007/s11235-011-9464-x;http://dx.doi.org/10.1007/s11235-011-9464-x,Journal Article
IoT for Agricultural Information Generation and Recommendation: A Deep Learning-Based Approach,"Agriculture is the foundation of national economy. Therefore, countries all over the world—developed and developing countries—attach great importance to the sustainable development of agriculture. With the rapid development of Internet of Things (IoT) technology, advance applications are being designed to enhance agricultural economy. With the application of IoT, the production mode of traditional agriculture has been restructured and rationalized. Based on the applications of IoT in agriculture, this paper presents a method to automatically classify and recommend agricultural information. The standard domain-related theories and information service system are exploited to promote IoT technology in the construction of agricultural informatization. A convolutional neural network (CNN) model is used to classify agricultural information based on the vector file generated after preprocessing textual agricultural data. With the clustering method, the influence of unbalanced number of documents in the dataset is minimized. Finally, an information recommendation method based on multimodal interaction behavior is proposed for agricultural information recommendation. Potential features from textual information are extracted which are then fed to long short-term memory (LSTM) in connection with the interaction behavior. LSTM is used for the prediction of the possibility of interaction with respect to the information recommendations system. The experimental results show the feasibility of CNN in agricultural information classification problem. A commendable clustering accuracy is obtained for the agriculture category containing a large number of documents. However, the category with fewer documents is less clustered. The model may be used to effectively extract and classify agricultural information and has great significance in structuring and shaping agricultural information for convenient use in agricultural decision-making.",,"Wang H,Zhao Y,Shao C,Tirunagari S",,2022,,10.1155/2022/7378755,https://doi-org.proxy.bnl.lu/10.1155/2022/7378755;http://dx.doi.org/10.1155/2022/7378755,Journal Article
Perspectives on Teaching Computer Architecture in Developing Countries,"Different areas of computer science have different challenges with respect to curriculum development, textbook writing, and teaching. Computer architecture represents one extrema in this hyper-dimensional space: it is an applied discipline, the theory for many sub-areas is still in development, and unlike traditional theoretical areas, problem-solving and tutorial based teaching is still uncommon. Students typically appreciate the rigor of traditional algorithm courses because they are similar to the courses they have seen in high school, however computer architecture is an engineering discipline: it is a science, an art, and sometimes a combination of both. As the name suggests, it is quite like traditional ""architecture"" (designing beautiful buildings...). To further complicate matters, most textbooks in this subject have originally been written in the late nineties and fail to capture most of the developments that have happened over the last two decades, which are fundamental and revolutionary at the same time. In the late nineties the field was an applied discipline where the design of systems, the specific design choices made by engineers, and a description of important architectures was considered proper pedagogical practice. This was probably acceptable in developed countries where students had an adequate exposure to computers since their teenage years. However, this approach did not work in India and a few other developing countries where I taught. Hence, there was a need to grow the field and instead focus on providing timeless insights that are not dependent on specific architectures or specific technologies. There was a need to bring the field in line with other fields such as chemistry, physics, biology, and of course traditional CS (data structures, algorithms, discrete math, etc.). This was the nature of the challenge that I faced when I started writing my two books on computer architecture (www.basiccomparch.com and www.advcomparch.com). There was a need to formalize basic notions, provide a theoretical framework, and create a discipline that is more in line with traditional theoretical subjects, which the students (at least in developing countries) are more used to. Hence, a novel pedagogical approach was developed where the connect between Turing machines and basic architectures was shown. The same approach was followed to explain all the major concepts by relating them to other concepts in traditional math or CS: caches were connected with hashtables, virtual memory was connected with indexing, I/O was connected with networking, OOO processing was connected with graph algorithms, and memory models were re-explained with novel concepts developed by the verification community. The end result of doing so was 1500 pages of computer architecture (captured in my two books). Many concepts appear to be intuitive such as memory consistency and routing algorithms in on-chip networks. However, this never leads to a deeper understanding and cannot lead to practical designs because there is not enough knowledge to write real code and capture corner cases. This is where formal training and a connect with well-established CS concepts is required. In my second book on Advanced Computer Architecture, especially in the later half, I focused on power management, security, reliability, and architectures for AI/ML. Here again, the theory was not well developed and thus there were no popular mathematical formalisms. New notations and new theoretical tools were introduced to put these areas on a sound footing. This is by far the most important contribution of my two books, which is to formalize and ""mathematize"" concepts in computer architecture that hitherto had very applied definitions, which were often prone to misinterpretations. Towards the end of the talk, we will also digress into some of my other academic adventures such as co-chairing a committee to create the CBSE CS curriculum for schools, a lab course on processor design and kernel hacking (future of OS teaching), and my take on the future of teaching and learning (in general).",,Sarangi SR,,2022,7,10.1145/3561833.3568498,https://doi-org.proxy.bnl.lu/10.1145/3561833.3568498;http://dx.doi.org/10.1145/3561833.3568498,Conference Paper
A Data Driven Pre-Cooling Framework for Energy Cost Optimization in Commercial Buildings,"Commercial buildings consume significant amount of energy. Facility managers are increasingly grappling with the problem of reducing their buildings' peak power, overall energy consumption and energy bills. In this paper, we first develop an optimization framework -- based on a gray box model for zone thermal dynamics -- to determine a pre-cooling strategy that simultaneously shifts the peak power to low energy tariff regimes, and reduces both the peak power and overall energy consumption by exploiting the flexibility in a building's thermal comfort range. We then evaluate the efficacy of the pre-cooling optimization framework by applying it to building management system (BMS) data, spanning several days, obtained from a large commercial building located in northern Australia. The results from simulations show that optimal pre-cooling reduces peak power by over 50%, energy consumption by up to 30% and energy bills by up to 37%. Next, to enable ease of use of our framework, we also propose a shortest path based heuristic algorithm for solving the optimization problem and show that it has comparable performance with the optimal solution. Finally, we describe an application of the proposed optimization framework for developing countries to reduce the dependency on expensive fossil fuels, which are often used as a source for energy backup. We conclude by highlighting our real world deployment of the optimal pre-cooling framework on the IBM Bluemix cloud. Our pre-cooling methodology, based on the gray box optimization framework, incurs no capital expense and relies on data readily available from a BMS, thus enabling facility managers to take informed decisions for improving the energy and cost footprints of their buildings.",,"Vishwanath A,Chandan V,Mendoza C,Blake C",,2017,157–167,10.1145/3077839.3077847,https://doi-org.proxy.bnl.lu/10.1145/3077839.3077847;http://dx.doi.org/10.1145/3077839.3077847,Conference Paper
NSDR '11: Proceedings of the 5th ACM Workshop on Networked Systems for Developing Regions,"Welcome to the 5th ACM Workshop on Networked Systems for Developing Regions (NSDR'11). NSDR has emerged as one of the leading venues for reporting early results in applying technology towards the challenges of developing regions. Starting in 2010, NSDR came to encompass WiNSDR, a prior workshop at MobiSys that also focused on developing regions.The NSDR workshop reflects a growing interest on the part of many computer science researchers to do work that is relevant to the poorest communities around the world. As technologies such as the mobile phone are penetrating even the most remote and economically disadvantaged regions, there is a large potential to utilize technology to provide services that were previously out of reach.However, developing regions often have unique constraints, spanning connectivity, power, affordability, as well as non-technical factors such as language and literacy. For these reasons, the technologies used to address developing regions often need to be different than those for rich countries. While applying technology towards development goals is a richly interdisciplinary field, spanning economics, sociology, political science, and other disciplines, NSDR focuses specifically on the technical problems in this realm, and what we as computer scientists can contribute to the picture.This year's workshop drew 21 submissions, and we selected 11 papers for presentation. Mobile technology accounts for approximately half of the papers, with a focus both on core infrastructure as well as new applications in the space of healthcare and education. Other papers focus on computer security in the developing world, as well as new techniques for accelerating Web access from lowbandwidth environments. We are also very fortunate to have a keynote address from Joel Selanikio, M.D., who is co-founder and CEO of DataDyne, a pioneering social venture in this space.",,,,2011,,,,Book
Learning Sustainable Locust Control Methods in Virtual Reality,"Invasion of locust swarms has affected the crops in many countries in Africa and Asia, which is a significant threat to food security. Therefore, different approaches are adopted to monitor and control the locust swarms to save the crops. Furthermore, it has been proved in various studies that technology can help in agriculture through drones, real-time data monitoring, or teaching the farmers with the latest tools. Following the UN sustainability goals for food security, this research has presented a Virtual Reality(VR) based educational application to teach sustainable locust management strategies. Using hand tracking technology in the Oculus Quest lets users learn how farmers can deal with locusts without pesticides. Based on a storytelling approach, the methods presented are profitable for the farmers and free of any harm to crops regarding food security. This application can help motivate the adoption of these sustainable locust control strategies in broader interventions for environmental recovery.",,"Hahn N,Fuchs B,Fortna M,Cobb E,Iqbal MZ",,2022,271–274,10.1145/3505284.3532973,https://doi-org.proxy.bnl.lu/10.1145/3505284.3532973;http://dx.doi.org/10.1145/3505284.3532973,Conference Paper
What Improves Citizens' Privacy Perceptions toward RFID Technology? A Cross-Country Investigation Using Mixed Method Approach,"Empirically, we compared RFID-related privacy measures in Australia and Bangladesh.We explore the privacy concerns and solutions from citizens' perspective.Basic privacy-perceptions among users from different cultures are not contradictory.We developed six contributing factors that may enhance citizens' privacy in RFID-use. Privacy is a serious concern to radio frequency identification (RFID) technology. Worldwide, several companies scrapped RFID projects because of high resistance from consumers and their advocacy groups - which actually demand RFID-specific privacy policies. This concern is even more acute when RFID is used in public applications; because, in general case, citizens cannot refuse to provide data, and the data collected by a government agency would offer serious threats if are shared among third parties. Limited research has been performed in this specific issue; they all agree that perceived privacy increased RFID acceptance. But, what drives privacy perceptions are yet to be researched - this study closes this research gap. In order to conduct the current research, mixed method of research approach has been adopted. In the qualitative research stage, the authors conducted two focused-group discussion sessions and eight in-depth interviews in two different countries: Australia and Bangladesh; arguing that the status, and the perceptions and tolerance of the citizens on privacy are different in these two regions. The explored factors have been examined with empirical data obtained from these two countries. It is found that, there are distinct differences in perceptions in developed and developing countries. The detail findings offer practical suggestions to the agency managers so that they can ensure better privacy of the citizens. As a significant theoretical contribution, this study enhances existing literature identifying the antecedents of privacy, which play even different roles in different cultural backgrounds.",,,,2014,711–719,10.1016/j.ijinfomgt.2014.07.002,https://doi-org.proxy.bnl.lu/10.1016/j.ijinfomgt.2014.07.002;http://dx.doi.org/10.1016/j.ijinfomgt.2014.07.002,Journal Article
A Framework for Predicting Adherence in Remote Health Monitoring Systems,"Remote health monitoring (RHM) systems have shown potential effectiveness in disease management and prevention. In several studies RHM systems have been shown to reduce risk factors for cardiovascular disease (CVD) for a subset of the study participants. However, many RHM study participants fail to adhere to the prescribed study protocol or end up dropping from the study prior to its completion. In a recent Women's Heart Health study of 90 individuals in the community, we developed Wanda-CVD, an enhancement to our previous RHM system. Wanda-CVD is a smartphone-based RHM system designed to assist participants to reduce identified CVD risk factors by motivating participants through wireless coaching using feedback and prompts as social support. Many participants adhered to the study protocol, however, many did not completely adhere, and some even dropped prior to study completion. In this paper, we present a framework for analyzing baseline features to predict adherence to prescribed medical protocols that can be applied to other RHM systems. Such a prediction tool can aid study coordinators and clinicians in identifying participants who will need further study support, leading potentially to participants deriving maximal benefit from the RHM system, potentially saving healthcare costs, clinician and participant time and resources. We analyze key contextual features that predict with an accuracy of 85.2% which participants are more likely to adhere to the study protocol. Results from the Women's Heart Health study demonstrate that factors such as perceived health threat of heart disease, and perceived social support are among the factors that aid in predicting patient RHM protocol adherence in a group of African American women ages 25-45.",,"Alshurafa N,Eastwood J,Pourhomayoun M,Liu JJ,Nyamathi S,Sarrafzadeh M",,2014,1–8,10.1145/2668883.2669586,https://doi-org.proxy.bnl.lu/10.1145/2668883.2669586;http://dx.doi.org/10.1145/2668883.2669586,Conference Paper
Integrating Web 2.0 into an Academic Library in Tanzania,"Purpose - This paper aims to demonstrate work undertaken by Muhimbili University of Health and Allied Sciences MUHAS Library in an effort to integrate Web 2.0 technologies in its functions to enhance the quality of its services in Tanzania. Design/methodology/approach - The study conducted an exploratory questionnaire survey to assess user requirements among undergraduate medical students at MUHAS, developed Library 2.0 services, conducted training and created awareness. Findings - The paper shows that Web 2.0 technologies can be implemented effectively according to university goals, user's needs, deployment of user friendly tools, and capacity building among librarians and users. Students positively supported the adoption of Library 2.0 services at MUHAS. Library 2.0 services improved the quality of MUHAS library services, despite various challenges related to infrastructure, awareness, literacy, inadequate staff, security and ownership of Web 2.0 services. Research limitations/implications - The study findings may not be widely replicated because this article is based on a case study of the integration of Web 2.0 technologies into the library functions of MUHAS. This study did not examine the use of Library 2.0 applications among library users such as faculty and students which could illuminate further the case study. Practical implications - Most academic libraries in Africa have not yet adopted Web 2.0 technologies to improve their services. The user preferences, technology adoption, and challenges faced from the present study can help other libraries to plan and integrate their Library 2.0 technologies in their services. Originality/value - MUHAS Library offers a practical example of how Web 2.0 services can be adopted to enhance the quality of academic library services in an African context. This paper is of significance to academic libraries that are still considering their options with regard to the application of Web 2.0 technologies.",,Tandi Lwoga E,,2014,183–202,10.1108/EL-06-2012-0058,https://doi-org.proxy.bnl.lu/10.1108/EL-06-2012-0058;http://dx.doi.org/10.1108/EL-06-2012-0058,Journal Article
Safe Farming as a Service of Blockchain-Based Supply Chain Management for Improved Transparency,"Precision agriculture is based on the idea of utilizing technology to improve the efficiency of agriculture industry. Blockchain technology has great potential to revolutionize the agricultural industry. Furthermore, various Internet of Things (IoT) based solutions are proposed to enhance production such as crops condition monitoring system. These technologies aim to address various stages of the agricultural supply chain by improving the processes. The key issue faced by farmers is to ensure the protection of their crops from animals during all stages of a harvest. This paper proposes a IoT-based prevention system to tackle this issue for safe farming. The solution is based on the input of the sensor nodes deployed in the field to detect animal attacks. These sensors report a hazard to a Repelling and Notifying System (RNS) in the field. The RNS produces human-safe ultrasonic sound waves that are unbearable for animals, thus they leave the field. The proposed RNS system also reports all hazard incidents to a centralized Farm Management System (FMS) maintained by the farmer. The paper also proposes a way the FMS can add value to a broader arena by becoming a service to an Agricultural Blockchain system. As a blockchain node, the FMS maintains a shared ledger as part of blockchain to share the details of incidents with meta information with other nodes in the blockchain. This information is vital for other blockchain nodes of the agricultural blockchain such as other FMS, companies, authorities and standard bodies. The low-cost RNS performance evaluation concludes that it is power efficient that makes it even affordable for developing countries, and as a Blockchain service it enables novel applications for the agriculture industry.",,"Iqbal R,Butt TA",,2020,2139–2150,10.1007/s10586-020-03092-4,https://doi-org.proxy.bnl.lu/10.1007/s10586-020-03092-4;http://dx.doi.org/10.1007/s10586-020-03092-4,Journal Article
Understanding and Improving Modern Web Traffic Caching,"The WorldWide Web is one of the most popular and important Internet applications, and our daily lives heavily rely on it. Despite its importance, the current Web access is still limited for two reasons: (1) the Web has changed and grown significantly as social networking, video streaming, and file hosting sites have become popular, requiring more and more bandwidth, and (2) the need for Web access also has grown, and many users in bandwidth-limited environments, such as people in the developing world or mobile device users, still suffer from poor Web access. There was a burst of research a decade ago aimed at understanding the nature of Web traffic and thus improving Web access, but unfortunately, it has dropped off just as the Web has changed significantly. As a result, we have little understanding of the underlying nature of today’s Web traffic, and thus miss traffic optimization opportunities for improvingWeb access. To help improveWeb access, this dissertation attempts to fill the missing gap between previous research and today’s Web.For a better understanding of today’sWeb traffic, we first analyze five years (2006-2010) of real Web traffic from a globally-distributed proxy system, which captures the browsing behavior of over 70,000 users from 187 countries. Using this data set, we examine major changes in Web traffic characteristics that occurred during this period. We also develop a new Web page analysis technique that is better suited for modern Web page interactions. Using our analysis technique, we analyze various aspects of page-level changes, and present a simple Web traffic model that we develop based on our findings. Finally, we investigate the redundancy of this traffic, using both traditional object-level caching as well as content-based approaches that use the caching technique at the sub-object or packet level. Among many findings, we observe a huge potential benefit of the content-based caching approaches - the byte hit rate is almost twice as large as that of the traditional object-level caching approach. Motivated by the possible benefits from content-based caching approaches, we also develop Wanax, a scalable and flexible wide-area network (WAN) accelerator that is designed for low-bandwidth and resource-limited developing world environments. It uses a novel multi-resolution chunking (MRC) scheme that provides high compression rates and high disk performance for a variety of content, while using much less memory than existing approaches. Wanax exploits the design of MRC to perform intelligent load shedding to maximize throughput even when running on resource-limited shared platforms. Finally, Wanax exploits mesh network environments, instead of just the star topologies common in enterprise branch offices. Equally importantly, the designs of Wanax can be applied to enterprise environments, providing the same benefits.",,Ihm S,,2011,,,,Ph.D. Thesis
"Evaluation and Comparison of Satellite-Based Rainfall Products in Burkina Faso, West Africa","The performance of seven operational high-resolution satellite-based rainfall products – Africa Rainfall Estimate Climatology ARC 2.0, Climate Hazards Group InfraRed Precipitation with Stations CHIRPS, Precipitation Estimation from Remotely Sensed Information using Artificial Neural Networks PERSIANN, African Rainfall Estimation RFE 2.0, Tropical Applications of Meteorology using SATellite TAMSAT, African Rainfall Climatology and Time-series TARCAT, and Tropical Rainfall Measuring Mission TRMM daily and monthly estimates – was investigated for Burkina Faso. These were compared to ground data for 2001–2014 on a point-to-pixel basis at daily to annual time steps. Continuous statistics was used to assess their performance in estimating and reproducing rainfall amounts, and categorical statistics to evaluate rain detection capabilities. The north–south gradient of rainfall was captured by all products, which generally detected heavy rainfall events, but showed low correlation for rainfall amounts. At daily scale they performed poorly. As the time step increased, the performance improved. All except TARCAT provided excellent scores for Bias and Nash–Sutcliffe Efficiency coefficients, and overestimated rainfall amounts at the annual scale. RFE performed the best, whereas TARCAT was the weakest. Choice of product depends on the specific application: ARC, RFE, and TARCAT for drought monitoring, and PERSIANN, CHIRPS, and TRMM daily for flood monitoring in Burkina Faso.",,"Dembélé M,Zwart SJ",,2016,3995–4014,10.1080/01431161.2016.1207258,https://doi-org.proxy.bnl.lu/10.1080/01431161.2016.1207258;http://dx.doi.org/10.1080/01431161.2016.1207258,Journal Article
EC '06: Proceedings of the 7th ACM Conference on Electronic Commerce,"The papers in these proceedings represent the technical contributions to the 7th ACM Conference on Electronic Commerce -- EC'06, held June 11-15, 2006, at the University of Michigan in Ann Arbor, Michigan, USA. Since its inception in 1999, ACM EC has served as the leading scientific conference on advances in theory, systems, and applications for electronic commerce. The natural focus of the conference is on computer science issues, but the conference is interdisciplinary in nature, addressing a number of facets of electronic commerce including (1) theory and foundations; (2) languages; (3) automation, personalization, and targeting; (4) security, privacy, encryption, and digital rights; (5) applications and empirical studies; and (6) social factors. In addition to the main technical program, EC'06 featured four workshops, four tutorials, and invited keynote presentations from UC Berkeley School of Information Professor Hal Varian and Harvard Economics Professor Drew Fudenberg.The call for papers attracted 127 submissions from authors in academia and industry from around the world, including Africa, Asia, Canada, Europe, the Middle East, and the United States. Each paper was reviewed by at least three program committee members on the basis of scientific novelty, technical quality, and importance to the field. After discussion and deliberation among the program committee and program chairs, 36 papers were selected for publication in these proceedings and for presentation at the conference.",,,,2006,,,,Book
Making a Community Network Legal within the South African Regulatory Framework,"Community networks often operate at the fringe of legality with respect to spectrum, network infrastructure and providing services. We have been involved with such a network in a rural community, and together with them, have devised a way to become legal within the South African regulatory framework. A not-for-profit co-operative was formed and successfully applied for license exemption to operate the network infrastructure and offer services. Revenue is used to sustain the network and can also be used for other community needs. The network has equipment that is not 100% type-approved, and operates at a higher output power than is allowed. However, we have a simple plan to comply with such regulations. This paper offers our experience as a precedent for how to go about making a community network completely legal in South Africa and other countries that have a similar regulatory environment.",,"Rey-Moreno C,Tucker WD,Cull D,Blom R",,2015,,10.1145/2737856.2737867,https://doi-org.proxy.bnl.lu/10.1145/2737856.2737867;http://dx.doi.org/10.1145/2737856.2737867,Conference Paper
EC '10: Proceedings of the 11th ACM Conference on Electronic Commerce,"The papers in these proceedings represent the technical contributions to the 11th ACM Conference on Electronic Commerce -- EC'10, held June 7-11, 2010, at Harvard University in Cambridge, Massachusetts, USA. Since its inception in 1999, ACM EC has served as the leading scientific conference on advances in theory, systems, and applications for electronic commerce. The natural focus of the conference is on computer science issues, but the conference is interdisciplinary in nature, addressing a number of facets of electronic commerce including (1) theory and foundations; (2) languages; (3) automation, personalization, and targeting; (4) security, privacy, encryption, and digital rights; (5) applications and empirical studies; and (6) social factors. In addition to the main technical program, EC'10 features two workshops, four tutorials, and invited keynote presentations from MIT Sloan School of Management Professor Erik Brynjolfsson and Microsoft's General Manager for Experimentation Platform Ronny Kohavi. EC'10 is also co-located this year with the 9th Workshop on the Economics of Information Security (WEIS 2010), the Trading Agent Competition (TAC 2010) and the 25th IEEE Conference on Computational Complexity (CCC 2010).The call for papers attracted 136 submissions from authors in academia and industry from around the world, including Africa, Asia, Canada, Europe, the Middle East, and the United States. Each paper was reviewed by at least three program committee members and one senior program committee member on the basis of scientific novelty, technical quality, and importance to the field. After discussion and deliberation among the program committee, senior program committee and program chairs, 45 papers were selected for presentation at the conference; most of these papers appear in these proceedings. For a few accepted papers and at the authors' request, only abstracts are included along with pointers to full versions of these ""working papers."" This accommodates the practices of fields outside of computer science in which journal rather than conference publishing is the norm and conference publishing sometimes precludes journal publishing. It is expected that many of the papers in these proceedings will appear in a more polished and complete form in scientific journals in the future.",,,,2010,,,,Book
Identifying More Sustainable Technological Solutions for the Oil Refining Industry,"Sustainability is a paradigm, or way of thinking, to ensure that human activities remain within the global economic, social and environmental constraints imposed either externally, by the environment, or internally, by society itself. Making the transition to sustainability requires society to follow a path of sustainable development, which requires an understanding of the impacts of alternative activities in their entirety so that choices made lead increasingly to a sustainable way of life. To facilitate this adjustment, analytical approaches have been developed that promote holistic thinking. Some of these analytical approaches are driven by the concept of 'life cycle thinking' (LCT), which refers to the consideration of impacts throughout a complete supply chain or 'product system' with dimensions in time and space. By embracing life cycle approaches, decision makers can capture all potential effects of a decision and ensure that improvements made in one area of time or space do not lead to equal or greater decline elsewhere - a phenomenon referred to as 'shifting burdens'. Approaches have also been developed to aid decision makers in identifying the various ways that the choices they make can impact upon people and the planet and enable these decision makers to make, sometimes difficult, trade-offs between these impacts through a structured and transparent decision making process. This Engineering Doctorate (EngD) considers the concept of sustainable development in relation to a well established and important industry within the fabric of modem society: the oil refining industry. The first question that this project aims to answer is: 'Are oil refineries on a path of sustainable development?' The answer to this question is sought through learning about the principles and practices at a specific UK oil refinery, currently wholly owned by Chevron (Pembroke Refinery, Wales). Although it is recognised that there are questions over whether the conclusions apply to the industry as a whole, the findings are expected to provide some reflection of the wider industry because all multinational oil companies are influenced by the same societal pressures and have evolved through information sharing as well as individual enterprise. A further caveat when seeking to answer this question is the dependency of the oil refining industry on a non-renewable resource - oil - which makes it inherently unsustainable. That said, the alternatives to an oil based economy are still in their infancy and the oil industry is likely to underpin society for the foreseeable future with growth anticipated in developing countries. Promoting sustainable development in the oil refining industry is therefore a case of making the present less unsustainable, rather than the future more sustainable. To answer the stated question, the policies and strategic objectives of Chevron Global Manufacturing (GMfg) and their translation into project prioritisation and execution were critically reviewed against a set of principles for sustainable development. The main findings from this work are summarised as follows: 1. Chevron GMfg gives the protection of people and the local environment utmost priority within its corporate principles and strategic objectives; 2. These principles translate into a well developed hierarchy of decision processes that are designed to handle multiple criteria; 3. Inclusion of environmental criteria in project prioritisation is mainly confined to projects driven by compliance or corporate risk assessments. Where projects are not driven by compliance or risk, environmental criteria tend to be used to aid project prioritisation when economic and strategic criteria are inconclusive; 4. The full potential of the 'Chevron Project Development and Execution Process' (CPDEP) to handle multiple criteria may not always be realised when a narrow range of criteria dominate the capital allocation decision processes; 5. In-house decision support processes mainly focus on site impacts specific to the immediately surrounding area - they do not encourage life cycle thinking in a spatial sense (although it should be noted that 'life cycle costs', relating to the displacement of financial costs across time, are considered). 6. Some legislation, such as clean fuels legislation and biofuels mandates, moves environmental impacts from one part of the fuel chain to another; partly as a result of this legislation, there is some indication that life cycle approaches are starting to come into use at a corporate level. Based on these findings, it is proposed that current practices within Chevron GMfg are not fully aligned with sustainable development although it is recognised that the organisation is continuously improving its approach to incorporate environmental and social aspects into decision making.",,Weston N,,2010,,,,Ph.D. Thesis
Learned Features Are Better for Ethnicity Classification,"Ethnicity is a key demographic attribute of human beings and it plays a vital role in automatic facial recognition and have extensive real world applications such as Human Computer Interaction (HCI); demographic based classification; biometric based recognition; security and defense to name a few. In this paper, we present a novel approach for extracting ethnicity from the facial images. The proposed method makes use of a pre trained Convolutional Neural Network (CNN) to extract the features, then Support Vector Machine (SVM) with linear kernel is used as a classifier. This technique uses translational invariant hierarchical features learned by the network, in contrast to previous works, which use hand crafted features such as Local Binary Pattern (LBP); Gabor, etc. Thorough experiments are presented on ten different facial databases, which strongly suggest that our approach is robust to different expressions and illuminations conditions. Here we consider ethnicity classification as a three class problem including Asian, African-American and Caucasian. Average classification accuracy over all databases is 98.28%, 99.66% and 99.05% for Asian, African-American and Caucasian respectively. All the codes are available for reproducing the results on request.",,"Anwar I,Ul Islam N",,2017,152–164,10.1515/cait-2017-0036,https://doi-org.proxy.bnl.lu/10.1515/cait-2017-0036;http://dx.doi.org/10.1515/cait-2017-0036,Journal Article
Informal Interactive Bring Your Own Device Strategising Practices of South African Small and Medium Enterprises,"The purpose of this study is two-fold: firstly, to identify factors affecting BYOD adoption in South African SMEs and secondly understand strategies SMEs engage in with regard to BYOD. Following an interpretive approach, and interviews as a means of data collection; the findings show that SMEs are consciously aware of BYOD but are not eager to formalise their strategy. Instead, SMEs engage in informal interactive strategising despite their concerns of cost, security and privacy. SMEs did not view BYOD as a cost saving approach but rather as a double end sword that brought privacy and security concerns; unintended consequence; and no improvement in productivity. Further findings show how SMEs, specifically employees advocated the need for a policy prescribing how mobile devices should be used and in so doing perpetuating the interactive strategising. External factors such as changing laws and regulation were perceived as barriers towards a formalised BYOD strategy.",,,,2018,257–295,,,Journal Article
Virtual Currency as an Inclusive Monetary Innovation for the Unbanked Poor,,,Chipere M,,2018,37–43,10.1016/j.elerap.2018.01.004,https://doi-org.proxy.bnl.lu/10.1016/j.elerap.2018.01.004;http://dx.doi.org/10.1016/j.elerap.2018.01.004,Journal Article
EC '07: Proceedings of the 8th ACM Conference on Electronic Commerce,"These proceedings present the technical contributions to the 8th ACM Conference on Electronic Commerce EC'07, held June 11-15, 2007, at the Federated Computer Research Conference in San Diego, California, USA. Since its inception in 1999, ACM EC has served as the leading scientific conference on advances in theory, systems, and applications for electronic commerce. The natural focus of the conference is on computer science issues, but the conference is interdisciplinary and addresses many facets of electronic commerce including (1) theory and foundations; (2) languages; (3) automation, personalization, and targeting; (4) security, privacy, encryption, and digital rights; (5) applications and empirical studies; and (6) social factors. In addition to the main technical program, EC'07 featured three workshops and four tutorials.The call for papers attracted 154 submissions from academia and industry around the world, including Africa, Asia, Canada, Europe, the Middle East, and the United States. Each paper was reviewed by at least three program committee members on the basis of scientific novelty, technical quality, and importance to the field. The program committee selected 42 papers for presentation at the conference, of which 40 are published in the proceedings. At the authors' request, only abstracts for the remaining papers are included along with pointers to full versions of these ""working papers"". This accommodates the practices of fields outside of computer science in which journal rather than conference publishing is the norm and conference publishing sometimes precludes journal publishing. Several papers were invited to a special issue of Games and Economic Behavior. Moshe Tennenholtz (Technion University) will join David Parkes as a guest editor of this special issue and invited papers will be subject to a thorough round of additional review. In addition, a couple of papers will be invited for fast-track journal publication in the ACM Transactions on the Web (TWEB).",,,,2007,,,,Book
Deploying PAWS: Field Optimization of the Protection Assistant for Wildlife Security,"Poaching is a serious threat to the conservation of key species and whole ecosystems. While conducting foot patrols is the most commonly used approach in many countries to prevent poaching, such patrols often do not make the best use of limited patrolling resources. To remedy this situation, prior work introduced a novel emerging application called PAWS (Protection Assistant for Wildlife Security); PAWS was proposed as a game-theoretic (""security games"") decision aid to optimize the use of patrolling resources.This paper reports on PAWS's significant evolution from a proposed decision aid to a regularly deployed application, reporting on the lessons from the first tests in Africa in Spring 2014, through its continued evolution since then, to current regular use in Southeast Asia and plans for future worldwide deployment. In this process, we have worked closely with two NGOs (Panthera and Rimba) and incorporated extensive feedback from professional patrolling teams. We outline key technical advances that lead to PAWS's regular deployment: (i) incorporating complex topographic features, e.g., ridge-lines, in generating patrol routes; (ii) handling uncertainties in species distribution (game theoretic payoffs); (iii) ensuring scalability for patrolling large-scale conservation areas with fine-grained guidance; and (iv) handling complex patrol scheduling constraints.",,"Fang F,Nguyen TH,Pickles R,Lam WY,Clements GR,An B,Singh A,Tambe M,Lemieux A",,2016,3966–3973,,,Conference Paper
Adapting Motorbikes for Independent Use by People with Disability,"In much of the world motorbikes are the dominant means of transportation. In the developing world motorbikes are often the only form of motorized transportation affordable to the majority of the population. Unfortunately this method of transportation has not been widely exploited anywhere in the world for use by people with severe mobility impairments, especially those who must use wheelchairs. To provide affordable transport for a wide range of mobility-impaired people I have developed an inexpensive sidecar adaptation for motorbikes, capable of transporting a wheelchair user (Figure 1). This adaptation is referred to as the SideScooter. The SideScooter can be operated independently with hand controls from the sidecar or from the motorbike seat, depending on the needs of the operator. Given the vast number of motorbikes in the world and the scarcity of independent transportation for wheelchair users this device has the potential to improve the quality of life for many disabled people in developing as well as the more affluent regions of the world.",,Owens J,,2009,,10.1145/1592700.1592704,https://doi-org.proxy.bnl.lu/10.1145/1592700.1592704;http://dx.doi.org/10.1145/1592700.1592704,Conference Paper
Risk Analysis of Enterprises’ Investment in Infrastructure in Developing Countries Based on Structural Equation Model,"In order to control the risks of Chinese enterprises in infrastructure investment in developing countries, the corresponding evaluation system is constructed through a structural equation model algorithm to analyze these risks, so as to achieve risk prediction and risk controllability. Structural equation modeling is a method to establish, estimate, and test causality. It can replace multiple regression, path analysis, factor analysis, covariance analysis, and other methods and clearly analyze the effect of individual indicators on the population and the interrelationship between individual indicators. It is a multivariate statistical modeling technology mainly applied to confirmatory model analysis. Due to the guidance of national policies, there are more and more opportunities for Chinese enterprises to invest abroad. However, due to the influence of political, economic, and environmental factors, overseas investment is facing many difficulties. This paper analyzes the risks from four aspects: bilateral policy risk, legal difference and litigation risk, international economic risk, and technical risk through structural equation model algorithm. Aiming at these risks, the simulation software of the algorithm is constructed in MATLAB big data analysis software, and the risk control measures are put forward. Finally, with the support of China’s policies, in order to ensure the investment income, we should carry out risk intervention for the foreseeable risk and reduce the impact of risk on the investment income as much as possible, so as to improve the risk prevention and management awareness of overseas investment business. By analyzing the characteristics of venture capital and the various kinds of risks affecting venture capital, the risk structure model estimation of risk sneak attack is established by using the principle of structural equation model, and the impact of various risks on investment risks can be analyzed, so that the risk measurement and control of venture capital provides the basis of theoretical knowledge.",,"Lu H,Wang L,Khattak HA",,2022,,10.1155/2022/4790726,https://doi-org.proxy.bnl.lu/10.1155/2022/4790726;http://dx.doi.org/10.1155/2022/4790726,Journal Article
"Pre-Entry Experience, Postentry Adaptations, and Internationalization in the African Mobile Telecommunications Industry","We study the evolution of the African mobile telecommunications industry from its effective beginning and explore the sources of ownership advantages among indigenous firms, by assembling historical qualitative and quantitative firm-level data. Our historical qualitative findings suggest that a few start-ups gained industry-specific knowledge through their pre-entry experience, directed their postentry development of capabilities toward adaptations to challenging market and operational conditions, and leveraged their adaptive capabilities to enter and compete in other African countries. Using our quantitative panel data, we show that these firms successfully internationalized across the continent. In particular, compared with other start-ups, they had higher rates of foreign entry in African countries that had relatively weaker rule of law, and greater market reach in African countries that had relatively larger low-income consumer segments. These patterns corroborate that their capabilities for overcoming the industry’s challenging market and operational conditions were their key ownership advantages. Through our triangulated analysis, we show that inherited industry knowledge provides a foundation for postentry capability development, and entrepreneurial leadership guides this process to create ownership advantages for regional internationalization.",,"Jahanbakht M,Mostafa R,Veloso F",,2022,969–990,10.1287/orsc.2021.1470,https://doi-org.proxy.bnl.lu/10.1287/orsc.2021.1470;http://dx.doi.org/10.1287/orsc.2021.1470,Journal Article
Cybercrime and Business: Strategies for Global Corporate Security,"Cybercrime and Business: Strategies for Global Corporate Security examines the three most prevalent cybercrimes afflicting todays corporate security professionals: piracy, espionage, and computer hacking. By demonstrating how each of these threats evolved separately and then converged to form an ultra-dangerous composite threat, the book discusses the impact the threats pose and how the very technologies that created the problem can help solve it. Cybercrime and Business then offers viable strategies for how different types of businessesfrom large multinationals to small start-upscan respond to these threats to both minimize their losses and gain a competitive advantage. The book concludes by identifying future technological threats and how the models presented in the book can be applied to handling them. Demonstrates how to effectively handle corporate cyber security issues using case studies from a wide range of companies around the globe Highlights the regulatory, economic, cultural, and demographic trends businesses encounter when facing security issues Profiles corporate security issues in major industrialized, developing, and emerging countries throughout North America, Europe, Asia, Latin America, Africa, and the Middle East",,Moskowitz S,,2017,,,,Book
A Postal System Based Digital Network and a Distance Learning Application,"In this thesis, we propose the novel approach of turning storage media transported by the postal system into a general-purpose and transparent digital network, extending pervasive, high-bandwidth, and low-cost connectivity to places such as rural areas in developing countries. We call such a system the Postmanet . To fully realize its potential, however, an end user needs better support than being told to burn discs and toss them into the mail bin. We describe the systems support that we provide in order to achieve the generality, transparency, efficiency, and scalability goals of the system. The issues that we address include: managing ""DVD robots"" that automate mass-processing of DVDs, application-specific marshaling and unmarshaling of messages, providing best-effort reliable and secure delivery, simultaneous exploitation of conventional connectivity, and a mechanism for distributing and updating application code. Two additional support features are of particular importance. The first is a distributed object repository that makes available a single name space, on which any sites, including those that lack conventional networking access, can perform read, write, navigation, search, and other operations. This high-level abstraction makes it easier to construct distributed Postmanet applications. It also helps us realize a powerful ""network effect,"" as spontaneous connections are established among sites that enjoy shared access to a common repository. The second is scalable routing. Simply leaving end users to directly swap discs with each other does not scale well, because as many as N 2 discs may need to be exchanged at once in an N -node network. We solve this problem by multiplexing/de-multiplexing data to/from a smaller number of discs in transit. This can occur multiple times at dedicated nodes inside the network, or at peer end user nodes. We present routing topologies that can result in a good balance between simultaneously minimizing the number of discs involved and the end-to-end postal latency. We have built and deployed a real-world application, a rural distance learning system called the Digital StudyHall, on top of the Postmanet . It consists of a network of hubs and spokes, where the hubs are urban centers of excellence, which ""radiate"" content and methodology into poor villages and slum schools. Our experiences in rural India not only have provided us insights on the type of the systems support that we need, but also have allowed us to study mediation-based pedagogy that has proved promising in extending high-quality education to a needy population. For more information, please visit: http://dsh.cs.washington.edu .",,Garg N,,2006,,,,Ph.D. Thesis
ICT for Human Development in South Pacific,"The author worked past year as Professor & Head of School of Computing, Information and Mathematical Sciences and Director of Japan Pacific ICT Centre at the University of South Pacific. The South Pacific Region has many problems related to environmental and economic issues. The University of the South Pacific (USP) is an ideal platform for provision of development of Human Resources and enhancement of Human Security in the Pacific. Current dynamic Internet developments and continuous demand for the ubiquitous connectivity combined with the next generation of networks contributes towards creation the future cyberspace infrastructures worldwide. Implementation of the cyberspace in the government and corporate infrastructures, contributes towards creation of new paradigm in the decision making processes. Decisions that are currently governed by the human intelligence knowledge and intuition may be influenced by the cyber-data and processes. Future cyberspace will ultimately impact the decision making processes by government, corporate, industrial and academic institutions worldwide. Main objective of ICT Development is to build human-resource for ICT capacity development all across developing countries in the south pacific while “bridging the digital divide”. Main Goal was to provide the south pacific region with appropriate resources to take the lead role in driving the Pacific through Human Resource Development Programs. Specific Goal is to focus on developing and strengthening ICT skills applicable at the e-services level. The author discusses the development of new Japan Pacific Information Communications Technologies (ICT) Centre at the University of the South Pacific and ICT for Human Development and Security in the South Pacific Region. The main objective is to accommodate increasing demand for ICT-related education and training in the region and to accelerate research and development activities in the Pacific. The ICT Centre will play a facilitator role for ICT related education, Training; and Research and Development for the Pacific. In conclusion the author promotes discussion on the role of the role of the ICT in the South Pacific Region. The author opens discussion on ICT's social and ethical impact in the south pacific region in the context of governance vs. privacy.",,Babulak E,,2010,621–624,10.1109/MINES.2010.229,https://doi-org.proxy.bnl.lu/10.1109/MINES.2010.229;http://dx.doi.org/10.1109/MINES.2010.229,Conference Paper
Scalable Non-Invasive Pediatric Cerebral Visual Impairment Screening with the Higher Visual Function Question Inventory (HVFQI),"Cerebral Visual Impairment (CVI), vision loss due to brain injury in early childhood, is the leading cause --- approximately 40% --- of bilateral visual impairment in children from industrialized countries and is the most rapidly growing cause among children in developing countries. Typical causes of CVI include abnormal brain development or brain damage, often consequential to birth-related complications such as hypoxic ischemic encephalopathy, meningitis, hydrocephalus, and head injury. The current gold standard for clinical diagnosis require a trained clinician to administer visual-motor integration tests in conjunction with a thorough review of a patient's clinical history. This approach does not scale to meet the needs of undiagnosed children with CVI. Recently, non-invasive screening methods such as administering a higher visual function question inventory (HVFQI), have been shown to accurately capture the observations of teachers and guardians of children. Analysis of those observations has been shown to have very high correlation with visual-motor integration tests (p values < 0.01). In this poster, we present a clinical database and information system, the HVFQI web app, which delivers a scalable, non-invasive pediatric CVI screening that is currently administered by a clinician, but has the potential to be self-administered in the near future, with follow-up clinical consultation for participants that screen positive or near-positive.The HVFQI web app is an online clinical diagnostic tool and database system designed to gather participant responses to over 50 questions and follow-up questions. No personally identifying information is stored in the database. The web app provides three critical functions: (1) scalable and accurate administration of the HVFQI; (2) global coordination of screening efforts; and (3) a consolidated database for efficacy analysis studies and rapid iteration of the HFVQI to maximize impact, accuracy, and accessibility.The HVFQI consists of a question inventory, a scoring rubric, and a conditional intervention strategy list. The question inventory is carefully curated and designed to capture observations from teachers and guardians that may indicate specific pediatric higher visual function deficits (HVFDs). In many instances HVFDs are accompanied by normal visual acuity, making diagnosis difficult, and requiring multiple questions to elicit HVFDs. These questions are presented at random to avoid leading the participant. The responses are scored according to the HVFQI scoring rubric, which indicates which HVFDs may be present, and relevant intervention strategies. The rubric responds to varying degrees of affirmative responses in a 5-category Likert scale, which includes 3 non-applicable responses with different causes. The rubric also responds to categorical responses for multiple-choice and multiple-answer questions. The web app prepares a report of relevant intervention strategies for the participant, along with the questions and responses as context.The web app is also designed to coordinate international clinical efforts. The web application administrators, led by a superadministrator, can create, manage, and remove centers and staff corresponding to physical (or virtual) locations as demand grows. Each center has a local administrator that can authorize staff to administer the HVFQI, interpret and discuss the results and relevant strategies with participants.The data is stored in a central secure database, accessible only to authorized researchers, and the site administrator. All data are anonymous and non-personally identifying. Authorized researchers can analyze the results for their local centers to estimate efficacy and submit suggestions to update the HVFQI to meet the needs of local participants. Researchers authorized by the super-administrator have full access to the entire database and can perform global analysis for rapid improvement.The HVFQI web app is the result of a collaboration between the SeeLab of the Smith-Kettlewell Eye Research Institute, and the Kulkarni Group of the Computer Science Department at San Francisco State University.",,"Wong M,Ghahghaei S,Chandna A,Kulkarni A",,2021,,10.1145/3459930.3469495,https://doi-org.proxy.bnl.lu/10.1145/3459930.3469495;http://dx.doi.org/10.1145/3459930.3469495,Conference Paper
Q2SWinet '15: Proceedings of the 11th ACM Symposium on QoS and Security for Wireless and Mobile Networks,"It is our great pleasure to welcome you to the 2015 ACM Symposium on QoS and Security for Wireless and Mobile Networks -- Q2SWinet'15. This year's symposium continues its tradition of being the premier forum for exchanging ideas, discussing solutions, and sharing experiences among researchers, professionals, and application developers, both from industry and academia. As with the previous editions of the Q2SWinet symposium series, the scope of this year's symposium will remain on general issues related to QoS and security in wireless and mobile networks networking and computing.The call for papers attracted several submissions from Asia, Canada, Europe, Africa, and the United States. The program committee accepted 25 full papers and 2 short papers that cover a variety of topics, including next generation access control models, engineering and analysis techniques for QoS, and security administration. We hope that these proceedings will serve as a valuable reference for researchers and developers in QoS, security and wireless networking.",,,,2015,,,,Book
Automatic Detection and Compression for Passive Acoustic Monitoring of the African Forest Elephant,"In this work, we consider applying machine learning to the analysis and compression of audio signals in the context of monitoring elephants in sub-Saharan Africa. Earth's biodiversity is increasingly under threat by sources of anthropogenic change (e.g. resource extraction, land use change, and climate change) and surveying animal populations is critical for developing conservation strategies. However, manually monitoring tropical forests or deep oceans is intractable. For species that communicate acoustically, researchers have argued for placing audio recorders in the habitats as a cost-effective and non-invasive method, a strategy known as passive acoustic monitoring (PAM). In collaboration with conservation efforts, we construct a large labeled dataset of passive acoustic recordings of the African Forest Elephant via crowdsourcing, compromising thousands of hours of recordings in the wild. Using state-of-the-art techniques in artificial intelligence we improve upon previously proposed methods for passive acoustic monitoring for classification and segmentation. In real-time detection of elephant calls, network bandwidth quickly becomes a bottleneck and efficient ways to compress the data are needed. Most audio compression schemes are aimed at human listeners and are unsuitable for low-frequency elephant calls. To remedy this, we provide a novel end-to-end differentiable method for compression of audio signals that can be adapted to acoustic monitoring of any species and dramatically improves over näive coding strategies.",,"Bjorck J,Rappazzo BH,Chen D,Bernstein R,Wrege PH,Gomes CP",,2019,,10.1609/aaai.v33i01.3301476,https://doi-org.proxy.bnl.lu/10.1609/aaai.v33i01.3301476;http://dx.doi.org/10.1609/aaai.v33i01.3301476,Conference Paper
Introduction to Global Health IT Strategies and Applications Minitrack,"The global proliferation of information and communication technologies, along with improved mobile computing accessibility, enhanced security and cloud-based data exchanges have germinated interests in those seeking to apply existing and emerging information technologies to address health issues throughout diverse regions of the world. This minitrack will examine similarities and differences in how regions as diverse as North America, Africa, Asia and the Middle-East apply technology for improving health and healthcare systems. The focus will be on emerging trends for applying innovative health IT solutions to improve general population and community health care across the globe, including low-cost, mobile and other emerging health technological applications.",,"Tan J,Dohan MS,Patrick S",,2014,2570,10.1109/HICSS.2014.323,https://doi-org.proxy.bnl.lu/10.1109/HICSS.2014.323;http://dx.doi.org/10.1109/HICSS.2014.323,Conference Paper
Extending Y-STR Loci in Portugal for Forensic and Population Studies: The PowerPlex® Y23 Experience,"The specific properties of the Y chromosome make it highly informative not only for tracing human migration and evolution through male lineages but also for forensic studies. While different kind of Y-polymorphisms were proved to be useful in routine forensic casework, up to now short tandem repeats (STRs) have been the most commonly used due to their high levels of diversity when compared to other polymorphisms. In this study, the recently released PowerPlex® Y23 System (Promega) was evaluated viewing its application in forensic casework in the Portuguese population. The 23 Y-chromosomal STRs included in the kit (DYS576, DYS389I, DYS448, DYS389II, DYS19, DYS391, DYS481, DYS549, DYS533, DYS438, DYS437, DYS570, DYS635, DYS390, DYS439, DYS392, DYS643, DYS393, DYS458, DYS385, DYS456, GATA H4) were typed in 250 samples from unrelated Portuguese males. A total of 236 different haplotypes were found, among which 14 were shared by two individuals. The overall haplotype diversity (HD) was 0.9996. Since the same sample had been previously typed with the AmpFISTR Yfiler™ Amplification Kit (Applied Biosystems), a comparison was undertaken in terms of concordance and HD. Two discrepancies were found between both kits for the loci DYS385 and GATA H4, which were further demonstrated to be caused by a silent allele and a sequence variant, respectively. It was also assessed which new loci in PowerPlex® Y23 contributed more to HD increment in this sample: by fixing the haplotypes generated with the YFiler™ loci and adding the six new PowerPlex® Y23 loci one by one, the markers that increased HD the most were DYS576 and DYS481. In this sample, the same HD was obtained with or without the DYS643 locus. Since a group of samples had been previously typed for Y-SNP markers, a haplogroup predictor software was used to evaluate which set of Y-STR markers is currently more efficient in the prediction of haplogroups. The 17 Y-STR set of Yfiler™ showed to be slightly more accurate than the 23 Y-STR set of PowerPlex® Y23. Population comparisons with several worldwide population data were undertaken through MDS based on FST values, which has illustrated the sharp clustering of three major groups of populations: European, African and Eastern Asian. At the European level, geography was found to account considerably for the pattern of population substructuring, although inconsistencies were not rarely observed in the relationship between geographical and genetic distance. The same analysis based on 17 Y-STR data has led to similar results, but the FST values based on this panel of markers were on average higher than produced with the set of PowerPlex® Y23. This finding can possibly be explained by the high levels of diversity and mutability rates of the 6 additional loci, which consequently, when used to extend the number or markers in Yfiler™ might imply some loss in the ability to capture inter-population diversity.",,Salazar RA,,2013,,,,Masters Thesis
An Integrated Mobile Veld Fire Detection and Sharing Platform for Southern Africa,"While there are clear efforts towards managing veld fires, it comes as a concern that in Southern Africa, the role of local communities in fire control has weakened and veld fires have grown to be a major threat. Current systems and technologies to share veld fire information have several challenges. These include; being unable to detect burning fires in the forests, poor to almost missing veld fire local alerting systems, and malfunctioning local veld firefighting communities. Against this background, a mobile veld fire detection and sharing application prototype was developed using a qualitative data approach and experimental design. Weather data and scientific models of different areas were used to create fire-danger indices based on forecasted weather data and weather station information on the ground. These were programmed into the system to trigger alerts for the veld fire prediction component. For the identification of already burning fires, this was linked to the MODIS system of firefighting stakeholders (EMA Zimbabwe). Results revealed that conditions that promote veld fires can be predicted and local residents can thus be warned instantly to avoid activities that cause fires. For already burning fires, the mobile application was able to instantly communicate to users registered to the system.",,"Jere NR,Scott MS,Taruvinga A",,2017,,10.1145/3129416.3129439,https://doi-org.proxy.bnl.lu/10.1145/3129416.3129439;http://dx.doi.org/10.1145/3129416.3129439,Conference Paper
ISLPED '14: Proceedings of the 2014 International Symposium on Low Power Electronics and Design,"It is our great pleasure to welcome you to the 2014 ACM/IEEE International Symposium on Low Power Electronics and Design -- ISLPED'14, in the beautiful city of La Jolla, CA, USA. The mission of our symposium is to provide education and technical enrichment for professionals in the area of low power electronics and design and promote advancement of the state-of-the-art in the same area. It provides a forum for technical discussions and a platform for examining new ideas and research topics.This year, the call for papers attracted 184 submissions from Asia, Africa, Europe, and North & South America. The Technical Program Committee (TPC), consisting of 78 experts from industry and academia, accepted a total of 63 papers divided into 43 for full-length presentations and 20 for poster presentations. The accepted papers cover a variety of low-power topics in technologies, circuits, logic & architecture, CAD Tools & methodologies, systems & platforms, and software and applications. We are very thankful to the authors for contributing to ISLEPD'14. We are also grateful to our TPC members for volunteering their valuable time and effort in reviewing the papers, attending the in-person review meeting, and providing feedback to the authors.In addition to the above accepted papers, this year's program features: Three Keynote Speeches on ""Low Power Design Techniques in Mobile Processes"" by Dr. Karim Arabi, VP Engineering, Qualcomm Technologies, Inc.; ""Accelerator-Rich Architectures --- From Single-chip to Datacenters"" by Prof. Jason Cong, Chancellor's Professor at the Computer Science Dept., UCLA, and ""The New (System) Balance of Power and Opportunities for Optimizations"" by Dr. Partha Ranganathan, Principal Engineer, Google.Industry Focus Session on the Challenges of Low Power Analog Circuits, Exploiting FD-SOI for Energy Efficient SoCs, and Using Embedded STT-MRAM for Mobile Applications.Four Embedded Invited Papers on Emerging Interconnect Technologies, Low Power Processor Design, Leakage Mitigation in Smartphone SoCs, and Powering the Internet of Things.Embedded Tutorial by industry DA experts on ""Failing to Fail - Achieving Success in Advanced Low Power Design using UPF"".We hope the above talks will complement our main program by providing you with an in-depth understanding of the low-power state-of-the-art as well as gives you valuable insights into future trends. Finally, we hope that you will find the overall program interesting and thought-provoking and that the symposium will provide you with a valuable opportunity to share ideas with other researchers and practitioners from institutions around the world.",,,,2014,,,,Book
EC '08: Proceedings of the 9th ACM Conference on Electronic Commerce,"These proceedings present the technical contributions to the Ninth ACM Conference on Electronic Commerce EC'08, held July 8-12, 2008 in Chicago, Illinois, USA. Since its inception in 1999, ACM EC has served as the leading scientific conference on advances in theory, systems, and applications for electronic commerce. The natural focus of the conference is on computer science issues, but the conference is interdisciplinary and addresses many facets of electronic commerce including (1) theory and foundations; (2) languages; (3) automation, personalization, and targeting; (4) security, privacy, encryption, and digital rights; (5) applications and empirical studies; and (6) social factors. In addition to the main technical program, EC'08 featured two workshops on Ad Auctions and Prediction Markets and four tutorials on Computational Advertising, Economic Aspects of Social Networks, Communication Requirements of Economic Mechanisms and Automated Mechanism Design.The call for papers attracted submissions from academia and industry around the world, including Africa, Asia, Canada, Europe, the Middle East, and the United States. Each paper was reviewed by at least three program committee members on the basis of scientific novelty, technical quality, and importance to the field. The program committee selected 38 papers for presentation at the conference out of 198 submissions, most of which are published in the proceedings. At the authors' request, only abstracts for the five remaining papers are included along with pointers to full versions of these ""working papers"". This accommodates the practices of fields outside of computer science in which journal rather than conference publishing is the norm and conference publishing sometimes precludes journal publishing. It is expected that many of the papers in these proceedings will appear in a more polished and complete form in scientific journals in the future.After the review process the chairs selected six papers that had some of the strongest reviews of the approximately 200 papers submitted to the conference, as candidates for Outstanding Papers. An Outstanding Papers committee of Barry Smyth, Subhash Suri, and Moshe Tennenholtz was established to choose among these six finalists. The committee eventually chose two papers as the Outstanding Papers of EC'08. Those papers are: Self-Financed Wagering Mechanisms for Forecasting; Nicolas Lambert, John Langford, Jennifer Wortman, Yiling Chen, Daniel Reeves, Yoav Shoham, David PennockUncoordinated Two-Sided Markets; Heiner Ackermann, Paul Goldberg, Vahab Mirrokni, Heiko Roeglin, Berthold VoeckingThe Chairs thank the Outstanding Papers Committee for their distinguished advice, and offer congratulations to the authors for their excellent papers.",,,,2008,,,,Book
A Pilot Study of the Challenges Associated with ELearning Developments in Saudi Universities,"The ongoing developments in Information and Communication Technologies (ICTs) lead IT professionals of the academic environments worldwide to adjust the eLearning Management Systems of their universities' domains in this reality by adopting the new ideas and recommendations. As a direct consequence, the influence on teaching and learning environments is more than emphatic and the challenges revealing the all the more increasing need in utilizing modern learning applications, procedures, and policies more apparent than ever before. Nevertheless, their institutions remain teaching organizations with their core processes focused on the need for education and training of their student bodies often increasing in size, especially in the emerging economies and developing countries. The Middle East and especially the Gulf Council Countries' (GCC) higher education systems are no exception. Saudi Arabia in particular can be considered a special case in the GCC due to its numerous and rather crowded higher education institutions. In this research, a number of diverse types of administrative, technical, and general challenges and issues related to eLearning are covered in order to examine the current situation of eLearning progress in Saudi universities, investigate the obstacles preventing high rates of eLearning development, and discover what kind of learning procedures people of Saudi prefer to accommodate their educational preferences. A pre-tested questionnaire was used for the purpose of data collection. The data were gathered from students of these educational institutions in Saudi Arabia and from other individuals from all walks of life and of various employment statuses. This pilot research study suggested that the main reason behind the slow progress of eLearning in Saudi Arabia is the result of problems in the local telecommunications and other infrastructure, as noted by the survey participants, and far less the outcome of weaknesses of the established procedures and facilities available from the local eLearning institutions.",,"Xanthidis D,Nikolaidis P",,2014,63–79,10.4018/ijtd.2014100105,https://doi-org.proxy.bnl.lu/10.4018/ijtd.2014100105;http://dx.doi.org/10.4018/ijtd.2014100105,Journal Article
"An Investigation into the Adoption of Electronic Commerce by South African Consumers, from a Social, Technological and Business Perspective","The Internet is a phenomenon of the modern world, conceived during the last few years of the second millennium. As a global, digital communication tool that links together as many as 350 million people worldwide, it has enormous commercial possibilities (Jones: 1998). It is a new and dynamic medium that poses special challenges for business, marketers and consumers. Many predictions see this burgeoning electronic marketplace becoming a significant component of the world economy (Birch: 1994). E-commerce is nothing but the commercial application of the Internet and may have a huge impact on the way business will be conducted in future. Consumers already use the Internet extensively to shop and bank online. In most first world countries, more than 50% of the online population uses the Internet to purchase goods and services. This is also the case in South Africa, although only 5% of the population currently has Internet access. Those that buy online spend approximately ZAR3900 per annum online. The reasons most Internet users gave that have not purchased online yet are: concerns about security, no access to credit cards and not familiar with the technology. Those that buy online find it convenient, easy and enjoy the variety offered online. The adoption of e-commerce has been found to be below expectation in South Africa. The above reasons/factors that impede the rate-of-adoption of e-commerce by consumers were determined by an original, positivistic research project conducted in November 2000. The results of the study concur with the view of many experts that the adoption of technology by consumers is a complex process (Rogers: 1983). Taking a philosophical view on the role of technology in society, it becomes clear that new technology is not an unparallel blessing for humankind. It often has negative or unpredicted consequences for society. One view of its impact on society is to expect flaws, realise its limitations and enjoy its benefits—we should have what Florman (1981) calls a tragic view of technology, i.e. accept responsibility with the good and the bad it brings.",,"Van Der Merwe JP,Villiers C,Plooy ND",,2002,,,,Ph.D. Thesis
Q2SWinet '14: Proceedings of the 10th ACM Symposium on QoS and Security for Wireless and Mobile Networks,"It is our great pleasure to welcome you to the 2014 ACM Symposium on QoS and Security for Wireless and Mobile Networks -- Q2SWinet'14. This year's symposium continues its tradition of being the premier forum for exchanging ideas, discussing solutions, and sharing experiences among researchers, professionals, and application developers, both from industry and academia. As with the previous editions of the Q2SWinet symposium series, the scope of this year's symposium will remain on general issues related to QoS and security in wireless and mobile networks networking and computing.The call for papers attracted 30 submissions from Asia, Canada, Europe, Africa, and the United States. The program committee accepted 14 full papers and 3 short papers that cover a variety of topics, including next generation access control models, engineering and analysis techniques for QoS, and security administration. In addition, during the symposium days attendees will have the possibility to attend the keynote speech by Dr. Ian F. Akyildiz on Wireless Sensor Networks in Challenged Environments such as Underwater and Underground. We hope that these proceedings will serve as a valuable reference for researchers and developers in QoS, security and wireless networking.",,,,2014,,,,Book
Flipping 419 Cybercrime Scams: Targeting the Weak and the Vulnerable,"Most of cyberscam-related studies focus on threats perpetrated against the Western society, with a particular attention to the USA and Europe. Regrettably, no research has been done on scams targeting African countries, especially Nigeria, where the notorious and (in)famous 419 advanced-fee scam, targeted towards other countries, originated. How- ever, as we know, cybercrime is a global problem affecting all parties. In this study, we investigate a form of advance fee fraud scam unique to Nigeria and targeted at Nigerians, but unknown to the Western world. For the study, we rely substantially on almost two years worth of data harvested from an on-line discussion forum used by criminals. We complement this dataset with recent data from three other active forums to consolidate and generalize the research. We apply machine learning to the data to understand the criminals' modus operandi. We show that the criminals exploit the socio-political and economic problems prevalent in the country to craft various fraud schemes to defraud vulnerable groups such as secondary school students and unemployed graduates. The result of our research can help potential victims and policy makers to develop measures to counter the activities of these criminal groups.",,"Mba G,Onaolapo J,Stringhini G,Cavallaro L",,2017,1301–1310,10.1145/3041021.3053892,https://doi-org.proxy.bnl.lu/10.1145/3041021.3053892;http://dx.doi.org/10.1145/3041021.3053892,Conference Paper
Building Virtual Pentesting Labs for Advanced Penetration Testing - Second Edition,"Key Features Learn a systematic process for professional security and penetration testing Explore and build intricate architectures that allow you to emulate an enterprise network Examine and perform research to identify the latest vulnerabilities and, build a lab and test them! Learn methods to bypass common enterprise defenses and leverage them to test the most secure environments. Book Description In this book you will be introduced to a proven professional security and penetration testing methodology that has trained thousands of professional testers. Your experience from reading this book will prepare you for participation in professional security testing teams, both as a red team and a blue team member. Within the book you will learn how to take advantage of the power of virtualisation to build a multi-layer enterprise architecture and then deploy targets to test inside it. Additionally, you will learn a systematic process for discovering vulnerabilities and then a way to test these on your own private network. By practising the techniques throughout the book, you will be able to hone and enhance your skills in professional security and penetration testing. Building Virtual Pentesting Labs for Advanced Penetration Testing will teach you the process of how to build your own labs and a proven process to test these labs that is currently used in Industry by global penetration testing teams. You will start with an introduction to professional security testing and deciding where pen testing fits; then you will be introduced to proven leading Industry testing methodologies. Once the introduction has completed, you will start building the machines; once you have built them you will learn how to build and test layered architectures. After you have mastered the layers you will plan specific attacks based on the platforms you are going up against. The book will show you a process for discovering new vulnerabilities for systems and networks, and how to apply these to your developed range and discover what the vulnerability means to your potential clients. Building Virtual Pentesting Labs for Advanced Penetration Testing uses extensive labs and illustrations to take you from the beginning (building and attacking an enterprise architecture) to methods to bypass and avoid common enterprise architecture defences. What you will learn Proven security testing and penetration testing techniques How to build multi-layered complex architectures to test the latest network designs Applying a professional testing methodology Determining whether there are filters between you and the target and how to penetrate them How to deploy and then find weaknesses in common firewall architectures. Advanced techniques to deploy against hardened environments Methods to circumvent endpoint protection controls About the Author Kevin Cardwell currently works as a freelance consultant and provides consulting services for companies throughout the world. He developed the Strategy and Training Development Plan for the first Government CERT in the country of Oman and developed the team to man the first Commercial Security Operations Center there. He has worked extensively with banks and financial institutions throughout Middle East, Africa, Europe, and the UK. He currently provides consultancy to Commercial companies, governments, major banks, and financial institutions across the globe. He is author of Backtrack: Testing Wireless Network Security, Building Virtual Pen Testing Lab for Advanced Penetration Testing First Edition, and Advanced Penetration Testing of Highly Secured Environments 2nd Edition.",,Cardwell K,,2016,,,,Book
Q2SWinet'18: Proceedings of the 14th ACM International Symposium on QoS and Security for Wireless and Mobile Networks,"It is our great pleasure to welcome you to the 2018 ACM International Symposium on QoS and Security for Wireless and Mobile Networks (ACM Q2SWinet 2018), to be held at Montreal (Canada). As with the previous seven editions of the Q2SWinet symposium series, this year's symposium continues its tradition being a meeting point and a forum for exchanging ideas, discussing solutions, and sharing experiences among researchers, professionals, and application developers, both from industry and academia on general issues related to QoS and security in wireless and mobile networking and computing.The call for paper attracted submissions from North-America (USA and Canada), South-America (Brazil, etc.), Europe (France, Portugal, etc.), Asia (China, Singapore, etc.) and Africa, which demonstrates the World-Wide range of the symposium. The program committee reviewed all papers and accepted 18 technical papers for presentation, which represents an acceptance ratio of 29%.",,,,2018,,,,Book
"Testing Foundational Tenets of Stable Isotope Ecology Analyses in Neotropical Mammalian Communities, and Implications for Terrestrial Paleoecology","Stable isotope analyses are powerful tools for reconstructing ancient ecologies and ecosystems, as they are independent of morphology and directly reflect dietary ecology. The application of stable isotope analyses, however, is not without limitations, as determination of food web dynamics using these methods often relies on poorly tested assumptions. The guiding thread of this thesis is the testing of foundational cornerstones on which these methods rely, in order to validate the suitability of applying these techniques to different mammalian clades, and to more reliably and confidently interpret the isotopic signals preserved in extinct organisms.The first chapter of this thesis tests the validity of an important assumption behind the interpretation of stable carbon isotope analyses for understanding diet in terrestrial mammalian herbivores: if, as assumed for almost two decades, mammalian bioapatite δ13C is enriched by 14‰ relative to dietary δ13C. By analyzing new isotopic data from a never before assessed herbivorous group spanning a broad range of body masses—sloths (Xenarthra, Mammalia)— and other mammals with experimentally controlled or observationally known diets, I discovered considerable variation in diet–bioapatite δ13C enrichment among mammals. Statistical tests (ordinary least squares, quantile, robust regressions, Akaike information criterion model tests) documented independence from phylogeny, and a previously unrecognized strong and significant correlation of δ13C enrichment with body mass for all mammalian herbivores. A single-factor body mass model outperformed all other single-factor or more complex combinatorial models evaluated, including for physiological variables (metabolic rate and body temperature proxies), and indicated that body mass alone predicts δ13C enrichment. These analyses, spanning more than 5 orders of magnitude of body sizes, yield a size-dependent prediction of isotopic enrichment across Mammalia and for distinct digestive physiologies, permitting reconstruction of foregut versus hindgut fermentation physiologies for fossils and refined mean annual paleoprecipitation estimates based on δ13C of mammalian bioapatite.Second, I sought to evaluate the existing paradigm governing identification of closed canopy rainforests in the fossil record using mammalian δ13C data: the presence of mammals with dietary δ13C <-31‰, which has only been observed in closed canopy rainforests in Equatorial Africa, the only other tropical ecosystem sampled extensively. This chapter provides a characterization of δ13Cbioapatite, δ13Chair and δ15Nhair of a modern mammalian community in western Amazonia, in Peru, to test if the isotopic structure of mammals in this Neotropical ecosystem is similar to those in African tropical rainforests. The results indicate that despite their marked geographical and taxonomic differences, median δ13Cdiet values from closed canopy rainforests in Amazonia (-27.4‰) and equatorial Africa (-26.9‰) are not significantly different. Amazonian mammals, however, seem to exploit a narrower spectrum of dietary resources than equatorial African mammals, as depicted by the absence of highly negative δ13Cdiet values previously proposed as indicative of rainforests (<-31‰). I hypothesize that differential effects of late Pleistocene extinction may be responsible for the ecological disparities among the two rainforests, by significantly reducing evolutionary time and dietary breadth reflected in the modern Amazonian mammalian community.Finally, the third chapter of this dissertation evaluates assumptions behind δ15N amino acid compound specific analyses in order to test the controversial hypothesis of carnivory and consumption of proteins of animal origin in fossil sloths. This analytical technique relies on three main assumptions. First, that the offset between the δ15N of glutamic acid (δ15NGlx) and phenylalanine (δ15NPhe) in the organism under study will increase with increasing trophic level. Second, that the offset between δ15NGlx and δ15NPhe at the base of the food chain is relatively constant and has a value of -8.4‰ for C3 ecosystems. Third, that the trophic discrimination factor in all ecosystems (the difference in δ15NGlx relative to δ15NPhe with increasing trophic level) is 7.6‰. The results of my experiments conducted on extant xenarthrans (sloths and anteaters) with controlled diets document that only the first assumption holds true. Rather than relying on an equation with constants introducing uncertainties and that are not applicable to organisms feeding on a combination of items of different origin (e.g., C3 + C4 plants), δ15NGlx and δ15NPhe values by themselves can accurately reconstruct the trophic position of organisms. Indeed, the results on δ15NGlx and δ15NPhe herein obtained for five xenarthran species in controlled feeding experiments, combined with mammalian data available from the literature, show strong and significant correlations between these two AAs and with trophic positions. Both the TP equation and the regression analyses of δ15NGlx and δ15NPhe suggest that the Pleistocene fossil ground sloths Mylodon darwinii and Nothrotheriops shastensis were not pure herbivores as commonly presumed, but rather that they were both mixed feeders/omnivores, incorporating items of animal origin in their diets.",,Lara JV,,2020,,,,Ph.D. Thesis
An Empirical Investigation on Acceptance of Mobile Payment System Services in Jordan: Extending UTAUT2 Model with Security and Privacy,"Several developed and developing countries have launched a mobile payment system service, which is known in Jordan as Jordan Mobile Payment (JoMoPay) system to overcome the drawbacks of traditional payment system. The system supports payment transactions by utilising mobile phones applications. However, the acceptance of JoMoPay system in Jordan is still below the level of expectation. This study was undertaken to understand and explain the acceptance of JoMoPay system based on extending the unified theory of acceptance and use of technology (UTAUT2) model in the Jordanian context. The model was extended by considering two additional constructs namely; security and privacy. Utilising a self-reported survey, collected data was analysed using structural equation modelling (SEM) to test the research model. Five constructs were found to be the determinants of behavioural intention to use JoMoPay system, namely performance expectancy, social influence, price value, security and privacy. Together they account for 61.4% of the variance in behavioural intention. However, effort expectancy, facilitating condition and hedonic motivation did not have a significant impact on behavioural intention to use JoMoPay system and hence the related hypotheses were not supported. Lastly, conclusions, limitations and future research directions will be discussed further in the last section of the paper.",,"Al-Okaily M,Rahman MS,Ali A,Abu-Shanab E,Masa'deh R",,2023,123–152,10.1504/ijbis.2023.128306,https://doi-org.proxy.bnl.lu/10.1504/ijbis.2023.128306;http://dx.doi.org/10.1504/ijbis.2023.128306,Journal Article
The Tao of Open Source Intelligence,"The Internet has become the defining medium for information exchange in the modern world, and the unprecedented success of new web publishing platforms such as those associated with social media has confirmed its dominance as the main information exchange platform for the foreseeable future. But how do you conduct an online investigation when so much of the Internet isn't even indexed by search engines? Accessing and using the information that's freely available online is about more than just relying on the first page of Google results. Open source intelligence (OSINT) is intelligence gathered from publically available sources and is the key to unlocking this domain for the purposes of investigation. Product overview The Tao of Open Source Intelligence provides a comprehensive guide to OSINT techniques, for the investigator: It catalogues and explains the tools and investigative approaches that are required when conducting research within the surface, deep and dark webs. It explains how to scrutinise criminal activity without compromising your anonymity - and your investigation. It examines the relevance of cyber geography and how to get around its limitations. It describes useful add-ons for common search engines, as well as considering metasearch engines (including Dogpile, Zuula, PolyMeta, iSeek, Cluuz and Carrot2) that collate search data from single-source intelligence platforms such as Google. It considers deep-web social media platforms and platform-specific search tools, detailing such concepts as concept mapping, entity extraction tools and specialist search syntax (Google kung fu). It gives comprehensive guidance on Internet security for the smart investigator, and how to strike a balance between security, ease of use and functionality, giving tips on counterintelligence, safe practices and debunking myths about online privacy. OSINT is a rapidly evolving approach to intelligence collection, and its wide application makes it a useful methodology for numerous practices, including within the criminal investigation community. The Tao of Open Source Intelligence is your guide to the cutting edge of this information collection capability. About the author Stewart K. Bertram is a career intelligence analyst who has spent over a decade working across the fields of counterterrorism, cyber security , corporate investigations and geopolitical analysis. The holder of a master's degree in computing and a master of letters in terrorism studies, Stewart is uniquely placed at the cutting edge of intelligence and investigation, where technology and established tradecraft combine. Stewart fuses his academic knowledge with significant professional experience, having used open source intelligence on such diverse real-world topics as the terrorist use of social media in Sub-Saharan Africa and threat assessment at the London Olympic Games. Stewart teaches courses on open source intelligence as well as practising what he preaches in his role as a cyber threat intelligence manager for some of the world's leading private-sector intelligence and security agencies.",,,,2015,,,,Book
Personalized Persuasion to Promote Positive Work Attitudes in Public Workplaces,"This paper investigates how mobile persuasive system targeting African audience could be designed and tailored to promote employee's commitment to the ideals, visions, and mission of an organization. We conduct a qualitative study with two categories of workers to uncover core factors that influence employee's attitudes to their jobs and map our findings to their matching social influence persuasive techniques. We propose that a persuasive system (PS) employing the social influence strategies could motivate workers towards acceptable positive pro-workplace behaviors and etiquette. The PS allows workers too compare their behaviors against set goals and acceptable standards, compete and compare performances with peers, view and respond to peers' activities, and receive recognition for accomplishing a target task. The system ensures the security of worker's data via the authentication of login credentials while showing them a personalized persuasive display of essential workplace information. We present a prototype persuasive system called ""PAULApp"" for motivating pro-workplace behaviors and plans for evaluation. PAULApp was designed using the iterative design process and was informed by the findings from the user studies.",,"Nkwo M,Orji R",,2019,185–190,10.1145/3314183.3323858,https://doi-org.proxy.bnl.lu/10.1145/3314183.3323858;http://dx.doi.org/10.1145/3314183.3323858,Conference Paper
IH&MMSec '20: Proceedings of the 2020 ACM Workshop on Information Hiding and Multimedia Security,"It is our great pleasure to welcome you to the 8th ACM Workshop on Information Hiding and Multimedia Security (IH&MMSEC 2020). This year's workshop continues its tradition of being the premier events for presentation of research results and experience reports on multimedia security and attracts researchers from all over the world.The workshop focuses on information hiding topics, such as digital watermarking, steganography, steganalysis, anonymity, hard-to-intercept communications, and covert/subliminal channels. It also covers a variety of multimedia security topics including multimedia identification and authentication, signal forensics, and biometrics.The mission of the workshop is to share novel solutions that fulfill the needs of heterogeneous security applications and identify new directions for future research and development. IH&MMSEC gives researchers and practitioners a unique opportunity to share their perspectives with others interested in the various aspects of multimedia security.The call for papers attracted submissions from Asia, Canada, Australia, Europe, Africa, and the United States. We are proud to announce that this year both the quality and number of submissions was extremely high.",,,,2020,,,,Book
The Gulf Cooperation Council Monarchies After 2011 : Redefining Security Perceptions and Rethinking Threat Analysis,"Since 2011, a series of events – including popular upheavals, civil wars, the empowerment of non-state actors, economic volatility and increased geopolitical confrontation between states – hinted at the beginning of a transformative period for the Middle East and North Africa (MENA) region. Notwithstanding the region's modern history has provided several instances of treacherous conjunctures, seldom like in the aftermath of 2011 so many different challenges of different types have risen simultaneously on a regional and domestic scale. As this transformative wave spread towards the countries of the Gulf Cooperation Council (GCC), challenging the existing balance of power, the local regimes' security perceptions were profoundly impacted. Arguably, these became so substantially divergent at the level of each state that, in the span of only six years, the GCC was hit by two of the gravest internal political crises in its history. Hence, new questions emerged regarding the existence of a shared prioritization of threats and the interaction of endogenous and exogenous dangers when they materialize simultaneously, that don't seem to find answers in the existing body of scholarship yet. Focusing on the post-2011 environment and the issues emerged as crucial amid the 2014 and 2017 intra-GCC crises, this thesis aims to provide new analytical tools for addressing such questions and enhancing the understanding of evolving security perceptions. In order to do so, drawing from the literatures of security studies and area studies, an original theoretical framework is elaborated, which introduces a distinction between threats and risks and a categorization system addressing the emergence of multidimensional, 'intermestic' threats. The framework is subsequently applied to perform an analysis of threat perceptions in each of the six GCC states. Finally, the author will attempt to draw conclusions on threat prioritization in the region and the status of the much-debated notion of 'Gulf security'.",,Bianco C,,2020,,,,Ph.D. Thesis
Administrators' Benevolent and Corrective Humor and Suspensions of African American School Age Females,"African American female students' suspension rates are rising and outpacing those of all categories of students, including African American males. This is due, in part, to an administrator's over-reliance on zero-tolerance disciplinary policies, as well as a teacher's negative perception of African American female students. Soaring rates of suspension contribute to school disengagement, high dropout rates, and a formidable risk of involvement in the school to prison pipeline. In addition to the toll these risks take on human lives, society loses the benefit of human productivity, taxable wages, and stable, thriving communities. As attention to the rising rate of suspension of African American females increases, there are several suggested interventions to reverse the trend. Some researchers offer strategies like Positive Behavior Interventions (PBI) that modify students' behavior. Others recommend culturally sensitive, professional learning for teachers. Administrators are encouraged to replace zero-tolerance policies with restorative justice practices. There is, however, a paucity of research that offers humor, specifically, benevolent and corrective humor, as a tool that administrators can apply, instead of handling office referrals with harsh, disciplinary decrees when they exercise their authority to suspend a student.The purpose of this study was to examine whether a principal's benevolent and corrective humor score was predictive of the suspension rate of African American female students. Guided by established research in transformational leadership, school suspensions, and Black Girlhood Studies, a newly created, self-administered, Benevolent, and Corrective Humor Scale was distributed to a nationwide population of educational leaders. Benevolent humor is compassionate and for the benefit of the individual. Corrective humor is a moral-based mockery with a sympathetic heart. The blend of these evidence-based approaches was used to address the complex and multi-layered lives of African American females and their heightened risk for suspension. Results indicate a weak, positive relationship between corrective humor question 2 and the percentage of African American female students r (.149), p = .040. Recommendations for educational leaders and future research are provided.",,"Bacon RE,Gnanadass E,Platt R",,2020,,,,Ph.D. Thesis
Fundamental Computing Forensics for Africa: A Case Study of the Science in Nigeria,"This book presents a general introduction to the computational aspects of forensic science, covering the different tools needed for forensic investigations, the importance of forensics and biometrics, and the use of Benfords law for biometrics and network traffic analysis. It specifically focuses on the application of these techniques in Africa, and how they can be of benefit in the investigation of crime in Nigeria in particular.",,Iorliam A,,2018,,,,Book
Shared Value in Sustainable Development Goals: The Case of Arla,"This study analyzes and discusses sustainability reporting at Arlacompany which is one of the largest organic dairy producer in the world. Geographically its operations span across Europe, North America, North and West Africa and Asia. This study evaluates the company's communication of its shared value creation and how it contributes towards the accomplishment of the Sustainable Development Goals. In order to do so, it analyzes two of the company's annual Corporate Social Responsibility reports (2016 and 2019). A theoretical framework, combining Corporate Social Responsibility, Creating Shared Value, Triple Bottom Line and Legitimacy Theory is synthesized and applied to the subject matter. This research is a holistic single-case study, employing continuous literature review and a qualitative content analysis. The results of the research are presented discussing the merits and shortcomings of the company's sustainability communication practices. It finds that Arla'ssustainability communication is heavily based on societal and environmental topics. Its main strategies of shared value creation operationalize the expansion and strengthening of markets and product quality assurance. The study also concludes that Arlaavoids the communication of sensitive issues such as animal welfare or greenhouse gas emissions emerging from farms. Finally, the study advocates for the benefits of failure reporting, both in terms of compliance with the Sustainable Development Goals and sustainability practices, arguing that this might lead to greater legitimacy and possible support from other stakeholders.",,Lizikeviciute-Grisine J,,2020,,,,Ph.D. Thesis
EC '09: Proceedings of the 10th ACM Conference on Electronic Commerce,"These proceedings present the technical contributions to the Tenth ACM Conference on Electronic Commerce EC'09, held during July 6-10, 2009 in Stanford, California, USA. Since its inception in 1999, ACM EC has served as the leading scientific conference on advances in theory, systems, and applications for electronic commerce. The natural focus of the conference is on computer science issues, but the conference is interdisciplinary and addresses many facets of electronic commerce including (1) theory and foundations; (2) languages; (3) automation, personalization, and targeting; (4) security, privacy, encryption, and digital rights; (5) applications and empirical studies; and (6) social and human factors. In addition to the main technical program, EC'09 featured two workshops on Ad Auctions and The Economics of Networks, Systems, and Computation and four tutorials on Convergence of Nash Dynamics: Equilibria and Nearly-Optimal Solutions, A Computational Perspective on Game-Theoretic Solution Concepts, Information Exchange, Bidding Languages and Competition in Sponsored Search, and Mechanism Design in Dynamic Settings.The call for papers attracted 161 submissions from academia and industry around the world, including Africa, Asia, Canada, Europe, the Middle East, and the United States. Each paper was reviewed on average by a total of 6 reviewers, 4 from the program and 2 from the senior program committees respectively, on the basis of scientific novelty, technical quality, and importance to the field. The program committee selected 40 papers for presentation at the conference and most of them are published in the proceedings. At the authors' request, only abstracts for the three remaining papers are included along with pointers to full versions of these ""working papers"". This accommodates the practices of fields outside of computer science in which journal rather than conference publishing is the norm and conference publishing sometimes precludes journal publishing. It is expected that many of the papers in these proceedings will appear in a more polished and complete form in scientific journals in the future.The committee has singled out two papers as co-winners of the outstanding paper award.• Eliciting Truthful Answers to Multiple-Choice Questions, by Nicolas Lambert and Yoav Shoham, Stanford University• An Optimal Lower Bound for Anonymous Scheduling Mechanisms, by Itai Ashlagi, Harvard University, Shahar Dobzinski, Hebrew University and Ron Lavi, TechnionThe first paper was also named as the winner of the best student paper award. EC '09 also hosted two invited keynote addresses.• Susan Athey, Harvard University and Microsoft (joint with TARK XII)• Michael Moritz, Sequoia CapitalEC'09 introduced for the first time in this conference series the two-tier reviewing processing involving 13 senior program committee members and 90 program committee members. We sincerely thank all of them for their hard work in ensuring the quality and fairness of the paper reviewing and final program selection process. We'd like to also thank the authors for their submissions and participation in the feedback.",,,,2009,,,,Book
The Cloud Computing Adoption in Higher Learning Institutions in Kenya: Hindering Factors and Recommendations for the Way Forward,,,"Njenga K,Garg L,Bhardwaj AK,Prakash V,Bawa S",,2019,225–246,10.1016/j.tele.2018.10.007,https://doi-org.proxy.bnl.lu/10.1016/j.tele.2018.10.007;http://dx.doi.org/10.1016/j.tele.2018.10.007,Journal Article
Dynamics of the Relationship between NDVI and SWIR32 Vegetation Indices in Southern Africa: Implications for Retrieval of Fractional Cover from MODIS Data,"Fractional cover of photosynthetic vegetation FPV, non-photosynthetic vegetation FNPV, and bare soil FBS has been retrieved for Australian tropical savannah based on linear unmixing of the two-dimensional response envelope of the normalized difference vegetation index NDVI and short wave infrared ratio SWIR32 vegetation indices VI derived from Moderate Resolution Imaging Spectroradiometer MODIS reflectance data. The approach assumes that cover fractions are made up of a simple mixture of green leaves, senescent leaves, and bare soil. In this study, we examine retrieval of fractional cover using this approach for a study area in southern Africa with a more complex vegetation structure. Region-specific end-members were defined using Hyperion images from different locations and times of the season. These end-members were applied to a 10-year time series of MODIS-derived NDVI and SWIR32 from 2002 to 2011 to unmix FPV, FNPV, and FBS. Results of validation with classified high-resolution imagery indicated major bias in estimation of FNPV and FBS, with regression coefficients for predicted versus observed data substantially less than 1.0 and relatively large intercept values. Examination with Hyperion images of the inverse relationship between the MODIS-equivalent SWIR32 index and the Hyperion-derived cellulose absorption index CAI to which it nominally approximates revealed: 1 non-compliant positive regression coefficients for certain vegetation types; and 2 shifts in slope and intercept of compliant regression curves related to day of year and geographical location. The results suggest that the NDVI–SWIR32 response cannot be used to approximate the NDVI–CAI response in complex savannah systems like southern Africa that cannot be described as simple mixtures of green leaves, dry herbaceous material high in cellulose, and bare soil. Methods that use a complete set of multispectral channels at higher spatial resolution may be needed for accurate retrieval of fractional cover in Africa.",,"Hill MJ,Zhou Q,Sun Q,Schaaf CB,Southworth J,Mishra NB,Gibbes C,Bunting E,Christiansen TB,Crews KA",,2016,1476–1503,10.1080/01431161.2016.1154225,https://doi-org.proxy.bnl.lu/10.1080/01431161.2016.1154225;http://dx.doi.org/10.1080/01431161.2016.1154225,Journal Article
The Network Analysis for AgriTech and FoodTech Start-up and Support Organisations: Twitter Analytic Perspective in Thailand,"Over the last decade, entrepreneurial ecosystem research has grown popular among researchers, policymakers, and practitioners. There have been many research attempts to investigate the gap in the entrepreneurial ecosystem between academics and practice. However, there still a lacks of research that provides an understanding of the interconnectedness and networking among entrepreneurial actors or supporting organisations, particularly in developing countries, for example, Thailand. In order to provide valuable opportunities to enhance and understand start-ups and support organisations networking in the entrepreneurial ecosystem, this paper applied social media analysis to demonstrate how social media data can be used to evaluate networks and explore interconnectedness. The research also helps to understand the relationship between AgriTech, FoodTech start-ups and support organisations in the entrepreneurial ecosystem.Using Twints and the Twitter API, data were retrieved from Twitter's databases for start-ups, incubators or accelerators, and other support organisation information within the entrepreneurial ecosystem. The results indicate a lack of interconnections among AgriTech and FoodTech startups and support organisations in Thailand's ecosystem, such as incubators and accelerators. According to an analysis of social media, most start-up aid organisations in Thailand originated from local or national organisations, not global ones. These represent the fragility of Thai start-ups’ support organisations which lack a connection with international opportunities.This research utilised social media analysis as an alternate evaluation approach for assessing the connection or network activity among local entrepreneurial actors. Furthermore, the evaluation findings can be used to identify the weak points in an ecosystem and suggest policies for growing and enhancing a start-up entrepreneurial ecosystem in developing nations such as Thailand.",,"Pintoo E,Guo Y,Bal J",,2022,547–555,10.1145/3556089.3556145,https://doi-org.proxy.bnl.lu/10.1145/3556089.3556145;http://dx.doi.org/10.1145/3556089.3556145,Conference Paper
Simulating DDoS Attacks on the US Fiber-Optics Internet Infrastructure,"Network-based attacks like the distributed denial-of-service (DDoS) attacks are not new, but we are beginning to see attacks of unprecedented scale. Examples of such attacks include the 2016 attack on DYN INC that crippled a part of the Internet for hours, and the attack on Liberia, which partially brought down the African nation. Limitations in identifying vulnerable Internet infrastructure and testing possible defense strategies are a part of the problem. We need a simulation testbed that can reflect the complexity of the Internet, yet allows to swiftly test attacks, providing insights that can apply to real-world attack scenarios. In this research, we have designed a test-bed that mirrors the Internet infrastructure of the US and can simulate the Internet traffic flow patterns for different attack targets. We also estimate the degradation in the quality-of-service and the number of users impacted in two attack scenarios.",,"Kumar S,Carley KM",,2017,,,,Conference Paper
Agent-Based Simulation of Local Soy Value Chains in Ghana,"The assessment of changes in the relationships between supply chain agents is considered fundamental for market transformation. This paper reports on the application of a Value Chain Lab that supports the measurement of behavioral change in vertically structured supply-chain relationships. A participative gaming approach is used that enables to identify changes in mutual trust, transaction costs and risk behavior that result from value chain support and co-operation. The Value Chain Lab comprises value chain analysis, value chain games and multi-agent simulation. The paper describes the multi-agent simulation of a soy value chain in northern Ghana. The research was conducted in the context of the 2SCALE program, aiming to improve rural livelihoods and food and nutrition security in a number of African countries by developing agricultural supply chains including local smallholder farmers. The study confirms the positive effects of trust and loyalty in value chain relationships. Furthermore, it demonstrates the usefulness of agent-based simulations for exploring potential consequences of alternative interventions.",,"Verwaart T,Dijkxhoorn Y,Plaisier C,van Wagenberg C",,2019,654–666,10.1007/978-3-030-30244-3_54,https://doi-org.proxy.bnl.lu/10.1007/978-3-030-30244-3_54;http://dx.doi.org/10.1007/978-3-030-30244-3_54,Conference Paper
CyberBullet - Share Your Story: An Interactive Game for Stimulating Awareness on the Harm and Negative Effects of the Internet,"Increased internet connectivity across the African continent through mobile phones not only opens numerous opportunities, but also increases cybercrimes such as online child abuse and sexual exploitation. Previous national studies have shown that Namibia has experienced a surge in cybercrimes, which leaves children vulnerable to predators. While a national reporting portal has been launched, children are less likely to report incidents of cyber bullying or online abuse. This study aimed at investigating how an interactive game-based approach can be used for preventing online child abuse and the study also creates a fully functional game prototype. We wanted to gain insight into the current online experiences in Namibia. After administering an online survey and conducting focus group interviews at a local high school, we then conducted two game design workshops with stakeholders namely students, teachers, parents, and game developers. We found that most girls liked storytelling games whereas boys were more drawn to action games. This led to the development of the game called CyberBullet - Share Your Story. The study contribution is in the application of game-based approach to sensitize and prevent children from becoming victims of online abuse.",,"Mikka-Muntuumo J,Peters A,Jazri H",,2018,,10.1145/3283458.3283482,https://doi-org.proxy.bnl.lu/10.1145/3283458.3283482;http://dx.doi.org/10.1145/3283458.3283482,Conference Paper
Harnessing Nigeria's Investment in Satellite Technology for Sustainable Agriculture and Food Security,This paper examines the relevance of satellite technology in promoting and sustaining agricultural development and food security in Africa and Nigeria in particular. Some of the common problems facing agricultural development in Nigeria and Africa as a whole are discussed. The authors justify the relevance of Nigeria's investment in satellite technology for improving agricultural production in Nigeria and Africa as a whole. The paper also presents selected applications of NigeriaSat-1 in sustainable agriculture and food security as embarked on by the government of Nigeria through the National Space Research and Development Agency. Policy recommendations were made to further boost agricultural production and food security in Africa and particularly Nigeria.,,"Opeyemi ZA,Akinyede JO",,2012,63–72,10.4018/jagr.2012010106,https://doi-org.proxy.bnl.lu/10.4018/jagr.2012010106;http://dx.doi.org/10.4018/jagr.2012010106,Journal Article
The Identification of Mammalian Species through the Classification of Hair Patterns Using Image Pattern Recognition,"The identification of mammals through the use of their hair is important in the fields of forensics and ecology. The application of computer pattern recognition techniques to this process provides a means of reducing the subjectivity found in the process, as manual techniques rely on the interpretation of a human expert rather than quantitative measures. The first application of image pattern recognition techniques to the classification of African mammalian species using hair patterns is presented. This application uses a 2D Gabor filter-bank and motivates the use of moments to classify hair scale patterns. Application of a 2D Gabor filter-bank to hair scale processing provides results of 52% accuracy when using a filter-bank of size four and 72% accuracy when using a filter-bank of size eight. These initial results indicate that 2D Gabor filters produce information that may be successfully used to classify hair according to images of its patterns.",,"Moyo T,Bangay S,Foster G",,2006,177–181,10.1145/1108590.1108619,https://doi-org.proxy.bnl.lu/10.1145/1108590.1108619;http://dx.doi.org/10.1145/1108590.1108619,Conference Paper
Acoustic Communication of Rare and Threatened Crocodilians and Its Use for Population Monitoring,"Freshwater animal populations can be more vulnerable to human impact than those in terrestrial and marine ecosystems, but may receive less conservation investment, often due to limited availability of information. In this thesis I explore strategies for the conservation monitoring of crocodilians, an iconic group of apex predators that play a key role in tropical freshwater ecosystem regulation, but are unfortunately vulnerable to human activities. Population assessments, crucial in developing conservation programs, require robust methodologies that take advantage of our knowledge of organismal biology and ecology. Traditional crocodile survey techniques that rely on spotlight or aerial counts are well established for the more conspicuous species, but can provide limited information when applied to species that are shy or difficult to access. These hard-to-survey species are also often the ones that are most vulnerable to habitat modification, and are consequently of greater conservation concern. Crocodiles are the most vocal of reptiles, which opens up the potential for novel methods of surveying. Here I provide baseline information on general ecology and acoustic communication in three threatened crocodilian genera in Africa and Southeast Asia—Mecistops, Osteolaemus, and Tomistoma—and then go on to test how the crocodile vocalisations can be exploited in a monitoring and survey context. I find that: (i) sympatric African crocodiles are highly partitioned in their habitat preferences, and so monitoring methods need to be tailored to individual species ecology, even when species are found in close proximity; (ii) West African slender-snouted crocodiles, Mecistops cataphractus, of all size classes produce distress calls and will respond to pre-recorded calls of their own species, but while the calls produced by small individuals attract conspecifics of all size classes, calls emitted by adults tend to repel them; (iii) spotlight surveys incorporating playback of Mecistops distress calls show greater detection rates compared to spotlight-only surveys; (iv) spotlight-only surveys detect a greater number of Mecistops than passive acoustic monitoring; (v) adult Central African dwarf crocodiles, Osteolaemus tetraspis, produce four distinct vocalisation types previously unreported in crocodylids, and are readily detected during passive acoustic monitoring; (vi) adult Sunda gharials, Tomistoma schlegelii, produce a range of previously-unreported underwater acoustic signals, but these appear restricted only to direct mating activities, therefore limiting utility of acoustic monitoring for their population assessment. This research provides an insight into the diversity of crocodilian acoustic repertoires, offers potential for acoustic-based survey methodologies in conservation, and opens up exciting new directions in reptile behavioural ecology.",,Staniewicz AM,,2020,,,,Ph.D. Thesis
"Workshop Report: Reducing Internet Latency, 2013","This paper reports on a workshop convened to develop an action plan to reduce Internet latency.Internet latency has become a focus of attention at the leading edge of the industry as the desire to make Internet applications more responsive outgrows the ability of increased bandwidth to address this problem. There are fundamental limits to the extent to which latency can be reduced, but there is considerable capacity for improvement throughout the system, making Internet latency a multifaceted challenge. Perhaps the greatest challenge of all is to re-educate the mainstream of the industry to understand that bandwidth is not the panacea, and other optimizations, such as reducing packet loss, are at odds with latency reduction.For Internet applications, reducing the latency impact of sharing the communications medium with other users and applications is key. Current Internet network devices were often designed with a belief that additional buffering would reduce packet loss. In practice, this additional buffering leads to intermittently excessive latency and even greater packet loss under saturating load. For this reason, getting smarter queue management techniques more widely deployed is a high priority. We can reduce these intermittent increases in delay, sometimes by up to two orders of magnitude, by shifting the focus from packet loss avoidance to delay avoidance using technology that we already have developed, tested, implemented and deployed today.There is also plenty of scope for removing other major sources of delay. For instance, connecting to a website could be completed in one roundtrip (the time it takes for packets to travel from source to destination and back again) rather than three or four, by folding two or three rounds of flow and security set-up into the first data exchange, without compromising security or efficiency.Motivating the industry to deploy these advances needs to be aided by the availability of mass-market latency testing tools that could give consumers the information they need to gravitate towards low latency services, providers and products. There is no single network latency metric but several alternatives have been identified that compactly express aggregate delay (e.g. as relationships or a constellation), and tools that make use of these will give greater insight into the impact of changes and the diversity of Internet connections around the world.In many developing countries (and in rural regions of developed countries), aside from Internet access itself, there are significant structural issues, such as trombone routes through the developed world and a lack of content distribution networks (CDNs), that need to be addressed with more urgency than Active Queue Management (AQM) deployment, but the `blank slate' of new deployments provides an opportunity to consider latency now. More widespread use of Internet exchange points for hosting local content and fostering local interconnections is key to addressing some of these structural challenges.",,Ford M,,2014,80–86,10.1145/2602204.2602218,https://doi-org.proxy.bnl.lu/10.1145/2602204.2602218;http://dx.doi.org/10.1145/2602204.2602218,Journal Article
"A Glamorous Nightmare: HIV/Aids, American Literature, and the Viral Power of Whiteness","This dissertation traces the strange racial history of the HIV/AIDS pandemic through recent fiction and drama. Cultural theorists like Cindy Patton and Simon Watney have long insisted that nonfictional depictions of HIV/AIDS shored up white exceptionalism by figuring a racialized threat to both immunity and sovereignty emanating from Africa. From a literary archive that spans 1980s and 90s works by Tony Kushner, Bret Easton Ellis, and Neal Stephenson, among others, I extract an unlikely characterization of the disease: one that betrays a counterintuitive identification with whiteness. By engaging narrative texts as a cultural unconscious uniquely capable of distilling otherwise inchoate discourses, I demonstrate that the plague's devastation of this premise of racial security necessitated a reformulation of white superiority based in the virus, which furnished a plausible substitute for many of the capacities—flexibility, mobility, and ruthless self-interest—once lodged in immunity itself. I thus diagnose how a viral threat to the hegemony of whiteness ultimately availed that hegemony's reconstitution. This identification with virality, I contend, recapitulates a strategy of dominance operative since the 1970s when, as several scholars have argued, white exceptionalism coopted the narrative of victimhood invoked by the very movements believed to threaten white power: civil rights and affirmative action campaigns. My dissertation also discovers an insurgent response to this racializing discourse of virality. From the works of Hortense Spillers, Sapphire, and Gary Fisher I limn a counter discourse that accrues power and vibrancy for blackness from the plague's morbid economy.",,"Alexander TT,Dore F,Avilez G,Eng D",,2020,,,,Ph.D. Thesis
CODASPY '13: Proceedings of the Third ACM Conference on Data and Application Security and Privacy,"It is our great pleasure to welcome you to the third edition of the ACM Conference on Data and Application Security and Privacy (CODASPY 2013), which follows the successful first and second editions held in February 2011 and 2012. This conference series has been founded to foster novel and exciting research in this arena and to help generate new directions for further research and development. The initial concept came up in a conversation between the two co-founders when both happened to be at the same meeting. This was followed by discussions with a number of fellow cyber security researchers. Their enthusiastic encouragement persuaded us to move ahead with the always daunting task of creating a high-quality conference.Data and applications that manipulate data are crucial assets in today's information age. With the increasing drive towards availability of data and services anytime and anywhere, security and privacy risks have increased. Vast amounts of privacy-sensitive data are being collected today by organizations for a variety of reasons. Unauthorized disclosure, modification, usage or denial of access to these data and corresponding services may result in high human and financial costs. New applications such as social networking and social computing provide value by aggregating input from numerous individual users and the mobile devices they carry and computing new information of benefit to society and individuals. To achieve efficiency and effectiveness in traditional domains such as healthcare there is a drive to make these records electronic and highly available. The need for organizations to share information effectively is underscored by rapid innovations in the business world that require close collaboration across traditional boundaries. Security and privacy in these and other arenas can be meaningfully achieved only in context of the application domain. Data and applications security and privacy has rapidly expanded as a research field with many important challenges to be addressed.In response to the call for papers of CODASPY 2013 a total of 107 papers were submitted from Africa, Asia, Australia, Europe, and North America. The program committee selected 24 fulllength research papers, which is three more than last year. These papers cover a variety of topics, including privacy of social networks, novel privacy techniques and applications, and access control and security of smart appliances and mobile devices. The program committee also selected nine short papers for presentation. This year for the first time the program also includes a poster paper session presenting exciting work in progress. The program is complemented by keynote speeches by Mike Reiter and by Ronnie Killough, as well as a panel (topic not yet decided at press time).",,,,2013,,,,Book
Requirements Engineering in an Emerging Market,"The growing importance of requirements engineering RE in software development cannot be overemphasized. A faulty requirements gathering exercise and the emergent requirements document could mislead the entire software development drive, resulting in a software product that falls short of user expectation in terms of meeting needs and delivering within budget, time and scope. Achieving the objective of a well articulated and coordinated requirements document in an ideal economic environment is tasking let alone in an emerging market characterized by macro-economic variables such as high cost of doing business, weak institutions, poor infrastructure, lack of skilled and competitive workforce, among others coupled with micro-economic personal tendencies like resistance to change, vested interest, technophobia and insider abuse. This paper reports on industrial experience of designing and implementing an n-tier enterprise application in an African university using service oriented software engineering SOSE approach. The application is meant to facilitate the actualization of the 25-year strategic plan of the institution. We applied design and software engineering skills: Literature were examined, requirements gathered, the n-tier enterprise solution modeled using unified modeling language UML, implementation achieved using Microsoft SharePoint and the results evaluated. Though success was recorded, the challenges encountered during the requirements engineering stage were quiet reflective of the challenges of software project management in a typical relatively unstable macroeconomic environment. The outcome of this study is a compendium of lessons learnt and recommendation for successful RE in the context of an emerging economy like Africa in the hope that this will guide would-be software stakeholders in such a business landscape.",,Okewu E,,2015,476–491,10.1007/978-3-319-21410-8_37,https://doi-org.proxy.bnl.lu/10.1007/978-3-319-21410-8_37;http://dx.doi.org/10.1007/978-3-319-21410-8_37,Conference Paper
A Developing World Perspective on the Design of Wireless Enabled Humanitarian Relief Services,"In the absence of adequate state support, societies in the developing world have long relied on community support for humanitarian relief. Such community networks provide a readily available platform for delivery of humanitarian relief services. Wireless technologies can play an important role in enabling humanitarian relief applications that strengthen these community networks by facilitating the flow of information amongst the community members. Nevertheless, given the welfare nature of the activity, these applications face some strict design constraints that emerge from the larger socio-political-economic landscape. This paper presents a systematic approach to unearth the requirements that these domains may impose on the design of wireless enabled information and communication oriented humanitarian relief services, wiHRS. We describe SEAM, a systems thinking inspired conceptual framework that provides the theoretical underpinnings of the modeling apparatus used in this paper. As an example, we demonstrate the relevance of this framework to the design of wiHRS by analyzing the economics of enhanced information flow in community networks and how this analysis can be exploited to reflect on the financial viability of such services by, say, soliciting support from financial risk management instruments like insurance schemes.",,"Saxena A,Wegmann A",,2011,357–364,10.1145/2185216.2185312,https://doi-org.proxy.bnl.lu/10.1145/2185216.2185312;http://dx.doi.org/10.1145/2185216.2185312,Conference Paper
CODASPY '14: Proceedings of the 4th ACM Conference on Data and Application Security and Privacy,"It is our great pleasure to welcome you to the fourth edition of the ACM Conference on Data and Application Security and Privacy (CODASPY 2014), which follows the successful three editions held in February 2011, 2012 and 2013. This conference series has been founded to foster novel and exciting research in this arena and to help generate new directions for further research and development. The initial concept came up in a conversation between the two co-founders when both happened to be at the same meeting. This was followed by discussions with a number of fellow cyber security researchers. Their enthusiastic encouragement persuaded the co-founders to move ahead with the always daunting task of creating a high-quality conference.Data and applications that manipulate data are crucial assets in today's information age. With the increasing drive towards availability of data and services anytime and anywhere, security and privacy risks have increased. Vast amounts of privacy-sensitive data are being collected today by organizations for a variety of reasons. Unauthorized disclosure, modification, usage or denial of access to these data and corresponding services may result in high human and financial costs. New applications such as social networking and social computing provide value by aggregating input from numerous individual users and the mobile devices they carry and computing new information of benefit to society and individuals. To achieve efficiency and effectiveness in traditional domains such as healthcare there is a drive to make these records electronic and highly available. The need for organizations to share information effectively is underscored by rapid innovations in the business world that require close collaboration across traditional boundaries. Security and privacy in these and other arenas can be meaningfully achieved only in context of the application domain. Data and applications security and privacy has rapidly expanded as a research field with many important challenges to be addressed.In response to the call for papers of CODASPY 2014 a total of 119 papers were submitted from Africa, Asia, Australia, Europe, North America, and South America. The program committee selected 19 full-length research papers (less than 16% of acceptance rate). These papers cover a variety of topics, including privacy of social networks, novel privacy techniques and applications, and access control and security of smart appliances and mobile devices. The program committee also selected nine short papers for presentation. This year for the second time the program also includes a poster paper session presenting exciting work in progress. The program is complemented by keynote speeches by Dongyan Xu and by Jarret Raim, as well as a panel (topic not yet decided at press time).",,,,2014,,,,Book
The Coming African Tsunami of Information Insecurity,"As the affordability and use of mobile phones in Africa increase, so too will security vulnerabilities.",,"Goodman S,Harris A",,2010,24–27,10.1145/1859204.1859215,https://doi-org.proxy.bnl.lu/10.1145/1859204.1859215;http://dx.doi.org/10.1145/1859204.1859215,Journal Article
The Perception of Hypertension among Haitian Adults: A Focused Ethnography,"The terms ""health disparities"" and ""social justice"" are popular buzz words in health care. These terms have been applied locally and internationally when examining the current health conditions and health resources. Since 2010, Haiti has gained much attention from the world with relief efforts and increased attention on the apparent health needs in the country. Despite these efforts, the overall health statistics of the country have not improved (Brown, 2010; Garfield & Berryman, 2012; WHO, 2014). In 2010, in response to the global epidemiological transition, the World Health Organization (WHO) shifted its attention to worldwide non-communicable diseases (NCDs) such as cardiovascular disease, cancer, diabetes, and respiratory diseases. With this shift in attention, hypertension has been identified as a worldwide health concern. The purpose of this focused ethnography is to describe Haitians' perceptions of hypertension which contribute to the meaning of and beliefs about this chronic illness, in order to more fully understand the needs of Haitian adults with hypertension. Kleinman's Explanatory Model of Illness (Kleinman, A., Eisenberg, L., & Good, B., (1978) serves as the theoretical background for the study. The overarching theme identified is that Haitians perceive hypertension to be a feeling that one gets which should be treated at that moment to prevent falling down. This feeling presents differently and can vary with occurrences and individuals. The feelings identified as being associated with hypertension can actually be a variety of symptoms to include: headache, blurry vision, dizziness, burning, weakness, and shortness of breath. These feelings, known as symptoms in allopathic medicine, are consistent with presenting clinical manifestations of hypertension as well as consistent with the complication of stroke often associated with uncontrolled hypertension. The findings in this study can be expanded upon to inform management and treatment options for this population as well as provide recommendations for healthcare providers serving in developing countries.",,"Feurer AE,Lindgren T,Beneson I,Chase S",,2020,,,,Ph.D. Thesis
Multi-Objective Cluster Head Using Self-Attention Based Progressive Generative Adversarial Network for Secured Data Aggregation,,,"Sindhuja M,Vidhya S,B S J,Shajin FH",,2023,,10.1016/j.adhoc.2022.103037,https://doi-org.proxy.bnl.lu/10.1016/j.adhoc.2022.103037;http://dx.doi.org/10.1016/j.adhoc.2022.103037,Journal Article
Mapping Twenty Years of Antimicrobial Resistance Research Trends,,,"Luz CF,van Niekerk JM,Keizer J,Beerlage-de Jong N,Braakman-Jansen LM,Stein A,Sinha B,van Gemert-Pijnen JE,Glasner C",,2022,,10.1016/j.artmed.2021.102216,https://doi-org.proxy.bnl.lu/10.1016/j.artmed.2021.102216;http://dx.doi.org/10.1016/j.artmed.2021.102216,Journal Article
IWCMC '06: Proceedings of the 2006 International Conference on Wireless Communications and Mobile Computing,"It is our great pleasure to welcome you to the International Wireless Communications and Mobile Computing Conference (IWCMC 2006). This year's Conference continues its tradition of being the premier forum for presentation of research results and experience reports on leading edge issues of Wireless Networking and Communications as well as Mobile Computing and Security. The mission of the Conference is to share novel basic research ideas as well as experimental applications in the wireless areas in addition to identifying new directions for future research and development. IWCMC 2006 gives researchers a unique opportunity to share their perspectives with others interested in the various aspects of Wireless Networking, Wireless Communication, Information Theory and Mobile Computing.The call for papers attracted more than 550 submissions from Asia, Canada, Europe, Africa, and the United States. The program committee accepted 270 papers that cover a variety of topics, including next generation networks, MIMO Systems, Cross-Layer Design, Multimedia over Wireless, Mobile Computing, Wireless LNAs and MANs, Next Generation Networks, Ad-Hoc and Sensor Networks, Information Theory and security of computers and networks. In addition, the program includes four Tutorials and a keynote speech by Philippe Jetté, General Manager, Marketing & Business Development at Bell Mobility Canada, on Enabling the Future Mobile Environment. We hope that these CDs will serve as a valuable reference to researchers and developers in the area.",,,,2006,,,,Book
The Relative Importance of Monetary and Non-Monetary Drivers for Information and Communication Technology Acceptance in Rural Agribusiness,"Traditionally the information and communication technology for development ICT4D literature assessed technology interventions in developing countries from an economic viewpoint, typically measuring income increases or other economic gains. However numerous ICT4D studies revealed that technology adopters only secure a small, single-digit monetary benefit, thus suggesting the importance of other i.e. non-monetary drivers of information and communication technology ICT acceptance. Seeking to address the issue and to identify the relative importance of monetary vs. non-monetary drivers for the acceptance of ICT in rural agribusiness, this study investigates the key motivational drivers monetary vs. non-monetary for the acceptance of a digital procurement e-purjee system by sugarcane growers in rural Bangladesh. The e-purjee system is a simple SMS-based purchase order system that replaces a paper-based procurement order system. Treating the acceptance of e-purjee system as sugarcane growers’ decision-problem, and applying a multi-criteria decision-making approach [e.g. Zionts & Wallenius. 1976. An interactive programming method for solving the multiple criteria problem. Management Science, 226, 652–663] to that problem, the study identifies the trade-offs growers appear to make between non-monetary and monetary decision criteria. In addition, by analyzing interviews with local growers from the perspective of the human capability approach [Sen. 1999. Development as freedom. New York, NY: Oxford University Press], this study offers new explanations for their preferences and reasoning. The findings indicate that non-monetary incentives, namely procedural fairness and uncertainty reduction, can be more important than positive monetary benefits. Interview responses also suggest that non-monetary benefits affect small-scale growers more than the large-scale growers. Considering growers’ preferences related to non-monetary incentives, the e-purjee system appears to affect three out of five types of instrumental freedoms postulated by Sen [1999. Development as freedom. New York, NY: Oxford University Press]. The study offers several practical and theoretical recommendations about the structuring of incentive systems for rural technology-based development projects, and about decision modeling for a relatively untrained informant group.",,"Alam MM,Wagner C",,2016,654–671,10.1080/02681102.2016.1155142,https://doi-org.proxy.bnl.lu/10.1080/02681102.2016.1155142;http://dx.doi.org/10.1080/02681102.2016.1155142,Journal Article
The Still Untapped Potential of Social Media for Health Promotion: The WHO Example,"Social media platforms are a network and a communication tool for populations and a powerful marketing channel for the private sector. But are public health organizations taking advantage of its full potential to reach communities directly, influence their (un)healthy behaviors and reduce inequities? Performance indicators of World Health Organization (WHO) Facebook pages were monitored over 5 months in 2019. Simple and multivariate statistics were applied to identify patterns of social media performance, limitations and improvement opportunities. The WHO global page has a totally different profile than the other WHO pages, being by far the most successful: 4.132.925 followers, average of 3 posts per day posts and average of 2.429 total reactions, comments, shares per post. However, one could expect an even better performance given the number of followers. Performance of regional offices pages (Western, Pacific, South-East Asia, Pan American, Europe, Eastern Mediterranean and African Regional Offices) is more worrisome: 14.149 to 309.104 followers per page, quite irrelevant interaction indicators (30 to 208 total reactions, comments, shares per post, 16 to 106 likes per post, 2 to 6 comments per post and 8 to 133 shares per post). Pan American Regional Offices strategy to create different countries and themed Facebook pages doesn't seem to work out. Posts are mainly published in a neutral way, compliant with a traditional public health institutional approach to disseminate health information. However, nowadays fake and erroneous news dissemination take a more sensationalist, sentimental approach, more in tune with a social media environment. One could wonder if WHO's more traditional way to transmit health information is indeed still the key to changing health behaviors. The existence of several WHO pages could be an opportunity to segment audiences, adjust the message to particular needs and thus improve the equity gap, since smaller, more at risk groups could be targeted. However, the current fragmentation of WHO approach isn't inducing a better social media performance. Our analysis was based on publicly available information (organic not payed reach). Owners of pages though have the exact return of health promotion investments in social media. Practice shows that social media investments necessary are much more cost effective than regarding other media (press, television among others). International institutions with a public health motivation such as WHO should rethink their social media strategies. A traditional and institutional approach of publishing information online isn't the more effective way to reach populations through this channel and spun behavior changes. Additionally, adequately done social media communication represents an easy and effective way to reach out to communities and accurately calculate the return on investment of these health promotion actions.",,Bacelar-Nicolau L,,2019,125,10.1145/3357729.3357755,https://doi-org.proxy.bnl.lu/10.1145/3357729.3357755;http://dx.doi.org/10.1145/3357729.3357755,Conference Paper
Proposal for a Platform for the Continuity of Distance Learning in African Schools and Universities at the End of the Politico-Military Crisis in the Face of Covid-19: Case of the Central African Republic,"Following the perpetual political-military crises, most of the rural areas of the Central African Republic (CAR) are occupied by armed groups. This leads to human insecurity in these areas. Children, adolescents, and youth are out of school. Primary and secondary school teachers are unable to travel to unsafe areas. Due to the problem of human insecurity in rural areas and especially poverty in several rural areas of CAR, parents are unable to finance the education of their children who have taken the baccalaureate exams to travel to the capital Bangui to study at Bangui University alone. In this article, we propose a platform for the continuity of educational activities in the Central African Republic. Our initially proposed platform solution allows the creation of a distance primary and secondary school in bimodal mode in the rural areas of the CAR. In a second step, it allows the creation of a complete distance university training coupled with traditional education for young people from all rural areas of the Central African Republic. This platform has been tested at the Higher Institute of Technology (Department of Computer Science and Telecommunications) and the Faculty of Science of the University of Bangui and has enabled the partial resumption of pedagogical activities in these institutions. It has been applied in the field of STEM (science, technology, engineering and mathematics) and can be extended to other disciplines. Access to resources is efficient thanks to the coupling of the WireGuard VPN server and the Apache Guacamole server which is a gateway using standard protocols via a browser. It also uses VXLAN technology which moves the WireGuard VPN server subnet from OSI Layer 3 to Layer 2 and allows the organization of practical work that requires being in the same local subnet. Access to this platform provides learners in the Central African Republic with a complete and secure distance learning environment for courses, assignments and tutorials.",,"Mervyl Saint-Juste Kossingou G,Dégboé B,Gladys Gladys Ndassimba N,Ouya S,Mendy G",,2021,,10.1145/3454127.3456603,https://doi-org.proxy.bnl.lu/10.1145/3454127.3456603;http://dx.doi.org/10.1145/3454127.3456603,Conference Paper
"ICT Supported Extension Services in Conservation Agriculture Information Access for Small Holder Farmers in Laikipia County, Kenya","This paper examines how Information Communication Technologies (ICTs) are used in the Conservation Agriculture (CA) knowledge pathways. It discusses the parallel knowledge pathways smallholder farmers' use to access conservation agriculture information. The objective of this study is to develop an effective dissemination model that exploits the use of ICTs in existing pathways in order to improve Conservation Agriculture knowledge flows. Design, Methodology and Approach: A total of one hundred and twenty-five respondents were interviewed, with a purposively selected sample of 110 CA farmers interviewed using semi-structured questionnaires. Using the snowball method, 15 key informant interviews were conducted with Policy makers and ICT service providers. 69% of the respondents were female and 51% male. The focus of the study was on the different ICTs used in the CA knowledge pathways, the dissemination pathways, institutional and socio-economic factors. Data was analyzed manually and using SSPS ver. 21. Preliminary findings indicate that a variety of knowledge pathways exist through which farmers can access CA knowledge. Four different kinds of ICT led models were identified, Government ICT led models, Government-NGO led models, Government-NGO-Private Sector ICT led model and NGO ICT led models. However the study notes a lack of harmonization and weak linkages between institutions in the utilization of the existing ICT models, utilization of the ICTs access and ownership at the household level and harmonization of the CA messages. Furthermore, even though there was 100% mobile phones ownership, 94% radio, 64% television, 10% Laptop and 9% Computer (desktop) access and ownership among the respondents, farmers and a large number of the extension staff lacked the skills to fully exploit the use of these tools to access CA knowledge. Implication: New emerging and existing communication technologies have a very high potential to improve agricultural knowledge flows if taken advantage of by the ""change agents"" in the diffusion process of new innovations. New online technologies known as Web 2.0 and ‘social media’ are slowly emerging as platforms for collaboration, sharing of product and market information. Open chain models of village information centres also provide numerous economic opportunities, and do also network communities while providing public/private services. The opportunity of using real time communication tools has been greatly embraced especially by the youthful farmers and the advantage of these new emerging tools is their unique attributes, similar to the traditional oral cultures of communication seen in the African social systems where one can see, hold a discussion, get immediate feedback and use the written media to convey messages. The high percentage of ownership and accessibility of ICTs among the survey population also offers the potential to fully exploit ICTs in the improvement of the CA knowledge pathways and agricultural information provision.",,"Cox AJ,Sseguya H",,2015,1–6,10.1109/ISTAS.2015.7439408,https://doi-org.proxy.bnl.lu/10.1109/ISTAS.2015.7439408;http://dx.doi.org/10.1109/ISTAS.2015.7439408,Conference Paper
Fractional Optimal Control with Fish Consumption to Prevent the Risk of Coronary Heart Disease,"According to the World Health Organization (WHO), Chronic Heart Disease (CHD) is one of the greatest defies currently confronting humankind which is sweeping the whole globe, with an expanding trend in developing countries. In this paper, a mathematical model (MM) was proposed to study the connection between fish consumption and CHD mortality in Egypt, by considering a system of ordinary differential equations (ODEs) involving time-fractional derivative (FD). We considered here the study on Egypt for the ease of obtaining real data, but the method and approach adopted here is not limited to Egypt only and can be applied to any country in the world with the information of the real data related to the subject of the study. Additionally, the control function which represents the metabolic and the behavioural risk factors of CHD that help to reduce the number of mortality due to CHD is incorporated in the proposed MM. A fractional optimal control problem (FOCP) with a proposed control is formulated and studied theoretically using the Pontryagin maximum principle, to minimize the susceptible population and also to decrease the mortality rate of CHD. Moreover, firstly we discussed the positivity and boundedness of solutions; then, the model equilibria are determined and their local stability analysis was investigated; furthermore, we use the improved forward-backward sweep method (FBSM) based on the predictor-corrector method (PCM) in order to obtain the solution of proposed FOCP. In addition, some numerical simulations were performed to show the effect of the proposed optimal control (OC) besides the impact of fish consumption on the mortality of CHD.",,"Ameen I,Hidan M,Mostefaoui Z,Ali HM,Guo X",,2020,,10.1155/2020/9823753,https://doi-org.proxy.bnl.lu/10.1155/2020/9823753;http://dx.doi.org/10.1155/2020/9823753,Journal Article
"Information and Communication Technology for Development for Africa: First International Conference, ICT4DA 2017, Bahir Dar, Ethiopia, September 25-27, 2017","This book constitutes the proceedings of the First International Conference on Information and Communication Technology for Development for Africa, ICT4DA 2017, held in Bahir Dar, Ethiopia, in September 2017. The 31 revised full papers presented were carefully reviewed and selected from 72 submissions. The papers address the impact of ICT in fostering economic development in Africa. In detail they cover the following topics: e-services, natural language processing, intelligent systems, mobile and wireless communication, privacy and security.",,"Mekuria F,Nigussie EE,Dargie W,Edward M,Tegegne T",,2018,,,,Book
Cryptographic Credit Control in Pre-Payment Metering Systems,"Abstract: We describe the successful introduction of cryptology into a new application area-protecting prepayment electricity meters from token fraud. These meters are used by a number of utilities from Scotland to South Africa, and they present some interesting security challenges.",,"Anderson RJ,Bezuidenhout SJ",,1995,15,,,Conference Paper
Automatic Detection of Trypanosomes on the Blood Stream,"Despite the major advancements in Medical Science of the XXI century, there are still dangerous diseases spread worldwide. Two of them chagas disease and the sleeping sickness are potentially life-threatening illnesses caused by the protozoan parasites: Trypanosoma cruzi (T. cruzi) and Trypanosoma brucei (T. brucei) respectively. These diseases are mainly found in Latin America and Africa being transmitted to humans and animals by small insects like triatomine bugs and tsetse flies either by bite or contact with their faeces, killing a high number of people by late diagnostic.The chagas disease has two phases, the initial (acute) phase lasts about two months after infection, in this phase a high number of parasites circulate in the blood stream, but in most cases the symptoms are absent or mild. In the second phase, chronic phase, the parasites are hidden mainly in the heart and digestive muscles, in later years the infection can lead to sudden death or heart failure caused by progressive destruction of the heart muscle and digestive system.The sleeping sickness also has two phases, the initial phase, last from some weeks to one or two years depending of the sub-species of the T.brucei parasite, in this phase a high number of parasites circulate in the blood stream, and some patients suffer from fever and aches. In the second phase, the parasite reaches the nervous system causing mental deterioration and other neurological problems leading to the death of the patient.The objective of this work was to create a mobile solution that can help detect both the diseases in the initial stages by detecting the trypanossomes parasites. With this application, the user takes a photo of a thin blood smear sample of a patient using an adapter that attaches the mobile device to a microscope, then the image is segmented in order to separate the components of the blood from its background. Later, the application will try to confirm if the parasites segmented are the correct ones, informing the user if the donor of the blood is infected. With this, it becomes possible to make the detection of the diseases in countries where the health services are poorly developed and people do not have good access to it. This mobile application is fast and reliable due to its analysis sensibility of 97.37% and its short execution time of approximately 32 seconds on a high-end android device.",,"Faria JR,Pascoal Faria JC,Henriques da Cunha Abreu PM",,2016,,,,Ph.D. Thesis
"Drivers, Enablers and Barriers of Developing Commercialisation in an Oil-Dependent Economy : The Case of Saudi Arabia","Faced with depleted oil stocks from 2035 onwards, the Kingdom of Saudi Arabia has a strategy of becoming a more knowledge-based economy by indigenously exploiting transferred technologies. However, despite significant investment in university-based technology transfer and incubation facilities, there is little progress in establishing high-growth advanced technology companies outside of the oil sector. The thesis explores the commercialisation of university knowledge in university-based incubators in the Kingdom of Saudi Arabia and contrasting with arrangements in the UK, to identify the cause of low spinout rates and what can be done. Beginning with a systematic review of literature on innovation and entrepreneurship as it applies to commercialisation, the thesis identifies gaps and areas of controversy in the literature. From previous research an initial conceptual framework is developed to guide data gathering, its presentation and analysis. Using a qualitative method a sample of twenty-four Saudi interviews and eight UK interviews is justified. Significant new data on Saudi incubator policy, processes and outcomes is presented alongside new data from the UK. This is then analysed from an in-case and cross-case perspective and then re-integrated with literature. A revised conceptual framework is presented and conclusions for theory and practice drawn. The thesis adds to the multidisciplinary bodies of knowledge for example by updating Gerschenkron's (1966) theory of catch-up, challenging the validity of Etzkowitz's (1983) triple helix theory, and arguing that North's (1990) neo-institutional theory is ethnocentric. I argue that culture is a major influence on commercialisation in developing countries and consciously changing cultures necessary for Saudi Arabia's transition from a rentier state. I find little empirical evidence in either the UK or Saudi Arabia for theories (Shane 2004) of academic entrepreneurship. I conclude that major reforms of Saudi universities and commercialisation processes are necessary if the strategy is to contribute significantly towards diversification of the economy.",,Alakeel AA,,2017,,,,Ph.D. Thesis
Securing the Human: Broadening Diversity in Cybersecurity,"Recent global demand for cybersecurity professionals is promising, with the U.S. job growth rate at 28%, three times the national average. Lacking qualified applicants, many organizations struggle to fill open positions. In a global survey, 2,300 security managers reported that 59% of their security positions were unfilled, although 82% anticipated cyberattacks to their systems. At the same time, the cybersecurity field is broadening, not only in technical concepts but also in human factors, business processes, and international law. The field has not become culturally diversified, however. Professionals hired in 2018 included only 24.9% women, 12.3% African Americans, and 6.8% Latinos. These facts create an opportunity for higher education: diversify the profession while increasing the numbers of skilled computer scientists. New and integrated methods of attracting student populations in the field of cybersecurity are needed. The working group goal is to evaluate the effectiveness of approaches used in higher education to diversify the cybersecurity field through literature review, analysis of the findings, and a survey on techniques used for diversification of the cybersecurity field.",,"Azhar M,Bhatia S,Gagne G,Kari C,Maguire J,Mountrouidou X,Tudor L,Vosen D,Yuen TT",,2019,251–252,10.1145/3304221.3325537,https://doi-org.proxy.bnl.lu/10.1145/3304221.3325537;http://dx.doi.org/10.1145/3304221.3325537,Conference Paper
Group Profiling for Understanding Social Structures,"The prolific use of participatory Web and social networking sites is reshaping the ways in which people interact with one another. It has become a vital part of human social life in both the developed and developing world. People sharing certain similarities or affiliates tend to form communities within social media. At the same time, they participate in various online activities: content sharing, tagging, posting status updates, etc. These diverse activities leave behind traces of their social life, providing clues to understand changing social structures. A large body of existing work focuses on extracting cohesive groups based on network topology. But little attention is paid to understanding the changing social structures. In order to help explain the formation of a group, we explore different group-profiling strategies to construct descriptions of a group. This research can assist network navigation, visualization, and analysis, as well as monitoring and tracking the ebbs and tides of different groups in evolving networks. By exploiting information collected from real-world social media sites, extensive experiments are conducted to evaluate group-profiling results. The pros and cons of different group-profiling strategies are analyzed with concrete examples. We also show some potential applications based on group profiling. Interesting findings with discussions are reported.",,"Tang L,Wang X,Liu H",,2011,,10.1145/2036264.2036279,https://doi-org.proxy.bnl.lu/10.1145/2036264.2036279;http://dx.doi.org/10.1145/2036264.2036279,Journal Article
Employee Perceptions of BYOD in South Africa: Employers Are Turning a Blind Eye?,"As mobile Information and Communication Technologies (ICTs) become greater entrenched in society and with the nature of work changing, more and more international organizations are embracing and/or considering formalizing the phenomenon of 'Bring Your Own Device' (BYOD). The gist of BYOD is the use of privately owned devices and software to access and work with organizational resources. There is however little that is known about the degree to which organizations in South Africa are embracing the BYOD phenomenon. In this paper, we explored how employees in organizations in South Africa perceive the use of their privately owned devices for work. The results from 61 employees suggest that there is a strong awareness of the BYOD concept among employees. Employees also appear to believe that although their employers are aware of the use of privately owned devices for work, the employers are reluctant to formally create BYOD organizational strategies. The findings suggest that the laxity of employers in South Africa to deal with the BYOD phenomenon as an issue of strategic importance could result in considerable security challenges for organizational data.",,"Twinomurinzi H,Mawela T",,2014,126–131,10.1145/2664591.2664607,https://doi-org.proxy.bnl.lu/10.1145/2664591.2664607;http://dx.doi.org/10.1145/2664591.2664607,Conference Paper
Essays on Discrimination and Spatial Inequality,"The dissertation investigates institutional sources of spatial inequality, such as government-sponsored ""redlining"" and family structure. Throughout my doctorate, I developed an interest in the geographic sources of socioeconomic gaps that characterize US society according to race, ethnicity and gender. My research has focused on institutional features of the social environment that shape ""neighborhood effects,"" such as place-based government interventions, family arrangements, and peer influences. Methodologically, I developed empirical strategies to estimate causal effects in observational settings where a control group is not immediately apparent. To carry out my research plans, I built and analyzed complex spatial datasets using a variety of geographic software programs and machine learning algorithms that have proven useful in defining valid and innovative control groups.The first chapter is ""The Effects of Federal ""Redlining"" Maps: a Novel Estimation Strategy,"" joint work with Disa M. Hynsjö. Redlining, the systematic denial of credit to residents of a community, is often cited by activists and policymakers as one cause of enduring urban inequality. It is widely understood that the federal government started redlining in the 1930s. Government maps, identifying disadvantaged neighborhoods with the color red, have become a symbol of institutional discrimination. However, historians have disputed the ultimate influence of such maps on access to credit, and evidence of any causal economic impacts is scarce due to a lack of data and estimation challenges.This paper investigates the causal effects of the Home Owners' Loan Corporation (HOLC) maps and the neighborhood grades they assigned to summarize lending risk in the second half of the 1930s. In particular, we estimate the effects of different grades on homeownership rates, property values and shares of African-Americans between 1940 and 2010. In their time, the HOLC maps were a data analytics tool at the forefront of real estate appraisal techniques that soon became influential in the housing market at large. Our study illustrates how institutional practices can coordinate individual choices and amplify their discriminatory consequences.To measure the short and long-term effects of the HOLC mapping intervention, we propose a new estimation strategy. Spatial discontinuity designs, often used in the literature on this topic, suffer from endogeneity concerns: multiple authors documented socioeconomic differences on opposite sides of boundaries traced by the agency, indicating that the HOLC did not assign border locations and grades randomly. Instead, we exploit an exogenous population threshold that determined which cities were mapped and a machine learning algorithm drawing HOLC maps in control cities. Using the grades predicted by the machine learning model, we apply a grouped difference-in-differences design to measure the causal effects of the HOLC intervention. The causal effects are identified by differences between neighborhoods in treated cities and areas in control cities that would have received the same grade, but were not mapped. This empirical strategy is possible thanks to a new spatial dataset we constructed geocoding full-count Census records between 1910 and 1940. In addition, geographic coordinates let us join tract-level Census data for 1960-2010 and CoreLogic real property data to measure long-term outcomes.We find a substantial reduction in property prices and a 2.4 percentage points decrease in homeownership rates in the lowest grade (red) areas in the short term. For this same grade, the HOLC maps caused a 1.6 percentage points increase in the local share of African-American residents in 1940. We also find a sizable house value reduction in the second to last grade (yellow) areas, showing that the causal impacts were not confined to red areas. Such negative effects for property prices persisted until the early 1980s, shortly after the federal government introduced legislative measures to counteract redlining.The second chapter is titled ""The Long Term Effects of Exposure to Non-Traditional Family Structures."" Single-mother households have become common in the US over the past fifty years. Economists, sociologists, and psychologists have documented that children from single-headed families have lower intergenerational mobility because of a lack of resources and the type of parenting they receive. However, little is known about the effects of children from single-mother families on their school peers. Taking advantage of the Add Health panel data structure, I estimate the effect of this feature of the adolescents' social environment on educational achievement and long-run labor market outcomes. My identification strategy is based on cohort-to-cohort variation in the percentage of children without a father figure within a school. The preliminary estimates indicate that exposure to peers with a higher rate of father absence does not have much of an effect on education, employment, or wages.",,"Perdoni L,Cormac O' Dea,Eric Humphries J",,2022,,,,Ph.D. Thesis
Towards a Theoretical Understanding of Workarounds Emerging from Use of a Referral Mobile Application: A Developing Country Context,,,"Kapepo MI,Van Belle JP,Weimann E",,2022,533–541,10.1016/j.procs.2021.12.046,https://doi-org.proxy.bnl.lu/10.1016/j.procs.2021.12.046;http://dx.doi.org/10.1016/j.procs.2021.12.046,Journal Article
Accidental Infrastructure for Groundwater Monitoring in Africa,"A data deficit in shallow groundwater monitoring in Africa exists despite one million handpumps being used by 200 million people every day. Recent advances with smart handpumps have provided accelerometry data sent automatically by SMS from transmitters inserted in handles to estimate hourly water usage. Exploiting the high-frequency noise in handpump accelerometry data, we model high-rate wave forms using robust machine learning techniques sensitive to the subtle interaction between pumping action and groundwater depth. We compare three methods for representing accelerometry data (wavelets, splines, Gaussian processes) with two systems for estimating groundwater depth (support vector regression, Gaussian process regression), and apply three systems to evaluate the results (held-out periods, held-out recordings, balanced datasets). Results indicate that the method using splines and support vector regression provides the lowest overall errors. We discuss further testing and the potential of using Africa's accidental infrastructure to harmonise groundwater monitoring systems with rural water-security goals. A data deficit exists in shallow groundwater monitoring in Africa.Our smart handpump has low-cost accelerometers mounted in the handle.We show that machine learning methods applied to the accelerometry can estimate aquifer depth.We demonstrate that we can use the accidental infrastructure of handpumps for estimating groundwater levels.",,"Colchester FE,Marais HG,Thomson P,Hope R,Clifton DA",,2017,241–250,10.1016/j.envsoft.2017.01.026,https://doi-org.proxy.bnl.lu/10.1016/j.envsoft.2017.01.026;http://dx.doi.org/10.1016/j.envsoft.2017.01.026,Journal Article
"Expanding the Knowledge Economy: Issues, Applications, Case Studies - Volume 4 Information and Communication Technologies and the Knowledge Economy - Two Volume Set","Commercializing and exploiting applied Information and Communication Technologies (ICT) research results is critical in reducing the global Digital Divide and building a sustainable Knowledge Economy. A major challenge for leveraging ICT around the world is taking account of local and regional differences. This requires meaningful cross-border communication between researchers, government and industry, as well as strategic dialogues in relation to regulation and policy. While it is clear that the Digital Divide can be challenging in Europe and the Americas, clearly there are greater difficulties to be overcome in Asia and Africa. That acknowledged, it is often striking how once basic infrastructure and regulatory issues have been addressed and how learning from the experiences of others can enhance the impact of leveraging ICT. This book brings together a comprehensive collection of over 210 in ten broad thematic areas. These are: ICT for Networked Enterprise; e-Government and e-Democracy; eHealth, Collaborative Working Environments; Networked, Smart and Virtual Organizations; SME Issues; Technology Enhanced Learning and ICT Skills; Security and Identity Management; and Mobility and Digital Content. Papers within each thematic area are grouped as Issues, Applications and Case Studies, reflecting their primary focus.IOS Press is an international science, technical and medical publisher of high-quality books for academics, scientists, and professionals in all fields. Some of the areas we publish in: -Biomedicine -Oncology -Artificial intelligence -Databases and information systems -Maritime engineering -Nanotechnology -Geoengineering -All aspects of physics -E-governance -E-commerce -The knowledge economy -Urban studies -Arms control -Understanding and responding to terrorism -Medical informatics -Computer Sciences",,"Cunningham P,Cunningham P,Cunningham M",,2007,,,,Book
UMS-Dev-Sec: A Proposed Framework to Address Security Concerns of UMS Devices,"USB Mass Storage (UMS) devices are affordable, convenient, practical, and have the ability to interface easily with a number of operating systems, and various hardware systems. UMS devices are found among virtually any person in the workplace today. However, UMS devices hold significant security risks, often overlooked. These risks arise mainly due to their compact size, storage capacity or the fact that UMS technology is integrated into Smartphones, cameras, music players, to name most significant. Considering that UMS technology is incorporated into devices that employees need daily to perform their work, such as Smartphones, it is not a feasible option to ban UMS technology from the workplace. However, without a guiding framework, UMS devices pose security risks to both organization and individual users.This paper reports on research conducted at the research institution in South Africa to evaluate the employee's understanding of the security risks to the organization and to them in person. From the results derived from the research conducted, a framework was developed to address the security concerns associated with UMS devices.",,"Molotsi K,Tait BL",,2013,72–76,10.1145/2513456.2513487,https://doi-org.proxy.bnl.lu/10.1145/2513456.2513487;http://dx.doi.org/10.1145/2513456.2513487,Conference Paper
On Food Price Implications from Expanded Bioenergy Production,"Bioenergy has been put forward as a solution to energy security and at the same time to climate change. It is, however, dependent on productive agricultural land, which is a limited resource. Introduction of bioenergy on a large scale will thus compete with food production and natural forests for productive land, a competition expected to affect food prices.In this thesis I focus on poverty nourishment issues related to changing food prices and on the mechanisms of land-use competition and how they affect food prices. In the first paper we use two established indicators for poverty and sensitivity to food-price changes, to capture peoples' vulnerability to rising food-prices, in four Sub-Sahara African countries/regions. In contrast to previous studies, we include all food products instead of just one or a few main staples. We found that the vast majority of people are net consumers of food and that the inclusion of more than main staples increases their net position as consumers and thus vulnerability to high food prices. In paper two and three a conceptual and transparent partial equilibrium model of global land-use competition is developed, analyzed and applied. The model is to a large degree analytically explored and price differentials between crops are derived. The model is subjected to a detailed characterization of its mechanisms and parameters in which parameters that are critical to results and conclusions from the model are detected and their impacts depicted. We conclude that the total amount of productive agricultural area is of crucial importance to the price impacts from large-scale introduction of bioenergy. Yields of bioenergy crops are also important since they determine the amount of land required to produce the bioenergy.",,Bryngelsson DK,,2012,,,,Ph.D. Thesis
IoT-Sys '15: Proceedings of the 2015 Workshop on IoT Challenges in Mobile and Industrial Systems,"It is our great pleasure to welcome you to the Workshop on IoT challenges in Mobile and Industrial Systems -- IoT-Sys 2015.The new IoT-Sys workshop, at its first edition, is intended to be a forum for exchanging new ideas about the challenges and symbiosis between the Internet of Things, Mobile Computing, and Industrial Systems. The workshop aims at providing a significant contribution by fostering fruitful and critical discussions between attendees in order to facilitate the growth of the main pillars of Mobile-IoT applications and Industrial-IoT systems, and, more importantly, pave the road towards networked systems of Information in IoT applications.The workshop program includes a mix of papers on mobile computing for the IoT, communication protocols, and security issues. The program includes also a keynote speech on the Physical Web paradigm, based on the integration between the IoT and web technologies.The workshop program includes an interesting mix of papers and talks on three broad themes: health and wellness, human and social sensing, and wireless tracking. The program includes two keynote talks, three other invited talks, and six refereed paper presentations, covering a mix of blue sky research and systems in deployment by startups. We have included a discussion period at the end of each session, with a moderator who will engage the speakers and the audience in a discussion. We have also set aside time at the end of the workshop program to discuss topics outside of the session themes, including the future direction for the workshop.The call for papers attracted submissions from Asia, Australia, Europe, Africa, and the United States. The program committee reviewed 18 submissions with an acceptance rate of 50%.We also encourage attendees to attend the keynote speech. These valuable and insightful talks can and will guide us to a better understanding of how the Internet of Things will be shaped in the future: The Physical Web, Roy Want (who is currently at Google, Inc.).",,,,2015,,,,Book
The Transition to Sustainable Construction in Botswana : A Multi-Level Perspective,"Interest in sustainable construction has grown in developing countries. The Government of Botswana has over the years introduced a number of initiatives with the potential to transform the construction sector to sustainability. It is noted from a socio-technical perspective that transition depends on changes in technologies, infrastructures, institutions, social practices, markets and regulations among others. The multi level perspective (MLP) approach draws attention to ways in which these configurations change to fulfil societal functions. It has been used in the western world in historical case studies to trace socio-technical changes. What is less clear is how a combination of discrete initiatives can support a transition in a less economically developed country. The multi-level perspective (MLP) has been used to study transition from one socio-technical system to another. MLP in this study is applied to a single sector; the construction sector in Botswana, to analyse the contribution of a combination of different initiatives to an ongoing sustainable transition. These include a demonstration project, environmental impact assessment and a new construction board. Opportunities and challenges that have/are being created by the initiatives are examined in the process. A mixture of in-depth interviews and document analysis were used to study the impact of the initiatives on the construction sector, the reasons for these effects and their implications for the transition to sustainable construction. It has been found that the initiatives influenced on-going efforts towards professionalization in the construction sector. Other effects include introduction of new governance techniques from abroad, produced a valuable ecological profiling of the country and transferring knowledge and technology both within and outside of BOTEC. The MLP focus on visions, learning and development of social networks in transitions have been used to help account for these outcomes. The analysis identified a number of opportunities that the initiatives introduced, but which have yet to be fully exploited. The contribution of the thesis is a reflection on the potential of discrete innovations to contribute to a transition and the use of MLP to study an emergent transition in the construction sector in Botswana.",,Ntshwene K,,2019,,,,Ph.D. Thesis
MobiWac '07: Proceedings of the 5th ACM International Workshop on Mobility Management and Wireless Access,"We are pleased to welcome you to the 5th ACM International Workshop on Mobility Management and Wireless Access -- MobiWac 2007. We are proud of the great success of highly productive MobiWac workshops held in recent years in Fort Worth (TX), Philadelphia (PA), Maui (Hawaii), and Torremolinos (Spain). The 2007, MobiWac workshop continues to maintain the high standards set by its predecessors. MobiWac provides an excellent venue to discuss and present original ideas, recent results and achievements by researchers, students, designers, and system developers on various issues and challenges related to mobility management and wireless access. We hope this workshop will continue to fulfill its primary mission which is to provide active researchers and practitioners a unique opportunity to share and exchange their ideas and solutions with others while enjoying a stimulating program.The call for papers attracted 44 submissions from 25 countries (from Asia, North America, South America, Australia, Europe, and Africa). The program committee accepted 18 papers. In addition, the program also includes a poster session that hosts 10 relevant short papers. The accepted papers cover a variety of topics, including localization and tracking, wireless sensor networks, Internetworking, 3G/4G, Wireless Local Area Networks (WLANs), mobility management, modeling, performance analysis, Quality of Service (QoS) and scheduling, power management, cross-layer design, secure protocols, and privacy management. The accepted papers are from 21 countries demonstrating the strong international flavor and stature of the workshop.Based upon the MobiWac TPC recommendations, the following three papers have been selected as candidates for the MobiWac 2007 Best Paper Award: ""A Proposal for Unifying Mobility with Multi-homing, NAT, and Security,"" Randall Atkinson, Saleem Bhatti, Stephen Hailes""An Efficient Link Management Algorithm for High Mobility Mesh Networks,"" Husnain Mansoor Ali, Amina Naimi, Anthony Busson, Véronique Vèque""Evaluation of Impact Factors on RSS Accuracy for Localization and Tracking Applications,"" Tsenka Stoyanova, Fotis Kerasiotis, Aggeliki Prayati, George Papadopoulos.We are pleased to announce the Winner of the Best Paper selected for MobiWac 2006, in Torremolinos, Spain: ""Diagnosing Mobile Ad Hoc Networks: Two Distributed Comparison-Based Self-Diagnosis Protocols,"" by Mourad Elhadef, Azzedine Boukerche, Hisham Elkadiki.",,,,2007,,,,Book
Q2SWinet '20: Proceedings of the 16th ACM Symposium on QoS and Security for Wireless and Mobile Networks,"With great pleasure, we welcome you to the 2020 ACM symposium on QoS and Security for Wireless and Mobile Networks (ACM Q2SWinet 2020). This year edition was initially planned to be held at Alicante (Spain), but unfortunately, due to Covid-19 pandemic, we moved to remote meeting through video-conferencing tools. Despite this world-wide pandemic situation, Q2SWinet is still attracting academic and industrial interest and confirmed its place as a known meeting point for exchanging ideas, sharing experiences and discussing solutions among researches and professionals. This year's edition proposes an exciting program covering broad issues on QoS and security in wireless and mobile networked systems as well as in IoT systems.The call for paper attracted submissions from Europe (France, Italy, etc.), North America (USA and Canada), South America (Brazil, etc.), Asia (China, etc.) and Africa as well, which demonstrate the international dimension of the symposium. The program committee reviewed all papers and accepted 17 technical papers for presentation, which represents an acceptance ratio of 27%.",,,,2020,,,,Book
Mobile Agent-Based Approach for Modeling the Epidemics of Communicable Diseases,"The increase in the use of mobile phones generates the formation of mobile social networks which can make use of various purposes including education, public health and controlling epidemics. Social networks consist of the basic building blocks called as the communities within which the social interactions are intensive, but between which they are very weak. Everyone could observe that the spread of infectious disease inside communities often has the ability to cross countries borders and spread rapidly. With the widespread of diseases causing major public health problem, we argue that human mobility patterns not only influence the spreading, but are also useful for preventing and creating awareness of the diseases. In this paper, we present new opportunities offered by the field of mobile social networks for understanding the spread of infectious diseases. For this purpose we propose two models namely MABM (Mobile Agent Based Model) and SDC (Spread Discovery Control) model to understand the spread of communicable diseases between different regions. The proposed SDC model is used to comprehend the spread of diseases by extracting the community structures and the analysis of mobility pattern of each agent (user) within the mobile network. Moreover, the understanding of spread details helps us to propose the control strategy to avoid the spread of the epidemic disease on the specific region. To realize our proposed models in a better way, we have modeled one such communicable disease usually spreading every year in West African region.",,"Saravanan M,Karthikeyan P,Arathi A,Kiruthika M,Suganya S",,2013,16–20,10.1145/2492517.2492612,https://doi-org.proxy.bnl.lu/10.1145/2492517.2492612;http://dx.doi.org/10.1145/2492517.2492612,Conference Paper
SignSupport: A Mobile Aid for Deaf People Learning Computer Literacy Skills,This paper discusses a prototype of a learning aid on a mobile phone to support Deaf people learning computerliteracy skills. The aim is to allow Deaf people to learn at their own pace which in turn reduces the dependenceon a teacher to allow weaker learners be assisted. We studied the classroom dynamics and teaching methods toextract how lesson content is delivered. This helped us develop an authoring tool to structure lesson content forthe prototype. A prototype has been developed using South African Sign Language videos arranged accordingto the structure of pre-existing lessons. The technical goal was to implement the prototype on a mobile deviceand tie the resulting exported lesson content from the authoring tool to a series of signed language videos andimages so that a Deaf person can teach him/herself computer literacy skills. Results from the user testing foundthe prototype successful in allowing Deaf users to learn at their own pace thereby reducing the dependence onthe teacher.,,"G. Ng'ethe G,H. Blake E,Glaser M",,2015,501–511,10.5220/0005442305010511,https://doi-org.proxy.bnl.lu/10.5220/0005442305010511;http://dx.doi.org/10.5220/0005442305010511,Conference Paper
Building the Global Information Economy: A Roadmap from the Global Information Infrastructure Commission,"From the Publisher: As trade in goods and services becomes increasingly integrated, information and communications technologies have fundamentally altered the nature of global markets, transforming economic and social interactions, redefining work and causing shifts in labor markets. Factors that have spurred these phenomena include falling costs, rapid technological development and convergence between technologies. Technological change is occurring in a far shorter time frame than policies are able to respond, while an increasingly borderless world challenges national sovereignty. In the meantime, information gaps continue to exist between the developed and developing world, with the potential to disenfranchise entire communities who are at the periphery of the information revolution. This publication describes how globalization and rapid technological change define the parameters of the new global information economy, presenting huge opportunities, but also exposing countries and individuals to tremendous structural change, uncertainty and risk. Drawing upon the lessons learned by the Global Information Infrastructure Commission (GIIC) during its first three years, the GIIC presents a roadmap to governments and private sector players on how they can draw upon the huge opportunities of this revolution. According to the GIIC, four building blocks are needed to compete in the information age: (1) a basic information infrastructure at the national and regional levels to support applications and services; (2) a ""soft infrastructure"" of flexible regulatory frameworks and institutions responsive to changing technologies and increased demand needs to be developed; (3) an educated population toestablish and sustain a global competitive environment; (4) international cooperation and coordination to achieve a secure, efficient, cost-effective and seamless global communications environment.",,"Charles CA,Furar LE",,1998,,,,Book
"When the Playing Fields Aren't Even: Personalised Attention in the Multilingual, Varied-Ability Classroom","This paper describes an English-language application developed within an educational system undergoing radical change. Since the mid-1990s, South African education has moved from a dual system favouring one privileged class, to one that embraces integration and equality. This move has been complex and fraught with the difficulties inherent in incorporating disadvantaged pupils from multilingual backgrounds into previously-advantaged' classrooms with English as principle medium of tuition. In order to cope with widely-varying student needs, an interactive computer-based application has been created to enable students to diagnose their own specific weaknesses and to provide personalised assistance in overcoming them. Results demonstrate that the system is highly effective in improving students' performance, establishing a closer sense of personal attention, and in alleviating the pedagogical stressesexperienced by instructors in an extremely demanding educational environment.",,"Jacobs G,Meyer D",,2002,1346,,,Conference Paper
Two Methods for Large-Scale Nonlinear Optimization and Their Comparison on a Case Study of Hydropower Optimization,"This paper presents two methods for the optimization of structured large-scale problems: a decomposition method of dual type for nonlinear problems and a sequential quadratic programming based method. Practical details of application of the methods to the case study problem of the hydropower system of an African river are then given. Comparison of results is presented, indicating that both methods are useful and efficient, having however different features from a practical point of view. General remarks concerning the practical differences between a decomposition-based method and a method exploiting the problem structure within the framework of general purpose optimization routines are finally presented.",,"Arnold E,Tatjewski P,Wołochowicz P",,1994,221–248,,,Journal Article
The Digital Turn in Radio: A Critique of Institutional and Organizational Modeling of New Radio Practices and Cultures,"This article conducts a critical analysis of the use of Internet and mobile phone technologies by Capital radio in Malawi. It examines the uses of the Internet, social networking sites and mobile text-messaging by the radio station. Three central questions constitute the major concerns of the article: (a) To what extent do institutional and organizational contexts shape the uptake and uses of the Internet and mobile phones by radio journalists? (b) How do the uses of the Internet and mobile phones in turn influence the institutional cultures and organizational practices? (c) To what extent, if at all, does radio convergence reconfigure traditional radio to create new spaces that augment audience participation? The article argues that any meaningful critique of the technological affordances to the radio institution must critically engage with the complex questions of the dialectical relationship between technology, structure, and agency especially given the seductive myth of the so- called new media. It concludes that digital media technologies on radio are subject to organisational, institutional, and social shaping, and that questions about the emancipatory power of these technologies especially to audiences and citizens are often exaggerated because the question of power relations between actors or interests is often overlooked. The digital turn and the demotic turn on radio therefore must not be seen as synonymous with the participatory turn, especially in African countries where the regulation of corporate power in mass media is weak and where multiple forms of the digital divide that impede on consistent and meaningful use of digital media still persist.",,Moyo L,,2013,214–222,10.1016/j.tele.2012.10.003,https://doi-org.proxy.bnl.lu/10.1016/j.tele.2012.10.003;http://dx.doi.org/10.1016/j.tele.2012.10.003,Journal Article
"South Africa Crime Visualization, Trends Analysis, and Prediction Using Machine Learning Linear Regression Technique","South Africa has been classified as one of the most homicidal, violent, and dangerous places across the globe. However, the two elements that pushed South Africa high in the crime rank are the rates of social violence and homicide. It was reported by Business Insider that South Africa is among the most top 15 ferocious nations on earth. By 1995, South Africa was rated the second highest in terms of murder. However, the crime rate has reduced for some years and suddenly rose again in recent years. Due to social violence and crime rates in South Africa, foreign investors are no longer interested in continuing or starting a business with the nation, and hence, its economy is declining. South Africa’s government is looking for solutions to the crime issue and to redeem the image of the country in terms of high crime ranking and boost the confidence of the investors. Many traditional approaches to data analysis in crime-related studies have been done in South Africa, but the machine learning approach has not been adequately considered. The police station and many other agencies that deal with crime hold a lot of databases that can be used to predict or analyze criminal happenings across the provinces of South Africa. This research work aimed at offering a solution to the problem by building a model that can predict crime. The machine learning approach shall be used to extract useful information from South Africa's nine provinces' crime data. A crime prediction system that can analyze and predict crime is proposed. To accomplish this, South Africa crime data on 27 crime categories were obtained from the popular data repository “Kaggle.” Diverse data analytics steps were applied to preprocess the datasets, and a machine learning algorithm (linear regression) was used to build a predictive model to analyze data and predict future crime. The appropriate authorities and security agencies in South Africa can have insight into the crime trends and alleviate them to encourage the foreign stakeholders to continue their businesses.",,"Obagbuwa IC,Abidoye AP,Daneshvar Rouyendegh (B. Erdebilli) B",,2021,,10.1155/2021/5537902,https://doi-org.proxy.bnl.lu/10.1155/2021/5537902;http://dx.doi.org/10.1155/2021/5537902,Journal Article
Q2SWinet'19: Proceedings of the 15th ACM International Symposium on QoS and Security for Wireless and Mobile Networks,"We welcome you to the 2019 ACM symposium on QoS and Security for Wireless and Mobile Networks (ACM Q2SWinet 2019), to be held at Miami Beach (USA). Continuing in the tradition of Q2SWinet symposium series, which have been established as well known meeting points for exchanging ideas, sharing experiences and discussing solutions among researches and professionals, this year's edition proposes an exciting program covering broad issues on QoS and security in wireless and mobile networked systems.The call for paper attracted submissions from North-America (USA and Canada), South-America (Brazil, etc.), Europe (France, Italy, etc.), Asia (China, etc.) as well as Africa, which shows the international dimension of the symposium. The program committee reviewed all papers and accepted 13 technical papers for presentation, which represents an acceptance ratio of roughly 25%.We thank all the actors of this Q2SWinet edition: firstly, authors for providing the content of the program; secondly, members of the program committee for their valuable efforts in reviewing papers and providing feedback for authors.",,,,2019,,,,Book
STC '08: Proceedings of the 3rd ACM Workshop on Scalable Trusted Computing,"It is our great pleasure to welcome you to the Third ACM Workshop on Scalable Trusted Computing (ACM STC'08). This year's symposium builds on the success of ACM STC'06 and STC'07 as a premier forum for presentation of research results and experience reports on leading edge issues of fundamental technologies of trusted computing and its applications in large-scale systems.The call for papers attracted 34 submissions from Asia, Europe, Africa, and the United States. The program committee accepted 9 papers that cover a variety of topics, including architecture and implementation technologies for trusted platforms, remote attestation of trusted devices, and cryptographic support in trusted computing. In addition, the program includes two keynote speakers, Peter Loscocco from the National Security Agency and Sean Smith from the Dartmouth College. We hope that these proceedings will serve as a valuable reference for security researchers and developers.",,,,2008,,,,Book
Europe and MENA Cooperation Advances in Information and Communication Technologies,"This book contains a selection of articles from The Europe, Middle East and North Africa Conference on Technology and Security to Support Learning 2016 (EMENA-TSSL'16), held between the 3th and 5th of October at Saidia, Oujda, Morocco. EMENA-TSSL'16 is a global forum for researchers and practitioners to present and discuss recent results and innovations, current trends, professional experiences and challenges in Information & Communication Technologies, and Security to support Learning. The main topics covered are: A) Online Education; B) Emerging Technologies in Education; C) Artificial Intelligence in Education; D) Gamification and Serious games; E) Network & Web Technologies Applications; F) Online experimentation and Virtual Laboratories; G) Multimedia Systems and Applications; H) Security and Privacy; I) Multimedia, Computer Vision and Image Processing; J) Cloud, Big Data Analytics and Applications; K) Human-Computer Interaction; L) Software Systems, Architectures, Applications and Tools; M) Online Languages and Natural Language Processing N) E-content Development, Assessment and Plagiarism; O) Secure E-Learning Development and Auditing; P) Internet of Things and Wireless Sensor Networks.",,"Rocha Á,Serrhini M,Felgueiras C",,2016,,,,Book
A Roadmap to Proliferate Open Source Software Usage within SA Government Servers,"Open Source software (OSS) is increasingly being recognized by the government sector around the world as a viable choice to proprietary software, particularly in a number of areas of information technology (IT) such as on the network servers. In the OSS domain, it is perceived that OSS has the potential to deliver better value for money, high quality software, secure, flexible, stable and reliable network applications. The South African (SA) government acknowledges that OSS is a viable alternative to proprietary software especially on the servers. According to the data collected (survey) from various SA government departments and agencies, indications are that OSS is not fully implemented on the network servers, although the global trends indicate high usage of OSS within the network environment. The main aim of this paper is to propose a roadmap that can be used to aid SA ministries to increase OSS usage.",,"Mtsweni J,Biermann E",,2008,430–436,10.1109/BROADCOM.2008.82,https://doi-org.proxy.bnl.lu/10.1109/BROADCOM.2008.82;http://dx.doi.org/10.1109/BROADCOM.2008.82,Conference Paper
Ubuntu Unleashed 2014 Edition: Covering 13.10 and 14.04,"Ubuntu Unleashed 2014 Edition is filled with unique and advanced information for everyone who wants to make the most of the Linux-based Ubuntu operating system. This new edition has been thoroughly revised and updated by a long-time Ubuntu community leader to reflect the exciting new Ubuntu 13.10 and the forthcoming Ubuntu 14.04. Former Ubuntu Forum administrator Matthew Helmke covers all you need to know about Ubuntu 13.10/14.04 installation, configuration, productivity, multimedia, development, system administration, server operations, networking, virtualization, security, DevOps, and moreincluding intermediate-to-advanced techniques you wont find in any other book. Helmke presents up-to-the-minute introductions to Ubuntus key productivity and Web development tools, programming languages, hardware support, and more. Youll find new or improved coverage of Ubuntus Unity interface, various types of servers, software repositories, database options, virtualization and cloud services, development tools, monitoring, troubleshooting, Ubuntus push into mobile and other touch screen devices, and much more. Matthew Helmke served from 2006 to 2011 on the Ubuntu Forum Council, providing leadership and oversight of the Ubuntu Forums, and spent two years on the Ubuntu regional membership approval board for Europe, the Middle East, and Africa. He has written about Ubuntu for several magazines and websites and is the lead author of The Official Ubuntu Book. He works for Pearson Education writing technical documentation for educational testing software. Detailed information on how to Configure and customize the Unity desktop Get started with multimedia and productivity applications, including LibreOffice Manage Linux services, users, and software packages Administer and run Ubuntu from the command line Automate tasks and use shell scripting Provide secure remote access and configure a secure VPN Manage kernels and modules Administer file, print, email, proxy, LDAP, DNS, and HTTP servers (Apache, Nginx, or alternatives) Learn about new options for managing large numbers of servers Work with databases (both SQL and the newest NoSQL alternatives) Get started with virtualization Build a private cloud with Juju and Charms Learn the basics about popular programming languages including Python, PHP, Perl, and new alternatives such as Go and Rust Learn about Ubuntus work toward usability on touch-screen and phone devices Ubuntu 13.10 on DVD DVD includes the full Ubuntu 13.10 distribution for Intel x86 computers as well as the complete LibreOffice office suite and hundreds of additional programs and utilities. Free Kick Start Chapter! Purchase this book and receive a free Ubuntu 14.04 Kick Start chapter after Ubuntu 14.04 is released. See inside back cover for details",,Helmke M,,2013,,,,Book
Flexicurity for Investment Reimbursement of Micro Renewable Electric Energy Systems,"Even the most affordable renewable energy installation still needs an investment that is significant for local people, so that a co-financing party is often indispensable. This article investigates through field research in Tanzania and a technology survey, whether technology could be able to support such investment schemes. It would secure reimbursements in the same flexible and secure way people now pay for mobile communication services, thereby applying the success factors of mobile communications in Africa to micro renewable electric energy systems. Further areas for investigation are identified.",,"Van Acker B,Van Acker C,Van Acker V",,2012,149–154,10.1109/GHTC.2012.32,https://doi-org.proxy.bnl.lu/10.1109/GHTC.2012.32;http://dx.doi.org/10.1109/GHTC.2012.32,Conference Paper
FMSE '04: Proceedings of the 2004 ACM Workshop on Formal Methods in Security Engineering,"This volume contains the proceedings of the Second ACM Workshop on Formal Methods in Security Engineering (FMSE 2004) held in Washington D.C., October 29th, in conjunction with the 11th ACM Conference on Computer and Communications Security.The purpose of FMSE is to bring together researchers and practitioners from both the security and the software engineering communities, from academia and industry, who are working on applying formal methods to designing and validating large-scale security-critical systems. The scope of the workshop covers security and formal-methods related aspects of: security specification techniques, formal trust models, combination of formal techniques with semi-formal techniques like UML, formal analyses of specific security properties relevant to software development, security-preserving composition and refinement of processes, faithful abstractions of cryptographic primitives and protocols in process abstractions, integration of formal security specifications, as well as refinement and validation techniques in development methods and tools.The paper selection process was very competitive this year. The call for papers attracted 25 submissions from Asia, Canada, Europe, Africa, and the United States. The program committee accepted 9 papers for presentation at the workshop, which means that many high-quality papers had to be rejected. In addition, the program included an invited talk on ""Security Analysis of Network Protocols"" by John C. Mitchell as well as an invited talk on ""Model-driven development of Security Components"" by Prem Devanbu.",,,,2004,,,,Book
The Concept of Family Re-Enactment in Counselling Psychologists' and Psychotherapists' Training Groups,"This Portfolio contains a summary · of my work as a Counselling Psychologist in training. Although it is unable to provide a complete account of all my therapeutic work over the last 3 years, I hope it will give the reader a taste of my experience and how I relate to both practice and research. All clients referred to throughout the portfolio have been assigned a synonym in order maintain their anonymity. Before I begin, I feel it is appropriate that I give some insight into the reasons that drew me into training and a career as a counselling psychologist. My academic journey started 8 years ago when I began my psychology degree at Roehampton Institute. This had followed a particularly difficult time for me in which I had struggled to find a professional identity and career that I felt I could be happy with. Over the 3 years my research has focused on aspects of group therapy, my literature review explore.cl the concept of late-onset alcoholism and the effects of group therapy on an older population. This was perhaps driven by an earlier focus on lifespan development and ongoing transitions across the human lifecycle. My qualitative research examined the perceptions of Counselling Psychologists' and Psychotherapists' experience in training groups and the impact that they have on interpersonal learning and practitioner development. From my perspective, one of the most interesting findings was the concept of family re-enactment within the group and this formed the basis of my 3rd year quantitative study. I feel I was drawn to the concept of groupwork for a number of 'reasons. One of these was perhaps the very reason that attracted me to Counselling Psychology in the first place. As a child I spent a number of years at a boarding school where my experience was not a particularly happy one. At the time I felt that there was no escape, I felt both lonely and homesick and equally experienced a sense of shame at my inability to cope with the situation. This led me to consider the possible reasons for why people behave and react as they do, and whether earlier patterns of relating and experience are carried forwards into the future. Although the focus of my study was on family reenactment it is equally possible that alternative group situations may help construct an individual's communication style for example those with peers at school. I asked myself whether if an individual has . been a victim, a leader or a helper in the past whether this would be relived in a group. My focus on Psychotherapists and Counselling Psychologists was largely due to my belief that this population, through the rigours of their training, and the emphasis on development of self awareness and personal insight would be able to engage in a profound self-reflection on their experiences and emotions. I found that my focus on groups was of a particular value in my second year in which I had the opportunity of co-facilitating a_ psychotherapy group based on the ""Tavistock Tradition"". It was interesting for me as a trainee to examine some of the emotions that were stirred up by the group and reflect on how I may have interpreted these both in relation to my past and my research. It also allowed me to think about how different individuals relate in a group and how their interactions effect others whether this be for better or worse. Over the course of the last 3 years I have had the opportunity of working in a number of different settings. My first placement was based in a large multinational organisation, and for me, with some background in occupational psychology this proved to be enlightening. The clients I worked with came from a range of cultural and ethnic backgrounds, which included British, European, African, Asian and American. I felt this was important for me since it gave me an opportunity to reflect on how I a white British male may be seen by any of these other cultures and what implications this may have for my therapeutic work with clients. My clients were able to give me a valuable insight into their emotions, which often included anxiety due to difficulties at work and problems with colleagues that impacted on their wider lives. They were in some cases also able to provide me with an insight into what it might be like working in a culture that appeared alien to them. I found it useful to reflect on these issues in terms of my own previous experience, my research and my countertransference. Equally from a theoretical and academic perspective this led me to explore some of the models surrounding anxiety, and I produced a paper exploring the Freudian and Kohutian interpretation of its origin and treatment. My second year placement was in an adult psychotherapy unit, which adopted a psychodynamic approach in its work. I had four clients, one male and three female with whom I worked on a weekly basis for approximately 9 months. During this time I was able to think about my relationship with them and many of the ethical and professional issues that arose, many of these surrounded aspects of gender and age. In turn, this led me to contemplate the phenomenon of erotic transference, which I examined in a paper in my second year. I felt this was an appropriate time to explore this concept since it presented a particular challenge in relation to one of my female clients. In addition I felt it would provide me with an ""armoury"" and an ethical foundation for my future work as a Counselling Psychologist, which I believe to be an important and valuable part of my practice. Initially I had felt some trepidation in working with certain client populations. This became real for me in my third year in which I had two separate placements; the first was in a combined forensic and learning disability department that operated from a cognitive behavioural perspective. My clients were mainly learning disabled and my anxieties in working with this group stemmed from my own experience with dyslexia and whether we would both end up confusing each other. I frequently wondered whether the countertransference would in some way disempower or deskill me and was able to reflect on how these may be prominent concerns for the learning disabled. As I read more, I found that it was important that the cognitive model was applied within a much broader person centred approach (Emerson, Hatton, Bromley & Caine 1998), and I discovered that I felt more comfortable with this. From a cognitive behavioural perspective I am drawn to this when I consider the possible difficulties or stumbling blocks that may present themselves within the therapeutic relationship, and this I have considered in a paper in my academic dossier. Overall a person centred approach would seem to sit more easily with my belief that the three core conditions of empathy, congruence and unconditional positive regard (Rogers 1961) are central aspects facilitating positive change in therapy. I believe these are important ingredients of any model of therapy that I may wish to work within in the future. My second placement focused on working with mentally disordered offenders who had committed violent crimes; previously this had been an area of concern for me since I thought the experience would be profoundly disturbing. However, instead it has been extremely rewarding in that it has taught me more about human nature. I feel it has given me a greater insight into why people may commit crimes and a better understanding of their predicament. It has also enlightened me to the notion that very few people are beyond help or understanding, and that the application of cognitive behavioural therapy within a person centred framework can build a relationship even with those who have committed particularly violent acts. I hope this goes some way to outlining the content of this portfolio and clarifying the direction of my research and therapeutic work both over the last 3 years in my training and in my future work as a Counselling Psychologist.",,Hale A,,2006,,,,Ph.D. Thesis
Re-Architecting Internet Access and Wireless Networks for Rural Developing Regions,"In this work we focus on unique Internet connectivity challenges and opportunities in rural areas, especially in developing regions. Providing access in rural areas is particularly challenging due to its unique set of social and technical constraints. In order to quantify the challenges being faced by Internet users in rural areas, we conducted on-site and on-line interviews and analysed network traces from a rural network in Macha, Zambia. Our findings reveal severe local and global connectivity limitations that are unique to rural areas. Local wireless networks experience interference and packet loss due to poor network designs and limitations of WiFi in rural areas. Bandwidth-restricted Internet gateways are constantly congested during normal daytime usage periods. Users describe cost, limited availability and unreliability of Internet access as key barriers to on-line interaction. These obstacles prevent leisurely access to the Internet, including, amongst others, users generating and sharing media. This leads to a more transaction-like or ""deliberate interaction"" model for many rural users. Usage analysis reveals unique behaviour for users in rural areas. Most traffic is web-based traffic as opposed to peer-to-peer traffic in developed countries. Social media features even more prominently than in western countries, with Facebook being the most visited website; users are twice as likely to access Facebook than Google search. Further analysis of Facebook traffic shows a high degree of traffic between local users in the village. These unique patterns and constraints form the basis for ICT solutions we propose for rural regions. In order to avoid using congested rural Internet gateways, we propose a set of techniques to localize traffic. VillageShare, a Facebook-based localization application, facilitates file sharing amongst users in the village without the need to send media over the bandwidth-constrained gateway. VillageShare is also capable of time-shifting uploads to off-peak usage periods in order to avoid upload failures. In order to exploit locality of interest in mobile phone usage, we designed VillageCell, an open-source, low-cost pico-cell-like base station, that allows users to make free local cellular calls in a village. It takes advantage of the very high penetration rate of mobile phones in rural Africa, which occurs even though many villages lack cellular coverage. VillageCell is also able to support SMS to instant message client exchange as well as routing calls between VillageCell phones and phones on the public switched telephone network. The low population density and large village diameters in rural areas of Africa imply a need for novel solutions to spread wireless connectivity to individual homes. Current solutions, based on 802.11 require clear line-of-sight and have limited range. Those based on WiMax are not suitable due to high licence and deployment costs. We propose to use the recently freed TV spectrum bands known as ""white spaces"", encompassing frequencies from 52MHz to 698MHz, to cover vast distances in rural areas. Our solution, VillageLink, builds on the existing 802.22 white space standard to optimally utilize white space spectrum. We add a feature that allows base stations to allocate optimal channels using inter-cell probing across all available TV channels. Channel probing is critical as frequency selectivity is more dependent on antenna characteristics and non-linearity of RF components in the system than on the free-space propagation laws in the white space band. In order to evaluate our interventions, we deployed VillageShare and VillageCell in Macha, Zambia and evaluated VillageLink using simulations built on a real 3 km white space link in South Africa. We collected usage logs of these systems both through quantitative and qualitative studies. For qualitative studies we involved users in an iterative design process and made use of on-line interviews to understanduser perception of our solutions. Ultimately, we hope that this research will lead to better penetration of wireless networks and improved network performance for users in rural villages, culminating in a more inclusive and representative Internet that truly reflects all languages and cultures in the world. (Abstract shortened by UMI.)",,Johnson DL,,2013,,,,Ph.D. Thesis
Exploring Factors of Employability among African American Non-Veteran Homelessness: A Narrative Case Study Analysis,"Employment is vital to an individual's existence and oftentimes enhances one's wellbeing. However, individuals with disabilities and individuals experiencing homelessness often encounter difficulties in seeking, securing, and maintaining vocational outcomes. Utilizing a conceptual framework of Social Cognitive Career Theory and Chaos Theory of Careers, this study aimed to explore the experiences and factors among African American non-veteran individuals who identified as homeless at time of application for federal-state vocational rehabilitation supports. Participants were recruited from the North Carolina Division of Rehabilitation Services (DVRS) and reflected a total of three respondents. Results among the within-case analysis reflected the lack of relationship alliance, morality and self-determination, and diminished self-image to aid or impact employability. Cross-case analysis reflected the presence of isolation, chaos, and a lack of supports and resources as participants navigated services from vocational rehabilitation. Implications for rehabilitation counselors and rehabilitation counselor educators were identified. Additionally, limitations to this study and future research were discussed.",,"Battle TR,Whittaker TT,Dowden A,Brooks M,Moore C,Colleran H",,2020,,,,Ph.D. Thesis
FMSE '06: Proceedings of the Fourth ACM Workshop on Formal Methods in Security,"This volume contains the proceedings of the Fourth ACM Workshop on Formal Methods in Security Engineering (FMSE'06) held in Fairfax, Virginia, November 3rd 2006, in conjunction with the 13th ACM Conference on Computer and Communications Security.Information security has become a crucial concern for the commercial deployment of almost all applications and middleware. Although this is commonly recognized, the incorporation of security requirements in the software development process is not yet well understood. The deployment of security mechanisms is often ad hoc, without a formal security specification or analysis, and practically always without a formal security validation of the final product. Progress is being made, but there remains a wide gap between high-level security models and actual code development.The purpose of FMSE is to bring together researchers and practitioners from both the security and the software engineering communities, from academia and industry, who are working to apply formal methods to the design and validation of large-scale systems. The scope of the workshop -- as indicated by the call for papers -- covers the security and formal methods aspects of: security specification techniques, formal trust models, combination of formal techniques with semi-formal techniques like UML, formal analyses of specific security properties relevant to software development, security-preserving composition and refinement of processes, symbolic and computational models of security protocols, integration of security aspects into formal development methods and tools, access control policies, information flow, risk management and network security, formal analysis of firewalls and intrusion detection systems, trusted computing, and case studies.As for previous years, the paper selection process was very competitive. Our call for papers attracted 21 submissions from Asia, North Africa, Canada, Europe, Russia, and the United States. The program committee accepted 7 papers for presentation at the workshop. Many high-quality papers had to be rejected. In addition, the program includes invited talks from Joshua Guttman and Steve Zdancewic.",,,,2006,,,,Book
IWSPA '21: Proceedings of the 2021 ACM Workshop on Security and Privacy Analytics,"It is our great pleasure to welcome you to the 2021 ACM International Workshop on Security and Privacy Analytics (IWSPA 2021). This year's workshop is the seventh in the series and is co-hosted with the Eleventh ACM Annual Conference on Data and Application Security and Privacy (CODASPY 2021).IWSPA addresses important research topics associated with the application of data analytics tools (including statistical, machine learning, data mining, and natural language processing) to challenges that arise with security and privacy preservation. IWSPA provides a forum for the interaction between researchers in these areas, identifying and pursuing new topics that arise in the intersection between the fields of Artificial Intelligence and Cybersecurity.The IWSPA 2021 call for papers attracted 22 papers from four continents (Africa, Asia, Europe and North America). Each paper considered for presentation was evaluated by three reviewers, who were either committee members or assigned by committee members. The reviews were detailed and examined various aspects of the papers, including correctness and presentation. Five papers were accepted for presentation as full papers (11-page limit) and three were accepted as short papers (7-page limit).We thank the authors, reviewers, and program committee members, whose enthusiastic efforts make the workshop possible, and are critical in its success. We also thank the CODASPY publicity chairs, workshop chair and general chair, as well as the ACM Special Interest Group on Security, Audit and Control (SIGSAC), for supporting IWSPA '21. Special thanks are also due to Rakesh Verma, former IWSPA chair, for his guidance during the entire process of organizing IWSPA '21.",,,,2021,,,,Book
The Urgency for Investment on Local Data for Advancing Food Assessments in Africa: A Review Case Study for APSIM Crop Modeling,,,"Carcedo AJ,Vieira Junior N,Marziotte L,Correndo AA,Araya A,Prasad PV,Min D,Stewart ZP,Faye A,Ciampitti IA",,2023,,10.1016/j.envsoft.2023.105633,https://doi-org.proxy.bnl.lu/10.1016/j.envsoft.2023.105633;http://dx.doi.org/10.1016/j.envsoft.2023.105633,Journal Article
MobiWac '11: Proceedings of the 9th ACM International Symposium on Mobility Management and Wireless Access,"On behalf of the Technical Program Committee, it is our great pleasure to welcome you to the 9th ACM International Symposium on Mobility Management and Wireless Access - MobiWac '11 at Miami Beach, FL, USA. Following the successful previous events in Fort Worth (TX), Philadelphia (PA), Maui (Hawaii), Torremolinos (Spain), Chania (Greece) and Bodrum (Turkey), this year's symposium was created to advance our knowledge in mobility and wireless access, including models, systems, applications, and theory. The mission of the symposium is to share novel mobility management and wireless network solutions and identify new directions for future research and development.A number of people have contributed, creating a strong technical program. First of all, the call for papers attracted 58 submissions from Asia, Australia, Africa, Europe and North /South America. Second, a number of researchers served on the MobiWac'11 Technical Program Committee. They helped to evaluate paper submissions in a rigorous and fair, yet timely, manner and contributed significantly to the strength of the technical program. The program committee accepted 15 regular papers which represents an acceptance rate of 25.8%. These papers cover a variety of topics, including general wireless networks, mesh and vehicular networks, network mobility, optimization, and security administration. The program committee further accepted 6 papers as short papers. All submissions were fully peer reviewed. Finally, 12 poster/demo papers (solicited both by an open call and via invitations) have been selected. We hope that these proceedings will serve as a valuable reference for mobile and wireless systems researchers and developers.Based upon the MobiWac TPC recommendations, the following 3 papers have been selected as candidates for the MobiWac 2011 Best Paper Award: Tree-Based Double-Covered Broadcast for Wireless Ad Hoc Networks Weisheng Si; Roksana Boreli; Anirban Mahanti; Albert ZomayaSoftware-related Energy Footprint of a Wireless Broadband Module Mikael Asplund; Anton Thomasson; Ekhiotz Jon Vergara; Simin Nadjm-TehraniDegree of Node Proximity: a Spatial Mobility Metric for MANETs Elmano Ramalho Cavalcanti; Marco Aurelio Spohn",,,,2011,,,,Book
Enhancing E/M-Government Synergy in Kenya: Citizens’ Perspectives on the Driving Factors for M-Government Diffusion,"This research investigated the driving factors of mobile government diffusion as antecedents that expand the specificity and explanatory power of traditional technology adoption models in mobile contexts. A robust conceptual model for evaluating the adoption of electronic and mobile government services is proposed. Focusing on the first component of the model, namely, the drivers of m-government diffusion, five innovation attributes that influence the decision to adopt m-government were examined. Following a qualitative approach based on grounded theory, 91 mobile phone users were interviewed using a questionnaire. Five attributes of diffusion of innovation were investigated: relative advantage, complexity, compatibility, trialability, and observability. The qualitative data obtained was coded and analyzed for theme frequency distribution. The driving factors that emerged from the themes were: accessibility, efficiency, connectivity and time-saving (dominant factors); convenience, user-friendly, features and service provider (moderate factors), and cost and security (minor factors). Future research should consider how the key driving factors for m-government diffusion can be leveraged to facilitate greater adoption of and synergy between e- and m-government. Empirical validation of the conceptual model is recommended to confirm its appropriateness in enhancing the adoption of electronic and mobile government services in Sub-Saharan Africa.",,"Wakhu SM,Fuyuan X,Kakonge JO",,2020,121–137,10.1007/978-3-030-50350-5_11,https://doi-org.proxy.bnl.lu/10.1007/978-3-030-50350-5_11;http://dx.doi.org/10.1007/978-3-030-50350-5_11,Conference Paper
Admixture Aberration Analysis: Application to Mapping in Admixed Population Using Pooled DNA,"Admixture mapping is a gene mapping approach used for the identification of genomic regions harboring disease susceptibility genes in the case of recently admixed populations such as African Americans We present a novel method for admixture mapping, called admixture aberration analysis (AAA), that uses a DNA pool of affected admixed individuals We demonstrate through simulations that AAA is a powerful and economical mapping method under a range of scenarios, capturing complex human diseases such as hypertension and end stage kidney disease The method has a low false-positive rate and is robust to deviation from model assumptions Finally, we apply AAA on 600 prostate cancer-affected African Americans, replicating a known risk locus Simulation results indicate that the method can yield over 96% reduction in genotyping Our method is implemented as a Java program called AAAmap and is freely available.",,"Bercovici S,Geiger D",,2010,31–49,10.1007/978-3-642-12683-3_3,https://doi-org.proxy.bnl.lu/10.1007/978-3-642-12683-3_3;http://dx.doi.org/10.1007/978-3-642-12683-3_3,Conference Paper
Investigating the Role of Sensor Based Technologies to Support Domestic Activities in Sub-Saharan Africa,"In sub-Saharan Africa (SSA), homes face various challenges including insecurity, unreliable power supply, and extreme weather conditions. While the use of sensor-based technologies is increasing in industrialized countries, it is unclear how they can be used to support domestic activities in SSA. The availability of low-cost sensors and the widespread adoption of mobile phones presents an opportunity to collect real-time data and utilize proactive methods to monitor these challenges. This dissertation presents three studies that build upon each other to explore the role of sensor-based technologies in SSA. I used a technology probes method to develop three sensor-based systems that support domestic security (M-Kulinda), power blackout monitoring (GridAlert) and poultry farming (NkhukuApp). I deployed M-Kulinda in 20 Kenyan homes, GridAlert in 18 Kenyan homes, and NkhukuProbe in 15 Malawian home-based chicken coops for one month. I used interview, observation, diary, and data logging methods to understand participants' experiences using the probes. Findings from these studies suggest that people in Kenya and Malawi want to incorporate sensor-based technologies into their everyday activities, and they quickly find unexpected ways to use them. Participants' interactions with the probes prompted detailed reflections about how they would integrate sensor-based technologies in their homes (e.g., monitoring non-digital tools). These reflections are useful for motivating new design concepts in HCI. I use these findings to motivate a discussion about unexplored areas that could benefit from sensor-based technologies. Further, I discuss recommendations for designing sensor-based technologies that support activities in some Kenyan and Malawian homes. This research contributes to HCI by providing design implications for sensor-based applications in Kenyan and Malawian homes, employing a technology probes method in a non-traditional context, and developing prototypes of three novel systems.",,"Chidziwisano GH,Bree Holtz,B. Jordan S,Eduardo Nakasone,Kurtis Heimerl",,2022,,,,Ph.D. Thesis
MobiWac '16: Proceedings of the 14th ACM International Symposium on Mobility Management and Wireless Access,"On behalf of the organizing committees, it is our great pleasure to welcome you to the 14th ACM International Symposium on Mobility Management and Wireless Access -- MobiWac 2016. Previous editions of this symposium took place in Dallas/Fort Worth (TX, USA), Philadelphia (PA), Maui (Hawaii), Torremolinos (Spain), Chania (Greece), Vancouver (Canada), Tenerife (Spain), Bodrum (Turkey), Miami (FL, USA), Paphos (Cyprus), Barcelona (Spain) Montreal (Canada), Cancun (Mexico). This year MobiWac takes place in Malta, and it continues its successful track record of being a forum where researchers from academy and industry gather to discuss novel advances in mobility, wireless access and related topics, aiming at advancing knowledge and identifying new directions for future research and development.The call for papers attracted a large number of submissions from Africa, America, Asia and Europe. From these works, the program committee has reviewed all papers and selected 28% of the best papers and put together the program you have in front of you. Accepted papers cover a wide variety of topics, including mobility management and medium access, MANET networking, tracking, quality of service, security and applications. The accepted papers come from 11 countries (Brazil, Italy, UK, Turkey, USA, Spain, Germany, Canada, Germany, Norway, Greece), which reflects the international nature of the symposium.",,,,2016,,,,Book
MobiWac '15: Proceedings of the 13th ACM International Symposium on Mobility Management and Wireless Access,"On behalf of the organizing committees, it is our great pleasure to welcome you to the 13th ACM International Symposium on Mobility Management and Wireless Access -- MobiWac 2015. Previous editions of this symposium took place in Dallas/Fort Worth (TX, USA), Philadelphia (PA), Maui (Hawaii), Torremolinos (Spain), Chania (Greece), Vancouver (Canada), Tenerife (Spain), Bodrum (Turkey), Miami (FL, USA), Paphos (Cyprus), Barcelona (Spain) and Montreal (Canada). This year MobiWac takes place in Cancun, Mexico, and continues its successful track record of being a forum where researchers from academy and industry gather to discuss novel advances in mobility, wireless access and related topics, aiming at advancing knowledge and identifying new directions for future research and development.The call for papers attracted 37 submissions from Africa, America, Asia and Europe. From these works, the program committee accepted 12 as regular papers and 2 more as short papers, which represents an acceptance rate of 37.8%. Accepted works cover a wide variety of topics, including mobility management and medium access, MANET networking, tracking, quality of service, security and applications. The accepted papers come from 10 countries (Brazil, Canada, Germany, Greece, Japan, Luxemburg, Mexico, Sweden, Tunisia and USA), which reflects the international nature of the symposium.",,,,2015,,,,Book
Adopting Biometric Technology: Challenges and Solutions,"Many types of security technologies are currently in use, with biometrics being one of the latest and most cutting-edge forms that has been produced for mass application. Biometrics, while intriguing, is often broached with hesitation and poor understanding. Adopting Biometric Technology: Challenges and Solutions advocates increased implementation of biometric technology areas of the world where it has been least accepted, particularly in the United States. This book looks at several specific applications of biometric technology, challenging issues that have obstructed the use of biometrics in security and offering realistic solutions for increasing its worldwide utilization. It is divided into three sections, with the first discussing societal barriers against the adoption of biometric technology in security. The second section presents case studies of specific applications, such as e-passports and e-voting, that have already been implemented and could be expanded into regions where usage is low. The third section lays out a case for the general practicality and value that biometrics offers to relevant business sectors, including the benefits of implementing the currently controversial technology in place of the conventional forms of verification. While biometric technology has been poorly accepted and adopted in the United States as well as other developed nations, it is already a popular tool in developing nations in Asia, Africa, and Eastern Europe. Adopting Biometric Technology examines the societal resistance hindering the broader usage of biometrics and provides practical solutions for overcoming those barriers while showing how its increased application would be overall advantageous.",,Das R,,2016,,,,Book
Developing EJB 2.0 Components,"From the Book: Preface What Are Enterprise JavaBeans (EJB) If youve picked up this book and are reading this page, its probably because youre curious about or interested in Enterprise JavaBeans (EJB), Java 2 Enterprise Edition, or Java. Or perhaps you just liked the cute giraffe on the cover with its backdrop of Mt. Kilimanjaro and wondered what a giraffe was doing on a cover of a J2EE book. This book is about Enterprise JavaBeans 2.0. EJB is a component model for building scalable, reusable, portable, transactional, and distributed enterprise business applications. Well-designed EJBs encapsulate discrete business logic, and EJBs that encapsulate different kinds of business logic can be assembled to form a complete business application. For example, one could take discrete EJBs that implement user authentication, credit card authorization, shopping cart, order fulfillment, inventory management, and customer relationship management tasks and assemble them into one integrated application, then add the Web front end and ... presto! You have an e-commerce application, are ready for an IPO (well, that was true back in 1999 and early 2000), and can afford to buy a two-bedroom mansion in Silicon Valley. Well worry about profitability later. Why Should You Buy This Book There are several good books on EJBs, and Im sure there will be more in the future. Most current EJB books fall into two basic categories—standalone EJB books that focus solely on EJB and do a good job at it but usually lack context, and books that attempt to cover everything—all the J2EE technologies, CORBA, and even (in some cases) COMDCOM, in one humongous tome. Theselatter books have plenty of breadth but usually lack depth. This book, Developing EJB 2.0 Components, the first book in a three-book series, focuses exclusively on the practical aspects of how to implement EJB 2.0. It is an attempt to bring the right balance between depth and breath within the broader context of the Java 2 Enterprise Edition platform. The book incorporates a unique perspective from my experience as a J2EE developer and as an instructor who teaches Java programmers—nationally, internationally for Netscape, and currently for Sun—how to develop J2EE applications. The second book in the series, Developing Web Components and Web Services, will focus exclusively on the presentation and user-interaction aspects of the J2EE technologies. The third (as yet untitled) book will focus exclusively on the J2EE infrastructure technologies. Together, these three titles will provide the audience with the necessary practical knowledge, depth, and breadth to write robust J2EE applications. So you must be wondering, Why should I buy this book Following are some good reasons (my reasons, of course), but you be the judge. Go the Whole Nine Yards Lets face it: EJB has a steep learning curve. Implementing EJB applications can be complex, and knowing how to write an EJB component is just half of the challenge. After coding and compiling the EJB components, you need to package, assemble, and then deploy them—no trivial tasks for a beginning EJB developer. Unlike most EJB books, Developing EJB 2.0 Components not only has chapters that discuss the theory, the APIs, and the rules on writing various types of EJBs and their methods, but it also has a separate chapter that discusses step-by-step implementation details followed by packaging and deployment steps and information on how to run the sample application. This information is complete with a copious number of diagrams and screen captures of the steps to guide you to a successful completion for each type of EJB. If my instructor-lead training experience is any indication, most rookie EJB developers will appreciate my effort to show you how to go the whole nine yards (to borrow an expression from American football), or in other words, to implement a complete solution. Respect Your Intelligence One of the hardest tasks in writing a book like this is setting the level at which to write the sample applications. On one hand, they shouldnt be so long and complicated that they distract the attention of the audience from fundamental EJB concepts in the process of trying to figure out complicated and nifty algorithms. At the other extreme, the sample applications shouldnt be so simple that they add little or no value to the learning process. My view is that I respect your intelligence and dont need to impress you with complex examples, so Ive taken the middle path. The sample applications are not too complicated and not too long, so you can focus on the fundamentals of EJB without being too simplistic. Once youve mastered the concepts, you can take the sample application and use it to add real-world business complexity to your own applications. Most of the examples implement discrete business logic per EJB, and the last chapter takes all the EJBs youve implemented in the previous chapters and assembles them into an integrated EJB application, applying EJB design patterns and best practices. I hope Ive been successful with this approach. Advanced EJB Concepts...When Youre Ready for Them Ive purposely deferred discussing advanced and complex issues such as transactions, security, and design patterns in Part 3 of the book so as to focus on the fundamentals of EJBs. Once youve mastered the fundamentals of EJB 2.0, you can then dive into more advanced concepts in Part 3. In that section, we concentrate on how to implement EJB transactions, EJB security, EJB design patterns, and strategies for migrating from EJB 1.1 to EJB 2.0, with complete code examples. Repetition is the Key to Learning The approach this book takes is first to discuss the concepts, characteristics, APIs, and rules on how to write specific EJB in an introductory chapter and then follow up with an implementation chapter that repeats the key concepts and rules during step-by-step implementation of a sample code example. I believe the discussion of the EJB fundamentals, followed by reinforcement through repetition of key concepts with example code, helps reduce the learning curve. What This Book Doesnt Cover Ive told you all the great reasons why you should buy this book, so in the interest of fairness, Ill be up front and tell you that in some cases, you might not want to buy it. Heres why. Vendor Neutrality This book doesnt discuss packaging, assembling, and deploying using BEAs Weblogic, IBMs Websphere, Suns iPlanet, or any other brand-name application server. The EJB implementation details are standard, so theyre applicable to all application servers. This book focuses on packaging, assembling, and deploying EJB applications using Suns J2EE Reference Implementations deployment tool (deploytool), so the instructions are specific to that tool. The main reason I chose J2SDKEE RI is the resource requirements—it requires less than 15 MB of drive space, features ease of installation, runs with 128 MB of RAM, and is free. Most brand-name application servers require 100 MB of disk space and a minimum of 256 MB of RAM (or more), and installations can be challenging. Not a Java Programming Book This book doesnt teach you Java programming—only how to write business applications using EJBs. Servlet, JSP, SQL, JDBC, UML, or CORBA Not Covered This book assumes that the reader has real-world Java programming experience and also has familiarity with HTML, servlet, JSP, SQL, JDBC, and CORBA. I use UML diagrams and assume you have basic familiarity with such diagrams. Audience for This Book This book is primarily geared toward helping new EJB programmers and existing EJB 1.1 programmers learn how to write business logic in EJB 2.0. The ideal reader should have at least one year of Java programming experience and be familiar with HTML, servlet, JSP, SQL, rmi, and JDBC. How the Book Is Organized The book consists of eighteen chapters and is organized in three parts. Part 1: OverviewThe first part of this book consists of two introductory chapters that give an introductory, non-technical overview of the Java 2 Enterprise Edition and Enterprise JavaBeans 2.0. Audiences who will benefit from this section are novice Java programmers and nonprogrammers (such as technical managers, project managers, and so forth) who can read these two chapters and get a good grasp of the fundamentals of J2EE (1.3) and EJB (2.0) technologies. The information in these chapters will help them communicate effectively with EJB programmers. Chapter 1: Introduction to Java 2 Enterprise Edition 1.3 —This chapter is a basic, nontechnical introduction to Java 2 Enterprise Edition. It discusses the various Java technologies required by the J2EE 1.3 specification and how they all fit together. Chapter 2: Introduction to Enterprise JavaBeans 2.0 —This chapter is an introduction to Enterprise JavaBeans 2.0 and discusses the basics of Enterprise JavaBeans without overwhelming the reader with technical details. Part 2: Developing EJBs The second part of this book is geared toward Java programmers and EJB programmers who are interested in learning how to implement EJB 2.0. The chapters in this section deal with theory, followed by a step-by-step guide to implementing, packaging and deploying session, entity, and message-driven beans. Chapter 3: Overview of Sample Applications —This chapter offers a high-level implementation overview of sample applications of stateless and stateful session beans, bean-managed persistent and container-managed persistent entity beans, and message-driven beans. Chapter 4: The EJB Client View —This chapter discusses the local and remote client view of EJB and rules for implementing remote and local interfaces and their advantages and disadvantages. Chapter 5: Introduction to Session Beans —Here, youll be introduced to characteristics, APIs, and details of session beans. Chapter 6: Developing Stateless Session Beans —This chapter covers the life cycle of stateless session beans and provides a step-by-step guide to implementing, packaging, assembling, and deploying a stateless session bean sample application. Chapter 7: Developing Stateful Session Beans —This chapter describes the life cycle of stateful session beans and provides a step-by-step guide to implementing, packaging, assembling, and deploying a stateful session bean sample application. Chapter 8: Introduction to Entity Beans —This chapter discusses the basic characteristic of entity beans, their life cycle, API, and rules for writing entity beans. Chapter 9: Developing Bean-Managed Entity Beans —This chapter povides a step-by-step guide to implementing, packaging, assembling, and deploying a BMP entity bean sample application. Chapter 10: CMP 2.0: Abstract Persistence Model and EJB QL —This chapter helps you understand the concepts behind the abstract persistence schema and the EJB Query language. Chapter 11: Developing CMP 2.0 Entity Beans —These pages offer a step-by-step guide to implementing, packaging, assembling, and deploying a CMP entity bean sample application. Chapter 12: Java Message Service —This chapter discusses the JMS API, messaging models, and how to use JMS APIs. Chapter 13: Developing Message-Driven Beans —This chapter is a step-by-step guide to implementing, packaging, assembling, and deploying a message-driven bean sample application. Part 3: Advanced Topics The final section in the book discusses advanced EJB concepts, including transaction, security, EJB design patterns, and migration issues. Chapter 14: Transactions —This chapter discusses how to implement programmatic and declarative transactions in different types of EJBs. Chapter 15: Enterprise JavaBean Security —This chapter discusses security concepts and how to implement programmatic and declarative security in EJBs. Chapter 16: EJB Design Patterns, Interoperability, and Performance —This chapter discusses common EJB design patterns and issues with performance and interoperability in EJB 2.0. Chapter 17: Migrating EJB 1.1 Applications to the EJB 2.0 Container —This chapter discusses issues involved in migrating EJB 1.1 beans to EJB 2.0 containers. It includes details and steps for migration of a sample application. Chapter 18: Assembling the J2EE Online Registration Application —The final chapter takes some of the discrete EJBs components developed in Part 2 of the book, refactors them if necessary, applies appropriate design patterns, and creates a complete application. It also discusses some of the implementation issues. AppendixAt the back of the book, you will find instructions on how to download, install, and set up Suns J2SDKEE 1.3 Reference Implementation. For the latest updates, please go to the companion Web site at http: and download Appendix.pdf. Companion Web Site To keep the book within reasonable number of pages, the book lists no examples in their entirety but instead takes code snippets from the sample examples to elucidate key concepts during the discussion. The complete source code, along with compiled classes and deployable ear files, are available for download from the companion Web site, http: . I encourage you to return here for the latest on bug fixes, new articles, new sample examples, and my public speaking engagements. Feel free to send comments and suggestions to , and Ill do my best to respond within a few days. Whats a Giraffe Doing on the Front Cover of an EJB Book Im an avid adventure traveler, and as a server-side Java consultant working in Silicon Valley, Ive been able to travel two months a year for the past several years, thanks to the Internet boom (ah, those good old days!). I always had a vague notion of writing a Java book someday, and it became a real-life goal during a long trip across East Africa-to be precise, at the top of Mt. Kilimanjaro on January 1, 2000 at the first sunrise of the millennium. So the picture on the cover of the book is my tribute to the majestic Mt. Kilimanjaro (the highest mountain in Africa), its amazing wildlife, and the diverse cultures and peoples of Africa. AcknowledgmentsTo put it positively (and mildly), writing this book has been one of the most challenging experiences of my life. I know youve read such statements by other authors-and so have I-but it really is a challenge, trust me. Even though the book has only my name on it, there are many talented people whove played significant roles in making this book possible. Without their assistance, this book would not have become a reality.At Prentice Hall, I would like to express my gratitude to executive editor Greg Doench for accepting the J2EE book series proposal and bringing it to fruition. My thanks also go to marketing manager Debby vanDijk and acquisition editor Eileen Clark for all their efforts toward the completion of the book. I would also like to thank developmental editor Jim Markham for helping me with the development and structure of the book.Thanks also go to production coordinator, editor, and compositor Sybil Ihrig of Helios Productions, technical reviewer Rob Gordon, and editors Casey Andrysiak and Elizabeth Hayes for their meticulous reviews and suggestions regarding the content and style of the book. At Sun Press, I wish thank Michael Alread and Rachel Borden for their efforts in expediting production of the book.I would also like to express my thanks to three excellent authors-Phillip Heller (The Complete Java Certification), Marty Hall (Core Servlet and JavaServer Pages), and Peter Haggar (Practical Java Programming Guide) for their initial encouragement with my book. The acknowledgment list would be incomplete without expressing my deep appreciation to Jason Fish, business development manager at Sun Educational Services, for introducing me to Greg Doench at Prentice Hall and thus starting the ball rolling on the this book. Thanks, Jason.-Pravin Tulachan ",,Tulachan PV,,2002,,,,Book
The Freedom Economy: Gaining the Mcommerce Edge in the Era of the Wireless Internet,"From the Book: Foreword We at Nokia are part of the transformation of communication for the human race. Mobile phones and wireless technology are having that profound an impact on society around the world. Soon there will be a billion mobile phones in use and within just a few years the Internet will be fully mobile as well. We are seeing a democratization of technology uses and its impact through mobile communications. Until very recently, technology was meritocratic in that many countries lacked the infrastructures and capital required to put personal computers in schools and homes or to make it practical for businesses to exploit electronic commerce in the same way as in advanced economies. Any country now can enter the Internet age. The poorest villages in Africa and Asia as well as the most affluent cities in Europe and North America are crowded with mobile phone users. Society is being changed all over the globe. Business will change profoundly as well. Mobile commerce, or M-commerce, is already creating many innovations. The important insight that Peter Keen and Ron Mackintosh provides in this book is that the interests of society and business come together in M-commerce because it will succeed in creating freedoms for customers that change the limits of the possible in the structures of their everyday life. As the world's largest manufacturer of mobile handsets, we have seen wireless technology create such freedoms, and the authors offer many examples, ranging from shared phones in Bangladesh villages to field technicians having all the information they need moving with them as they do their work, even when they are many meters up a telephone pole! Peter and Ron's book looks at M-commerce from the demand side. Nokia is a leader in the supply side. In our own country of Finland, supply and demand have come together quickly and with dramatic results. We are in many ways the M-commerce nation. In 1999, just around 15 percent of Finns used a mobile phone. Now that's about the number that doesn't. Worldwide, we see the same explosive growth in the use of the wireless tools that, in Peter and Ron's phrase, create new freedoms in everyday life. Go to China, South Africa, Australia, Italy, Chile, or almost anywhere on planet Earth and you will see the same pattern of wireless phones moving into everyday life and changing how we live. And of course you'll see the other pattern that is reshaping society. That is the Internet. It's everywhere now as well. It has already changed how we live in so many ways that it is impossible to keep up with what is going on. The supply side of M-commerce is moving as fast as we and our partners and competitors in wireless technology can accelerate. The technology that is on the market today is almost already out of date. In Nokia, our products of just a few years ago are like museum pieces compared with what we are bringing out now, and what's in our development labs will make those museum pieces as well. As this book makes very clear, all this innovation can be bewildering for companies that want to make best use of it for M-commerce. It will probably get even more bewildering because the innovation is really only just starting! We have a selfish interest in M-commerce. All this new supply and innovation will be of no value unless they create services that provide value. This is why we welcome this book. It is very pragmatic in how it looks at the technology of today and how it can be used in the three areas of business that the authors call the ""value imperatives"" of M-commerce. These are new customer relationships, supply-chain management, and the mobilization of an organization's knowledge resources. It gives business managers a set of maps so they can move ahead and continue to take advantage of the technology innovations and make business innovations. It is also an international book. Nokia is a large company headquartered in a small country, but really the world is our ""country."" The wireless revolution knows no boundaries. Innovation is everywhere and so is the demand for value. Peter and Ron provide a guidebook to creating that value. They alert managers everywhere to the state of the best practice in technology and its uses around the globe. And they give examples! They tell managers what is really happening in M-commerce. The M-commerce innovation that changes everyday life will come from the creative and pragmatic interaction of the supply side and the demand side. The wireless technology and wireless services providers are building the new foundations for M-commerce, but it is businesses that will build on that foundation the applications that generate value. At Nokia, we are becoming a demand-side company as well as a supply-side one. For instance, we are working with credit card firms and banks to help create the new mobile payments systems that we believe will be valuable new freedoms for people everywhere in their everyday lives. Peter Keen and Ron Mackintosh have written a book that, in our view, every business manager should read. It is about the future, but not the distant future. M-commerce is here now, and with all the supply side and demand side innovation that is coming fast, the future is now. -Mikko Heikkonen Executive Vice President, Nokia Networks",,"Keen PG,Mackintosh R,Heikkonen M",,2001,,,,Book
Electric Signal Synchronization as a Behavioural Strategy to Generate Social Attention in Small Groups of Mormyrid Weakly Electric Fish and a Mobile Fish Robot,"African weakly electric fish communicate at night by constantly emitting and perceiving brief electrical signals (electric organ discharges, EOD) at variable inter-discharge intervals (IDI). While the waveform of single EODs contains information about the sender’s identity, the variable IDI patterns convey information about its current motivational and behavioural state. Pairs of fish can synchronize their EODs to each other via echo responses, and we have previously formulated a ‘social attention hypothesis’ stating that fish use echo responses to address specific individuals and establish brief dyadic communication frameworks within a group. Here, we employed a mobile fish robot to investigate the behaviour of small groups of up to four Mormyrus rume and characterized the social situations during which synchronizations occurred. An EOD-emitting robot reliably evoked social following behaviour, which was strongest in smaller groups and declined with increasing group size. We did not find significant differences in motor behaviour of M. rume with either an interactive playback (echo response) or a random control playback by the robot. Still, the robot reliably elicited mutual synchronizations with other fish. Synchronizations mostly occurred during relatively close social interactions, usually when the fish that initiated synchronization approached either the robot or another fish from a distance. The results support our social attention hypothesis and suggest that electric signal synchronization might facilitate the exchange of social information during a wide range of social behaviours from aggressive territorial displays to shoaling and even cooperative hunting in some mormyrids.",,"Worm M,Landgraf T,von der Emde G",,2021,599–613,10.1007/s00422-021-00892-8,https://doi-org.proxy.bnl.lu/10.1007/s00422-021-00892-8;http://dx.doi.org/10.1007/s00422-021-00892-8,Journal Article
African Buffalo Algorithm: Training the Probabilistic Neural Network to Solve Classification Problems,,,"Alweshah M,Rababa L,Ryalat MH,Al Momani A,Ababneh MF",,2022,1808–1818,10.1016/j.jksuci.2020.07.004,https://doi-org.proxy.bnl.lu/10.1016/j.jksuci.2020.07.004;http://dx.doi.org/10.1016/j.jksuci.2020.07.004,Journal Article
Front Matter,"These proceedings contain the final versions of the papers presented at the 7th International Workshop on Finite-State Methods and Natural Language Processing, FSMNLP 2008. The workshop was held in Ispra, Italy, on September 11--12, 2008. The event was the seventh instance in the series of FSMNLP workshops, and the third that was arranged as a stand-alone event. In 2008 FSMNLP was merged with the FASTAR workshop.The aim of the FSMNLP workshops is to bring together members of the research and industrial community working on finite-state based models in language technology, computational linguistics, web mining, linguistics, and cognitive science on one hand, and, on related theory and methods in fields such as computer science and mathematics, on the other. Thus, the workshop series is a forum for researchers and practitioners working on applications as well as theoretical and implementation aspects. The special theme of FSMNLP 2008 centered around high performance finite-state devices in large-scale natural language text processing systems and applications.In the context of FSMNLP 2008, we received in total 37 submisions, of which 13 were selected as regular papers, 6 as short papers and 1 as demo paper. The acceptance rate for regular papers was 46,4%. Most of the papers were evaluated by at least four Programme Committee members, with the help of external reviewers. Only 15% of the papers were reviewed by three reviewers. In addition to the submitted papers, four lectures were given by invited speakers. The invited speakers and the authors of the papers represented (at least) Croatia, Finland, France, Gemany, Italy, Luxembourg, Netherlands, Portugal, Puerto Rico, Sweden, U.K., and the USA.We would like to thank all workshop participants for their contributions and lively interaction during the two days. The presented papers covered a range of interesting NLP applications, including machine learning and translation, logic, computational phonology, morphology and semantics, data mining, information extraction and disambiguation, as well as programming, optimization and compression of finite-state networks. The applied methods included weighted algorithms, kernels, and tree automata. In addition, relevant aspects of software engineering, standardization, and European funding programmes were discussed.We are greatly indebted to the members of the Programme Committee and the external referees for reviewing the papers and maintaining the high standard of the FSMNLP workshops. The members of the Programme Committee of FSMNLP 2008 were Cyril Allauzen (Google Research, New York, USA), Francisco Casacuberta (Instituto Tecnologico De Informática, Valencia, Spain), Jean-Marc Champarnaud (Université de Rouen, France), Maxime Crochemore (Department of Computer Science, King's College London, U.K.), Jan Daciuk (Gdańsk University of Technology, Poland), Karin Haenelt (Fraunhofer Gesellschaft and University of Heidelberg, Germany), Thomas Hanneforth (University of Potsdam, Germany), Colin de la Higuera (Jean Monnet University, Saint-Etienne, France), André Kempe (Yahoo Search Technologies, Paris, France), Derrick Kourie (Dept. of Computer Science, University of Pretoria, South Africa), Andras Kornai (Budapest Institute of Technology, Hungary and MetaCarta, Cambridge, USA), Marcus Kracht (Univeristy of California, Los Angeles, USA), Hans-Ulrich Krieger (DFKI GmbH, Saarbrücken, Germany), Eric Laporte (Université de Marne-la-Vallée, France), Stoyan Mihov (Bulgarian Academy of Sciences, Sofia, Bulgaria), Herman Ney (RWTH Aachen University, Germany), Kemal Oflazer (Sabanci University, Turkey), Jakub Piskorski (Joint Research Center of the European Commission, Italy), Michael Riley (Google Research, New York, USA), Strahil Ristov (Ruder Boskovic Institute, Zagreb, Croatia), Wojciech Rytter (Warsaw University, Poland), Jacques Sakarovitch (Ecole nationale supérieure des Télécommunications, Paris, France), Max Silberztein (Université de Franche-Comté, France), Wojciech Skut (Google Research, Mountain View, USA), Bruce Watson (Dept. of Computer Science, University of Pretoria, South Africa) (PC co-chair), Shuly Wintner (University of Haifa, Israel), Atro Voutilainen (Connexor Oy, Finland), Anssi Yli-Jyrä (University of Helsinki and CSC --IT Center for Science, Espoo, Finland) (PC co-chair), Sheng Yu (University of Western Ontario, Canada), and Lynette van Zijl (Stellenbosch University, South Africa). The external reviewers were Marco Almeida, Marie-Pierre Beal, Oliver Bender, Jan Bungeroth, Pascal Caron, Loek Cleophas, Matthieu Constant, Stefan Hahn, Christopher Kermorvant, Sylvain Lombardy, Patrick Marty, Evgeny Matusov, Takuya Nakamura, Ernest Ketcha Ngassam, Jyrki Niemi, Sébastien Paumier, Maciej Pilichowski, Adam Przepiórkowski, Magnus Steinby, Yael Sygal, David Vilar, Hsu-Chun Yen, Francois Yvon, Artur Zaroda, and Djelloul Ziadi.FSMNLP 2008 was organised by the Institute for the Protection and Security of the Citizen of the Joint Research Centre (JRC) of the European Commission in Ispra, Italy, in cooperation with the host of the next FSMNLP event, the FASTAR group of the University of Pretoria in South Africa. The Organizing Committee in 2008 had five JRC representatives: Regina Corradini, Daniela Negri, Jakub Piskorski (OC chair), Hristo Tanev, and Vanni Zavarella, and two members from the Department of Computer Science, University of Pretoria, South Africa: Derrick Kourie and Bruce Watson. A complementary role in long-term planning and coordination was played by the Steering Committee: Lauri Karttunen (Palo Alto Research Center, USA and Stanford University, USA) Kimmo Koskenniemi (University of Helsinki, Finland), Kemal Oflazer (Sabanci University, Turkey) and Anssi Yli-Jyrä (University of Helsinki and CSC --IT Centre for Science, Espoo).The current year's event is pivotal to the series of FSMNLP workshops since it starts the tradition of organizing the workshops on a yearly basis. Locations for successive events, including FSMNLP 2008 in Ispra and FSMNLP 2009 in Pretoria were proposed already in FSMNLP 2007 in Potsdam. The success of FSMNLP 2008 indicates that there is a growing and wide interdisciplinary community with shared interest in finite-state methods and natural language processing. Therefore, we are looking forward to the FSMNLP 2009 that is to be held in Pretoria, South Africa next year!In October 2008Jakub Piskorski, Bruce Watson, Anssi Yli-Jyrä",,,,2009,i–viii,,,Conference Paper
Plenary Lecture 3: A Fuzzy Technologies of Weakly Structurable Systems' Modeling and Simulation,"The Plenary Speech will present the new approach to the study of optimization of weakly structurable fuzzy dynamic systems (Extremal Fuzzy Dynamic System (EFDS)). This approach is based on the six papers published in the Int. Journal of General Systems (by G. Sirbiladze, ""Modeling of Extremal Fuzzy Dynamic Systems"". Parts I-VI: 34,2, 2005, 107-138; 139-167; 169-198; 35, 4, 2006, 435-459; 35, 5, 2006, 529-554; 36,1 2007, 19-58). Different from other approaches where the source of fuzzy uncertainty in dynamic systems is expert, this approach considers time as long as an expert to be the source of fuzzy uncertainty. This notably widens the area of studied problems. All these is connected to the incomplete, imprecise, anomal and extremal processes in nature and society, where connections between the system's objects are of subjective (expert) nature, which is caused by lack of objective information about the evolution of studied system, for example in 1) engineering problems, 2) economics and business of developing countries, 3) management of evacuation processes in catastrophe areas, estimation of disease spreading in epidemical regions; 4) research of complex systems of applied physics, 5) conflictology, sociology, medical diagnosis, etc; One of our purposes is to create scenarios describing possible evolution of EFDS using methods of optimization developed by the framework of expert-possibilistic theory. This includes construction of algorithms of logical-possibilistic simulations of anomal and extremal process analysis.The plenary speech will cover the following topics: introduce the notions of extremal fuzzy time moments and intervals; construction of fuzzy processes with possibilistic uncertainty, the source of which is extremal fuzzy time intervals; the dynamics of EFDS's; questions of the ergodicity of EFDS's; Fuzzy-integral representations of controllable extremal fuzzy processes; Sufficient and necessary conditions for the existence of an extremal fuzzy optimal control processes.A separate consideration will be given to the case where an extremal fuzzy control process acting on the EFDS does not depend on an EFDS state. Applying Bellman's optimality principle and assuming that the gain-loss process exists for the EFDS, a variant of the fuzzy integral representation of an optimal control is given for the EFDS. This variant employs the instrument of extended extremal fuzzy composition measures. An example of constructing of the EFDS optimal control will be presented.",,Sirbiladze G,,2009,18,,,Conference Paper
Essays on Production Function Estimation,"This thesis contains three chapters. ""Copies available exclusively from MIT Libraries, libraries.mit.edu/docs |docs mit.edu""This first chapter develops a new method for estimating production functions with factor-augmenting technology and assesses its economic implications. The method does not impose parametric restrictions and generalizes prior approaches that rely on the CES production function. I first extend the canonical Olley-Pakes framework to accommodate factor-augmenting technology. Then, I show how to identify output elasticities based on a novel control variable approach and the optimality of input expenditures. I use this method to estimate output elasticities and markups in manufacturing industries in the US and four developing countries. Neglecting labor-augmenting productivity and imposing parametric restrictions mismeasures output elasticities and heterogeneity in the production function. My estimates suggest that standard models (i) underestimate capital elasticity by up to 70 percent (ii) overestimate labor elasticity by up to 80 percent. These biases propagate into markup estimates inferred from output elasticities: markups are overestimated by 20 percentage points. Finally, heterogeneity in output elasticities also affects estimated trends in markups: my estimates point to a much more muted markup growth (about half) in the US manufacturing sector than recent estimates. The second chapter develops partial identification results that are robust to deviations from the commonly used control function approach assumptions and measurement errors in inputs. In particular, the model (i) allows for multi-dimensional unobserved heterogeneity, (ii) relaxes strict monotonicity to weak monotonicity, (iii) accommodates a more flexible timing assumption for capital. I show that under these assumptions production function parameters are partially identified by an 'imperfect proxy' variable via moment inequalities. Using these moment inequalities, I derive bounds on the parameters and propose an estimator. An empirical application is presented to quantify the informativeness of the identified set. The third chapter develops an approach in which endogenous networks is a source of identification in estimations with network data. In particular, I study a linear model where network data can be used to control for unobserved heterogeneity and partially identify the parameters of the linear model. My method does not rely on a parametric model of network formation. Instead, identification is achieved by assuming that the network satisfies latent homophily — the tendency of individuals to be linked with others who are similar to themselves. I first provide two definitions of homophily: weak and strong homophily. Then, based on these definitions, I characterize the identified sets and show that they are bounded under weak conditions. Finally, to illustrate the method in an empirical setting, I estimate the effects of education on risk preferences and peer effects using social network data from 150 Chinese villages.",,"Demirer M,Finkelstein A",,2020,,,,Ph.D. Thesis
Quantifying the Strength and Durability of Induced Immunity to HIV Infection in Women Engaging in Unprotected Sexual Contacts with Infected Men,"Evidence is accumulating that exposure to human immunodeficiency virus (HIV) can lead to an increased resistance or immunity to subsequent infection. A multirisk model that permits either induced immunity or infection to develop after heterosexual inoculation with HIV is shown to be compatible with a wide spectrum of disparate male-to-female transmission data. When the model is applied to time-dependent, HIV-seroprevalence data, the probability that an unexposed woman would remain unexposed after an unprotected contact with an infected man was estimated to be greater than 0.95 on the average. Thus, it would require at least 14 unprotected sexual contacts with HIV-infected men for 50% of an unexposed cohort of women to become exposed to the virus. This suggests that there is a low probability that HIV virions will be found to have penetrated the mucosal barriers of the reproductive tract after a contact. The model also predicts, that the average woman whose mucosal barriers have been breached by HIV has a significant probability of developing immunity to the virus rather than infection. Modelling data for a cohort of unexposed Nairobi women leads to the prediction that the probability of acquiring induced immunity per contact is about 60% of the probability of acquiring the disease per contact. The modelling results also predict that those who had developed resistance to HIV run the small, but significant risk of becoming infected nonetheless by continuing high-risk behavior. For the common contact rate of ten per month, the modelling predicts that the HIV-transmission risk per contact for unexposed women in the Nairobi cohort is 1178 while the transmission risk for the cohort's immunized women is 11548. These numbers suggest that HIV infection is difficult to transmit through heterosexual intercourse on the average and that male-to-female HIV-transmission risk per contact for African women lies between 1178 and 11548. Direct confirmation of the predictions in the last paragraph has been subsequently observed in two completely independent studies. The Nairobi research team recently reported that a notable number of Nairobi prostitutes previously identified to be members of the HIV-resistant group became infected nonetheless. Second, in a study of 174 sexually monogamous, discordant couples in Rakai, Uganda reporting contacts rates of nine to ten per month, the male-to-female HIV-transmission risk per contact was found to be 1769 by direct measurement, a value that falls between the above limits of 1178 and 11548 predicted by the modelling. Thus, a second major prediction of this paper has been directly confirmed, and induced immunity to HIV is limited and not absolutely protective. Circumstantial evidence suggests that the induced immunity to HIV predicted by the model could be generated and/or initiated by nonspecific innate immune responses, specific immunological responses, including IgA-mediated mucosal immunity and cytotoxic T lymphocytes (CTL) immunity, or some combination of the above. It is suggested here, that a decrease in the ability of HIV virions to penetrate the protective mucus layer of the reproductive tract may be a prerequisite, cofactor, or the principle cause of the induced immunity or resistance demonstrated to exist in this paper. The value of the probability that induced immunity to HIV will develop after a contact is shown to be a sensitive function of the woman's human leucocyte antigen (HLA) supertype profile.",,"Kramer I,Shearer GM",,2002,1435–1458,10.1016/S0895-7177(02)00299-6,https://doi-org.proxy.bnl.lu/10.1016/S0895-7177(02)00299-6;http://dx.doi.org/10.1016/S0895-7177(02)00299-6,Journal Article
SEHS '18: Proceedings of the International Workshop on Software Engineering in Healthcare Systems,"Welcome to the workshop on Software Engineering in Healthcare Systems, co-located with ICSE 2018 in Gothenburg, Sweden! This year marks the tenth anniversary of holding an event at ICSE dedicated to discussing experiences, challenges and solutions related to engineering software in healthcare applications. Given the dramatic changes associated with digitalizing healthcare at a global scale, ICSE 2008 featured an Experience Track on Software Engineering in Health Care, an event that gave birth to a subsequent series of workshops and symposia. Ten years later, software-intensive systems and systems of systems have become pivotal components of most modern healthcare organizations. Still, healthcare systems continue to undergo significant transformations due to the innovation of new technologies and paradigms such as connected medical devices, the Internet of Things (IoT), Big Data analytics and context-aware, adaptive services. At the same time, advances in health care such as personalized and omics-based medicine have further increased the need for software-based knowledge-management and decision-support. In the developing world and in low-resource settings, softwareintensive healthcare delivery has shown tremendous success in improving efficiency and access to services. Notwithstanding these successes, there have been ongoing concerns and highly publicized failures pertaining to critical quality attributes of healthcare software, particularly with respect to safety, efficiency, security, interoperability and effectiveness. A principle challenge underlying most of these concerns is that despite the important role of software in modern health service delivery, health care remains a human-intensive industry and thus must be considered a socio-technical system. Integrating software components that have been engineered and quality assured in isolation into the larger context of a system of systems often results in unforeseen and potentially negative consequences. The purpose of this workshop is to bring together software engineering researchers and health informaticians with the goal to identify the key challenges faced in current healthcare software development and discuss potential approaches on how to address these challenges. This year's workshop program has its particular focus on healthcare process engineering, the development of medical device software and infrastructure, and emerging software engineering paradigms for connected health delivery systems. Our goal is to discuss recent research innovations and challenges in these areas, and to continue developing an interdisciplinary, international community with an effective research, educational and industrial agenda for supporting software engineering in the healthcare sector.",,,,2018,,,,Book
Evaluating a Mobile Tablet Project in Rural South Africa against Criteria to Comply with Being an Innovative Educational Ecosystem,The purpose of this paper is to evaluate a specific mobile technology rural schools project known as ICT4E in South Africa to determine if it complies to be regarded as an innovative educational ecosystem. The criteria that were used were sourced from the literature. The project was explained and why it is regarded as a rural school mobile tablet project in education. Some high-level results and lessons learnt from this project are also provided that was used to evaluate whether it can be regarded as an instantiation of an innovative educational ecosystem. The evaluation methodology was applied to do the evaluation. The main results were that based on the criteria and the findings from the ICT4E project it could be regarded as an instantiation of an innovative educational ecosystem as it complied to most of the criteria except for two where it only partially complied. The project was also seen as a good example of how a country's system of innovation can be supported from a socio-cultural context-specific perspective.,,"Herselman M,Botha A,Maremi K",,2019,215–220,10.1145/3345120.3355422,https://doi-org.proxy.bnl.lu/10.1145/3345120.3355422;http://dx.doi.org/10.1145/3345120.3355422,Conference Paper
Session Details: Demonstrations,"It is our great pleasure to welcome you to the Demo Track of WWW 2016, The 25th International World Wide Web Conference, held in Montreal, Canada, during April 11-15, 2016.The WWW 2016 Demo Track, like in the tradition of WWW Demo conference series, allows researchers and practitioners to demonstrate new systems in a dedicated session. Demo contributions are based on an implemented and tested system that pursues one or more innovative ideas in the interest areas of Web data and information management, Web search, Web intelligence tools, Web mining, social network applications and so forth. Topics of interest for the 2016 edition's conference include (but are not limited to) the following ones: Behavioral Analysis and PersonalizationBig Data on the WebCrowdsourcing Systems and Social MediaContent AnalysisGraph Data Management and MiningHigh-Performance Infrastructures for Data- Intensive Web TasksInternet Economics and MonetizationPervasive Web and MobilitySecurity and PrivacySemantic WebSocial Networks and Graph AnalysisWeb Information RetrievalWeb Infrastructure: Datacenters, Content Delivery Networks, and Cloud ComputingWeb MiningWeb ScienceWeb Search Systems and ApplicationsDemo contributions come from academic researchers, industrial practitioners with prototypes or inproduction deployments, as well as from any W3C-related activities. All have in common to show innovative use of Web-based techniques.The WWW 2016 Demo Track call for papers attracted 65 submissions from all over the world (USA, North America, South America, Europe, Australia, Asia, Africa). The program committee reviewed and accepted a very selected collection of 29 papers, and the final statistics is the following: WWW 2016 Demo Track Statistics Number of Submitted Papers 65 Number of Accepted Papers 29.",,"Cuzzocrea A,El Saddik A",,2016,,,,Conference Paper
SACMAT '11: Proceedings of the 16th ACM Symposium on Access Control Models and Technologies,"It is our great pleasure to welcome you to the 16th ACM Symposium on Access Control Models and Technologies (SACMAT 2011). This year's symposium continues its tradition of being the premier forum for presentation of research results on leading edge issues of access control, including models, systems, applications, and theory.The call for papers attracted 52 submissions from Asia, Canada, Europe, Africa, and the United States. The program committee accepted 16 papers that cover a variety of topics, including next generation access control models, engineering and analysis techniques for access control policies and models, and security administration. New this year in the program is a demo session with five demos covering topics such as security visualization, access control federation and social networks. In addition the program includes a panel on usability of access control models and systems from the perspective of the nonspecialist, and keynote talks by Professors David Basin and Jean-Pierre Seifert. We hope that these proceedings will serve as a valuable reference for security researchers and developers.",,,,2011,,,,Book
"Technology, Governance, and the Escalation of Ebola: Wicked Problems in Real Time","Digital technologies have been perceived as a means of facilitating governance in addressing complex, dynamic policy problems. Yet, technology alone cannot resolve interdependencies among divergent organizations operating at different levels of authority, access to resources, and experience in heterogeneous contexts. We report preliminary findings from an ongoing study of the recent Ebola outbreak in West Africa that revealed breakdowns in governance due in part to limited use of digital technologies to support systematic monitoring of the spread of the disease, search for, and exchange of, valid information and knowledge essential to manage a rapidly evolving, complex threat. We apply a systems theory approach to this wicked problem as a framework for examining the interactions between key actors involved in the response to this epidemic. Preliminary findings are drawn from a content analysis of news articles posted on the United Nations Relief Web and include a chronological record of the Ebola outbreak in West Africa from March through December 2014. A planned network analysis will measure the centrality of actors, as well as the strength and direction of ties among the participating actors: local, provincial, national, regional, and international. We anticipate developing a model of sociotechnical design for addressing complex policy problems",,"Bert J,Shin YA,Chalfont B",,2015,71–78,10.1145/2757401.2757434,https://doi-org.proxy.bnl.lu/10.1145/2757401.2757434;http://dx.doi.org/10.1145/2757401.2757434,Conference Paper
IWSPA '19: Proceedings of the ACM International Workshop on Security and Privacy Analytics,"It is our great pleasure to welcome you to the 2019 ACM Workshop on Security and Privacy & Analytics - IWSPA 2019. This year's workshop is the fifth in the series and continues the tradition of being the premier forum for presentation of research results and experience reports on leading edge issues and applications/adaptations of data analytics (by which we mean techniques drawn from data mining, machine learning, statistics and natural language processing) to security challenges. Data analysis techniques also have serious implications on privacy. Because of these reasons, the first four ACM international workshops on security and privacy analytics were organized in 2015, 2016, 2017 and 2018. The mission of IWSPA is to create a forum for interaction between data analytics and security/privacy experts and to examine the questions raised in the previous workshops.The call for papers attracted 14 abstracts and 13 papers from Asia, Africa, Europe, and North America. Each paper was assigned to at least three program committee members for review, a total of 39 reviews were obtained with every paper receiving exactly three reviews. The quality of the submissions was high. Consequently, five papers were accepted for presentation as full papers (11-page limit) and one was accepted as a short paper (7-page limit).",,,,2019,,,,Book
Understanding Actors in Complex Security Problems,"This article arose while working on the rhino poaching problem in South Africa and having to deal with the large number of stakeholders and complexity. The purpose of actor modelling is to develop a deeper understanding of how stakeholders and threats contribute to the complex security problems. This article is the author's reflection on two different attempts at modelling actors in the rhino problem. A framework is developed and a number of issues are raised with respect to actor modelling: First, values and perspectives are driven by actor needs. The knowledge acquired by actors is determined by perspectives. With a diversity of actors, there is a ""fragmentation of perspective"" that hampers addressing the problem. Thus, dealing with fragmentation of perspective, requires an approach that is inclusive of actors and different ways of knowing. The validity of actor modelling is limited by what can be determined about the values and interests of actors and this varies across actors. Second, actors have multiple identifications with multiple levels of relationality. For high levels of identification combined with low levels of relationality, there is a challenge for a researcher to understand actor behaviour. Third, actors operate in an autonomy-heteronomy space. This is not a continuum, but both autonomy and heteronomy experienced at the same time. When actors are autonomous they live out their values and interests and are most creative. When creativity is applied, there are many ways what of satisfying interests and living out values why, but actors do not behave randomly. Under autonomy, understanding motivation why is more important than what because why is more stable and what cannot be predicted. Actors are dynamic, non-deterministic and non-linear. Fourth, the model represents not only structure but also motivation or purpose and resources; thus, addressing certain aspects of subjective and objective fragmentation. Based on the argument advanced in the paper, the sources of actor complexity leading to novel emerging behaviour in social systems are actor needs and the corresponding values and perspectives, high levels of identification with low levels of relationality and autonomy.",,Gonçalves D,,2018,1–18,10.4018/IJSDS.2018040101,https://doi-org.proxy.bnl.lu/10.4018/IJSDS.2018040101;http://dx.doi.org/10.4018/IJSDS.2018040101,Journal Article
ACSC '08: Proceedings of the Thirty-First Australasian Conference on Computer Science - Volume 74,"The Australasian Computer Science Conference (ACSC) series is an annual forum, bringing together research sub-disciplines in Computer Science. The meeting allows academics and researchers to discuss research topics as well as progress in the field, and policies to stimulate its growth. This volume contains papers presented at the Thirty First ACSC in Wollongong, NSW, Australia. ACSC 2008 is part of the Australasian Computer Science Week which ran from Jan 22nd to 25th, 2008.The ACSC 2008 call for papers solicited contributions in all areas of computer science research. This years conference received submissions from Australia, New Zealand, China, France, India, Iran, Jamaica, Jordon, Malaysia, Pakistan, South Africa, Turkey, UK, and Taiwan. The topics addressed by the submitted papers illustrate the broadness of the discipline. The authors categorised their submissions into one or more of the following topics:- Algorithms (9 papers)- Artificial Intelligence (7 papers)- Communications and Networks (4 papers)- Computer Architecture (2 paper)- Computer Vision (4 papers)- Databases (5 papers)- Distributed Systems (6 papers)- E-Commerce (4 papers)- Formal Methods (6 papers)- Graphics (6 papers)- High Performance Computing (7 papers)- Human-Computer Interaction (8 papers)- Mobile Computing (6 papers)- Multimedia (1 paper)- Object Oriented Systems (3 papers)- Ontologies (1 paper)- Operating Systems (5 papers)- Programming Languages (4 papers)- Robotics (1 paper)- Scientific Computing (5 papers)- Security and Trusted Systems (5 papers)- Simulation (6 papers)- Software Engineering (5 papers)- Speech (1 paper)- Theory (3 papers)- Visualization (6 papers)- Web Services (3 papers)The programme committee consisted of 28 highly regarded academics from around the globe, including Australia, Brazil, Canada, Japan, New Zealand, Singapore and USA. All papers were sent to at least three programme committee members for review and every effort was made to obtain at least three reviews. Of the 47 papers submitted, 16 were selected for presentation at the conference.The programme committee invited Professor Joxan Jaffar, to give a keynote on Constraint Logic Programming for Program Analysis. Professor Jaffar has recently completed a stint as Dean of the School of Computing from 2001-2007 at the National University of Singapore. His interests are in programming languages and applications, with emphasis on the logic and constraint programming paradigms. Amongst his main contributions are the principles of constraint logic programming, and the widely-used CLP(R) system. The committee also invited Dr Benjamin Burton and Associate Professor Ewan Tempero to give invited talks. Dr Burtons talk was entitled Informatics Olympiads:Challenges in Programming and Algorithm Design. Associate Professor Temperos talk is entitled On Measuring Java Software.",,,,2008,,,,Book
The Carpathian-Balkans During the Holocene: Reconstructing Human Influences and Climatic Changes,"The Carpathian-Balkan region in south-eastern Europe is one of the longest inhabited regions in Europe, with evidence of some of the earliest examples of European agriculture, farming and metallurgy. Despite its importance for understanding past human activity and climate change, high-resolution reconstructions of Holocene hydroclimate variability and human impact are rare. This thesis provides a series of new high-resolution Holocene (the past 11,700 years) palaeoenvironmental records derived from peat bogs in the Carpathian Mountains of Romania, and the Dinaric Alps of Serbia, to investigate climate variation and human impact. Two peat-derived archives of environmental change in Romania are presented. First, a 7500-year record of minerogenic deposition from the Southern Carpathians linked to heavy rainfall events provides the first record of extreme precipitation for the Carpathians. Such minerogenic depositional events began 4000 calibrated years before present (yr BP, where present is 1950 CE), with increased depositional rates during the Medieval Warm Period (1150 – 850yr BP), the Little Ice Age (350 – 100 yr BP) and during periods of societal upheaval (e.g. the Roman conquest of Dacia). The timing of minerogenic events appears to indicate a teleconnection between the North Atlantic Oscillation (NAO) and hydroclimate variability in south-eastern Europe, which persists throughout the mid-to-late Holocene. Secondly, a 10,800-year record of geochemically-derived dust deposition and testate amoeba-derived local wetness from the Eastern Romanian Carpathians highlights several discrepancies between eastern and western European dust depositional records and the impact of highly complex hydrological regimes in the Carpathian region. Specifically, the record outlines the increased impact of Saharan dust after 6100 yr BP which is associated with the end of the African Humid Period. A lead (Pb) record from a peat bog in Western Serbia provides an unprecedented view on past pollution related to metal exploitation in the Balkans. Environmental Pb pollution is first observed in the very earliest Bronze Age, the oldest environmental Pb pollution in Europe. After 600 CE an almost linearly increasing Pb trend until the Medieval period is observed. Comparison with western European records suggests an alternative history of European metallurgy, one in which metal-related pollution does not cease with the fall of the Roman Empire, and which displays major Medieval pollution. Pb isotopes provide a valuable insight into the sources of Pb observed within a sample, allowing for the fingerprinting of their metal's geological source, or production site. Presented here is the application of a state of the art Bayesian mixing model to such a purpose, outlining a 'best practice' and testing of the approach via a number of real-world examples.",,Longman J,,2018,,,,Ph.D. Thesis
Public Perception of Mental Illness: Opportunity for Community-Based Collaborative Intervention,"We explore factors contributing to poor mental healthcare, treatments and help-seeking behaviors among communities in Nigeria and across Africa. The findings from the interview of 25 stakeholders reveal some socio-cultural factors such as negative perceptions, stigmatizations, religious beliefs, and absence of automated supports, which hinder mental healthcare and help-seeking. Also, delays in seeking appropriate medical attention and intake of untested local herbs could lead to severe depressive symptoms, suicidal risks, and adversely affect the mental health of clients. Based on our findings, and in collaboration with the stakeholders, we designed ""Gwam-Okwu"" [Talk to Me]; a culturally-appropriate interactive app that is hyper-localized, safe and secured, and tailored to support communication and collaboration between health workers and clients/relations, personalized self-monitoring, and guided self-learning for the clients.",,"Nkwo M,Suruliraj B,Orji R",,2020,1–7,10.1145/3334480.3383023,https://doi-org.proxy.bnl.lu/10.1145/3334480.3383023;http://dx.doi.org/10.1145/3334480.3383023,Conference Paper
MobiArch '14: Proceedings of the 9th ACM Workshop on Mobility in the Evolving Internet Architecture,"It is our great pleasure to welcome you to MobiArch 2014, the 9th Workshop on Mobility in the Evolving Internet Architecture. This year continues the tradition of presenting the most recent challenges of mobility architectures, covering aspects in system design and new protocols.We live in fast changing times. Wireless access devices by far outnumber stationary Internet hosts and an increasing share of traffic traverses at least one wireless link. With recent advances in technologies for wireless access and mobile devices, mobility has become a fundamental characteristic of today's Internet. Yet, basic architectural issues related to mobility such as efficient mobility management, the locator-identifier split, multi-homing, security, and various operational, deployment concerns are still not fully explored. Moreover, the Internet architecture itself, its endto- end principles and business models require rethinking due to the massive penetration of mobility into the Internet.The mission of MobiArch is to act as a forum where these changes can be discussed and new ideas can be shared. MobiArch has been giving researchers a dedicated environment where to share perspectives with others interested in the various aspects of the new mobile world.This year, we had 17 interesting submissions from the Americas, Asia, Africa, and Europe. The program committee accepted 11 papers that cover a variety of topics, divided in three large groups: new mobility protocol issues, virtualization challenges and solutions, and information centric approaches. We hope that our proceedings will serve as a valuable reference for future mobility research.We also encourage attendees to attend the two keynotes. These valuable and insightful talks can and will guide us to a better understanding of mobility future(s). We will have Marco Gruteser (Rutgers) discussing connected vehicles while Lili Qiu (University of Texas) will address the relevance of physical analytics.",,,,2014,,,,Book
"Wealth Composition, Capital Flows, and the International Financial System","International capital flows play a critical role in the development process. On the one hand, a stable stream of capital flows could augment the capital stock accumulation of a country and, hence, spur economic growth. On the other, volatile capital flows increase the risks that could induce financial and economic crises. Moreover, contrary to the efficient allocation implied by the neoclassical growth theory, Lucas (1990) poses the paradox of ""Why Doesn't Capital Flow from Rich to Poor Countries?"". Recent studies also demonstrate an even stronger phenomenon known as the allocation puzzle or upstream capital flows. That is, fast-growing emerging markets have associated with net capital outflows on average (e.g., Gourinchas and Jeanne 2013). While previous studies provide explanations about cross-country differences in human capital (Lucas 1990), institutional quality (Alfaro et al. 2008), I argue that the capital flows are also explained by differences in natural resources in the current era of financial globalization. In general, I demonstrate the role of initial wealth compositions.In this dissertation, I define capital stock more broadly than the standard neoclassical growth model in terms of wealth accumulation, comprising physical capital, human capital, natural capital, net foreign assets, social capital, and domestic financial capital (as in Gylfason 2004). By exploiting a recent database on wealth accounting by the World Bank, I find that the wealth composition matters in explaining capital flows across 108 countries over 1995-2015. More importantly, results of Chapter 1 suggest that initial abundance measures of subsoil natural resources and net foreign asset positions explain much of the subsequent annualized average net capital inflows. An alternative measure of net capital inflows also suggests a stabilizing role of the valuation effects in the international financial system. In sum, measures of wealth abundance and net capital inflows should be considered carefully in studying the patterns of international capital flows. Results from the typical measure suggest that capital mobility allows subsoil resource-rich countries to invest their resource rents abroad, so they could better smooth the use of resource windfalls. Therefore, the inclusion of natural capital emphasizes the role of economic management in whether to channel rents toward productive investment and human capital to industrialize the economy, or to accumulate foreign assets for exchange rates managements and for precautionary motives due to volatile international commodity prices. It should be noted that there is no evidence on the neoclassical allocative efficiency— the relationship between economic growth rates and net capital inflows.Due to the insignificant finding of the allocative efficiency, Chapters 2 and 3 extend and modify the first chapter's conceptual framework. Chapter 2 investigates not only international capital flows but also some explanations for the persistent global imbalances. Using a unified sustainable growth framework with a broad definition of total wealth, I demonstrate that there could be specific spillover effects (or specific complementarities) rather than an overall complementarity effect, which is simply proxied by real per capita growth rates. For instance, the interaction between human capital and physical capital generates a positive spillover effect, as explained by Lucas (1990). Thus, the departure from the focus on the overall complementarity to specific complementarities and tradeoffs in capital stocks provides us with a way of testing for 13 hypotheses, motivated by the broad literature of international finance and sustainable development. Some of these are about a human capital externality, the global saving glut argument, and negative spillover effects from natural capital on institutions and financial development. I also test for Blecker's (2005) argument on comparative advantage in selling financial assets and find supporting evidence. The implication of such findings implies that the current account (CA) deficit countries with highly developed financial systems have benefited from the current international monetary and financial system (IMFS) through the role of valuation effects. On the other hand, financial liberalization allows subsoil-rich economies to smooth the use of windfalls through foreign reserves accumulation. Other developing countries with CA surpluses due to excess savings, rather than low imports, reflect the flaws in the current IMFS.Chapter 3 is motivated by utilizing theoretical insights from overlapping generations (OLG) models with non-Ricardian equivalence, rather than the assumption of the infinitely lived agent as in previous chapters. I, therefore, examine not only net total capital inflows but also consider the distinction of private and official flows. In addition to the heterogeneities in economies' wealth compositions, I investigate the role of demographic structures by highlighting the aging population phenomenon. In other words, while using the unified sustainable growth framework with a broad definition of wealth, I distinguish between private and official capital flows, and between the relative ratios of young and old groups to the working-age population. All these factors relate to capital flow movements through their effects on saving-investment decisions. Overall findings support the adoption of OLG with non-Ricardian equivalence models in analyzing aggregated and disaggregated capital flows. Also, the inclusion of demographic factors seems to correct for the omitted variable bias. Moreover, cross-country differences in initial wealth compositions are of great importance for different types of disaggregated capital flows, and so policy implications differ accordingly.",,"Baqais UM,Bernasek A,Braunstein E,Koontz S",,2020,,,,Ph.D. Thesis
DRM '04: Proceedings of the 4th ACM Workshop on Digital Rights Management,"The area of Digital Rights Management (DRM) is a unique blend of many diverse subareas. These subareas include Mathematics and Cryptography, Legal and Social aspects, Signal Processing, Game Theory, Information Theory, Software and Systems Design and Business Analysis. The ACM Workshop on Digital Rights Management is an international forum that serves as an interdisciplinary bridge across these areas. Its purpose is to bring together researchers from the above fields for a full day of formal talks and informal discussions, covering new results that will spur new investigations regarding the foundations and practices of DRM.This year's workshop, the fourth in the series, continued this tradition. As in the previous years it was sponsored by ACM SIGSAC and was held in conjunction with the 11th ACM Conference in Computer and Communications Security (CCS). This volume contains the proceedings of the workshop and is a good representation of the diversity of disciplines that contribute to the complexity of DRM. The workshop received 27 submissions (from Africa, Asia, Australia, Europe and North America) out of which 10 were accepted for presentation after a rigorous refereeing process.There were two invited talks at the workshop that covered different ends of the spectrum of Digital Rights Management. The first talk, given by Jean-Jacques Quisquater and Francois-Xavier Standaert, presented a new system for digital cinema applications that was developed by a team of researchers from Louvain-la-Neuve, Belgium. The second invited talk, delivered by Reinaneh Safavi-Naini of the University of Wollongong, Australia, surveyed the cryptographic area of Traitor Tracing --- a basic primitive for DRM technology.",,,,2004,,,,Book
Design Automation of Paper Microfluidic Devices,"The emerging demands for healthcare where access is limited due to political, environmental, or socio-economic factors have been driving research into bio-medical devices that perform in both diagnostic and therapeutic roles at lower costs and greater accessibility. Paper microfluidic devices are used in many applications, particularly medical diagnostics and offer an excellent combination of utility and low cost making them particularly valuable in resource-limited applications and point-of-care usage across a wide variety environmental conditions. Microfluidic biological diagnostics continue to mature as researchers discover new ways to exploit the technological possibilities, and address liabilities. The increasing complexity of paper-based microfluidic devices beyond home pregnancy tests is driving the need to produce new tools and methodologies that enable more robust biological diagnostics and potential therapeutic applications. However, the process of developing new paper microfluidic devices is limited due to having to manually design and fabricate designs to research. Often, researchers must design scores of different devices to find a combination of parameters that functions as expected. In this work, a novel software framework to support automated development of paper-based microfluidic devices is introduced to facilitate both research and fabrication to accelerate the investigative process and reduce material utilization and manpower. Unlike to existing lab-on-a-chip technologies, paper-based microfluidics differs in terms of substrate technologies and use a passive flow method to deliver fluids and reagents for assays. While numerous analogies between microfluidics and semiconductor technologies have been espoused, the physical differences between the fluid dynamics and electrical current are significant which suggests that current trends in physical design for microfluidics must change course in order to be of practical use to designers. Within this framework, a methodology is introduced to address design automation such as dynamically placing and routing microfluidic components in a non-discrete design space while avoiding invalid design layouts, accounting for fluid volume usage, surface area utilization, and the timing required to perform specified biological assays and also optimizing device parameters, enabling researchers to focus on the science and thereby accelerating the development of new, low-resource paper microfluidic devices for a developing world.",,"Potter JM,H. Grover W,Victor Rodgers,Craig Schroeder,Daniel Wong",,2022,,,,Ph.D. Thesis
Integrated Security Framework for Low Cost RFID Tags,"Radio Frequency Identification (RFID) systems are becoming more popular today because of the wide area of applications. It is being used in several industries such as the transport industry, sports, medical and government institutions. Its advantages such as the capacity to store more information than other identification technologies as well as the ease with which data can be read (since it doesn't require line of sight and human intervention), have sparked its widespread use and implementation in the various industries.The most widely used class of RFID tags is the class 1 tag because it offers identification functionality at low cost. Class 1 tags have limited computational power and memory resources. Due to these limitations, common authentication protocols such as AES cannot be implemented on the class 1 tags. As these tags provide essential implementation capabilities for development countries such as South Africa, it is vital that researchers focus on providing adequate security solutions. Taking into account the information security needs and performance limitations of the class 1 tags, we present a lightweight protocol based on the Hopper and Blum's human authentication protocol.",,"Muwanguzi M,Biermann E",,2010,201–208,10.1145/1899503.1899526,https://doi-org.proxy.bnl.lu/10.1145/1899503.1899526;http://dx.doi.org/10.1145/1899503.1899526,Conference Paper
Social and Economic Barriers to Entrepreneurship Entry: A Study of African Women Immigrants in Canada,"This study explored the perceptions of African women immigrants (AWI) in Canada on the social and economic barriers that prevented them from entrepreneurship entry. The aim was to uncover, describe, and document impediments to their self-employment interests and therefore, socioeconomic mobility. A qualitative multiple case study design was adopted and was suitable to understand multiple perceptions and experiences of multiple entities and deconstruct the complex phenomenon studied. Nine participants took part in this study. Eight never engaged in entrepreneurship and one subject matter expert (SME) is a practicing entrepreneur, whose before and after experience and perception expanded knowledge. Data collection involved nine semistructured interviews, including face-to-face and telephone interviews, email follow ups, and participant and non-participant field observations. Open-ended interview questions aided a conversational style dialogue that increased understanding. Major findings revealed that both external and internal factors contributed to AWI's lack of entrepreneurship ambition. However, limited access to resources, opportunity, and support, weakened personal capacities, which compounded the barriers that limited them. These barriers were grounded in race, gender, ethnicity, class, and immigration, constituted as institutional discrimination, stereotyping, and undermined ethnic identity. This further marginalized them more than other female ethnic minority groups and thus, impeded entrepreneurship intentions. This research makes unique contributions by providing culturally sensitive evidentiary data about a unique group of women with complex experiences and informs current entrepreneurship dialogue, grounded in diversity, an area that is yet to be fully explored.",,Ogbolu OV,,2019,,,,Ph.D. Thesis
Near Real-Time Detection of Poachers from Drones in AirSim,"The unrelenting threat of poaching has led to increased development of new technologies to combat it. One such example is the use of thermal infrared cameras mounted on unmanned aerial vehicles (UAVs or drones) to spot poachers at night and report them to park rangers before they are able to harm any animals. However, monitoring the live video stream from these conservation UAVs all night is an arduous task. Therefore, we discuss SPOT (Systematic POacher deTector), a novel application that augments conservation drones with the ability to automatically detect poachers and animals in near real time [Bondi et al., 2018b]. SPOT illustrates the feasibility of building upon state-of-the-art AI techniques, such as Faster RCNN, to address the challenges of automatically detecting animals and poachers in infrared images. This paper reports (i) the design of SPOT, (ii) efficient processing techniques to ensure usability in the field, (iii) evaluation of SPOT based on historical videos and a real-world test run by the end-users, Air Shepherd, in the field, and (iv) the use of AirSim for live demonstration of SPOT. The promising results from a field test have led to a plan for larger-scale deployment in a national park in southern Africa. While SPOT is developed for conservation drones, its design and novel techniques have wider application for automated detection from UAV videos.",,"Bondi E,Kapoor A,Dey D,Piavis J,Shah S,Hannaford R,Iyer A,Joppa L,Tambe M",,2018,5814–5816,,,Conference Paper
Leveraging Artificial Intelligence Techniques for Smart Palm Tree Detection: A Decade Systematic Review,,,"Hajjaji Y,Boulila W,Farah IR",,2022,2823–2832,10.1016/j.procs.2022.09.340,https://doi-org.proxy.bnl.lu/10.1016/j.procs.2022.09.340;http://dx.doi.org/10.1016/j.procs.2022.09.340,Journal Article
A Critical Discourse Analysis of Governance Issues Affecting Public Private Partnership Contracting for Information Systems Implementations: A South African Case Study,"Public Private Partnership (PPP) contracts have drawn considerable media interest due to a number of problems such as cost overruns, mismanagement and failure. The purpose of this paper is to critically analyse media discourse relating to the failure of a PPP contract between the South African Department of Labour (DOL) and Siemens Information Services (SIS). The contract pertained to the provision and implementation of Information and Communication Technology (ICT) services for the DOL. The theoretical foundation for this research is Habermas' theory of communicative action which focuses on normative standards for communication and implications of public speech. Our research builds on a growing literature on critical discourse analysis (CDA) that systematically applies Habermas' validity claims to empirical research on public communication focused on revealing distortions concerning claims of truth, sincerity, legitimacy and comprehensibility. Our study contributes to understanding issues of public accountability of PPP contracts and extends the reach of critical research into PPP contracting for information systems (IS) services and highlights key challenges of the lack of public sector management competences in securing the public interest in PPP engagements.",,"Albertus R,Ngwenyama O,Brown I",,2015,,10.1145/2815782.2815800,https://doi-org.proxy.bnl.lu/10.1145/2815782.2815800;http://dx.doi.org/10.1145/2815782.2815800,Conference Paper
Strategic Market Entry Choices : Experience of Chinese Sme Managers,"There is intensive research in international business studies exploring strategic decisions relating to the choice of entry mode. As a frontier issue the choice of entry mode has been widely recognised as being one of the critical decisions in a firm's internationalisation. However, most of the research primarily focuses upon Western multi-national enterprises (MNEs) rather than small-medium sized enterprises (SMEs). Recently, interest in the international business activities of SMEs has been increasing. Nevertheless, little has been done in light of the choice of entry mode in the SME sector, especially for SMEs from developing countries. This study explored how Chinese SME managers make their strategic market entry choices when entering the UK to address the issue of whether Western MNEs' foreign investment theories are applicable to Asian SMEs. The decision making of entry mode choices involves complicated social processes such as social relationships both in and outside the firm. This research takes a social constructionist paradigm, trying to understand and interpret the Chinese SMEs decision maker's unique experiences, perceived values and embedded Chinese culture that can have great impact on their choice of entry modes. Cohering with this philosophical stance, 10 Chinese SMEs managers in the North East of the England were involved in qualitative interviews and data was analysed through template analysis. The findings of this thesis offer a more holistic picture of SME managers' decision making in terms of their entry mode choices. This study is inconsistent with the more classic motives of firms' internationalisation, such as securing raw materials and seeking low-cost labour as it reveals 2 previously unrecognised motives of Chinese SMEs' internationalisation, namely `seeking entrepreneurial freedom' and 'building their own international teams'. Moreover, 4 entry modes were used by the Chinese SMEs' entering the North East of England markets, including direct exporting, joint venture and wholly-owned subsidiary and internet entry mode. Interestingly, the joint venture mode used by Chinese SMEs in this study is operationally different from traditional joint ventures. Furthermore, a number of influencing factors emerged from the Chinese SME managers' accounts: firm-specific factors, strategy-factors, product-specific factors, networks and social culture factors and the decision maker's personal characteristics. In drawing upon their motives, influencing factors, and entry modes a 3-stage decision making process was discovered which combined rational and cybernetic strategic approaches that have been adopted by Chinese SMEs managers at different levels. Contributively, this study offers alternative understandings of the choice of entry mode. By drawing upon experiences of Chinese SME managers it extends the foreign investment theories based on Western-MNEs and offers a contribution to practice grounded in an Asian-SME context. Significantly, this thesis develops a practice-based framework by integrating factors into the whole decision making process, providing practical guidance for SME managers to inform their entry mode choices.",,Quan R,,2007,,,,Ph.D. Thesis
Chinese Petroleum Corporations' International Oil Trade in Africa,"In this paper, we focus on the international oil trade of Chinese petroleum corporations' in Africa. Initially, we are trying to understand the current oil reserves in Africa. Then the reasons and the situation of Chinese petroleum companies' international oil trade in Africa will be discussed. Afterwards, the weaknesses of Chinese petroleum enterprises' are investigated. Finally, a conclusion based on the research and recommendations that can not only help Chinese petroleum corporations but also apply to other similar corporations will be given.",,Long Z,,2012,279–283,,,Conference Paper
Global Variation in Attack Encounters and Hosting,"Countries vary greatly in the extent to which their computers encounter and host attacks. Empirically identifying factors behind such variation can provide a sound basis for policies to reduce attacks worldwide. However, the main current approach to identify these factors consists of expert opinions with limited empirical validation. In this work, we empirically test hypotheses regarding social and technological factors behind such international variation. We use Symantec's Intrusion Prevention System (IPS) telemetry data collected from around 10 million Symantec customers worldwide.We find that web attacks and fake applications are most prominent in Western Europe and North America. Our results indicate a relationship between countries' wealth and technological sophistication and attack exposure, indicating that attackers probably target developed countries to maximize their profits. Moreover, Eastern Europe hosts disproportionate quantities of attacks. Our statistical analysis reveals a relationship between attack hosting and the combined effect of widespread corruption and computing resources. Surprisingly, China is not among the top 10 attack hosting countries and Africa hosts the smallest quantities of attacks. Our work has important policy implications.",,"Mezzour G,Carley KM,Carley LR",,2017,62–73,10.1145/3055305.3055306,https://doi-org.proxy.bnl.lu/10.1145/3055305.3055306;http://dx.doi.org/10.1145/3055305.3055306,Conference Paper
SACMAT '04: Proceedings of the Ninth ACM Symposium on Access Control Models and Technologies,"It is our great pleasure to welcome you to the 9th ACM symposium on Access Control Models and Technologies - SACMAT 2004. This year's symposium continues its tradition of being the premier forum for presentation of research results and experience reports on leading edge issues of access control, including models, systems, applications, and theory. The mission of the symposium is to share novel access control solutions that fulfill the needs of heterogeneous applications and environments and identify new directions for future research and development. SACMAT gives researchers and practitioners a unique opportunity to share their perspectives with others interested in the various aspects of access control.The call for papers attracted 65 submissions from Asia, Canada, Europe, Africa, and the United States. The program committee accepted 18 papers that cover a variety of topics, including next generation access control models, engineering and analysis techniques for access control policies and models, and security administration. In addition, the program includes a panel on Security for Grid-based computing systems and a keynote speech by Bhavani Thuraisingham on Developments and Directions in Database Access Control. We hope that these proceedings will serve as a valuable reference for security researchers and developers.Putting together SACMAT 2004 was a team effort. First of all, we would like to thank the authors and panelists for providing the content of the program. We would like to express our gratitude to the program committee and external reviewers, who worked very hard in reviewing papers and providing suggestions for their improvements. We would also like to thank Elisa Bertino, this year's Panels Chair, Gail-Joon Ahn, our Proceedings Chair, and Charles Youman, our Registration Chair and Treasurer. Special thanks go to Konstantin Beznosov for maintaining the SACMAT 2004 web site and for his effort in advertising the symposium, to Barbara Carminati for her help in managing the review process, and to Reiner Sailer and Catherine Zhang of the Local Arrangements Committee for their work on the local arrangements. Finally, we would like to thank our sponsor, ACM SIGSAC, for their continued support of these successful meetings.",,,,2004,,,,Book
"Innovations and Interdisciplinary Solutions for Underserved Areas: Second International Conference, InterSol 2018, Kigali, Rwanda, March 2425, 2018","This book constitutes the refereed post-conference proceedings of the Second International Conference on Innovations and Interdisciplinary Solutions for Underserved Areas, InterSol 2018, and the 7th Collogue National sur la Recherche en Informatique et ses Applications, CNRIA 2018, held in Kigali, Rwanda, in March 2018. The 23 papers presented were selected from 56 submissions and issue the following themes: papers dealing with the evolution of performances of solar systems in Africa, papers addressing the issues is public health, telecom papers studying the business model of telecommunication, math models presenting the climatic phenomenon and finally health papers dealing with medical devices that are suitable to underserved areas. The proceedings also contain 7 papers from the co-located 7th CNRIA (Collogue National sur la Recherche en Informatique et ses Applications) focusing on network architecture and security, software engineering, data management, and signal processing.",,"Kebe CM,Gueye A,Ndiaye A,Garba A",,2018,,,,Book
Choosing Small Sets of Policy-Relevant Scenarios by Combining Vulnerability and Diversity Approaches,"Computer simulation models can generate large numbers of scenarios, far more than can be effectively utilized in most decision support applications. How can one best select a small number of scenarios to consider? One approach calls for choosing scenarios that illuminate vulnerabilities of proposed policies. Another calls for choosing scenarios that span a diverse range of futures. This paper joins these two approaches for the first time, proposing an optimization-based method for choosing a small number of relevant scenarios that combine both vulnerability and diversity. The paper applies the method to a real case involving climate resilient infrastructure for three African river basins (Volta, Orange and Zambezi). Introducing selection criteria in a stepwise manner helps examine how different criteria influence the choice of scenarios. The results suggest that combining vulnerability- and diversity-based criteria can provide a systematic and transparent method for scenario selection. Describes an optimization-based method for choosing a small number of scenarios.A combination of criteria related to vulnerability and diversity is used.The method is applied to a real case involving climate resilient infrastructure.",,"Carlsen H,Lempert R,Wikman-Svahn P,Schweizer V",,2016,155–164,10.1016/j.envsoft.2016.06.011,https://doi-org.proxy.bnl.lu/10.1016/j.envsoft.2016.06.011;http://dx.doi.org/10.1016/j.envsoft.2016.06.011,Journal Article
MobiCom '13: Proceedings of the 19th Annual International Conference on Mobile Computing & Networking,"Welcome to ACM MobiCom 2013, the 19th Annual International Conference on Mobile Computing and Networking. Over the years, MobiCom has established itself as a premier forum for publishing and presenting cutting-edge research in mobile systems and wireless networks, and this year's final program continues this wonderful tradition.The high quality and success of MobiCom can be credited to two groups. First and foremost is the authors of all of the submitted papers who submitted their very best research ideas and results. The 28 accepted papers represent leading-edge, and sometimes bleeding-edge, advances in a large variety of important mobile computing topics -- from traditional yet still very important topics, such as improving the efficiency of cellular and Wi-Fi networks, to topics focusing on the use of new wireless technologies in new mobile environments, such as enabling gesture recognition, managing indoor white space networks, and deploying performance improving femto cells. However, the hidden strength of MobiCom comes from the hard work of a very dedicated program committee, which consisted of 41 members from academia, government and industry spread across 7 different countries with expertise in many areas relevant to wireless networking and mobile computing.As Program Committee (PC) chairs, it was our task to keep MobiCom fresh and grow it to keep up with all the new challenges of the wireless and mobile community. This year, we introduced several new initiatives to further enhance the reach and quality of the conference. To increase MobiCom's visibility in industry, we included five PC members from product teams of Cisco, Google, Qualcomm and Broadcom. To ensure the strength and breadth, but most importantly the vision, of the PC, one-sixth of the PC were new members who had previously not served on the MobiCom PC. This year's MobiCom also has an invited industry session, in which speakers from Broadcom, Alcatel-Lucent, Google, and Microsoft will present the latest results and research challenges from industry in an effort to bridge the gap between academic research and how it is, or maybe isn't, relevant to industry.This year's call for papers attracted 207 submissions from five continents: Asia, Europe, Africa, North America and South America. We used HotCRP for handling the paper submission and reviewing, which was done in three phases. In the first phase, each paper was reviewed by at least three PC members, and the top 98 papers were selected for the next round. In the second phase, each paper was reviewed by at least two more PC members. In some cases when the paper was at the intersection of new topics, such as RADAR or robotics, additional expert opinions were solicited. The final phase was the PC meeting held on May 30th and 31st in Redmond, WA. A total of 34 members attended the PC meeting in-person, while 5 members attended the meeting on Skype. Over one and a half days, the PC extensively discussed the merits and flaws of the 60 toprated papers and ultimately accepted 28 papers for final publication in the conferences' proceedings. Across the three phases, each PC member reviewed about 25 papers, such that most round two papers had an average of 6 reviews (a high number for any top-tier conference). To ensure fairness and preserve the anonymity of all authors and reviewers, papers authored by PC chairs were mixed with a random selection of other papers and handled out-of-band by Alex Snoeren, who was the PC co-chair for MobiCom 2012.",,,,2013,,,,Book
Social Dimensions of Information and Communication Technology Policy: Proceedings of the Eighth International Conference on Human Choice and Computers ... Federation for Information Processing),"This book constitutes the refereed proceedings of the 8th International Conference on Human Choice and Computers (HCC8), IFIP TC9, held in Pretoria, South Africa on September 25-26, 2008. The IFIP series publishes state-of-the-art results in the sciences and technologies of information and communication. The scope of the series includes: foundations of computer science; software theory and practice; education; computer applications in technology; communication systems; systems modeling and optimization; information systems; computers and society; computer systems technology; security and protection in information processing systems; artificial intelligence; and human-computer interaction. Proceedings and post-proceedings of refereed international conferences in computer science and interdisciplinary fields are featured. These results often precede journal publication and represent the most current research. The principal aim of the IFIP series is to encourage education and the dissemination and exchange of information about all aspects of computing.",,"Avgerou C,Smith ML,van den Besselaar P",,2008,,,,Book
Baby Boomers’ Intention to Use Branch or Digital Banking Channels in South Africa: An Exploratory Study,"With Baby Boomers comprising a significant and profitable segment of the South African population, understanding the banking behaviour of seniors becomes an important task for banking institutions. With minimal literature dedicated to the banking behaviour of seniors, this study aimed to outline aspects which influenced Boomer's intention to use branch or digital channels and identify reasons for this. Literature highlighted the significant role of behavioural beliefs, trust, normative beliefs, perceived usefulness, perceived secureness and perceived control in banking intention, with gender, computer literacy, banking institution and banking channel used, being the major differences when choosing to walk into a branch or bank electronically. By applying the Kruskal-Wallis test on data collected from 281 Boomers across Gauteng, it was found that the gender of Boomers did not have any differences across branch and digital channels. By applying banking channel split, differences were found among the branch, digital and both channel groups for all six variables for both branch and DB intention. Reasons for these differences were also outlined within. Based on these findings, recommendations for the banking institutions of SA have been provided. Suggestions on future research initiatives are included, with linking banking intention to banking behaviour and conducting comparisons to other generational groups being notable topics.",,"Ramlall S,Hattingh M,Van Deventer P",,2020,74–84,10.1145/3410886.3410915,https://doi-org.proxy.bnl.lu/10.1145/3410886.3410915;http://dx.doi.org/10.1145/3410886.3410915,Conference Paper
"Running, Sweating, and Persistence Hunting [Numbers Don't Lie]","During the two years of its monthly appearance, this column has looked at many objects–cars, turbines, airplanes, windows, mobile phones, and nuclear reactors–made by humans. Todayźs focus is on the human body, specifically the way it keeps itself cool. Before the development of long-range projectile weaponry some tens of thousands of years ago, in Africa, our ancestors had only two ways to secure meat: by scavenging the leftovers of mightier beasts or by running down their own prey. Humans were able to occupy the second of those ecological niches thanks, in part, to two great advantages of bipedalism.",,Smil V,,2016,26,10.1109/MSPEC.2016.7607022,https://doi-org.proxy.bnl.lu/10.1109/MSPEC.2016.7607022;http://dx.doi.org/10.1109/MSPEC.2016.7607022,Journal Article
Speculative Vulnerability: Uncovering the Temporalities of Vulnerability in People's Experiences of the Pandemic,"Pandemic-tracking apps may form a future infrastructure for public health surveillance. Yet, there has been relatively little exploration of the potential societal implications of such an infrastructure. In semi-structured interviews with 23 participants from India, the Middle East and North Africa (MENA), and the United States, we discussed attitudes and preferences regarding the deployment of apps that support contact tracing to contain the spread of COVID-19. Through interpretive analysis, we examined the relationship between persistent discomfort and vulnerability when using such apps. Such an examination yielded three temporal forms of vulnerability: real, anticipatory, and speculative. By identifying and defining the temporalities of vulnerability through an analysis of people's pandemic-related thoughts and experiences, we develop the overlapping discourses of humanistic infrastructure studies and infrastructural speculation. In doing so, we explore the concept of vulnerability itself and present implications for the study of vulnerability in Human-Computer Interaction (HCI) and for the oversight of app-based public health surveillance.",,"Seberger JS,Obi I,Loukil M,Liao W,Wild DJ,Patil S",,2022,,10.1145/3555586,https://doi-org.proxy.bnl.lu/10.1145/3555586;http://dx.doi.org/10.1145/3555586,Journal Article
Domain Enriched Learning for Brain Image Analysis,"Medical imaging techniques such as magnetic resonance imaging (MRI), computed-tomography (CT), X-ray, ultra-sound, positron emission tomography (PET), and mammography have been widely used over the past few decades for diagnosis and treatment of many medical conditions. Often, medical image interpretation is performed by human experts such as neurosurgeons, pathologists, and radiologists. Given the complex nature of these pathological images and the burden on human experts, it is desired to at least partially automate medical image interpretation using computer-aided methods. With recent advance in machine learning techniques, the goal of achieving automated medical image analysis is now practically realizable. In this dissertation we address two important and challenging medical imaging problems: 1) Brain image super-resolution (SR) and 2) Brain image segmentation. Finally, we combine the frameworks of enhancement and segmentation to pave a way for cost-effective diagnosis of infant-hydrocephalic patients. The first part of the thesis addresses the issue of enhancing the resolution of MR brain images via deep learning techniques. High resolution Magnetic Resonance (MR) images are desired for accurate diagnostics. In practice, image resolution is restricted by factors like hardware and processing constraints. Recently, deep learning methods have been shown to produce compelling state-of-the-art results for image enhancement/super-resolution. Paying particular attention to desired hi-resolution MR image structure, we propose a new regularized network that exploits image priors, namely a low-rank structure and a sharpness prior to enhance deep MR image super-resolution (SR). Our contributions are then incorporating these priors in an analytically tractable fashion as well as towards a novel prior guided network architecture that accomplishes the super-resolution task. This is particularly challenging for the low rank prior since the rank is not a differentiable function of the image matrix (and hence the network parameters), an issue we address by pursuing differentiable approximations of the rank. Sharpness is emphasized by the variance of the Laplacian which we show can be implemented by a fixed feedback layer at the output of the network. As a key extension, we modify the fixed feedback (Laplacian) layer by learning a new set of training data driven filters that are optimized for enhanced sharpness. Experiments performed on publicly available MR brain image databases and comparisons against existing state-of-the-art methods show that the proposed prior guided network offers significant practical gains in terms of improved SNR/image quality measures. Because our priors are on output images, the proposed method is versatile and can be combined with a wide variety of existing network architectures to further enhance their performance.The second part of the thesis leverages the discriminative power of sparse signal representation to segment post-operative CT hydrocephalic images. Hydrocephalus is a medical condition in which there is an abnormal accumulation of cerebrospinal fluid (CSF) in the brain. Segmentation of brain imagery into brain tissue and CSF (before and after surgery, i.e. pre-op vs. post-op) plays a crucial role in evaluating surgical treatment. Segmentation of pre-op images is often a relatively straightforward problem and has been well researched. However, segmenting post-operative (post-op) computational tomographic (CT)-scans become more challenging due to distorted anatomy and subdural hematoma collections pressing on the brain. Most intensity and feature based segmentation methods fail to separate subdurals from brain and CSF as subdural geometry varies greatly across different patients and their intensity vary with time. We combat this problem by a learning approach that treats segmentation as supervised classification at the pixel level, i.e. a training set of CT scans with labeled pixel identities is employed. Our contributions include: 1) a dictionary learning framework that learns class (segment) specific dictionaries that can efficiently represent test samples from the same class while poorly represent corresponding samples from other classes, 2) quantification of associated computation and memory footprint, and 3.) a customized training and test procedure for segmenting post-op hydrocephalic CT images. Experiments performed on infant CT brain images acquired from the CURE Children's Hospital of Uganda reveal the success of our method against the state-of-the-art alternatives. We also demonstrate that the proposed algorithm is computationally less burdensome and exhibits a graceful degradation against number of training samples, enhancing its deployment potential.In the final contribution of the thesis, deep-learning frameworks for simultaneous enhancement and segmentation are developed for infant hydrocephalic patients. Post-infectious Hydrocephalus in infants is a major health problem in majority of the sub-Saharan African countries. As described above, segmentation of the affected brain imagery into brain tissue and cerebrospinal fluid (CSF) plays a crucial role in evaluating the surgical treatment. Owing to cost constraints, often computational tomographic (CT) scans are used for analysis and treatment purposes. However, exposing infants to radiation for a longer time is not recommended. To counter these challenges, deployment of low-field Magnetic Resonance (MR) devices is considered as an viable option in the developing nations. However, the scans obtained from these devices often result in poor quality images which are difficult to interpret and hence may lead to inaccurate volumetric analysis. Recently, deep learning frameworks have pushed the boundaries for several image enhancement and segmentation applications. In this work, we propose a deep learning framework that jointly enhances and segments the brain images and extends the range of biological phenomena observed by low field MR image devices. We demonstrate the capability of our proposed framework on our own database of infant hydrocephalic patients and on normal brain images obtained from National Institute of Health (NIH) data base. The results obtained on extremely noisy and un-interpretable images are so revealing that developing a reliable low- field MR imaging is now made a realistic possibility. The thesis is then concluded by pointing towards appropriate future directions wherein domain-enrichment for brain image analysis can be achieved via other sophisticated deep networks that learn important structural properties of the brain.",,"Cherukuri V,Vijaykrishnan Narayanan",,2020,,,,Ph.D. Thesis
Mastering MySQL 4,"From the Publisher:MySQL is the leading open-source database platform, and its popularity has grown with the widespread adoption of PHP for interactive web applications. With version 4, MySQL has added many new features and shown itself to be ready for widespread use. Ideal for both administrators and developers and suited to SQL beginners and veterans alike, Mastering MySQL 4 delivers a comprehensive guide to this new version as well as to database design and normalization -- functions often ignored in database development. You'll learn how to configure and optimize MySQL for high-volume usage and get the most out of its transactional and nontransactional table types. You'll also discover how to perform such standard database tasks as backup, maintenance, and replication plus manage security issues and program with MySQL (references are included for most popular programming languages, including PHP, Perl, Java, Python and C/C++). Ian Gilfillan is lead developer for South Africa's premier news portal, has developed and taught web and database programming courses, and has written technical articles on MySQL.",,Gilfillan I,,2002,,,,Book
System Architecture for Delay Tolerant Media Distribution for Rural South Africa,"Wireless communication offers access to information even to users living in areas where little to no access to affordable communication channels is available. Delay Tolerant Networks (DTNs) enable content distribution in such areas, using mobility of devices and avoiding the need for traditional network infrastructure. In DTNs, data is passed from mobile device to mobile device, whenever possible, in an intelligent way. DTNs have the potential to reach out to under-served regions where cellular Internet access (3G, LTE, and beyond) might be expensive or unavailable. We are interested in DTNs for distributing media from cities to under-served rural areas. The content is distributed to the target destinations, using either public transportation or commuting vehicles such as taxis, equipped with wireless DTN-enabled devices. At each target destination, a micro-entrepreneur business is established with the help of our network: Micro-entrepreneurs use DTN-enabled projectors (also referred to as cinemas-in-a-backpack) to deliver entertainment content at low cost, and exploit the opportunity to create a micro-business around the show events. In this paper, we introduce the DTN system setup, present performance results of laboratory tests and test with a local commuter train of periodic and predictable mobility. Further, we present the target scenario and specific technical challenges. We aim to explore opportunities for a rural, under-served region in the north of Pretoria, South Africa.",,"Galati A,Bourchas T,Siby S,Mangold S",,2014,65–72,10.1145/2643230.2643239,https://doi-org.proxy.bnl.lu/10.1145/2643230.2643239;http://dx.doi.org/10.1145/2643230.2643239,Conference Paper
Modelling Mixed Crop-Livestock Farms for Supporting Farmers’ Strategic Reflections: The CLIFS Approach,,,"Le Gal PY,Andrieu N,Bruelle G,Dugué P,Monteil C,Moulin CH,Penot E,Ryschawy J",,2022,,10.1016/j.compag.2021.106570,https://doi-org.proxy.bnl.lu/10.1016/j.compag.2021.106570;http://dx.doi.org/10.1016/j.compag.2021.106570,Journal Article
A Context-Aware Multi-Channel Messaging Framework for African Banks: Design and Implementation,"Customers of Financial Service Institutions (FSIs) subscribe to different types of alerts occurring on their accounts. The Single Channel Messaging (SCM) model is predominantly used by most Banks in Africa. However, the number of supported platforms and messaging formats limits the SCM Model and in the case where FSIs make use of multiple channels, these are not integrated. In addition, SCM does not provide a way of distinguishing between communication channels based on urgency or priority of the messages which need to be delivered to the customers. Consequently, this research work investigated and reviewed the existing approaches, publicly available platforms, web and mobile applications used by FSIs for interacting with their clients. Based on this, we derived the technical requirements for the implementation of a model for Multi-Channel Messaging (MCM) that addresses the weaknesses of SCM. Further, in this paper we present the proposed framework for the MCM model. The model was implemented using the problem-centred approach of the Design Science Research Methodology to derive the requirements of the MCM system from the SCM system, we further used Use-Case evaluation method to analyse the outcome of the design.",,"Salami O,Mtsweni J",,2019,224–230,10.1145/3305160.3305162,https://doi-org.proxy.bnl.lu/10.1145/3305160.3305162;http://dx.doi.org/10.1145/3305160.3305162,Conference Paper
EuroSys '11: Proceedings of the Sixth Conference on Computer Systems,"It is our great pleasure to present the proceedings of the 6th ACM EuroSys Conference on Computer Systems -- EuroSys 2011. We are confident that this year's conference continues to grow the reputation and influence of EuroSys with a set of excellent papers. These cover a wide range of topics, from real-world configuration management and debugging to hardware reliability, from mobile devices to clouds, from fault tolerance to energy management.This year's call for papers attracted a record number of 161 submissions, of which 78 came from North America, 49 from Europe, 25 from Asia, two each from South America and Australia, and one each from Central America and Africa. Seven papers were rejected for violating formatting requirements, which left 154 papers to be reviewed by the PC in two rounds with limited help from outside experts. In a first round of reviewing, each paper received a minimum of three reviews, leading to the elimination of 86 papers. Their authors were notified immediately, to maximise their opportunity to improve the paper for a possible re-submission to another conference.The remaining 68 papers each received a minimum of three further reviews in the second round, after which reviews were sent to authors for rebuttal. For the vast majority of papers the reviews were quite consistent, reflecting the quality and commitment of the PC members. In a few cases where there was significant disagreement between reviews, further reviews were obtained before the PC meeting. The vast majority of reviews were written by the PC members themselves.The rebuttal process was a first for EuroSys, and in a number of cases helped to provide valuable clarifications (including one case where a paper's strongest supporter downgraded their score after reading the rebuttal). A second novelty was the move to double-blind reviewing, where PC members did not know the authors of the papers they reviewed and discussed. Authors' identities were only revealed at the end of the PC meeting. There was strong consensus that this had worked well, that in the vast majority of cases PC members really did not know the origin of a paper and that this helped the PC to minimise bias.At the meeting, the PC applied very high quality standards, and in the end selected 24 papers. A particularly rigorous standard was applied to papers authored by PC members, and only two of nine such papers were accepted. Given the high standard applied, it is particularly pleasing that ten European papers were accepted, giving Europe the highest acceptance rate of all geographic regions.EuroSys 2011 takes place in Salzburg, Austria. The conference begins on April 10 with a day dedicated to workshops and tutorials held at the Faculty of Law of the University of Salzburg. In total, eight workshops and two tutorials have been selected to provide a forum for the latest research in multicore, cloud computing, embedded, security, and social network systems. The main conference is held on April 11-13 at the newly renovated Aula of the University of Salzburg across the street of the world-famous Festspielhaus.We hope that you will find these proceedings interesting and thought provoking, and that those who attend the conference and its associated workshops and tutorials will find this a stimulating experience.",,,,2011,,,,Book
Editorial Message: Special Track on Web and E-Business Applications,"The World WideWeb has become the standard computing platform for the development of new-generation information systems. A new tide of Web-based e-business applications (such as corporate portals, network-based supply chains and market places, etc.) is driving the need for a more open, flexible and distributed infrastructure, together with appropriate development methodologies and theoretical settings. Today's web applications involve skills from many different areas of computer science, including databases, AI and agent based applications, programming languages and algorithms, distributed computing, information retrieval, semantic modeling, etc. For this reason we proposed a track on Web and E-business applications based on the following main topics: data models for the World Wide Web, Web data management, languages for the World Wide Web and XML, E- business and Web services, transactions on the World Wide Web, security and integrity issues for the WWW, query systems for the World Wide Web, management and storage of Web information, information retrieval and search engines for the Web, Web semantics, data integration over the World Wide Web, data-intensive applications on the World Wide Web, Web architectures.We received 30 submissions, which were extensively reviewed for originality, significance, technical soundness and clarity of presentation. The submitted papers covered most of the proposed topics. The number of submissions distributed on each continent has been the following: 16 from Europe (53%), 9 from North America (30%), 3 from Asia (10%), 1 from Africa (3%) and 1 from Australia (3%). 12 papers corresponding to the 40% of the submitted papers have been selected for presentation at the conference, with the following distribution: 7 from Europe, 3 from America, 1 from Asia and 1 from Australia.",,"Comai S,Tanca L",,2002,1086–1087,10.1145/508791.509005,https://doi-org.proxy.bnl.lu/10.1145/508791.509005;http://dx.doi.org/10.1145/508791.509005,Conference Paper
"Computational Models and Tools for Analysis, Prediction, and Control of Infectious Diseases","Infectious disease modeling is used to examine pathogen transmission retrospectively and forecast outbreaks preemptively. Model results help public health authorities to optimize disease control measures, preventing catastrophic loss of lives in humans and animals. Yet, several fundamental challenges arise in infectious disease modeling. A critical problem involves modeling new and evolving pathogens for realistic simulations and reliable predictions of outcomes. Another concern is the lack of data related to infectious diseases. Epidemic modelers often face data inadequacy with host networks and disease incidence. This dissertation proposes remedies to challenges associated with infectious disease modeling, outbreak prediction, and host movement data.In response to vector-borne disease modeling challenges, this dissertation first takes a mechanistic approach. To realistically model the infection process, a novel interconnected network model is designed for the mosquito-vectored Zika virus, which links homogeneous vector populations with heterogeneous human contact networks. The model incorporates seasonal variations in mosquito abundance and characterizes hosts based on age group and gender. The aim is to develop a detailed model for an accurate representation of pathogen dynamics while keeping it computationally tractable. An event-based simulation tool is developed based on the non-Markovian Gillespie algorithm. This work investigates effects of seasonal variations on outbreak size, the role of sexual transmission in sustaining the pathogen, and relative contributions of key model parameters using a sensitivity analysis.A framework to improve machine learning performance for predicting dengue fever cases is developed in a data-driven approach. The goal is to fill in temporally limited human case data from spatially adjacent populations. The method ranks and sorts time-series data from peripheral locations around a target location as predictor variables commonly referred to as features. Metrics are computed from windowed time-shifted cross-correlation of incidence data, spatial distance, and historical prevalence to rank feature variables. A window detection method presented in this work analyzes incidence data to identify time intervals with significant outbreaks. The framework achieves improved prediction performance and works well with recurrent neural network (RNN) architectures. Performance gains are compared using different time window allocation methods for three distinct prediction models: linear, long short-term memory (LSTM), and gated recurrent units (GRU).Availability of data also affects applicability of mechanistic models. In the United States, farm animal movements are not tracked by a central authority. Lack of animal movement data is a significant obstacle in using network models to analyze infectious outbreaks in meat-producing industries. As an immediate solution, a novel method is presented to generate movement networks from limited data available in the public domain. A custom configuration model is developed for network generation that uses aggregate data from farm animal movement-related surveys and the U.S. agricultural census. A hypothetical spread of the African swine fever virus (ASFV) is simulated in a generated network to analyze how network structure affects pathogen dispersal. A node centrality-based analysis is performed to identify important farm operation types and evaluate how targeted control measures affect outbreaks.The experience of working with infectious disease models for the U.S. meat-producing industry revealed fundamental problems linked to trust and business data sharing. The U.S. beef cattle industry lacks adequate traceability, as most farm owners consider such data confidential, possibly harming their businesses if exposed. Blockchains, also known as distributed ledgers, have gained popularity in industrial supply chains because of their unique features of data immutability and transparency. A smart contract-based supply chain framework is designed using a private blockchain network. This system supports anonymity for users to protect their identities and lets everyone store data locally while ensuring the blockchain records any change in data with cryptographic proofs. The framework presented contains functionalities to perform business transactions, transfer animal data, conduct anonymous surveys, and trace animals.This work has original contributions in network epidemic models, data-driven prediction tools, network generation algorithms, and data management frameworks. It combines knowledge from social network analysis, graph theory, epidemiology, machine learning, statistics, cryptography, computer networks, and computational science to improve infectious disease modeling, analysis, and control. The knowledge gained here is generalizable to applications beyond specific cases presented in this dissertation.",,"Ferdousi T,Lee Cohnstaedt,Don Gruenbacher,David Amrine",,2021,,,,Ph.D. Thesis
Eukasimbiosys: A Stochastic Discrete Event-Based Simulation Software for in-Silico Study of Insulin Signaling and Metabolism in Cardiac Myocytes,"In this dissertation we first elucidate how the Stochastic Discrete Event Simulation (SDES) could be applied in capturing the behavior of biological processes as sets of biological events (bioevents) with random holding times . Then we introduce the architecture of ‘ eukaSimBioSys ’ which is designed for system-wide simulation of a eukaryotic cell. The model repository is one of the essential components of our proposed architecture, which comprises reusable modules of parametric models. Each of these parametric models once coupled with a proper parameter set is then applied to capture the holding time of a specific bioevent. These models are physicochemical models that attempt to abstract bimolecular interactions (i.e. modifications, associations, translocations, localizations, etc.) into a parametric probability distribution function of time. Typical interactions include: reaction, receptor-ligand binding, protein-protein binding, chromatin remodeling, transcription, translation, splicing, etc. The previous researchers have already started building this model library and in this work we add four new models (i) ligand-receptor binding, (ii) DNA fluctuations, (ii) chromatin remodeling, and (iii) splicing. For the first one we have developed both the eukaryotic and prokaryotic variants of the model, where as the rest are specific to eukaryotes. These models have been validated with the published experimental data where empirical results were available. Cell activity is the product of an intricate interaction among three main cellular networks: Signal Transduction Network (STN), Transcription Regulatory Network (TRN), and Metabolic Network (MTN). Each cellular function composed of one or more edges within or across these networks. Hence, system-wide study of a cell requires clear and explicit definition of these networks. We have incorporated the semantic of these networks in ‘ eukaSimBioSys ’ by designing an object-oriented database to hold the layout of these three networks along with their inter-relationships. We have populated these databases for ‘ human BCell ’ and ‘ human cardiac myocyte ’ from data available in literature and other databases. Despite the advances in health science, and discovery of new drugs, still heart disease is the most life threatening disease in both industrial and developing countries. Cardiac myocytes are the main players of the perpetual heart contraction function and are among the most energy consuming tissues in the body. Any changes in their normal metabolism can lead to severe consequences for an individual. Glucose and fatty acids comprise the major sources of energy for the myocardial cells, the interplay between these two sources is predominantly controlled by insulin. As the ultimate goal of this dissertation we have incorporated all the models developed in this dissertation and elsewhere into ‘ eukaSimBioSys ’ and utilized that to conduct unique in-silico experiments for studying the effects of insulin on metabolism of heart muscles. We exploit the features and capacities of our software by conducting six in-silico experiments where we proved its outstanding potentials in regenerating the experiential data and performing hypothesis testings by applying the experimental conditions in-silico. The biological facts that we validated in-silico briefly include: plasticity of cardiac myocytes, contributions of exogenous glucose and fatty acid in myocardial energetics, transcription regulation of insulin, and the effect of genetic null-mutations on metabolic pathways. One of the unique features of ‘ eukaSimBioSys ’ that was demonstrated throughout an in-silico experiment was the ability of the software to perform the system-wide simulation of myocardial cellular networks fora prolonged time (48 hours). To construct the SRN, TRN, and MTB for the experiment we incorporated the information from three major databases (i.e. KEGG, BiGG, HumanCyc) along with data from exhaustive literature searches. ‘ eukaSimBioSys ’ features variety of promising applications in the biology and health science. It could be applied to suggest the more promising experimental condition for the experimentalist or help investigating new pathways and regulatory mechanisms. Another very important application of this software is in rebuilding the disease scenarios such as hyperglycemia, diabetes, hypertension, ischemia, etc. In-silico investigation on the effects and side-effects of a new drug is another potential application of this emerging software. Note that utilizing ‘ eukaSimBioSys ’ for the above purposes might subject to certain case based enhancements to the current version of the software. (Abstract shortened by UMI.)",,Mazloom AR,,2008,,,,Ph.D. Thesis
Mobility Practices of the Urban Poor in Ahmedabad (India),"Urban poverty, a prominent issue in the rapidly urbanising developing world, consists of many interrelated aspects in poor people's lives. One such aspect is accessibility, which determines the crucial links between housing, labour markets and other amenities. Relatively little is known about how poor people negotiate the complexities of their daily lives in relation to their mobility choices with respect to existing transport systems, especially in Indian cities. This thesis argues that the poor should be viewed as 'disadvantaged citizen' rather than 'disadvantaged commuters' or 'vulnerable road users' as often described in the transportation studies in India and elsewhere. It is important to ask why the poor make certain mobility-related choices and how these choices shape their own efforts to deal with poverty. This thesis develops a conceptual model linking poverty and mobility debates by employing social practice theory for understanding and structuring mobility related practices of the poor. Further, the conceptual model is pitched in the larger international debates of informality, poverty alleviation and sustainable mobility. To situate the mobility practices of the poor, Ahmedabad is selected as a case-study which represents the dynamics of poverty, informality and intraurban relocation and displacement coupled with some innovative urban projects which, at least in terms of rhetoric, are engaged in developing more sustainable mobility and with poverty alleviation. This study adopts an inductive research strategy based around 'building theory' where the focus is on understanding the poor's own efforts to deal with their mobility and poverty. A mixed methods approach is followed involving qualitative narratives of individuals and a quantitative household survey, supported by secondary documentary analysis. This thesis extensively uses the qualitative narratives of the poor to build empirical knowledge about the differential sub-groups within the poor and to understand the dynamics of poverty in their mobility related decision-making. A range of social practices was identified by the research, which have developed around the low affordability of transport. The poor people are largely dependent on the human-powered transport modes like cycling and walking. The poor are found to seek shelter-livelihood-mobility balance variably across their locations, and differing based on their livelihoods and other social categories like gender. The prevailing informality in housing or job markets is often helpful for poor households to not only minimise transport but to also move out of poverty over a period of time, at least, in some cases. However, the current mobility practices of the poor based on walking, cycling and use of shared or public transport, in spite of their low energy consumption, are being marginalised in the official urban and transport planning in Ahmedabad. The poor face an intrinsic paradox in their mobility to access the various facets of the city; on one side, they resist motorised trips due to low-affordability and on the other side, even if some of them want to travel longer distances to access better opportunities, they are constrained in the absence of an affordable and reliable transport service in the city. Finally, this thesis makes a case for more inclusive and integrated policies around shelter-security, livelihood protections and sustainable transport linked infrastructure provision for the poor people in the cities of India. It is crucial that improved articulation and understanding of the social dimensions of transport should attract greater research and policy attention in India in future years.",,Joshi R,,2014,,,,Ph.D. Thesis
Challenges and Solutions Implementing an SMS Text Message-Based Survey CASI and Adherence Reminders in an International Biomedical HIV PrEP Study (MTN 017),,,"Brown W,Giguere R,Sheinfil A,Ibitoye M,Balan I,Ho T,Brown B,Quispe L,Sukwicha W,Lama JR,Carballo-Diéguez A,Cranston RD",,2018,78–86,10.1016/j.jbi.2018.02.018,https://doi-org.proxy.bnl.lu/10.1016/j.jbi.2018.02.018;http://dx.doi.org/10.1016/j.jbi.2018.02.018,Journal Article
Session Details: Tutorials,"It is our great pleasure to welcome you to the WWW 2016 Tutorials. We received 21 proposals from all around the world covering a broad range of topics. We evaluated them regarding relevance, quality, and novelty, selecting 5 half-day tutorials and 2 full-day tutorials. We also took in account the coverage of the different areas related to WWW as well as the potential audience, to schedule them in two consecutive days with the minimal audience interest overlap.The morning of the first day includes the following four tutorials: Computational Social Science for the World Wide WebCentrality Measures on Big GraphsThe afternoon of the first day includes the following four tutorials: Computational Social Science for the World Wide Web (continued)Cryptographic Currencies Crash CourseThe second day starts with three tutorials: Building Decentralized Applications for the Social WebAutomatic Entity Recognition and Typing in Massive Text CorporaMining Big Time-series Data on the WebThe final afternoon includes the last three tutorials: Building Decentralized Applications for the Social Web (continued)Analyzing sequential User Behavior on the WebThe call for tutorials attracted submissions from United States, Europe, Asia, Africa and South America. Review and acceptance statistics are as follows: WWW 2016 Tutorials Reviewed -21 Accepted - 7.We believe that the program provides a good balance between several trending topics such as deep learning, social media analysis, graph mining, crowdsourcing, knowledge databases, mobile data, etc. Hence we hope that you will find the tutorial program interesting, providing you with a valuable opportunity to learn and share ideas with other researchers and practitioners from institutions around the world.",,"Tiropanis T,Weber M",,2016,,,,Conference Paper
Spatiotemporal Anomaly Detection: Streaming Architecture and Algorithms,"Anomaly detection is the science of identifying one or more rare or unexplainable samples or events in a dataset or data stream. The field of anomaly detection has been extensively studied by mathematicians, statisticians, economists, engineers, and computer scientists. One open research question remains the design of distributed cloud-based architectures and algorithms that can accurately identify anomalies in previously unseen, unlabeled streaming, multivariate spatiotemporal data. With streaming data, time is of the essence, and insights are perishable. Real-world streaming spatiotemporal data originate from many sources, including mobile phones, supervisory control and data acquisition enabled (SCADA) devices, the internet-of-things (IoT), distributed sensor networks, and social media.Baseline experiments are performed on four (4) non-streaming, static anomaly detection multivariate datasets using unsupervised offline traditional machine learning (TML), and unsupervised neural network techniques. Multiple architectures, including autoencoders, generative adversarial networks, convolutional networks, and recurrent networks, are adapted for experimentation. Extensive experimentation demonstrates that neural networks produce superior detection accuracy over TML techniques. These same neural network architectures can be extended to process unlabeled spatiotemporal streaming using online learning. Space and time relationships are further exploited to provide additional insights and increased anomaly detection accuracy.A novel domain-independent architecture and set of algorithms called the Spatiotemporal Anomaly Detection Environment (STADE) is formulated. STADE is based on federated learning architecture. STADE streaming algorithms are based on a geographically unique, persistently executing neural networks using online stochastic gradient descent (SGD). STADE is designed to be pluggable, meaning that alternative algorithms may be substituted or combined to form an ensemble. STADE incorporates a Stream Anomaly Detector (SAD) and a Federated Anomaly Detector (FAD). The SAD executes at multiple locations on streaming data, while the FAD executes at a single server and identifies global patterns and relationships among the site anomalies. Each STADE site streams anomaly scores to the centralized FAD server for further spatiotemporal dependency analysis and logging. The FAD is based on recent advances in DNN-based federated learning.A STADE testbed is implemented to facilitate globally distributed experimentation using low-cost, commercial cloud infrastructure provided by Microsoft™. STADE testbed sites are situated in the cloud within each continent: Africa, Asia, Australia, Europe, North America, and South America. Communication occurs over the commercial internet. Three STADE case studies are investigated. The first case study processes commercial air traffic flows, the second case study processes global earthquake measurements, and the third case study processes social media (i.e., Twitter™) feeds. These case studies confirm that STADE is a viable architecture for the near real-time identification of anomalies in streaming data originating from (possibly) computationally disadvantaged, geographically dispersed sites. Moreover, the addition of the FAD provides enhanced anomaly detection capability. Since STADE is domain-independent, these findings can be easily extended to additional application domains and use cases.",,"Siegel B,Chong E,Maciejewski A,Young P",,2020,,,,Ph.D. Thesis
The Global Agricultural Concept Scheme and Agrisemantics,"Key concepts from three thesauri about agriculture and nutrition--AGROVOC, CAB Thesaurus, and NAL Thesaurus--have been merged into a Global Agricultural Concept Scheme (GACS). The respective partner organizations--Food and Agriculture Organization of the UN (FAO), CAB International (CABI), and the USDA National Agricultural Library (NAL)-- undertook this initiative in 2013 with the goal of facilitating search across databases, improving the semantic reach of their databases by supporting queries that freely draw on terms from any mapped thesaurus, and achieving economies of scale from joint maintenance. The GACS beta release of May 2016 has 15,000 concepts and over 350,000 terms in 28 languages.The creation of GACS began by mapping three sets of 10,000 frequently used concepts from the three thesauri to each other, pairwise. Mappings were vetted by experts; vetted mappings were algorithmically checked for awkward clusters, or ""lumps""; and lumps were resolved through discussion on teleconferences and in meetings--for example, by drawing a line between ""energy intake"" (related to organisms) and ""energy consumption"" with the narrower ""fuel consumption"" (related to natural resources). Mappings were manually corrected, and GACS was iteratively regenerated, until the set of concepts was considered stable enough for publication as GACS Beta.Some inevitable results of this process of aggregation, such as overlapping labels, have already been fixed. Other issues, such as concepts with multiple hierarchical relations (""polyhierarchy""), have yet to be tackled. The working group has revived a classification scheme, developed jointly in the 1990s, to tag concepts by thematic group. Concepts are being typed as chemical, geographical, organisms, products, or topics. Alongside generic thesaurus relations to broader, narrower, and related concepts, organisms will be related to relevant products.GACS is seen as a first step for Agrisemantics, an emerging community network of semantic assets relevant to agriculture and food security. Within Agrisemantics, the general-purpose, search-oriented concepts of GACS are intended to serve as a hub for concepts defined, with more precision, in a multitude of ontologies modeled for specific domains. Ontologies, in turn, are intended to provide global identity to concepts used in a vast diversity of quantitative datasets, such as sensor readings and crop yields, defined for a multitude of software applications and serialization formats.Such semantic authority control of data elements could support, for example, an analysis of the yield gap in sub-Saharan Africa. A wheat data element, labeled 'GW' in a phenotype dataset, could be mapped to the concept 'grain weight' as defined and globally identified in the CGIAR Crop Ontology. In turn, the Crop Ontology concept could be mapped to the broader concept 'Grain' in GACS. Searches could return not only datasets about grain weight, but references to published papers where the weight of the grain was studied.Agrisemantics aims at improving the discoverability and semantic interoperability of agricultural information and data for the benefit of researchers, policy-makers, and farmers with the ultimate goal of enabling innovative responses to the challenges of food security under conditions of climate change. Achieving these goals will require innovation in processes for the cooperative maintenance of linked semantic assets in the modern Web environment.",,"Baker T,Caracciolo C,Doroszenko A,Finch L,Suominen O,Suri S",,2016,14–15,,,Conference Paper
The Worth of Water: Evaluating Interventions to Achieve Financially and Operationally Sustainable Urban and Rural Water Systems in Low-Income Settings,"""We only know the worth of water when the well is dry,"" said Benjamin Franklin, quoting an old English proverb. He might have been talking about how the world is not on track to achieve the Sustainable Development Goals (SDGs) for clean water, despite clear evidence that investments in sustainable and resilient water and sanitation services pay for themselves by saving lives. Inadequate access to safe water, sanitation, and hygiene (WASH) causes 829,000 annual deaths globally, 1.9% of the global burden of disease, and economic losses estimated at $260 billion annually (WHO 2012, WHO 2016). In East Africa, severe and persistent drought periods interspersed with flooding have plagued the region for years, affecting water access and the health and livelihoods of millions. Unreliable and unsafe surface water increases the dependence of rural populations in Ethiopia on expensive mechanized boreholes to extract groundwater. However, borehole performance is sub-par without adequate resource allocations, effective monitoring of borehole functionality, and reliable maintenance services. Runtime sensors attached to 185 boreholes in the Afar Region of Ethiopia report that breakdowns occur 3-6 times a year, downtimes last for months, and uptimes range from 60 to 80 percent. Piped services managed by water utilities face similar reliability issues. There are significant budget gaps for capital maintenance globally, making it difficult for water utilities to conduct needed upgrades and repairs to reduce non-revenue water and improve service reliability and coverage. In four case water utilities studied representing low-income, middle, and high-income countries, maintenance budgets were insufficient to keep on top of repairs and replace assets at end of life, which will lead to declining service levels. In some cases, water services are partly or mostly subsidized, but not enough to fill funding gaps. To fill funding gaps and sustain what is already built, national governments and international donors should acknowledge that tariffs will not be able to cover the life cycle costs of water services. Subsidizing ongoing operations and maintenance (O&M) will continue to be critical to improving service quality and provider performance. The water sector is a system made up of actors from governments, regulators, and funders. Factors inside the system boundary that support sustainability include governance, financing, policy, regulations, technical capacity, environmental considerations, and accountability mechanisms. Together, these actors and factors form a complex system with non-linear and disproportional feedback relationships. For example, when actors fail to coordinate and plan asset management together, the ad-hoc maintenance systems they develop not only reach sub-par functionality levels but waste resources. In Afar, poor quality construction, maintenance, and management of water schemes made the region a candidate for development interventions in asset management and real-time monitoring. However, implementers failed to consider how merely collecting more monitoring data without understanding the data usage and support requirements would impact under-resourced staff. In this work, system dynamics modeling is used to quantify how increased support for O&M could turn around sub-par service delivery models by prioritizing capacity building and long-term reliability over constantly falling behind on fixing failures. Sub-par water service delivery significantly impacts the most vulnerable and last-mile communities. The rural pastoral population in Afar has been poorly served by policy establishing community management bodies to operate and maintain complex motorized borehole systems. This institutional model does not work without external support or in places where populations are mobile, leading to low functionality levels. A survey of households in Mille Woreda, Afar finds that low borehole reliability and daily usage are correlated with higher water insecurity and emotional distress. This proves that increased safe water consumption makes more of a difference to water security and well-being than does mere access to an unreliable source. We need to shift aid and development funding for WASH from the failed service delivery model of project cycles that build it and forget it to one that makes the best use of funds to build sustainable services. Will systems approaches to sustainability or climate-resilient services for water security become the next casualties in the grave of sector buzzwords? Or will nations significantly step up contributions and pay reparations towards public life-giving services, as promised by the human right to water? All that can be said is that - partly - the approach doesn't matter. Water sector actors understand the problem and want real solutions that address the lack of available resources, not buzzwords. What matters is supporting service providers with the funds to build their capacity according to their needs in the long term.",,"Libey AK,Amy Javernick-Will,Karl Linden,Jeffrey Walters,John Butterworth",,2022,,,,Ph.D. Thesis
U-NET '09: Proceedings of the 1st ACM Workshop on User-Provided Networking: Challenges and Opportunities,"It is with great pleasure that we welcome you to the 1st ACM Workshop on User-Provided Networking -- U-NET'09. This workshop is dedicated to the debate of concepts, challenges, and opportunities concerning user-provided networking. User-provided networks relate to scenarios where users cooperate by sharing network services, such as Internet connectivity and resources. In addition, end-users may or may not provide other network functionality such as local mobility management, as well as persistent storage and forwarding services. This new role is disruptive in what concerns Internet service models, since there is no distinction between what is today known as end-device and network device: in the future, end-user devices will actively participate as part of the network. In contrast, the Internet has been up to now mostly the means for end-users to obtain some form of network service, originally related to connectivity, person-to-person communication, or information retrieval.The U-NET workshop programme includes presentations of peer-reviewed papers and a discussion panel composed of keynote speakers from industry and academia. We envision U-NET as a forum aiming to ignite a debate concerning technical challenges and impact (negatively or positively) that user-provided networking may have on Internet communication models.This first edition of U-NET attracted 19 submissions from Europe, Middle East, Africa, Asia/Pacific, and the United States. Out of the submissions, 7 have been selected by the Technical Programme Committee. They cover a variety of topics, including user-provided access models, incentives and security, social-aware user-provided networks and cooperative user-provided networks.The discussion panel included in the programme is composed of four speakers from both academia and industry, and has as main purpose to ignite a discussion on the potential of user-provided networking, which the U-NET organizing committee expects to serve as a valuable reference for future research in the field of networking and Internet architectures.",,,,2009,,,,Book
The Method of Simulated Annealing for the Optimal Adjustment of the Nigerian Horizontal Geodetic Network,"The Horizontal Geodetic Network of Nigeria is made up of terrestrially arranged chains of triangles augmented by precise traverses. The work on the network began early 19th century but by 1930 the work was discarded and a re-observation of the network was carried out to the highest possible accuracy then, enhanced with high order geodimeter traverses which linked to other neighboring African networks.The full network consists of 515 stations, with 2411 observations which comprise 2197 angular observations, 40 Laplace azimuths and 174 measured distances, part of which substituted for the sparse triangulation observations especially in the southern part of the country. The added observations contributed to strengthening of the network in the 1977 adjustment which however was not a holistic optimized adjustment, but rather, a phase adjustment. Based on the 1977 state of adjustment of the network, no meaningful distortion monitoring exercise can take place until the network is adjusted by an optimized simultaneous technique in order to ascertain the state and consistency of the network. The use of the simulated annealing method, which has been successfully applied in other fields, is presented for the classical geodetic problem of simultaneous adjustment of the entire triangulation net using the least squares observation equation method. This method is an iterative heuristic technique (a method of solving problems by learning from past experience and investigating practical ways of finding a solution) in operations research. It uses a thermo dynamic analogy (Cooling theory) to adjust a network of unstable stations (changes to gaseous state) through fairly stable station coordinates (liquid state) to a stable station coordinates (solid state) so as to offer a solution that converges in a probabilistic sense (statistically based) to the global optimum. The simulated annealing method of optimization serves to help determine the position of all triangulation stations by means of minimizing the volume of the error hyper ellipsoid inherent in the solution to give an optimal configuration of the geodetic network. Computer programs were developed using Matlab Software and run on an adequately configured Pentium IV computer. Creation of an intelligent database was achieved through the interactive network of the data storage, processing, manipulation, analysis and retrieval of results of the adjustment.The result of the new adjustment produced a generally consistent trend of changes in the distances and azimuths compared to the previous adjustments. Error analysis of all lines were carried out and the respective standard errors in distances and azimuths were determined. Relative and absolute error ellipses of all stations were determined and plotted. Statistical plots and analysis of the error ellipses of the network stations were also determined. The absolute and relative weakness/strength of the network stations coordinates after adjustment were shown and confirmed by the error plots to have the following geometry error distributions. That is, 90.5% of the 515 Network Stations fell within Network Standard deviation of 1- Sigma, 94.2% within 2-Sigma, while 98.3% fell within 3- Sigma The distributions confirmed the high reliability of the Nigerian Horizontal Geodetic Network and its data quality. Re-strengthening exercise would be necessary using either the 1-Sigma or 2-Sigma region of network standard deviation .A data structure for the entire network was developed and necessary conclusions and recommendations are made for further action to update/upgrade the precision of the Nigerian horizontal geodetic network for future study.",,Omogunloye OG,,2010,,,,Ph.D. Thesis
Web Information Management: A Cross Disciplinary Textbook,"Summary This is a cross-disciplinary text book on web-based information management for students, faculty and practitioners (in business, industry and government). The Web has emerged as a universal space of information, occasioning proliferation of electronic publications. Though efforts have been made in developing tools and methods such as search engines, metadata, portals, subject directories and subject gateways aimed at enhancing the organization of and accessibility to information on Web, more remains to be done. The book addresses gaps in the existing Web-based tools and methods for information management. Key Features 1.Cross disciplinary - e.g. information science, information systems, computer science, business and records management 2.Addresses topical issues in web information management - such as content management, e-records readiness, e-government, portals and intranets, open source software, emerging technologies-WiMax, Bluetooth, etc 3.Targets audience in tertiary education, government, business and industry The Authors Dr Stephen Mutula is a senior lecturer at the Department of Library and Information Studies, University of Botswana.He teaches in the areas of web-based information systems. He won three professional awards of excellence in 2002 and 2005 from the Emerald Literati Club (UK) and another award from the Standing Conference of East and Southern Africa Librarians (SCECSAL) in 2000 for research and scholarly publications. Dr Justus W. Wamukoya is currently senior lecturer, Department of Library and Information Studies, University of Botswana. He teaches in the area of records and archives management. He has carried out a wide range of consultancy and research work in records and archives management, and has widely published on the subject.Readership Students and faculty in tertiary education, practitioners in business and industry as well as in government Contents Internet and World Wide web applications Information and knowledge management Content management E-records management Electronic mail management Digital literacy E-government Electronic publishing Intranets and web portals Web-based services Organising internet resources Open source software Emerging information and communication technologies and applications Information security E-readiness assessment and information assets Managing copyright in a digital environment Disaster management in a digital environment Conclusion Bibliography Index",,"Mutula MS,Wamukoya MJ",,2007,,,,Book
Professional Mobile Radio — the BT Airwave Public Safety Service and the Path for Technology and Service Evolution,"Professional mobile radio (PMR) has often been perceived as the 'Cinderella' of the mobility market; however, a wide range of corporate mobile business communications people use PMR intensively, because both its unique functionality and its performance match their requirements bettern than other mobile technologies.Although there are several new digital PMR technologies, the new ETSI TETRA (terrestrial trunked radio) standard, which offers enhanced speech and data facilities, is a major factor in the quiet revolution in which more and more business users are adopting PMR to meet their mobile communications needs. This market is estimated to be worth over £ 10bn by 2004.TETRA is also receiving global acceptance outside Europe with contracts and/or commitments already in Asia Pacific, Latin America, Africa, the Middle East and China, with interest being shown in North America (Com-Net Ericsson/Marconi contract for Florida).BT Quadrant and BT Airwave have been very successful in capturing a significant part of the UK 'public safety' (police, fire, ambulance, etc) market. A Public Private Partnership (PPP) contract worth £2.5bn was secured at the beginning of this year for the supply of PMR communications to the police and other 'blue light' services. This contract has a lifetime of over 15 years and it is therefore important to look ahead at how the present capabilities may be evolved to enhance the service, at a time when the capabilities of public cellular networks will be enhanced by new 3rd generation developments.This paper reviews the BT Airwave service for public safety users and discusses how it may be developed to provide users with greater flexibility and increased working effectiveness.",,Tattersall PR,,2001,142–148,10.1023/A:1009677432169,https://doi-org.proxy.bnl.lu/10.1023/A:1009677432169;http://dx.doi.org/10.1023/A:1009677432169,Journal Article
Monetary Policy and Monetary Integration in the Economic Community of West African States (Ecowas) : Dynamic Macro Panel Analyses,"Monetary integration has constituted an overarching objective of the Economic Community of West African States (ECOWAS) as reflected in the introduction of the ECOWAS Monetary Cooperation Programme (EMCP) in 1987 that defines the collective policy measures necessary to achieve a harmonised monetary system. The adoption in 2014 of a pragmatic single-track approach to monetary integration by the year 2020 and the formal adoption in June 2019 of the 'Eco' as the name for the single currency under the proposed ECOWAS monetary union underscore the ECOWAS authorities' determination to forge ahead with establishment of the monetary union. In spite of the progress in instituting the requisite policy and institutional framework, there has been a dearth of empirical studies to meaningfully inform the process of harmonization of monetary policy and the transition from country-specific monetary policy to a common monetary policy framework. This thesis contributes to filling the gap through research on important monetary policy issues in the ECOWAS by drawing on the advancement in macro panel estimation techniques. The first empirical paper investigates the viability of monetary targeting as a policy framework for the proposed common monetary policy by assessing the stability of the conventional money multiplier and the postulates of the endogenous money theory. Employing both first- and second-generation panel unit root tests and advanced panel cointegration and granger causality techniques, the results indicate that the money multiplier has been unstable. In addition, causality is shown to run from bank credit to the monetary base, broadly in line with postulates of the endogenous money theory. The second essay investigates the determinants of the demand for money in the ECOWAS by applying the common correlated effects mean group (CCEMG) and the augmented mean group (AMG) estimators. The study identified real income and inflation as the key determinants of money demand, but uncovered evidence of long-run instability in the money demand function. The final empirical chapter examined the transmission mechanism of monetary policy in the ECOWAS region by adopting the dynamic heterogeneous panel structural VAR technique developed by Pedroni (2013). Monetary policy was found to be ineffective, with weak or inoperable channels of monetary transmission. Indications of asymmetries in the country-specific responses to the common monetary policy shock were revealed. The study concludes that monetary targeting is inappropriate as a framework for conducting a common monetary policy and that a framework based on interest rate signalling, such an Inflation Targeting (IT) Lite, could be adopted at the inception of the union while the prerequisites for a full-fledged IT regime are being instituted. Deliberate efforts to deepen financial markets and strengthen policy credibility are needed to enhance the effectiveness of the common monetary policy.",,Tucker JA,,2019,,,,Ph.D. Thesis
Factors Affecting Cloud Computing Adoption in Sierra Leone: A Quantitative Analysis Based on Technology-Organization-Environment Framework (Toe) & Human-Organization-Technology Fit (Hot-Fit) Theory,"Information Technology (IT) services are now dynamic, movable, and interactive. This represents a paradigm shift in the IT industry, as IT companies and organizations can now rapidly migrate IT infrastructure, data, and software applications to the cloud to optimize the IT environment and reduce high operational costs. This paradigm shift is, however, limited to developed countries. Using Sierra Leone as a case study, this research examined the factors affecting the adoption rate of Cloud Computing in government organizations. It looked into the roles of top management support and self-efficacy as moderators on the relationship between technology readiness and cloud computing adoption and IT knowledge and cloud implementation adoption. This study used both the technology–organization–environment (TOE) framework and the Human-Organization-Technology fit (HOT-fit) theory to develop a model that examined the adoption challenges in SL. The research model was tested with twenty-five government organizations, using four hundred and four questionnaires. Furthermore, the study used SmartPLS software to analyze the collected data quantitatively. The findings showed that internet connectivity, perceived benefits, information security, technology readiness, and IT knowledge are significant factors affecting the adoption of cloud computing in Sierra Leone. . This research contributes to organizational IT adoption literature in Africa. In addition, it serves as a guide to understanding the factors that significantly influence organizations in adopting cloud computing in Sierra Leone. Although this research provides a sound theoretical model that adequately predicts cloud computing adoption in Sierra Leone, some limitations should be considered for future research. One such limitation is that the sample data is limited to only the government employees of Sierra Leone, which implies that this research reflects only the perspective of government employees. Therefore, the findings of this study cannot be used for non-governmental organizations. It should also be noted that only two theories and four factors were considered for testing in this study. I recommend that future research consider developing and testing other theories and hypotheses to obtain new findings for governmental and non-governmental organizations. Keywords: Information Technology, Cloud Computing, Sierra Leone, TOE Framework HOT-fit model, SmartPLS.",,"Cooper JD,Kenneth Cromer,Carole Angolano",,2022,,,,Ph.D. Thesis
Why Do People Seek Anonymity on the Internet? Informing Policy and Design,"In this research we set out to discover why and how people seek anonymity in their online interactions. Our goal is to inform policy and the design of future Internet architecture and applications. We interviewed 44 people from America, Asia, Europe, and Africa who had sought anonymity and asked them about their experiences. A key finding of our research is the very large variation in interviewees' past experiences and life situations leading them to seek anonymity, and how they tried to achieve it. Our results suggest implications for the design of online communities, challenges for policy, and ways to improve anonymity tools and educate users about the different routes and threats to anonymity on the Internet.",,"Kang R,Brown S,Kiesler S",,2013,2657–2666,10.1145/2470654.2481368,https://doi-org.proxy.bnl.lu/10.1145/2470654.2481368;http://dx.doi.org/10.1145/2470654.2481368,Conference Paper
Modelling of Ultra High Frequency Television Band Radio Signal Propagation in Underground Mine Environment,"Solutions operating at ultra high frequency television band (UHF TV) have been deployed in above ground networks and communication systems providing data connectivity for different applications, but its' deployment for underground communications haven't been tested to date. Signal level and throughput measurements have been organised inside a mine environment that exploits platinum ore in the Republic of South Africa. Signal propagation has been modelled using scatter and direct signal component for which losses follow 15th and 6th power of the distance on the crossing and in the tunnel respectively. Measurements taken in the mine showed a connectivity with UDP level throughputs in the order of 1 Mb/s for distances of 350 m. It was demonstrated that the proposed deployment could be combined with Wi-Fi networks in 2.4 and 5 GHz for connecting standard smart devices from underground to above ground communication systems including the provision of internet services. It is expected that communication systems based on UHF TV band could supplement or even replace existing communications that are usually based on leaky feeders since they can provide wireless broadband data connectivity in a mining environment.",,"Vujić DS,ă?Ertić JD",,2019,2117–2128,10.1007/s11276-018-1801-5,https://doi-org.proxy.bnl.lu/10.1007/s11276-018-1801-5;http://dx.doi.org/10.1007/s11276-018-1801-5,Journal Article
Deep-Learning-Based Diagnosis of Cassava Leaf Diseases Using Vision Transformer,"Viral diseases are major causes leading to the poor yields of cassava, which is the second-largest source of food carbohydrates in Africa. As symptoms of these diseases can usually be identified by inspecting cassava leafs, visual diagnosis of cassava leaf diseases is of significant importance in food security and agriculture development. Considering the shortage of qualified agricultural experts, automatic approaches for the image-based detection of cassava leaf diseases are in great demand. In this paper, on the basis of Vision Transformer, we propose a deep learning method to identify the type of viral disease in a cassava leaf image. The image dataset of cassava leaves is provided by the Makerere Artificial Intelligence Lab in a Kaggle competition, consisting of 4 subtypes of diseases and healthy cassava leaves. Our results show that Vision-Transformer-based model can effectively achieve an excellent performance regarding the classification of cassava leaf diseases. After applying the K-Fold cross validation technique, our model reaches a categorization accuracy 0.9002 on the private test set. This score ranks top 3% in the leaderboard, and can get a silver medal prize in the Kaggle competition. Our method can be applied for the identification of diseased plants, and potentially prevent the irreparable damage of crops.",,Zhuang L,,2022,74–79,10.1145/3508259.3508270,https://doi-org.proxy.bnl.lu/10.1145/3508259.3508270;http://dx.doi.org/10.1145/3508259.3508270,Conference Paper
Digital Apartheid: An Ethnographic Account of Racialised Hci in Cape Town Hip-Hop,"We describe findings from a 15-month ethnography of hip-hop performers in Cape Town, South Africa. Mobile communications and social media are hugely important to the development of these performers' careers, opening access to collaborators, production tools, audiences and distribution channels. This group go to extraordinary lengths to gain and maintain access to these technologies, often by exploiting their social capital through musical and ethnic networks. We document that even after nearly twenty years of democracy, a ridged separation along racial lines persists, which can be seen in all areas of life including access to and proficiency in digital technologies. We illustrate how hip-hop performers harness these divisions both on and offline in order to distinguish themselves from other artists. Our research raises a number of implications for post-colonial computing, highlighting difficulties related to discontinuous access, and how international preconceptions of identity and authenticity emerge as a consequence of the increased use of communication technology.",,"Pritchard GW,Vines J",,2013,2537–2546,10.1145/2470654.2481350,https://doi-org.proxy.bnl.lu/10.1145/2470654.2481350;http://dx.doi.org/10.1145/2470654.2481350,Conference Paper
"ICT Use Patterns, Mental Health Symptoms and the Well-Being of the Open Distance Learning Student: A Replication Study with Historically Advantaged Students","In addressing an area of research that has not received any attention, a recent study conducted in the South African context of disparities concluded that medium ICT use patterns hold no risk factors for the mental health and well-being of the historically disadvantaged open distance learning student. To determine if the basic findings can be applied to a different subgroup of students who are known to have better access to ICT resources, a replication study with historically advantaged students was done. Key findings from data collected from 699 students revealed no significant relationships between total ICT use scores, mental health and psychological and emotional well-being. Other than facets of social well-being, results were relatively consistent with the original study. In the context studied, it was concluded that ICT use patterns hold no risk factors for the mental health and well-being of the open distance learning student in general.",,Merwe TM,,2020,360–383,10.1504/ijlt.2020.113884,https://doi-org.proxy.bnl.lu/10.1504/ijlt.2020.113884;http://dx.doi.org/10.1504/ijlt.2020.113884,Journal Article
Secure Telematic Applications for National Scale Projects - Volume 20 NATO Science for Peace and Security Series - D: Information and Communication Security ... and Communitcation Sercurity- Vol. 20),"The NATO Advances Research Workshop (ARW) 'Secure Telematic Applications for National Scale Projects' is organized in the frame of the National Week of Information Technologies and Sixth International Congress 'Scientific and Methodological Facilitation of the Development of Informatization and the System of Scientific and Technical Information in the Republic of Belarus. Secure Telematic Applications in National and International Projects'. The participants of the event came to Minsk from all regions of Europe, as well as from Asia and Africa. Besides the general ARW topic, the participants also discussed the progress in the activity of the High Technologies Park, the creation of the corporate network of libraries of Belarus on the basis of the National Library of Belarus and the newest technologies of e-government. The presentations are also oriented on creation of the National Scientific and Research Computer Network of the Republic of Belarus on the basis of Academic Network BASNET. It is expected that the scientific and practical achievements of the forum will considerably influence both the development of the information technologies and their utilization for advantage of the national economy of Belarus and all participating countries.",,"Fontaine JG,Fontaine JG,Makhaniok M",,2009,,,,Book
Land-Use Competition and Agricultural Greenhouse Gas Emissions in a Climate Change Mitigation Perspective,"Productive land for food production, bioenergy, or preservation of nature is a limited resource. Climate change mitigation puts additional pressure on land via higher demand for bioenergy to replace fossil fuels and via restrictions on deforestation—two processes that limit the availability of land for food production, and may thus also raise food prices. Methane and nitrous oxide emissions from agriculture may also need to be reduced to efficiently mitigate climate change. This thesis deals with this in three ways.In papers I–II, we estimate greenhouse gas emissions from food production for current diets and expected future developments, together with alternative dietary developments and potential technical improvements in the agricultural sector. Costs and possibilities for reaching climate goals are analyzed for the different diets. The results indicate that a phase out of ruminant products would cut mitigation cost in half, for staying below a 2°C limit, and it may be necessary if the climate sensitivity is high.In papers III–IV, a conceptual and transparent partial equilibrium model of global land-use competition is developed, analyzed and applied. The model is to a large degree analytically explored and price differentials between crops are derived. The model is subjected to a detailed characterization of its mechanisms and parameters that are critical to the results. We conclude that the total amount of productive agricultural area and bioenergy yields are of crucial importance to the price impacts from large-scale introduction of bioenergy. We also show how limiting bioenergy production to marginal land could be difficult to implement in practice.In paper V, we use two established indicators for poverty and sensitivity to food-price changes to capture peoples' vulnerability to rising food-prices in four Sub-Sahara African countries/regions. In contrast to previous studies, we include all food products instead of just one or a few main staples. We found that the vast majority of people are net consumers of food and that the inclusion of more than main staples increases their net position as consumers and thus vulnerability to high food prices.",,Bryngelsson D,,2015,,,,Ph.D. Thesis
Renewing Local Planning to Face Climate Change in the Tropics,"This book is open access under a CC BY 4.0 license. This book aims to inspire decision makers and practitioners to change their approach to climate planning in the tropics through the application of modern technologies for characterizing local climate and tracking vulnerability and risk, and using decision-making tools. Drawing on 16 case studies conducted mainly in the Caribbean, Central America, Western and Eastern Africa, and South East Asia it is shown how successful integration of traditional and modern knowledge can enhance disaster risk reduction and adaptation to climate change in the tropics. The case studies encompass both rural and urban settings and cover different scales: rural communities, cities, and regions. In addition, the book looks to the future of planning by addressing topics of major importance, including residual risk integration in local development plans, damage insurance and the potential role of climate vulnerability reduction credits. In many regions of the tropics, climate planning is growing but has still very low quality. This book identifies the weaknesses and proposes effective solutions.",,"Tiepolo M,Pezzoli A,Tarchiani V",,2017,,,,Book
Emerging Biomarker Sensors for Personalized Medicine,"In this dissertation, three technological challenges pertaining to the field of personalized medicine are addressed. These challenges include development of high throughput and multiplexed proteomics technologies, reducing the diagnostic cost per test, and automation and integration of biological sample preparation prior to sensing. Accordingly, a multiplexed platform for protein analysis is developed, a low cost cytometer using contactless impedance sensing is presented, and a two-component microfluidic platform for depletion of cells and unwanted highly abundant proteins from the input sample is demonstrated. To facilitate a multiplexed platform for protein analysis, along a single microfluidic channel, an array of proteins is patterned, where each element is targeting a specific secondary protein coated on micron-sized beads in the subsequently introduced sample solution. Below each element of the array, there is a pair of addressable interdigitated electrodes. By selectively applying voltage at the terminals of each interdigitated electrode pair, the produced negative dielectrophoresis (nDEP) force detaches protein-bound beads from each element of the array, one by one, without disturbing the bound beads in the neighboring regions. The detached beads can be quantified optically or electrically downstream. Here, to allow for robust actuation of micron-sized beads, the relatively weak DEP force was enhanced by two orders of magnitude (beyond the strength of protein-protein interactions) by fabricating high voltage tolerant corrosion proof electrodes. This was achieved by depositing a protective pinhole free nanometer-scale thin film layer on electrodes, using Atomic Layer Deposition technique. In parallel, a comprehensive design space was developed to analyze this enhanced DEP system from both a circuit analysis and electrothermal viewpoints. In our developed model, various phenomena and constraints such as voltage degradation (due to the presence of the protecting oxide layer), oxide breakdown, instrumentation limitations, and thermal effects have been taken into account. The results from this analysis were used to maximize the DEP force in our system. For proof of concept, 16-plex actuation capability of the device is illustrated to elute micron-sized beads that are bound to the surface through anti-IgG and IgG interaction which is on the same order of magnitude in strength as typical antibody--antigen interactions. Next, a novel contactless impedance sensing scheme to perform low-cost cytometry in whole blood is demonstrated. In particular, a disposable microfluidic impedance cytometer is developed using electrodes that can be reused, without the need for microfabrication of the electrodes. This disposable device can be inserted onto a printed circuit board (PCB) which has a non-disposable, yet inexpensive, electronic reading apparatus. This significantly reduces the manufacturing costs, making it suitable for low resource settings, such as point-of-care testing in the developing countries. In the third platform, a microfluidic system that can deplete cells with 100% efficiency and abundant serum proteins from blood with 95% efficiency is presented. The platform consists of two components. The first component is a microfluidic mixer, which mixes beads containing antibodies against the highly abundant proteins with the whole blood. This complex mixture (consisting of beads, cells, and serum proteins) is then injected into the second component of our microfluidic platform, which comprises a filter trench to capture all the cells and the beads. The size-based trapping of the cells and beads into the filter trench is significantly enhanced by leveraging the enhanced DEP force to push the micron sized particles (cells and beads which have captured the highly abundant proteins) down into the trench, allowing the serum proteins of lower abundance to flow through.",,Emaminejad S,,2014,,,,Ph.D. Thesis
Diversity and Management of Sclerotinia Sclerotiorum in Brassica Spp. in Bangladesh,"Sclerotinia sclerotiorum is an important pathogen of many crops including rapeseed-mustard (Brassica spp.). The pathogen has emerged relatively recently in Bangladesh and there is no information available regarding its population structure in mustard-growing regions of the country. A series of experiments were conducted to determine the variability of S. sclerotiorum isolates from different regions of Bangladesh and from Ohio, USA. In 2014, a total of 132 S. sclerotiorum isolates were collected from 11 locations in Bangladesh and Ohio. Morphological characteristics including mycelial radial growth and number and fresh weight of sclerotia were measured. Genetic variability was also assessed by Internal Transcribed Spacer (ITS) rDNA sequencing, microsatellite markers and mycelial compatibility grouping. Overall, isolates from Bangladesh showed low variability based on morphological and molecular characteristics. Mycelial radial growth of isolates from one location in Bangladesh was significantly higher than the radial growth of isolates from Ohio. No significant difference was observed in the number of sclerotia produced by isolates from the 11 locations. The weight of sclerotia produced by isolates from Tangail, Mirzapur was smaller than that of isolates from Ohio and from Tangail, Ghatail and Shirajganj, Chalakpara, Bangladesh. No significant variation was observed among isolates from any location based on their ITS rDNA sequences. Based on eight informative microsatellite loci, 91% of the variation was within the isolates and 9% was due to location, indicating low divergence among the populations from the 11 locations. Principle Component Analysis (PCA) separated Ohio isolates from isolates from nine locations in Bangladesh; however, isolates from Jamalpur were in the same PCA quartile as isolates from Ohio. Twenty-seven microsatellite haplotypes were identified from 118 isolates from 11 locations, and one haplotype (haplotype 1) was predominant in ten locations in Bangladesh. The Ohio population contained two isolates with two separate haplotypes (haplotype 25 and haplotype 26). Thirty-four mycelial compatibility groups (MCGs) were identified among 80 S. sclerotiorum isolates; those from Ohio formed four groups while the remaining 30 groups were from Bangladesh. Fifty-one isolates from ten Bangladesh populations were in haplotype 1, which shared 14 MCGs. On the other hand, four MCGs contained more than one haplotype. Based on morphological and genetic characters, the S. sclerotiorum populations from Bangladesh and from Ohio were different, however, populations from Bangladesh had low variability.Fungicide application is the primary tactic widely used to manage white mold in mustard and other crops. However, no information is available on sensitivity of S. sclerotiorum to fungicides in Bangladesh. Sensitivity of S. sclerotiorum to iprodione, propiconazole, fluazinam and penthiopyrad was determined using isolates collected from 11 locations in Bangladesh and Ohio, USA in 2014. Sensitivity was assessed using discriminatory doses and concentrations and 50% mycelial inhibition (EC50) values were determined. Compared with the EC50 of the fungicides to S. sclerotiorum from the published literature, none of the tested S. sclerotiorum isolates were resistant to iprodione, propiconazole, fluazinam or penthiopyrad. However, some isolates of S. sclerotiorum exhibited reduced sensitivity to propiconazole. The EC50 values obtained in the first experiment ranged from 0.18 - 0.50 ppm, 0.12 - 0.78 ppm, 0.0019 - 0.0044 ppm and 0.012 - 0.429 ppm for iprodione, propiconazole, fluazinam and penthiopyrad, respectively. In the second experiment, EC50 values ranged from 0.16 - 0.36 ppm, 0.02 - 0.93 ppm, 0.0024 - 0.0050 ppm and 0.08 - 0.83 ppm for iprodione, propiconazole, fluazinam and penthiopyrad respectively. Relative toxicity index (RTI) values, using iprodione as the standard, were 103.2 and 67.6 (experiments 1 and 2, respectively) for fluazinam, and 6.0 and 1.6 (experiments 1 and 2, respectively) for penthiopyrad. Propiconazole was similar to iprodione in toxicity to S. sclerotiorum. Fluazinam and penthiopyrad are not registered in Bangladesh. Iprodione and propiconazole are registered for other diseases, but not resigtered for white mold management in mustard, therefore the EC50 values of fluazinam and pethiopyrad determined in this study can be considered baseline sensitivity levels for future efforts to monitor development of resistance to these fungicides in S. sclerotiorum in Bangladesh.Development of rapeseed-mustard varieties partially or fully resistant to S. sclerotiorum would enhance the disease management toolbox and reduce or eliminate the need for fungicides to control this disease. Fourteen varieties and one breeding line developed by the Bangladesh Agricultural Research Institute (BARI) were screened to determine their reactions to S. sclerotiorum. Twenty S. sclerotiorum isolates were pre-evaluated for virulence and a highly virulent isolate was selected. Isolate SCS1 caused large lesions 24 h after inoculation in a detached leaf assay. This isolate was used in cotyledon and petiole inoculation assays. In screening with cotyledon inoculation, the smallest lesions were observed in BARI Sharisa 14. There were no significant differences among the varieties/line in percentage of infected cotyledons. In petiole inoculation screening, variation in the reactions of the rapeseed-mustard varieties/line to S. sclerotiorum was insignificant, except for breeding line SS 75 in both experiments. This line showed significantly higher resistance to S. sclerotiorum than BARI Sharisa 10 in first experiment and Tori 7 in the second experiment. Although the results obtained using two inoculation methods were inconsistent, both BARI Sharisa 14 and SS 75 may prove to be useful as sources of resistance to S. sclerotiorum upon more extensive evaluation.Integrated management is the most durable management strategy. Two experiments were conducted at Rangpur and Jamalpur to evaluate different treatments separately and in combination to control white mold disease of mustard. The fungal biocontrol agent Trichoderma harzianum isolate BHT-N1 (ThBHT-N1) and five fungicides in different groups (carbendazim, thiophanate-methyl, propiconazole, iprodione and azoxystrobin + difenoconazole) were tested separately and in combination with ThBHT-N1 in natural field conditions. In Burirhut, Rangpur, the incidence of white mold disease was low. However, azoxystrobin + difenoconazole-treated plots had significantly lower disease incidence and higher yield than non-treated control plots. In Jamalpur, white mold was not observed, but Alternaria blight was recorded. All fungicide treatments and ThBHT-N1 significantly reduced disease severity compared to the non-treated control, but azoxystrobin + difenoconazole and iprodione treatments were more effective than the other treatments.Information generated from this study will enhance our understanding of population structure of S. sclerotiorum in Bangladesh, its diversity and sensitivity to fungicides and sources of resistance to S. sclerotiorum. This information will be helpful for increasing production and ultimately will contribute to food security of Bangladesh, a developing country.",,"Islam MM,Dorrance A,Paul P,Taylor C",,2018,,,,Ph.D. Thesis
One Digital Day: How the Microchip Is Changing Our World,"From the Publisher: No invention in history has spread so quickly throughout the world, or revolutionized so many aspects of human existence, as the microchip. Little more than a quarter century since its invention, there are now nearly 15 billion microchips in use worldwide -- the equivalent of two powerful computers for every man, woman, and child on the planet. The microprocessor is not only changing the products we use, but also the way we live, and, ultimately, the way we perceive reality. One Digital Day is the result of a unique project designed to make people aware of the thousands of microprocessors we unknowingly encounter every day. Rick Smolan, creator of the award-winning 'Day in the Life' photography books and the bestseller 24 Hours in Cyberspace, sent 100 of the world's most talented photojournalists around the globe on July 11, 1997. Their mission: to depict intimate and emotional stories of how this tiny chip-a square of silicon the size of a fingernail, weighing less than a postage stamp -- has transformed our human culture forever. The book features more than 200 compelling photographs, taken on that single day, revealing a world that only science-fiction writers once dared envision. Thanks to microchips, it is a world where science, entertainment, business, health, sports, education, and countless other fields are progressing faster than we can imagine. How pervasive is the microchip If you took the microchips out of every application in which they are now used, the results would be stunning and frightening. Microwave ovens, dishwashers, and many other kitchen appliances would stop working. Televisions and VCRs would fade to black; stereos would grow mute; and most clocks would stop. Cars wouldn't start, and airplanes would be unable to leave the ground. The phone system would go dead, as would most streetlights, thermostats, and, of course, a half-billion computers. And these are only the most obvious applications. Every factory in the industrial world would also shut down, as would the electrical grid, stock exchanges, and the global banking system. Pacemakers would stop, too, as would surgical equipment and fetal monitoring systems in obstetrics wards. This infinite variety of applications is vividly illustrated by the images captured last July for One Digital Day. A brief sample of what the hundred photographers came back with: Johannesburg, South Africa -- Once on the verge of extinction, cheetahs at the DeWildt Center are implanted with microchips that contain genetic information. This information, read by a scanner, is crucial to the center's efforts to build up the world population, because in-breeding is a big threat to the genetic strength of the cats. Hollywood, California -- The Jurassic Park River Adventure roller coaster is a completely automated ride which was designed with the help of paleontologists and robotics engineers, at a cost of $100 million. This completely automated ride includes ""animatronic"" dinosaurs which roar, lunge and even spit at riders in passing boats. Bury, England -- Ida Schofield, a 69-year-old grandmother, had never touched a computer or thought she had any need for one until she volunteered as a guinea pig for a state-of-the-art desktop system, with video-conferencing. She now uses it to communicate with family members around the world. Lacey, Washington -- Sprinter Tony Volpentest, born with no hands or feet and only partially formed arms and legs, uses ultra-light artificial feet designed with the help of sophisticated computer modeling programs. He now runs the 100-meter dash only 1.5 seconds slower than the world record holder. Singapore -- The foul-smelling but delicious tropical fruit known as durian is adored throughout Asia, but devotees dread carrying it home in their cars or keeping it around the house. Now connoisseurs of the odoriferous delicacy can order it online from 717 Trading Company and have it delivered just when they're ready to eat it. Since 717 launched its Web site in early 1996, about 20 percent of its sales have come from customers shopping online. Fort Bragg, NC and Sarajevo, Bosnia -- U.S. Army Lieutenant Frank Holmes, stationed 5,000 miles from home in Bosnia, gets his first look at his six-week-old daughter, Morgan, by using a PC-based videoconferencing system. The smooth images that reunited Frank, Morgan, and mom Andrea ran over normal phone lines between computers running ProShare Technology. Frank's commanding officer notes that videoconferencing is the single greatest morale boost for my troops in a long time. As Andrew S. Grove, Chairman and CEO of Intel Corporation, writes in his foreword, As you turn these pages, you'll see a world being reshaped by technology in ways previously unthinkable. One Digital Day makes it fascinatingly clear that there is no place on, above, or below the earth, that the microprocessor hasn't touched.",,"Smolan R,Erwitt J,Malone MS",,1998,,,,Book
Long-View Player Detection Framework Algorithm in Broadcast Soccer Videos,"In this paper, we propose an efficient video analysis framework to assign broadcast soccer video shots into their respective view classes, and then detect players in long view shots. Our technique is built on dominant color region based segmentation for soccer playfield extraction. A long-view shot classifier uses a combination of ""grass-area"" ratio and ""top-grass"" analysis. A player detector applies the distinctive uniform knowledge of interesting objects based on colors referring from the result of playfield. In order to verify the player region segmented using colour, we introduce the four-seed edge features which prune the redundant edges denoting the noise of court lines or audiences. The player detection performance is suitable to employ tracking methods in order to exploit higher semantic information from the games. Experimental evaluation of the framework is extensively demonstrated in numerous challenging test sequences of the 2010 FiFa World Cup South Africa. The results show the robustness of our framework, and the potential future-work.",,"Tran Q,Tran A,Dinh TB,Duong D",,2011,557–564,10.1007/978-3-642-25944-9_72,https://doi-org.proxy.bnl.lu/10.1007/978-3-642-25944-9_72;http://dx.doi.org/10.1007/978-3-642-25944-9_72,Conference Paper
ONISW '08: Proceedings of the 2nd International Workshop on Ontologies and Information Systems for the Semantic Web,"The emergence of the World Wide Web made massive amounts of data available and data exists in many scattered electronic data sources (e-sources) over the Web. Even though some of the data isin well-organized data sources, interoperability and integration with data from other sources, semantic coordination and conflict resolution are required for its full exploitation. Semantic Web enabled applications can potentially produce better results for semantic integration, interoperability and search. In particular, ontologies are widely regarded as the best solution toglobal information integration and semantic interoperability.The main objective of the 2nd International Workshop on Ontologies and Information Systems for the Semantic Web (ONISW 2008) is to bring together researchers in Information Management interested in the relation between ontology and information models, to present results and to discuss theoretical aspects and good practice. The Call for Paper solicited contributions that cover topics such as Ontology and epistemology in information systems, Ontology learning, Semantic interoperability, Ontology-based schema mapping/matching and integration, Ontology mapping tools, languages, and visualization, Schema transformation, Ontology-based data transformation and data migration tools, Ontology-based query mediation, Querying the Semantic Web, Ontology-driven application system and Web service design, etc.The Call for Papers attracted 21 submissions from Africa, Asia, Canada, Europe, and the United States. Each paper carefully reviewed by at least three members of program committee. Finally, the program committee accepted 16 papers. This volume of the proceedings contains papers presented in the 2nd International Workshop on Ontologies and Information Systems for the Semantic Web (ONISW 2008), which was held in Napa Valley, California, October 30, 2008. The workshop was held in conjunction with ACM 17th Conference on Information and Knowledge Management (CIKM).",,,,2008,,,,Book
GPCE '11: Proceedings of the 10th ACM International Conference on Generative Programming and Component Engineering,"These are the proceedings of the 10th ACM International Conference on Generative Programming and Component Engineering (GPCE'11), collocated with Systems, Programming, Languages and Applications: Software for Humanity 2011 (SPLASH'11). This year's conference continues its tradition of being the premier venue for researchers and practitioners interested in techniques that use program generation and component deployment to increase programmer productivity, improve software quality, and shorten the time-to-market of software products. In addition to exploring cutting-edge techniques of generative and component-based software, the goal of the conference is to foster further cross-fertilization between the software engineering and the programming languages research communities, a goal supported both by a strong technical program bringing in contributions and researchers from both research communities and from our collocation with SPLASH.The call for papers attracted 55 submissions from Africa, Asia, Europe, and North and South America. The program committee accepted 18 papers that cover a variety of topics within the key areas of domain-specific languages, program generation and components, and also novel topics such as empirical studies of software product lines and model-based tool development for robotics. The program includes two keynotes: Matthias Felleisen from Northeastern University on Multilingual Component Programming in Racket, and Gary Shubert from Lockheed Martin Space Systems Company on the Application of Model Based Development to Flexible Code Generation. In addition, the program includes two technical talks that provide in-depth treatment of selected research results, namely inter-derivation of formal semantics and the industrial application of domain-specific languages to cryptography.",,,,2011,,,,Book
Interaction Patterns and ICT Use to Support the Livelihoods of Microenterprises,"This paper reports on the nature of interaction patterns and use of Information and Communication Technologies (ICT) to support the livelihood of microenterprises. The study focused on the case of South Africa where Small, Micro and Medium Enterprises (SMMEs) act as a means for addressing unemployment and poverty. The study used qualitative data to understand the interaction patterns and how ICT such as computers, mobile phones and internet are used to support the livelihoods of microenterprises. The findings showed that vertical and horizontal interactions supported the flow of information and sharing of knowledge used in decisions for reducing vulnerabilities in the livelihoods of microenterprises. ICT were improving the interactions that supported the livelihoods of microenterprises. The study recommends use of existing social networks for microenterprises when designing interventions for supporting microenterprises.",,"Makoza F,Chigona W",,2014,20–40,10.4018/ijictrda.2014010102,https://doi-org.proxy.bnl.lu/10.4018/ijictrda.2014010102;http://dx.doi.org/10.4018/ijictrda.2014010102,Journal Article
Challenges of Housing Finance in Nigeria: The Federal Mortgage Bank of Nigeria (FMBN) Experience,"With a population of 197 million inhabitants, Nigeria is the most populous nation in Africa. The contribution that housing makes to the socio-economic development of an economy is significant. Despite this assertion, there exists a housing deficit of 17 million in the country. The gap between demand and supply for housing is wide owing to several challenges encountered in providing mortgage to low and middle-income earners that form the majority of prospective beneficiaries of the home ownership scheme initiated by Federal Mortgage Bank of Nigeria (FMBN). This study examined the challenges encountered in the Nigerian housing industry by determining the affordability of individuals in terms of access to the National Housing Fund (NHF) loan and further explored a cost-effective means to providing affordable houses to low and middle-income earners. In addition, the work made effort towards determining the effectiveness of the strategies presently used by the bank (FMBN) in creating mortgages and its existing model for housing finance in Nigeria. The need for this research developed because of the high level of housing deficit in the country and the insufficient attention given to empirical research on housing finance in Nigeria. This research used qualitative study approach by conducting eleven semi-structured interviews with key officials and Chief Executive Officers within the Nigerian housing industry supported with archival records of the housing industry in the country to advance ways by which the lending bank can operate efficiently. Results of the study revealed that key challenges facing the industry were classified into three broad themes namely; financial, procedural and structural challenges. The financial challenges highlighted the constraints (low capitalisation) encountered by FMBN in successfully delivering quality and affordable houses to low and middle-income earners. The procedural challenges emphasise the difficulties (stringent processes) that NHF applicants experience in the process of obtaining mortgage loans. While the structural challenges describe the problems associated with the quality and the standard of the buildings provided by estate developers. Another key finding is the high level of political interference in the housing sector which had greatly impacted on the nation's housing delivery. In addition, the institutional structure of FMBN is also flawed with loopholes such as the lack of Information Technology (IT) integration, lack of involvement of major stakeholders in the executive management of the Bank and a need to review the existing regulatory framework. To meet the housing delivery target set by the government, it is recommended that reform of the Housing Act and restructuring the housing industry will help in the needed redress within the industry.",,Barhama BI,,2019,,,,Ph.D. Thesis
Characterization and Identification of Host Plant-Driven Plasticity of the Cabbage Looper (Trichoplusia Ni) Saliva,"Plant-insect dynamics are a complex network of chemical interactions. How insects are able to adapt to their host plants and how plants can resist or tolerate insects are questions of much importance for evolutionary biology, ecology, physiology, insect behavior, agriculture, food security, and science in general. For example, plants are capable of inducing defenses against insects after detecting insect specific cues. Insects on the other hand might suppress these defenses by releasing molecules present in secretions like saliva. Currently, there is limited information on the saliva composition of chewing insect herbivores and how it might affect plant defenses. The main objectives of this study were to 1. Characterize the saliva of the generalist insect pest, the cabbage looper (Trichoplusia ni) and 2. Identify the changes in the composition of insect saliva driven by two of its host plants - cabbage and tomato as compared to artificial diet. These objectives were approached using both transcriptomic (RNAseq) and proteomic techniques (iTraq). A transcriptome of 14,037 genes and a proteome of 434 proteins were established. Feeding on different host plant diets resulted in substantial remodeling of the gland transcriptomes and proteomes, with 4,501 transcripts and 63 proteins significantly differentially expressed across the three treatment groups. Gene expression profiles were most similar between cabbage and artificial diet, which corresponded to the two diets on which larvae perform best. Within these libraries, several interesting enzymes were identified that may play an important role in the cabbage looper's ability to establish on different hosts. Some of these enzymes that were further analyzed are a catalase and three potential myrosinases. Catalase activity was confirmed in the labial glands of the cabbage looper. It was also determined that catalase plays a role in detoxification by reducing the activity of peroxidases as well as herbivore offense by suppressing the induction of trypsin protease inhibitor in tomato. The myrosinase genes identified were differentially expressed in several tissues of the cabbage looper and under different diets. However, they appear to be broad-spectrum glucosidases rather than specific myrosinases. Finally, as part of the INTAD dual degree, the alternative use of water containing methyl isothiocyanate - a defensive secondary compound in the famine shrub Hanza (Boscia senegalensis), was investigated. Hanza waste water has a significant effect on seed germination of several plants and could potentially be used for weed management in small farms of West Africa. This is an example of the study of plant defensive compounds for the use in applied research. This dissertation provides information about caterpillar saliva, which can be used for future functional and ecological studies. Also, it enriches our knowledge about a usually neglected secretion from chewing insects.",,Rivera-Vega LJ,,2017,,,,Ph.D. Thesis
"Transnational Regulation, Lenders' Responses and the Needs of Consumer Borrowers in Nigeria","This thesis undertakes an analysis of the developing international paradigm and rationale for regulating consumer credit and their application to Nigeria. The thesis makes an original contribution by problematising the simple application of the transnational model to Nigeria, which currently produces counterproductive outcomes for consumer borrowers. The thesis argues that the emerging transnational paradigm since the Global Recession presents an opportunity for stronger consumer protection and access to credit in Nigeria only if the framework is adapted to the existing institutional structures. To effectively address the research question, the thesis engages with economic neoliberalism as an analytical framework, the influences of psychology (behavioural science) on law, and the historical relationship of Nigeria with transnational institutions. Through a careful analysis of existing literature and a small scale qualitative study, the thesis found: (a) links between transnational regulation and decreased lending by formal sector lenders to consumer borrowers in Nigeria; (b) a weak coalition of change agents at the national level to advance the interests of consumer borrowers, and thus, forsee a role for transnational actors as change agents within the Nigerian credit market; (c) that International Financial Institutions (IFIs) like the World Bank and the International Monetary Fund (IMF) must approach this role from a social protection prism that rests on a new pro-poor 'conditionality', distinct from how conditionality has traditionally been used in Africa. The thesis is a scoping exercise that engages with the research question primarily from the perspective of formal lenders, and designed to facilitate more in-depth studies of the issues in the longer term. It is hoped that the findings of this thesis will stimulate further study. While making policy recommendations, the thesis was careful to avoid broad generalisations.",,Omede PI,,2019,,,,Ph.D. Thesis
Global Change Implications of Adaptation to Climatic Variability,"In this thesis I have examined ecology and evolution in a globally changing environment to address how climate change is likely to differentially affect tropical and temperate populations. I have approached the problem theoretically by applying an evolutionary model to global climate data and I have tested specific hypotheses arising from the model and associated theories using checkerspot butterfly populations throughout North and Central America.The study of climatic variability and adaptation has a long history going back to Alexander von Humboldt. I briefly review this history in chapter 1. In chapter 2 I applied an evolutionary model based on the ""jack-of-all-trades is a master of none'' principle to global temperature and precipitation data to derive theoretical tolerance curves throughout the globe. Primarily, this model predicts lower thermal tolerance breadths for tropical organisms because over a given year they experience far less thermal variation than temperate populations. The pattern is more complicated when considering precipitation effects. Using IPCC projections of climate change for 2100 I then examined what the relative fitness effects would be across the globe and found that other than the Mediterranean region, areas of the tropics (e.g. Southeast Asia, Central America and Equatorial Africa) would be among the most heavily impacted regions in the world (Bonebrake and Mastrandrea 2010).In chapters 3 and 4 I built on work within temperate checkerspot butterfly systems to examine climatic heterogeneity impacts on ecological and evolutionary process. The heavily studied Euphydryas editha population of Jasper Ridge at Stanford University went extinct in 1998 partly due to increases in precipitation variability in recent decades and its effects on host plant dynamics. To evaluate the potential of habitat creation to buffer the impacts of climatic changes, I participated in a multidisciplinary effort to experimentally alter soil conditions to mimic the unique properties of serpentine grasslands. Our efforts showed some success in lowering the invasibility (invasive species being another threat to E. editha) of experimental sites but also showed that the results were highly contingent on the amount of rainfall in each year (Bonebrake et al. in review). I also studied the evolutionary relationship between oviposition behavior and offspring performance in closely related Euphydryas gillettii. This study showed large effects of inter-annual variation in temperature in a montane E. gillettii Colorado population and that, for example, warm years and oviposition preference can significantly accelerate larval development time (Bonebrake et al. 2010).Finally, in chapter 5 I examined the biophysical, morphological and physiological properties of adaptation to temperate and tropical climates using populations of the widely distributed butterfly Chlosyne lacinia. First, the biophysical model demonstrated that variation in air temperature poorly predicted variation in butterfly body temperature (Tb) and that diurnal variation in Tb was lower for butterflies in El Salvador than Tb variation in Arizona. Second, as for morphological characteristics relevant to butterfly thermoregulation, thorax size was consistently smaller in tropical populations while variation in other characters (fur length, body length and wing absorptivity) was not consistent across latitude. Third, physiological studies showed that C. lacinia in Arizona began flying at colder temperatures (Tb=24 °C) than C. lacinia in El Salvador (Tb=27 °C). As consequence of these factors combined, the model predicts increased effects of climate change for tropical C. lacinia in the form of greater increases in flight activity time and more prolonged exposure to lethal temperatures (Tb> 45 °C) relative to temperate butterflies (Bonebrake et al. in prep).",,Bonebrake TC,,2010,,,,Ph.D. Thesis
Human-Giraffe Interactions: Characterizing Poaching and Use of Parts as a Threat to Giraffe in Northern Kenya,"Giraffe (Giraffe spp.) are iconic wildlife species to Africa, yet relatively little conservation funding and research have been directed at protection of giraffe in the wild. A growing number of national governments and conservation organizations are implementing management strategies to address the threats that giraffe face. To inform these plans, there is a need for social science that examines the human pressures associated with decline of giraffe populations, including poaching and the use of giraffe parts. As the large majority of reticulated giraffe (Giraffa reticulata) range occurs outside formally protected areas, conservation plans must be made with pastoralist communities and other actors in northern Kenya where the land is shared between people, their livestock, and wildlife. The research presented in this dissertation was conducted as part of a community-based program focused on reticulated giraffe, called the Twiga Walinzi Initiative (""Giraffe Guards"" in Swahili), and represents the first quantitative study on the human dimensions of giraffe conservation. Goals of the research project were to examine key cognitions to human-giraffe interactions (i.e. attitudes, beliefs, perceptions), assess relationships between certain cognitions within areas that adopt a community-based conservation approach, and understand the extent and drivers of giraffe meat and part usage. Face-to-face interviews were conducted at two study sites over survey periods in 2016/17 (n = 579) and 2019 (n = 680). Results from these studies provide insights to how pastoralist communities view and act toward local giraffe. Factors that significantly influenced support for giraffe conservation differed between study sites, suggesting that local context is important to shaping human-giraffe interactions (Chapter 2). For instance, perceived benefits had stronger influence on normative belief in communities more recently connected with wildlife-based tourism. The linkages between perceived benefits, attitudes, and behaviors were further explored by assessing the relationships between these concepts within a community-based conservation setting (Chapter 3). Findings suggest a positive association between perceived benefits and attitudes toward giraffe, but there was less evidence that perceptions of wildlife-related benefits influenced use of giraffe meat/parts. As human behavior is of central interest to conservation, we also assessed levels of giraffe meat consumption (Chapter 4) and determinants of intention to consume giraffe meat (Chapter 5). Specialized questioning techniques were utilized to estimate prevalence of giraffe meat consumption preceding the two surveys. Estimated prevalence of giraffe meat consumption declined after establishment of the Twiga Walinzi. Perceived behavioral control had stronger relative influence than attitudes and subjective norms on future intention to consume giraffe meat. Collectively, these research findings are relevant for applied giraffe conservation efforts and provide a framework for understanding human-giraffe interactions and associated threats in diverse global settings.",,"Ruppert KA,Glikman J,De Urioste-Stone S,Noblet C,Rickard L",,2020,,,,Ph.D. Thesis
A Visual Analytics Framework for Spatiotemporal Trade Network Analysis,"Economic globalization is increasing connectedness among regions of the world, creating complex interdependencies within various supply chains. Recent studies have indicated that changes and disruptions within such networks can serve as indicators for increased risks of violence and armed conflicts. This is especially true of countries that may not be able to compete for scarce commodities during supply shocks. Thus, network-induced vulnerability to supply disruption is typically exported from wealthier populations to disadvantaged populations. As such, researchers and stakeholders concerned with supply chains, political science, environmental studies, etc. need tools to explore the complex dynamics within global trade networks and how the structure of these networks relates to regional instability. However, the multivariate, spatiotemporal nature of the network structure creates a bottleneck in the extraction and analysis of correlations and anomalies for exploratory data analysis and hypothesis generation. Working closely with experts in political science and sustainability, we have developed a highly coordinated, multi-view framework that utilizes anomaly detection, network analytics, and spatiotemporal visualization methods for exploring the relationship between global trade networks and regional instability. Requirements for analysis and initial research questions to be investigated are elicited from domain experts, and a variety of visual encoding techniques for rapid assessment of analysis and correlations between trade goods, network patterns, and time series signatures are explored. We demonstrate the application of our framework through case studies focusing on armed conflicts in Africa, regional instability measures, and their relationship to international global trade.",,"Wang H,Lu Y,Shutters ST,Steptoe M,Wang F,Landis S,Maciejewski R",,2019,331–341,10.1109/TVCG.2018.2864844,https://doi-org.proxy.bnl.lu/10.1109/TVCG.2018.2864844;http://dx.doi.org/10.1109/TVCG.2018.2864844,Journal Article
HT '09: Proceedings of the 20th ACM Conference on Hypertext and Hypermedia,"Welcome to Hypertext 2009, the 20th ACM Conference on Hypertext and Hypermedia. The conference continues in its tradition of a diverse and multidisciplinary approach to the study of the ""link"" in all its manifestations. Links between any types of objects, from documents and media to people, are at the center of Hypertext from many perspectives: their design, management, applications, semantics, presentation, dynamics, effects on society, and the knowledge that can be derived from their analysis.Hypertext 2009 also continues to be organized around tracks autonomously organized with separate chairs and program committees. This year we have three tracks in the technical program.The Information Structure and Presentation track represents a multitude of topics, which were traditionally represented at ACM Hypertext Conferences. The track program targets formal study of scholarly, structural, sculptural, spatial, open, dynamic and adaptive or any other type of hypertext (or Web-based information system). This track also focuses on how hypertext approaches and technologies can be applied to structure and present information in diverse domains, and how hypertext techniques can be exploited in classical and advanced applications.The People, Resources, and Annotations track explores social annotations, which have rapidly risen as one of the most exciting recent developments in Web science. Users can easily markup other authors' resources via collaborative mechanisms such as tagging, filtering, voting, editing, classification, and rating. These social processes lead to the emergence of many types of links between texts, users, concepts, pages, articles, photos, videos, tags, and so on. The track's focus is on design, analysis, and modeling of information systems driven by social linking.Finally, the Hypertext and Community track examines and reflects upon social cyberculture in electronic media, ranging from literary fiction and creative scholarship to blog and microblog networks, social sites, games, auctions, net art, and markets.The Hypertext 2009 technical program received 117 submissions, a 70% increase from the previous year, despite the global economic downturn. These submissions originated from 36 countries in Europe, Asia, Africa, Australia, North and South America. 37 papers (26 long and 11 short) were selected for presentation at the conference, yielding an acceptance rate of 31.6%. We feel that the papers contained in these proceedings represent a strong, diverse, and exciting program.We are fortunate to feature two stellar keynotes by Lada Adamic of the University of Michigan on The Social Hyperlink and by Ricardo Baeza-Yates of Yahoo! Research on Relating Content by Web Usage. In addition, the program includes a poster & demo session (also in these proceedings), an ACM student research competition, and four workshops: Web 3.0: Merging Semantic Web and Social Web organized by Federica Cena (University of Torino, Italy), Pasquale Lops (University of Bari, Italy), and Rosta Farzan (University of Pittsburgh, USA).Dynamic and Adaptive Hypertext: Generic Frameworks, Approaches and Techniques organized by Paul De Bra and Mykola Pechenizkiy (Eindhoven University of Technology, the Netherlands).New Forms of Xanalogical Storage and Function organized by Fabio Vitali and Angelo Di Iorio (University of Bologna, Italy), and Jamie Blustein (Dalhousie University, Canada).Tagging Dynamics in Online Communities organized by Vittorio Loreto and Andrea Capocci (Sapienza University of Rome, Italy).Finally, Hypertext 2009 attendees have a chance to experiment with applications mixing real-world data and on-line data. Active RFID tags in the badges of volunteers track the real-time relations of physical proximity between the attendees. The data collection and visualization systems is provided by the SocioPatterns.org project and exposes API methods that allow developers to mash up real-world links between the attendees with other types of linking information from the Web.We are proud that Hypertext 2009 takes place in Torino (Turin). The capital of the Piedmont region in northwestern Italy, Torino lies at the foot of the Alps, the majestic mountains that hosted the 2006 Winter Olympics. First capital of the Kingdom of Italy, then one of the European centers of baroque, today Torino is a dynamic city known for its industry (Fiat and Lancia car makers are headquartered here); art and culture (its 40+ museums include the world's largest Egyptian collection outside of Cairo or the National Museum of Cinema located inside the Mole Antonelliana); sports (home of Juventus FC and Torino FC); research and education (including the University of Torino, the Polytechnic, and the Institute for Scientific Interchange Foundation); and cuisine (solid chocolate was born here in the 18th century). We hope that all attendees have an opportunity to enjoy the many cultural, artistic, historic, architectural, and culinary pleasures that Torino has to offer.",,,,2009,,,,Book
ISLPED '12: Proceedings of the 2012 ACM/IEEE International Symposium on Low Power Electronics and Design,"It is our great pleasure to welcome you to the ACM/IEEE International Symposium on Low Power Electronics and Design -- ISLPED 2012. This year's symposium continues its tradition of being the premier forum for presentation of research results and experience reports on leading-edge issues of low-power design. The mission of the symposium is to share novel low-power solutions that fulfill the needs of electronics design and identify new directions for future research and development. ISLPED gives researchers and practitioners a unique opportunity to share their perspectives with others interested in the various aspects of low-power design.The call for papers attracted 213 submissions from Asia, Canada, Europe, Africa, and the United States. The program committee accepted 66 papers, including 34 for full-length presentations, 14 for half-length presentations, and 18 for poster presentations. They cover a variety of topics, including next-generation technology, architecture, analog and MEMS, design tools, methodology, and optimization techniques for low-power design. We thank the authors for providing the content of the program. We are grateful to the program committee members, who volunteered their valuable time and hard work in reviewing papers, attending the in-person review meeting, and providing feedback for authors. We hope that these proceedings will serve as a valuable reference for researchers and developers on these topics.This year's program features three keynote speakers over three days. On the first day, Dr. Pradip Bose (IBM) introduces energy-secure computing, a new topic on ensuring robust and secure functionality while meeting the energy-related constraints of the emerging green-computing era. On the second day, Dr. Kaushik Roy (Purdue University) presents using spin (as opposed to charge) as state variable for memory and logic computation to achieve high density and very low power. On the third day, Dr. Uming Ko (MediaTek USA) speaks on ultra-low power challenges for the next-generation ASICs. In addition, the program includes a special session on future needs for low-power electronic design from an industry perspective, a panel on green communication and storage systems, and an embedded tutorial on power and thermal management for smart phones.We hope that you will find this program forward-looking and thought-provoking and that the symposium will provide you with a valuable opportunity to share ideas with other researchers and practitioners from institutions around the world.",,,,2012,,,,Book
An Investigation into Supply Chain Risk Factors and Their Impact on Performance of Humanitarian Pharmaceutical Supply Chain in Sub-Sahara Africa - A Case Study of the Supply Chain System for UNICEF Tanzania,"This thesis investigated the presence of supply chain risk factors and their impact on performance of humanitarian health programs in Sub-Saharan Africa, particularly UNICEF Tanzania. Supply Chain Risk management (SCRM) approach has become a major contributor to supply chain performance and to program/business success. The aim of this study was to contribute to professional practice by suggesting risk management approach (prioritisation and mitigation) as the one possible solution to the criticism that; ""Supply chain management is our Achilles heel; we receive the most criticism for this"" (UNICEF 2014). This criticism triggered this research whose aim was to systematically identify, prioritise and mitigate critical risk factors that impact on supply chain performance metrics of time, cost and quality. To achieve this aim, this study addressed two key research questions of risk prioritisation and risk treatment. A number of Supply Chain Risk Management (SCRM) studies available in literature mainly identified risks factors without much focus on prioritisation using Failure Mode and Effect Analysis (FMEA) methodology. This enquiry was abductive in reasoning and mixed methods in approach using process FMEA to quantify and analyse process risks. Besides being industry relevant, the benefit of using FMEA for this investigation included an increased focus on most imminent risks, prioritisation of risks and development of effective risk mitigation strategies.The research findings confirmed that poor risk management is the primary cause of poor supply chain performance. It also found a causal relationship between detection capability and likelihood of occurrence on a few of the risks, and a zero relationship on most risks tested. Overall, the research confirmed the proposition that effective supply chain risk management approach (prioritisation and mitigation/treatment) contributes to an improvement in supply chain performance of health programs in Sub Saharan Africa. The research findings matter in that the established risk profiles by performance metrics of delivery time, cost and quality (the SCRM Iceberg Model) can be used by supply chain managers to anticipate and proactively manage the potential risks found in their operations. The knowledge on the relationship between investment on risk detection capability and the reduction in risk occurrence challenges managers to re-assess the potential benefit of every investment on risk detection. The suggested context specific challenges and opportunities identified in this study, if applied rationally can help effectively manage supply chain risks for humanitarian operations in Sub-Saharan Africa and similar context globally.",,Sheshe F,,2018,,,,Ph.D. Thesis
"A Geometallurgical Evaluation of the Ores of the Northern Kalahari Manganese Deposit, South Africa","The Kalahari Manganese Deposit (KMD) is the largest of five erosional relics of the Hotazel Formation that are located near Kuruman in the Northern Cape Province of South Africa. Manganese ores are exploited from the lowermost of three manganiferous beds that are interbedded with banded iron-formation (BIF) and hematite lutite, that together constitute the Hotazel Formation. Two major ore types have been delineated previously, viz. low grade braunite lutite of the Mamatwan-type, and high grade oxidic ores of the Wessels-type, with the latter spatially restricted to the northern KMD. Genesis of the ores was temporally distinct, with the Mamatwan-type ore considered as a sedimentary-diagenetic precursor to the hydrothermally altered Wessels-type ore.Drill core samples from the Nchwaning-Gloria area of the northern KMD were analysed, with the aim to better characterise ore genesis, with emphasis on ore alteration. A second part of the study aimed at the application of mineralogical and geochemical information to aspects of ore smelting for the production of Mn alloy for use in the steel industry. Methods employed were drill core logging, X-ray diffraction (XRD), petrography, electron probe microanalysis (EPMA), major and trace element (including REE) analysis (employing artificial neural networks for evaluation of elemental trends), and stable isotope (C and O) analysis. Significant effort was invested in method development for quantitative mineralogical modal analysis using Rietveld refinement of XRD data.The study shows that a number of ore types can be differentiated in the northern KMD on the basis of mineral assemblage, grade, texture and geochemical characteristics. The ores are broadly classified into least altered (LA), partially altered (PA) and advanced altered (AA) types. The LA ores are low grade (45 wt%Mn), are recorded. Trace elements of significance include Zn, associated with hausmannite, B, associated with massive braunite and a number of trace minerals, and P, typically present in trace quantities of apatite.In terms of ore genesis, mineralogical, geochemical and geological considerations suggest that Mn (and Fe) originated from submarine hydrothermal vents, from which it travelled in hydrothermal plumes, prior to rapid deposition 2.2 Ga ago. Diagenesis followed soon after deposition, through redox reactions involving organic matter and higher oxides of Mn to produce the braunite-carbonate assemblage primarily observed in LA ores. The carbonate:oxide ratio and nature of the carbonates varied slightly depending on fluctuations in organic matter flux to the sediment, as well as marine bicarbonate concentrations. Metamorphism, in relation to diagenesis and metasomatism, is poorly understood, but is perceived to have resulted in serpentine formation, as observed in LA and PA ores.Hydrothermal alteration affected the ores in the northern part of the KMD at 1.3-1.0 Ga. Alteration involved extensive carbonate dissolution, and introduction of Fe by remobilisation from the surrounding BIF, alkali elements, base metals (particularly Zn) and minimal LREE into the system, with the removal of CO2 and some minor elements. Temperatures of alteration probably reached a maximum of 150 °C. Incoming fluids are proposed to have been hotter, i.e., >200 °C, and reducing in character, as they carried ferrous iron. The hydrothermal system was characterised by a combination of redox and acid-base reactions. Redox reactions were relatively limited, and involved oxidation of ferrous to ferric Fe, whereas extensive reactions involving Mn minerals are proposed to have been non-redox in character. Acid-base equilibria suggest that the formation of the significant Mn mineral assemblages of braunitegroup-bixbyite- vs. hausmannite-rich ores were probably determined by the activity of Mn2+ in the fluid, in turn related to the fluid:rock ratio. Lower fluid:rock ratios are represented by braunite-group-bixbyite-rich ores, whereas higher fluid:rock ratios are represented by hausmannite-rich ores, which also represent the most advanced degree of alteration in the system. PA ores represent the lowest fluid:rock ratios, as well as the spent fluid. Further, the hydrothermal system was open for CO2, but conservative with respect to Mn, as indicated by O and C isotope systematics and Mn3+/Mn2+ ratios, respectively. Rare earth element behaviour is interpreted in terms of a conservative system that was predominantly protolithbuffered. Dissolution of diagenetic apatite, remobilisation and reprecipitation of REE in other trace minerals results in a power trend in the Ce anomaly that is not evident from traditional PAAS-normalised REE plots.Normal faults of N-S and E-W strike, as well as the thrust fault to the west in the study area, were important controls that localised the hydrothermal alteration system. These faults were conduits for hydrothermal fluids that effected alteration in the BIF, hematite lutite and braunite lutite units of the Hotazel Formation. The BIF, in particular, is viewed as the source of divalent Fe, generated through hydrothermal dissolution of Fe-bearing carbonates present in unaltered BIF. Fault morphology (displacement) appears to be such that fluids could enter the Hotazel Formation at any given stratigraphic level, resulting in random alteration patterns and different scales of alteration. Faults in the western part of the study area, however, did not facilitate large volumes of fluid movement compared with faults in the eastern part, resulting in more braunite-group-bixbyite-rich assemblages (lower fluid:rock ratio) in the west, compared with more hausmannite-rich assemblages (higher fluid:rock ratio) in the east. Such trends were controlled by thrust dynamics that produced compressional regimes to the west, and extensional regimes to the east.In terms of method development, this study represents the first successful attempt at quantifying the relative abundance of minerals in ores from the KMD using XRD without requiring an internal standard. An original method is, further, presented for derivation of bulk Mn oxidation budgets through quantifying minerals present, understanding their chemical composition and validating with bulk major element concentrations. Additionally, the two reference materials, SARM 16 and SARM 17, have been, for the first time, quantitatively characterised in terms of mineral modal abundance and derived bulk Mn oxidation budgetOn the basis of the quantitative mineralogical results, a proposal is presented for implementation on a Mn ore smelting plant, as a means for quality control of feeds to furnaces. Such implementation will allow for better understanding of ore distribution in the deposit, resulting in the application of selective mining and/or ore blending to achieve desired specifications. More accurate prediction of ore behaviour during smelting can be facilitated, and it is expected that implementation of the method will have significant impact on economising energy and reductant consumption in furnaces.The geochemical data gathered on the different ore types add significantly to the database on elemental distribution in ores of the northern KMD, and constitute benchmarks for these ores, particularly as stricter import/export regulations take effect. Again, the two reference materials, SARM 16 and SARM 17, have been extensively characterised, with a substantial trace element (including REE), LOI and moisture dataset added to the already certified major element composition.",,Chetty D,,2008,,,,Ph.D. Thesis
"Decrease the Number of Patients Lost to Follow-up in the Monitoring of PLHIV in Cross-Border Areas between The Gambia, Senegal and Guinea Bissau","The fight against AIDS in West Africa is a big challenge of public health. The main difficulties are related to the availability of antiretrovirals (AVR) and mainly to the observance of the treatment. In the cross-borderland areas of the Gambia, Senegal and Guinea Bissau, these difficulties of the observance of the treatment are amplified by the vulnerability, the high mobility of the populations and the problems of communication between the actors of the monitoring of PLHIV who speak three different languages. That's what makes difficult the fight against AIDS in those areas and justify the phenomenon of the patients lost to follow-up, which is a great indicator for the following of the People Living with HIV-virus (PLHIV). This paper presents the decrease of the number of patients lost to follow-up in the monitoring of the PLHIV in the cross-borderland areas of the Gambia, Senegal and Guinea Bissau by a multilingual semantic web platform of reference, counter reference and auto reference.",,"Diop I,Dieng Y,Faye Y,Malack CA,Cisse O,Diouf B",,2019,,10.1145/3361570.3361596,https://doi-org.proxy.bnl.lu/10.1145/3361570.3361596;http://dx.doi.org/10.1145/3361570.3361596,Conference Paper
Image Fusion for Enhanced Forest Structural Assessment,"This research explores the potential benefits of fusing active and passive medium-resolution satellite-borne sensor data for forest structural assessment. Image fusion was applied as a means of retaining disparate data features relevant to modelling and mapping of forest structural attributes in even-aged (4-11 years) Eucalyptus plantations, located in the southern KwaZulu-Natal midlands of South Africa. Remote-sensing data used in this research included the visible and near-infrared bands of the Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER), as well as a fine-beam (6.25 m resolution) Radarsat-1 image. Both datasets were collected during the spring of 2006 and fused using a modified discrete wavelet transformation. Spatially referenced forest-inventory data were also collected during this time, with 122 plots enumerated in 38 plantation compartments. Empirical relationships (ordinary and multiple regression) were used to test whether fused data sources produced superior statistical models. Secondary objectives of the article included exploring the roles of texture, derived from grey-level co-occurrence matrices, and scale in terms of forest modelling at the plot and extended plot levels (Voroni diagrams). Results indicate that single bands from both the optical and Synthetic Aperture Radar (SAR) datasets were not adept at modelling basal area and merchantable timber volume with adjusted R2 (coefficient of determination) values < 0.3. An optimized multiple-regression approach (adjusted R2) improved results based on mean, range and standard deviation statistics when compared to single bands, but were still not suitable for operational forest applications (basal area: R2 = 0.55, volume: R2 = 0.59). No significant difference was found between fused and non-fused datasets; however, optical and fused datasets produced superior models when compared to SAR results. Investigations into potential benefits of using textural indices and varied scales also returned inconclusive results. Findings indicate that the spatial resolutions of both sensors are inappropriate for plantation forest assessment. The frequency of the C-band Radarsat-1 data is, for instance, unable to penetrate the canopy and interact with the woody structures below canopy, leading to weak statistical models. The lack of variability in both the optical and SAR data lead to unconvincing results in the fused imagery, where, in some cases, the adjusted R2 results were worse than the single-dataset approach. It was concluded that future research should focus on high-spatial-resolution optical and Light Detection and Ranging (LiDAR) data and the development of automated and semi-automated forest-inventory procedures.",,"Roberts JW,Van Aardt JA,Ahmed FB",,2011,243–266,10.1080/01431160903463684,https://doi-org.proxy.bnl.lu/10.1080/01431160903463684;http://dx.doi.org/10.1080/01431160903463684,Journal Article
ICEGOV '13: Proceedings of the 7th International Conference on Theory and Practice of Electronic Governance,"The 7th International Conference on Theory and Practice of Electronic Governance, ICEGOV2013, took place in Seoul, Republic of Korea from 22 to 25 October 2013. The conference was organized under the patronage of the Ministry of Security and Public Administration of the Republic of Korea (MOSPA) by the National Information Society Agency and by Macao-based Center for Electronic Governance at United Nations University International Institute for Software Technology (UNU-IIST) as the founder and organizer of the ICEGOV series. The conference took place under the theme ""Beyond 2015"" Smart Governance, Smart Development"". It was co-located with the Global e-Government Forum, organized by MOSPA in collaboration with United Nations Department of Economic and Social Affairs (UNDESA).The ICEGOV series focuses on the use of technology to transform relationships between government and citizens, businesses, civil society and other arms of government (Electronic Governance). Established in 2007, the series looks beyond the traditional focus on technology-enabled transformation in government (Electronic Government) towards new forms, new paradigms, and new foundations for technology-enabled governance, collaboration and sustainable development. ICEGOV is a platform where researchers, policy-makers and practitioners meet; a platform where theories are tested, insights are shared and experiences are reported; a platform for network- and capacity-building where keynote lectures and paper sessions are complemented by plenary discussions, town hall debates and poster exhibitions; a platform for international dialogue attended by participants from developing, developed and transition countries, from the United Nations system, and from many academic, governmental, non-governmental and private sector organizations. Since its establishment, the series has traveled globally from Macao (ICEGOV2007), through Cairo (ICEGOV2008), Bogota (ICEGOV2009), Beijing (ICEGOV2010), Tallinn (ICEGOV2011) and Albany (ICEGOV2012), to Seoul (ICEGOV2013) all generating significant local interest and stakeholder engagement.The program of ICEGOV2013 was built upon contributions from researchers and practitioners from around the world. In response to the call for papers, the conference received 133 papers from 54 countries and economies. The papers were evaluated in five categories: 1) Completed Research Papers providing the outcomes of complete research in one or more aspects of EGOV, with proven capability to advance the state of research in the field, limited to 10 pages; 2) Ongoing Research Papers providing the outcomes of ongoing research in one or more aspects of EGOV, with potential capability to advance the state of research in the field, limited to 4 pages; 3) Completed Experience Papers describing completed experience concerning EGOV policy or practice innovations, with proven capability to advance the state of practice in the field, including critical success factors and insights on the challenges encountered and how they were addressed, limited to 10 pages; 4) Ongoing Experience Papers describing ongoing experience concerning EGOV policy and practice innovations, with potential capability to advance the state of practice in the field, including critical success factors and insights on the challenges encountered and how they are being addressed, limited to 4 pages; and 5) Poster Papers presenting novel ideas and initiatives with potential to advance the state of research or state of practice in the field, limited to 2 pages. In total, 43 Completed Research Papers, 45 Ongoing Research Papers, 17 Completed Experience Papers, 21 Ongoing Experience Papers and 8 Posters were received. After anonymous peer-review process carried out by the members of the Program Committee at least three independent reviews were obtained for each submission as a basis for acceptance decisions: 13 submissions were accepted as Completed Research Papers, 8 as Completed Experience Papers, 29 as Ongoing Research Papers, 11 as Ongoing Experience Papers and 21 as Poster Papers. All accepted submissions, revised to address review comments, and presented at the conference within 6 paper tracks, 11 thematic sessions and one poster session, are included in this volume. Among them, like the last three ICEGOV conferences, the authors of selected papers were invited to submit extended versions of their papers for possible publication in the special issue of Government Information Quarterly, Elsevier.Based on the submitted and invited contributions and continuing the ICEGOV tradition, ICEGOV2013 featured a rich academic, capacity-building and network-building program comprising keynote lectures, plenary discussions, town hall debates, paper tracks, thematic sessions and the doctoral colloquium and poster exhibition. The program engaged individuals from over 60 countries and economies as authors, reviewers, committee members or resource persons. The details of the program are provided below.The conference included six keynote lectures on various aspects of Electronic Governance (EGOV), conducted by distinguished experts and practitioners in the area: 1) Park Chan Woo, Vice-Minister of Security and Public Administration of the Republic of Korea; 2) Alikhan Baimenov, Chairman of the Agency for Civil Service Affairs of the Republic of Kazakhstan; 3) Moon Suk Ahn, Chair Professor of e- Government, Korea University, Republic of Korea; 4) Mohammed Ali Al, Chief Executive Officer, e-Government Authority, Kingdom of Bahrain; 5) Henk G. Sol, Professor of Business and ICT and Founding Dean, University of Groningen, Netherlands; and 6) Edwin Lau, Head of Division, Reform of the Public Sector, Organization for Economic Co-operation and Development (OECD).Three plenary sessions followed the keynote lectures on the second, third and fourth day of the conference, focusing on specific questions of interest to the EGOV research and policy community:1. Are international EGOV rankings having a mobilizing or distracting influence on development? Chaired by Tomasz Janowski, Head of the Center for Electronic Governance at UNU-IIST and attended by: Vincenzo Aquaro, Chief of E-Government Branch, Division for Public Administration and Development Management, UNDESA; Bikesh Kurmangaliyeva, Deputy Chairwoman of the Board of ""Zerde"" National CT Holding, Kazakhstan; Mohammed Ali Al Qaed, CEO of eGovernment Authority, Kingdom of Bahrain; Mesfin Belachew Tefera, Technical Advisor to the Minister, Ethiopian Ministry of Communication and Information Technology; and Saleem Zoughbi, Former Regional ICT Advisor, UNESCWA and consultant for UNU-IIST.2. Who should drive smart conversations for sustainable development experts, citizens or politicians? Chaired by Marijn Janssen, Professor of ICT and Governance at Technology, Policy and Management Faculty, Delft University of Technology, Netherlands and attended by: Sunil Choenni, Head, Department of Statistical Information Management and Policy Analysis, Research and Documentation Centre (WODC), Dutch Ministry of Security and Justice; Harekrishna Misra, Professor in IT and Systems at the Institute of Rural Management Anand (IRMA), India; Henk G.Sol, Professor of Business and ICT and Founding Dean, Faculty of Economics and Business, University of Groningen, Netherlands; and Evgeny Styrin, Senior Research Analyst and Associate Professor, National Research University Higher School of Economics, Russia.3. Is a common set of e-government principles, applicable to all countries and contexts, possible? Chaired by Samuel Chan, Member of Executive Committee, Macao Science and Technology Development Fund, Macao SAR Government and attended by: Wojciech Cellary, Head of the Department of Information Technology, Poznan University of Economics, Poland; Sharon Dawes, Senior Fellow, Center for Technology in Government, University at Albany, USA; Edwin Lau, Head of Division, Reform of the Public Sector, Public Governance and Territorial Development Directorate, Organization for Economic Co-operation and Development; and Jeremy Millard, Associate Research Fellow, Brunel University, UK.Three town hall debates took place in the afternoons of the first, second and third days of the conference. They focused on three salient questions for the EGOV research and policy community:1. Catalyzing Smart Transformation: What Makes Governments Smarter? Chaired by Samia Melhem, Lead Policy Specialist, Transform Practice, Chair, eDevelopment Community of Practice, Transport, Water and ICT, Sustainable Development Network, World Bank Group; and Oleg Petrov, Senior Program Officer, ICT, World Bank; and attended by: Jabiri Kuwe Bakari, CEO, e-Government Agency, Tanzania; Rajendra Kumar, Senior Officer, Indian Administrative Service and Joint Secretary (e-Governance), Department of Electronics and Information Technology, Government of India; Bikesh Kurmangaliyeva, Deputy Chairwoman of the Board of ""Zerde"" National CT Holding, Kazakhstan; Margareta Petrusevschi, Knowledge and Learning Coordinator, e-Government Centre, Government of the Republic of Moldova; James Saaka, Executive Director, National Information Technology Authority, Uganda; Mesfin Belachew Tefera, Technical Advisor to the Ethiopian Minister of Communication and Information Technology; and Jeongwon Yoon, Executive Director, National Information Society Agency, Korea. This town hall was organized by the World Bank.2. Is Good Governance a Pre-Condition or a Consequence of the Development of Knowledge Societies? Chaired by Andrea Cairola, Adviser for Communication and Information, UNESCO Office Beijing, Cluster Office to the Democratic People's Republic of Korea, Japan, Mongolia, People's Republic of China and Republic of Korea; and attended by: Johanna Ekua Awotwi, Director of Research and ICT Operations, Centre for e-Governance, Accra, Ghana; Antonio Cordella, Lecturer in Information Systems, London School of Economics and Political Sciences, UK; Marco Peres, Director, Observatory for Society, Technology and Government Information, University Externado of Colombia, Colombia; Margareta Petrusevschi, Knowledge and Learning Coordinator, e-Government Centre, Government of the Republic of Moldova, Moldova; and Jeongwon Yoon, Executive Director, National Information Society Agency, Republic of Korea. This town hall was organized by the UNESCO Information for All Programme.3. Striking the Balance of Security, Privacy and Openness: To Open or Not To Open? Chaired by Theresa Pardo, Director of the Center for Technology in Government, University at Albany, USA, and attended by: Sharon Dawes, Senior Fellow, Center for Technology in Government, University at Albany, USA; Ramon Gil-Garcia, Research Director, Center for Technology in Government, University at Albany, USA; Louise Thomasen, independent consultant and expert in EGOV and technology, Denmark; and Lei Zheng, Assistant Professor, Department of Public Administration, Fudan University, China.The program included six paper tracks, chaired by leading international experts in the corresponding areas, comprising presentations of three to six accepted papers: 1) Building Smart Government chaired by Theresa Pardo, Director of the Center for Technology in Government, University at Albany, USA and Gabriel Puron Cid, Professor at the Centre of Research and Teaching in Economic Sciences, Mexico; 2) Governing through Networks chaired by Sehl Mellouli, Associate Professor at Laval University, Canada and Adegboyega Ojo, Research Fellow and Leader of E-Government Group at INSIGHT, National University of Ireland, Ireland; 3) Policy and Governance Innovation chaired by Natalie Helbig, Senior Research Associate at the Center for Technology in Government, University at Albany, USA and Marijn Janssen, Professor in ICT and Governance at the Delft University of Technology, Netherlands; 4) Smart Governance for Smart Industries chaired by Wojciech Cellary, Professor and Head of the Department of Information Technology at the Poznan University of Economics, Poland and Antonio Cordella, Lecturer at the London School of Economics and Political Sciences, UK; 5) Smart Governance for Smart Societies chaired by Jeremy Millard, Associate Research Fellow at the Brunel University, UK; and 6) Ethics, Transparency and Accountability chaired by Jeanne Holm, Chief Knowledge Architect at the NASA Jet Propulsion Laboratory, USA. Each track took place across the whole duration of the conference, with tutorial introduction to the topic of the track organized on the first day, presentations of accepted papers on the second or third day, and workshop-style discussion on the last day.Complementing the paper tracks, 11 thematic sessions were organized and chaired by industrial, academic, government and international organizations active in the theme of the session, comprising presentations of up to four accepted papers: 1) EGOV for Development chaired by Nag Yeon Lee, ICT Consultant and Instructor for e-Government on behalf of the Asia Pacific Center on ICT for Development, United Nations Economic and Social Commission for Asia Pacific; 2) National Data Policies chaired by Zhanat Zhakhmetova, Head of the Office of State Informatization Policy, Department of State Information Technology Policy, on behalf of the Ministry of Transport and Communications, Republic of Kazakhstan; 3) Governing Ageing Society chaired by Toshio Obi, Professor, Institute of e-Government on behalf of Waseda University, Japan; 4) Governing Smart Cities chaired by Yoon Chang So, Smart Cities Country Leader, IBM Korea on behalf of IBM; 5) Open Government Data Impact chaired by Edwin Lau, Head of Division, Reform of the Public Sector, Public Governance and Territorial Development Directorate, on behalf of the Organization for Economic Co-operation and Development; 6) Interoperability Governance chaired by Jung Sik Hwang, Platform Strategy Lead at Microsoft Korea on behalf of Microsoft; 7) Government on Social Media chaired by Jeanne Holm, Evangelist, Data.Gov and Chief Knowledge Architect at the Jet Propulsion Laboratory, NASA on behalf of the World Wide Web Consortium; 8) Innovative EGOV Applications chaired by Oleg Petrov, Senior Program Officer, ICT, World Bank on behalf of the World Bank; 9) Participatory Government chaired by Bernd Friedrich, Head of the Information and Communications Technologies for Development Project at Deutsche Gesellschaft für Internationale Zusammenarbeit (GIZ) GmbH, Germany on behalf of GTZ; 10) Mobile Governance chaired by Nestor Eduardo Fajardo Infante, Advisor for Research, Development and Innovation, Ministry of Information Technology and Communication, on behalf of the Government of Colombia; and 11) Open Data Ecosystem chaired by Jeanne Holm, Evangelist, Data.Gov and Chief Knowledge Architect, Jet Propulsion Laboratory, NASA on behalf of the U.S. Government and Data.Gov.The program also included poster exhibition, organized in the reception style to allow authors to present their ongoing work, receive feedback and engage in discussions and networking; and an interactive doctoral colloquium, jointly organized by the Center for Electronic Governance at UNU-IIST, Macao, University of Groningen, Netherlands and Chuo University, Japan. The colloquium provided doctoral students from different disciplines an opportunity to discuss a variety of EGOV topics and methods related to their research work, dissertations and career plans. The colloquium was co-chaired by Elsa Estevez, Academic Program Officer, United Nations University International Institute for Software Technology, Macao; Hiroko Kudo, Professor of Public Policy and Public Management, Faculty of Law, Chuo University, Japan; and Henk G. Sol, Professor of Business and ICT and Founding Dean, Faculty of Economics and Business, University of Groningen, Netherlands; and attended by Adegboyega Ojo, Research Fellow and Leader of E-Government Group at the INSIGHT Center for Data Analytics, National University of Ireland, Ireland as invited academic.The conference awarded best paper titles in Best Research Paper and Best Experience Paper categories. The selection was carried out jointly by Elsa Estevez as the ICEGOV2013 Awards Chair, and Tomasz Janowski and Jeanne Holm as the ICEGOV2013 Program Chairs. Three papers were nominated to the Best Experience Paper award: 1) A Reputation Based Electronic Government Procurement Model by Hichem Klabi, Sehl Mellouli and Monia Rekik; 2) Government 3.0 in Korea: Fad or Fashion? byTaewoo Nam; and 3) Secure ID Management for Social Security and Tax Number System by Hisao Sakazaki, Dan Yamamoto, Akihiro Sugimoto and Shinji Hirata. The winner in this category was ""A Reputation Based Electronic Government Procurement Model"" by Hichem Klabi, Sehl Mellouli and Monia Rekik. Three papers were also nominated to the Best Research Paper award: 1) Harnessing the Duality of e-Participation Social Software Infrastructure Design by Lukasz Porwol, Adegboyega Ojo and John Breslin; 2) When Food Quality Control in China Meets Mobile and Wireless Technology: Interactions, Potentials and Pitfalls by Shuhua Liu; and 3) Cross-departmental Collaboration in Government One-Stop Center: Factors and Performance by Xinping Liu. The winner in this category was ""Harnessing the Duality of e-Participation Social Software Infrastructure Design"" by Lukasz Porwol, Adegboyega Ojo and John Breslin.Many people and institutions contributed to the organization of ICEGOV2013. We wish to thank the official patron of ICEGOV2013, the Ministry of Security and Public Administration of the Republic of Korea for endorsing and supporting the conference. Our sincere thanks go to the National Information Society Agency, Republic of Korea (NIA) as the local organizer of the conference, particularly to Jeongwon Yoon for his vision and leadership, and to Dohyoon Kim and the whole team in NIA for their hard work and dedication to making the combined ICEGOV2013 and Global e-Government Forum event successful. We wish to express our most sincere thanks to the key sponsors Macao SAR Government and Macao Foundation and the sponsor Electronic Government of the Republic of Kazakhstan whose generous contributions allowed many academics and practitioners from developing countries to attend the conference. Special gratitude is due to Macao SAR Government, its Public Administration and Civil Service Bureau, and Macao Foundation for continuing support to the ICEGOV conference series and the origin of the series e-Macao Program. We also wish to thank ICEGOV2013 partners for their presence, support and in-kind contributions: Brunel University, London, UK; Center for Technology in Government, University at Albany, USA; Data.Gov, U.S. Government; German Cooperation, Deutsche Zusammenarbeit and Deutsche Gesellschaft fur Internationale Zusammenarbeit, Germany; IBM; Information and Communication Technologies, World Bank; Microsoft; Ministry of Information Technology and Communication, Colombia (MINTIC); Organization for Economic Co-operation and Development; Poznan University of Economics, Poland; The Insight Centre for Data Analytics, National University of Ireland, Ireland; The Science and Technology Development Fund, Government of Macao SAR, Macao; UNESCO Information for All Programme; United Nations Asian and Pacific Training Centre for Information and Communication Technology for Development; Vive Digital Programme, MINTIC, Colombia; Waseda University, Japan; and the World Wide Web Consortium. We also wish to express our thanks to ACM Press for publishing the ICEGOV2013 conference proceedings. We are most grateful to the whole Advisory Committee for supporting the conference and to all members of the Program Committee and additional reviewers for their efforts to carry out quality reviews and to help build a strong conference program. We thank keynote speakers; organizers, chairs and moderators of the plenary sessions, town hall debates, paper tracks, thematic sessions, the doctoral colloquium, and the poster session; and all panelists and speakers for their intellectual contributions. Last but not least, we are most thankful to all authors for their efforts in preparing, submitting and presenting papers at ICEGOV2013.We hope that ICEGOV2013 will further contribute to building, growing and connecting global EGOV research, policy and practice communities, able to cross not only national and regional but also institutional and thematic borders, and that the contacts, discussions and ideas initiated in Seoul in October 2013 will continue well after the conference and towards ICEGOV2014 in Guimaraes, Portugal.",,,,2013,,,,Book
Decoding the Translation Initiation Mechanism of Maize Chlorotic Mottle Virus / Decodificación Del Mecanismo de Iniciación En La Traducción Del Virus Moteado CloróTico Del Maíz,"Maize chlorotic mottle virus (MCMV) is the key player of Maize Lethal Necrosis Disease (MLND). MLND is caused by the co-infection and synergistic interaction of MCMV and any potyvirus that infects grasses. Recent outbreaks of MLND have ravaged maize fields in Kenya and neighboring regions of East Africa. The catastrophic economic losses brought back the interest of stakeholders to learn more about the synergistic interaction between MCMV and potyviruses and find ways to use this knowledge to create MLND resistant lines.MCMV is a positive-sense single-stranded RNA virus from the Tombusviridae family. Similarly to other members of the Tombusviridae family, MCMV does not have a 5'-cap or a poly (A) tail and must use alternative mechanisms to translate viral proteins. The lack of a 5'-cap in the viral RNA does not prevent it from interacting with host translation factors. Instead of a 5'-cap, most tombusvirids are known to employ unconventional mechanisms to sequester translation factors to the viral RNA. One mechanism of recruiting host's translation factors is 3'-cap-independent translation elements (3'-CITEs). 3'-CITEs are secondary structures located at the 3'-end of the virus genome used to recruit the translation initiation machinery. The work presented in this dissertation revolves around finding out the translation initiation control elements of MCMV RNA.The unearthing of MCMV's translation initiation process began with a series of deletions at the 3'-end of the virus genome. The genome sequence mapping indicated that there was an essential sequence for virus translation between nucleotides 4164 to 4333. Structural RNA probing of this region showed the presence of a panicum mosaic virus 3-CITE (PTE) -like structure located in the 3'-untranslated region (UTR) of the viral RNA. Similar to other PTEs, the MCMV 3'-CITE secondary structure was composed of a hammer-like helix structure that branched out into two side loops connected by a pyrimidine bridge. On the main stem of the hammer-like structure is a single-stranded bulge that is hyper-modified by SHAPE probing reagents in the presence of magnesium. PTEs are characterized by a pyrimidine rich bridge composed of cytosines and a purine-rich bulge that interacts with each other forming a pseudoknot. In contrary to most PTEs, the MTE was predicted to have a weak pseudoknot.The PTE C-G pseudoknot formation enables the virus to interact with the cap-binding pocket of eIF4E. Although the establishment of a suitable pseudoknot between the C-G domains of MTE is questionable, MTE interacts with initiation factor 4E. Similar to other 3'CITEs, the MTE used long-distance base-pairing to bring the translation machinery to the 5' end. The eIF4E-MTE RNA-protein interaction model was investigated by mutating both the RNA and the protein. Comparison of electrophoretic mobility shift assays (EMSA) results using mutated eIF4E with other PTE-like structure indicated that even though MCMV interact with eIF4E, it might use a mechanism that has yet to be characterized.",,"Carino EJ,Whitham S,Lubberstedt T,Moss W,Yang B",,2020,,,,Ph.D. Thesis
Design and Development of a Semantic Music Discovery Engine,"Technology is changing the way in which music is produced, distributed and consumed. An aspiring musician in West Africa with a basic desktop computer, an inexpensive microphone, and free audio editing software can record and produce reasonably high-quality music. She can post her songs on any number of musically-oriented social networks (e.g., MySpace, Last.fm, eMusic) making them accessible to the public. A music consumer in San Diego can then rapidly download her songs over a high-bandwidth Internet connection and store them on a 160-gigabyte personal MP3 player. As a result, millions of songs are now instantly available to millions of people. This 'Age of Music Proliferation' has created the need for novel music search and discovery technologies that move beyond the ""query-by-artist-name"" or ""browse-by-genre"" paradigms. In this dissertation, we describe the architecture for a semantic music discovery engine. This engine uses information that is both collected from surveys, annotation games and music-related websites, and extracted through the analysis of audio signals and web documents. Together, these five sources of data provide a rich representation that is based on both the audio content and social context of the music. We show how this representation can be used for various music discovery purposes with the Computer Audition Lab (CAL) Music Discovery Engine prototype. This web application provides a music query-by-description interface for music retrieval, recommends music based on acoustic similarity, and generates personalized radio stations. The backbone of the discovery engine is an autotagging system that can both annotate novel audio tracks with semantically meaningful tags (i.e. a short text-based token) and retrieve relevant tracks from a database of unlabeled audio content given a text-based query. We consider the related tasks of content-based audio annotation and retrieval as one supervised multi-class, multi-label problem in which we model the joint probability of acoustic features and tags. For each tag in a vocabulary, we use an annotated corpus of songs to train a Gaussian mixture model (GMM) over an audio feature space. We estimate the parameters of the model using the weighted mixture hierarchies Expectation Maximization algorithm. When compared against standard parameter estimation techniques, this algorithm is more scalable and produces density estimates that result in better end performance. The quality of the music annotations produced by our system is comparable with the performance of humans on the same task. Our query-by-semantic-description system can retrieve appropriate songs for a large number of musically relevant tags. We also show that our audition system is general by learning a model that can annotate and retrieve sound effects. We then present Listen Game , an online, multiplayer music annotation game that measures the semantic relationship between songs and tags. In the normal mode, a player sees a list of semantically related tags (e.g., genres, instruments, emotions, usages) and is asked to pick the best and worst tag to describe a song. In the freestyle mode, a user is asked to suggest a tag that describes the song. Each player receives real-time feedback (e.g., a score) that reflects the amount of agreement amongst all of the players. Using the data collected during a two-week pilot study, we show that we can effectively train our autotagging system. We compare our autotagging system and annotation game with three other approaches to collecting tags for music (conducting a survey, harvesting social tags, and mining web documents). The comparison includes a discussion of both scalability (financial cost, human involvement, and computational resources) and quality (cold start problem, popularity bias, strong vs. weak labeling, tag vocabulary structure and size, and annotation accuracy). Each approach is evaluated using a tag-based music information retrieval task. Using this task, we are able to quantify the effect of popularity bias for each approach by making use of a subset of more popular (short head) songs and a set of less popular (long tail) songs. Lastly, we explore three algorithms for combining semantic information about music from multiple data sources: RankBoost, kernel combination SVM, and a novel algorithm which is called Calibrated Score Averaging (CSA). CSA learns a non-parametric function that maps the output of each data source to a probability and then combines these probabilities. We demonstrate empirically that the combining of multiple sources is superior to any of the individual sources alone, when considering the task of tag-based retrieval. While the three combination algorithms perform equivalently on average, they each show superior performance for some of the tags in our vocabulary.",,Turnbull DR,,2008,,,,Ph.D. Thesis
A Computational Analysis of Human Genetic Variation,"The cost of assaying individuals for SNPs has decreased rapidly over the past few years. This has paved the way for a more statistical and computational analysis of human genetic variations. Fortunately the Linkage Disequilibrium (LD) structure of the genome, whereby neighboring SNPs exhibit varying degrees of correlation, facilitates the analysis. The thesis comprises of three such studies. In the first study we exploit the LD structure to identify a smaller representative subset of SNPs (known as tagging SNPs) which can then be used to predict the remaining tagged SNPs. We propose greedy derandomized variants of recently developed matrix algorithms to address these issues. We evaluate them on genotypic data from 38 populations and four genomic regions (248 SNPs typed for approximately 2000 individuals). We also evaluate these algorithms on a second dataset consisting of genotypes available from the HapMap database (1336 SNPs for four populations) over the same genomic regions. Furthermore, we test these methods in the setting of a real association study, using a publicly available family dataset. Using a small set of carefully selected tSNPs we achieve very good reconstruction accuracy of ""untyped"" genotypes for most of the populations studied. Additionally, we demonstrate in a quantitative manner that the chosen tSNPs exhibit substantial transferability, both within and across different geographic regions. Finally, we show that reconstruction can be applied to retrieve significant SNP associations with disease, with important genotyping savings. The skewed nature of modern genetic datasets, with hundreds of individuals genotyped for millions of SNPs, demands the development of novel algorithms for genome-wide data. In the second study we describe a novel window definition, which divides long genomic datasets into contiguous non-overlapping windows of high linear structure which allows efficient extension of our tSNP selection method to genome-wide datasets. We used the algorithms in conjunction for the analysis of 2.5 million SNPs and four populations from the HapMap database. We show that 10-25% of these SNPs suffice to predict genotypes in the remaining SNPs with more than 95% accuracy. Replicating two real genome wide disease association studies (GWAS) made publicly available by Coriell institute, we demonstrate that carefully selecting less than half of these SNPs, would lead to the same association results. We also study the portability of our selection and prediction across different geographic regions using 1 million SNPs assayed for 1115 individuals from 11 diverse populations in HapMap phase 3 dataset. We compare the efficiency and accuracy of our approach to results obtained using the popular method implemented in Tagger. Recombination rate plays a key role in determining the linkage structure within a region. In the third study, we use change in SNP pattern, among extant haplo-types, as evidence of recombinations. Using biological insight this evidence can be used to infer the recombinational history of DNA segments. This history can be represented as phylogenetic networks consisting of both mutational and recombinational events. We study a mathematical model to merge such networks into a single consensus network. Since the problem is NP-complete, we introduce a polynomial time approximate algorithm which reduces the number of new recombinations introduced in the merger within a factor of of the optimal. Furthermore experimental results, computed using the X-chromosome in HapMap, detect both continental and population specific recombinations. A statistical comparison with mutation based analysis reveals further support of the generally accepted 'Out of Africa' hypothesis, and can be viewed as an indirect validation of our approach.",,Javed A,,2008,,,,Ph.D. Thesis
The Filter Bubble: What the Internet Is Hiding from You,"Author Q&A with Eli Pariser Q: What is a Filter Bubble? A: Were used to thinking of the Internet like an enormous library, with services like Google providing a universal map. But thats no longer really the case. Sites from Google and Facebook to Yahoo News and the New York Times are now increasingly personalized based on your web history, they filter information to show you the stuff they think you want to see. That can be very different from what everyone else sees or from what we need to see. Your filter bubble is this unique, personal universe of information created just for you by this array of personalizing filters. Its invisible and its becoming more and more difficult to escape. Q: I like the idea that websites might show me information relevant to my interestsit can be overwhelming how much information is available I already only watch TV shows and listen to radio programs that are known to have my same political leaning. Whats so bad about this? A: Its true: Weve always selected information sources that accord with our own views. But one of the creepy things about the filter bubble is that were not really doing the selecting. When you turn on Fox News or MSNBC, you have a sense of what their editorial sensibility is: Fox isnt going to show many stories that portray Obama in a good light, and MSNBC isnt going to the ones that portray him badly. Personalized filters are a different story: You dont know who they think you are or on what basis theyre showing you what theyre showing. And as a result, you dont really have any sense of whats getting edited out or, in fact, that things are being edited out at all. Q: How does money fit into this picture? A: The rush to build the filter bubble is absolutely driven by commercial interests. Its becoming clearer and clearer that if you want to have lots of people use your website, you need to provide them with personally relevant information, and if you want to make the most money on ads, you need to provide them with relevant ads. This has triggered a personal information gold rush, in which the major companies Google, Facebook, Microsoft, Yahoo, and the like are competing to create the most comprehensive portrait of each of us to drive personalized products. Theres also a whole behavior market opening up in which every action you take online every mouse click, every form entry can be sold as a commodity. Q: What is the Internet hiding from me? A: As Google engineer Jonathan McPhie explained to me, its different for every person and in fact, even Google doesnt totally know how it plays out on an individual level. At an aggregate level, they can see that people are clicking more. But they cant predict how each individuals information environment is altered. In general, the things that are most likely to get edited out are the things youre least likely to click on. Sometimes, this can be a real service if you never read articles about sports, why should a newspaper put a football story on your front page? But apply the same logic to, say, stories about foreign policy, and a problem starts to emerge. Some things, like homelessness or genocide, arent highly clickable but are highly important. Q: Which companies or Websites are personalizing like this? A: In one form or another, nearly every major website on the Internet is flirting with personalization. But the one that surprises people most is Google. If you and I Google the same thing at the same time, we may get very different results. Google tracks hundreds of signals about each of us what kind of computer were on, what weve searched for in the past, even how long it takes us to decide what to click on and uses it to customize our results. When the result is that our favorite pizza parlor shows up first when we Google pizza, its useful. But when the result is that we only see the information that is aligned with our religious or social or political beliefs, its difficult to maintain perspective. Q: Are any sites being transparent about their personalization? A: Some sites do better than others. Amazon, for example, is often quite transparent about the personalization it does: Were showing you Brave New World because you bought 1984. But its one thing to personalize products and another to personalize whole information flows, like Google and Facebook are doing. And very few users of those services are even marginally aware that this kind of filtering is at work. Q: Does this issue of personalization impact my privacy or jeopardize my identity at all? A: Research psychologists have known for a while that the media you consume shapes your identity. So when the media you consume is also shaped by your identity, you can slip into a weird feedback loop. A lot of people see a simple version of this on Facebook: You idly click on an old classmate, Facebook reads that as a friendship, and pretty soon youre seeing every one of John or Sues posts. Gone awry, personalization can create compulsive media media targeted to appeal to your personal psychological weak spots. You can find yourself eating the equivalent of information junk food instead of having a more balanced information diet. Q: You make it clear that while most Websites user agreements say they wont share our personal information, they also maintain the right to change the rules at any time. Do you foresee sites changing those rules to profit from our online personas? A: They already have. Facebook, for example, is notorious for its bait-and-switch tactics when it comes to privacy. For a long time, what you Liked on Facebook was private, and the site promised to keep it that way. Then, overnight, they made that information public to the world, in order to make it easier for their advertisers to target specific subgroups. Theres an irony in the fact that while Rolex needs to get Tom Cruises permission to put his face on a billboard, it doesnt need to get my permission to advertise my endorsement to my friends on Facebook. We need laws that give people more rights in their personal data. Q: Is there any way to avoid this personalization? What if Im not logged into a site? A: Even if youre not logged into Google, for example, an engineer told me there are 57 signals that the site uses to figure out who you are: whether youre on a Mac or PC or iPad, where youre located when youre Googling, etc. And in the near future, itll be possible to fingerprint unique devices, so that sites can tell which individual computer youre using. Thats why erasing your browser cookies is at best a partial solutionit only partially limits the information available to personalizers. What we really need is for the companies that power the filter bubble to take responsibility for the immense power they now have the power to determine what we see and dont see, what we know and dont know. We need them to make sure we continue to have access to public discourse and a view of the common good. A world based solely on things we Like is a very incomplete world. Im optimistic that they can. Its worth remembering that newspapers werent always informed by a sense of journalistic ethics. They existed for centuries without it. It was only when critics like Walter Lippman began to point out how important they were that the newspapers began to change. And while journalistic ethics arent perfect, because of them we have been better informed over the last century. We need algorithmic ethics to guide us through the next. Q: What are the business leaders at Google and Facebook and Yahoo saying about their responsibilities? A: To be honest, theyre frustratingly coy. They tend to frame the trend in the passive tense: Googles Eric Schmidt recently said It will be very hard for people to watch or consume something that has not in some sense been tailored for them, rather than Google is making it very hard Mark Zuckerberg perfectly summed up the tension in personalization when he said A squirrel dying in your front yard may be more relevant to your interests right now than people dying in Africa. But he refuses to engage with what that means at a societal level especially for the people in Africa. Q: Your background is as a political organizer for the liberal Website MoveOn.org. How does that experience inform your book? A: Ive always believed the Internet could connect us all together and help create a better, more democratic world. Thats what excited me about MoveOn here we were, connecting people directly with each other and with political leaders to create change. But that more democratic society has yet to emerge, and I think its partly because while the Internet is very good at helping groups of people with like interests band together (like MoveOn), its not so hot at introducing people to different people and ideas. Democracy requires discourse and personalization is making that more and more elusive. And that worries me, because we really need the Internet to live up to that connective promise. We need it to help us solve global problems like climate change, terrorism, or natural resource management which by their nature require massive coordination, and great wisdom and ingenuity. These problems cant be solved by a person or two they require whole societies to participate. And that just wont happen if were all isolated in a web of one.",,Pariser E,,2011,,,,Book
Web Programming with ASP and COM with CD-ROM,"From the Book: It was bound to happen sooner or later. We've come to take for granted the Internet and all it has to offer. We can research, shop, entertain ourselves, and communicate with others worldwide without leaving our PC. Your average Web surfer does not give any second thoughts to the magic behind the scenes of these Web sites that enable us to carry out these activities, but you are a Web application developer who provides these experiences for the Web-surfing masses. For many businesses, a Web site is not just an attractive marketing tool, but a mission-critical piece of their revenue stream. Your job is to ensure the best possible user experience for the Web surfer, and, unfortunately, the time-to-market for these important Web applications shrinks with each passing day. So, how are you, the software developer, planning to cope with this trend Fortunately for you, Web application development has taken turns for the better in recent years. Many new tools have become available that make life easier when programming interactive Web applications. A wonderful technology from Microsoft called Active Server Pages shines in this capacity. By using simple scripting languages, coupled with its ability to call on the services of reusable code objects, applications come together with ease. Active Server Pages hides the required low-level details of interactive communications with Web servers, and COM components allow us to take advantage of prepackaged application functionality. Together, Active Server Pages and COM deliver on the promise of rapid application development for the Web. Is This Book for You By now, you may have noticed that I have referred to you, the reader, as a software developer. This book is geared for those software developers who need to deliver first-rate Web applications as quickly as possible. I certainly don't expect you to be an expert in Web application development. In fact, I assume that you have little or no knowledge of how Web applications work. However, I am forced to set a few prerequisites. Since we are working with Web pages, a working knowledge of basic HTML would be very helpful, which means you should be familiar with the common HTML tags as well as forms. You should also be familiar with URLs. And, it's a good idea to be familiar with the Windows operating system fundamentals, such as file operations (moving, copying, etc.) and navigation. We will be constructing COM components that will be built using Microsoft Visual C++. If you have worked with the C++ language for some time and you feel that you have a good understanding of it, then you are ready to tackle the chapters dealing with COM component construction. If not, you should brush up on your C++ skills by supplementing this text with a good C++ manual. The two primary subject areas of this text vary from moderately easy to somewhat difficult. The most difficult areas deal specifically with the COM architecture and its relationship to C++. Many C++ programmers have varying levels of skill, so, throughout the book, I will thoroughly explain concepts that may be unfamiliar to developers who have limited knowledge in programming using C++ in the Windows environment. This is key since the goal of this book is to bring the subject of ASP and COM to a broader audience, not just to those who develop in C++ exclusively for the Windows platform. In addition, exposure to relational database management system fundamentals would be beneficial in the chapters dealing with the ""database-enabling"" of your Web application. If you are comfortable working with tables, records, and key constraints and have basic database administration skills, you should be ready for the database sections in the book. Note: Here are some books that are recommended for bringing you up to speed on the prerequisites to the material in this book: HTML 3: Electronic Publishing on the World Wide Web by Raggett, Lam, and Alexander, Addison-Wesley, 1996. The C++ Programming Language, Third Edition by Stroustrup, Addison-Wesley, 1997. Fundamentals of Database Systems, Third Edition by Elmasri and Navathe, Addison-Wesley, 1999. Focus and Goal of This Book Why is the focus of this book on Active Server Pages and COM components This book grew out of my own frustrations of trying to find a book that emphasized the importance of these technologies coupled together and how to best implement applications using them. I was also trying to locate a book that addressed the issue of writing efficient ASP applications, and I failed at that as well. My struggles to learn how to construct COM components with Visual C++ were worsened by finding only books that focused mostly on theory and that lacked the code examples I needed to start creating my own COM components quickly. This book will fill in the gaps that were left by others. Above all, my ultimate goal is to provide you with a flying start toward developing world-class Web applications easily and quickly. Active Server Pages and COM provide the best framework for this. Web application development, which was once possible only by using UNIX-based systems and CGI programs (more on CGI later), is now within the reach of friendly Windows-based servers. With easy-to-understand scripting languages and reusable code modules, developing Web applications becomes as easy as developing other types of programs. Software and Hardware Tools Used in This Book To develop ASP applications, you will need a 486 or Pentium-class PC. You will need a minimum of 16MB of RAM, although I advise you to upgrade to at least 32MB. You will find that all the development tools required will use quite a bit of memory. If in doubt, it will not hurt to add more RAM or hard-drive space to your computer! Ideally, your computer should have Windows NT Server 4.0 installed. NT Server 4.0 is required to install the latest version of Internet Information Server, version 4.0 (IIS version 5.0 is in beta). This configuration will allow you to take advantage of all the material we are going to cover in this book. I realize that having NT Server 4.0 installed on your computer may not be an option for you. Upgrading your PC with a new operating system, just to follow along with the exercises, may not seem worth it (although this author certainly hopes you think it's worth the trouble!). Windows NT Server 4.0 also carries a heavy price tag, which puts it out of the reach of many home-based PC users or hobbyist programmers. Also, if you have an older PC, its hardware may not be compatible with or powerful enough to run Windows NT. You will need Microsoft's Web server software, Internet Information Server (IIS). This requires Windows NT Server 4.0. However, if NT Server 4.0 is not possible for you to obtain, Microsoft has a solution for users with low-end hardware. Personal Web Server (PWS) 4.0 can run on Windows 9x and Windows NT Workstation. It provides all the features of Internet Information Server, but it is intended for low-volume Web sites and for development. PWS also cannot use many of the integrated security features of IIS, which makes it unsuitable when site security is an issue. Interactive debugging of Active Server Pages is also not possible with PWS. All that considered, it is to your advantage that you use Windows NT Server for your development. You may download IIS/PWS, which is part of the Windows NT 4.0 Option Pack (for both Windows 9x and NT), from Microsoft's NT Server Web site. This book also focuses on COM object development. The component development chapters of this book require Microsoft Visual C++ 5.0 or greater. In this book, examples will be shown using Visual C++ 6.0. If possible, it would be to your advantage to get v6.0 since it incorporates many time-saving features that make development easier. You may run Visual C++ on either Windows NT or Windows 9x. Depending on the installation, Visual C++ can use several hundred megabytes of disk space, so be selective when installing this software to ensure that you have room for all the other component pieces of the Active Server Platform. Optional Software There are some optional software packages that you may find useful to have around while you develop ASP applications. Microsoft Visual Studio includes Visual InterDev 6.0, which features a code editor specifically designed for editing Active Server Pages. Using sophisticated IntelliSense technology, the Visual Studio editor aids the programmer in coding by providing syntax highlighting and automatic statement completion. It is highly recommended, not just for ASP development, but also when building COM components in Visual C++. You can also use any full-featured text editor if you so desire. Let your conscience be your guide. We will be interacting with relational database management systems (RDBMSs) throughout the book. While a complete RDBMS package is not required, it will be helpful since the utilities provided in the RDBMS package will give you quick control over the administrative functions of the database. The examples in this book will use Microsoft Access databases. IIS and PWS will install all of the necessary database software required to talk to Microsoft Access databases. Installing the complete Microsoft Access package will enable you to quickly query data, administer the database, enter new data, run reports, and more. In later chapters, we will discuss how to migrate this data over to Microsoft SQL Server 7.0. Obviously, if you will be doing any of the exercises in that chapter, you will be required to install SQL Server. If you want to use some of the COM components that you create in other development environments, you can install those as well. As mentioned, COM components can be reused in many development environments. Visual Basic, Microsoft Office, and others are possible hosts for your COM components. Special Text Notations Key code lines are referenced in the text with a special symbol. Code lines that wrap are indicated with a soft return mark. Other Resources For your convenience, the enclosed CD-ROM contains all of the code used for our complete Web application, the Megabyte's Pizzeria. It also includes the other code samples scattered throughout the text. You will find installation instructions in Appendix A. Feedback I am always trying to improve the quality of this book. So, please send me your comments, suggestions, rants, raves, and so on. I can be reached at matt_ crouch@hotmail.com. I look forward to hearing from you!",,Crouch MJ,,1999,,,,Book
Malarial Retinopathy and Neurovascular Injury in Paediatric Cerebral Malaria,"Background Diseases of the brain are difficult to study because this organ is relatively inaccessible. Only one part of the central nervous system is available to direct, non-invasive observation – the retina. The concept of the retina as a window to the brain has created much interest in the retina as a source of potential markers of brain disease. Paediatric cerebral malaria is a severe neurological complication of infection with the parasite Plasmodium falciparum, which is responsible for death and disability in a significant number of children in sub-Saharan Africa. As with many neurological diseases, the precise mechanisms by which this infection causes damage to the brain remain unclear, and this hampers efforts to develop effective treatments. It may be that studying the retina in paediatric cerebral malaria could both illuminate pathogenesis specific to this disease, and also provide an illustration of how to approach retinal biomarkers in a new, and potentially more effective way.Methods I approached the aim of developing retinal features as markers of brain disease in paediatric cerebral malaria via several objectives. I made use of an existing clinical study to collect new retinal data from ophthalmoscopic examinations and fundus fluorescein angiograms from patients over three successive malaria seasons in Malawi, and added these to historical data obtained previously at the same site. I devised a new method for grading retinal images. I reviewed the biological plausibility of associations between retina and brain in cerebral malaria, and then considered analytical methods to interpret my retinal data effectively. Finally I estimated associations between retinal features, outcomes, and a radiological measure of brain swelling using combinations of regression models.Results My review of retinal and cerebral histopathology, vascular anatomy and physiology indicated that certain retinal and brain regions may be similarly prone to damage from sequestration as a result of interactions between aberrant rheology and microvascular geometry, such as branching patterns and arteriole to venule ratios. My review of evaluations of analogy and surrogacy suggested that biological similarities between retina and brain could be used to justify statistical evaluation of the amount of information the subject and object of the inference share about a common outcome, as used to assess surrogate end points for clinical trials. This kind of approach is able to address questions about whether a particular retinal feature is effectively equivalent to an analogous disease manifestation in the brain. I report analyses on three overlapping groups of subjects, all of whom had retinopathy positive cerebral malaria: children with admission ophthalmoscopy (n=817), children with admission fluorescein angiography (n=260), and children with admission angiography and MRI of the brain (n=134). Several retinal features are associated with death and longer time to recover consciousness in paediatric cerebral malaria. Broadly speaking, these features appear to reflect two processes: neurovascular sequestration (e.g. orange vessel discolouration and death), and neurovascular leakage (e.g. >5 sites of punctate leak and death). Respective adjusted odds ratios and 95% confidence intervals for these particular associations are: 2.88 (1.64-5.05); and 6.90 (1.52-31.3). Other related processes may also be important, such as ischaemia, which can be extensive. Associations between retina and brain are less clear, in part because of selection bias in the samples.Conclusions Neurovascular leak is important in fatal paediatric cerebral malaria, suggesting that fatal brain swelling may occur primarily as a result of vasogenic oedema. Other processes are also likely to be involved, particularly neurovascular sequestration, which is visible on retinal imaging as orange vessels or intravascular filling defects. Sequestration may plausibly cause leak through direct damage to tight junctions and by increasing transmural pressure secondary to venous congestion. Several types of retinal leakage are seen and some of these may represent re-perfusion rather than acute injury. Future work to investigate temporal changes in retinal signs may find clearer associations with radiological and clinical outcomes. The steps taken to evaluate retinal markers in cerebral malaria illustrate a more rigorous approach to retinal biomarkers in general, which can be applied to other neurological diseases.",,MacCormick IJ,,2016,,,,Ph.D. Thesis
Mobile Network Infrastructure Security in Developing Countries – A Kenya Case Study,"The usage of mobile network infrastructure to access internet resources for organizations is getting higher year by year in sub-Saharan Africa. However, there was an increase in malicious attacks on mobile networks and devices accessing mobile network infrastructure, targeting organizations' private information. Grounded in Bandura's social cognitive theory, the purpose of this multiple case study was to explore strategies security managers used to secure mobile network infrastructures from cyberattacks. Participants comprised four security managers in Kenya in two major cities who successfully implemented strategies to mitigate cyberattacks on the mobile network infrastructures. Data were gathered from video conference, face-to-face, semi-structured interviews, and review of organizations' documents comprised of policies and procedures, internal reports, and training procedures. Thematic analysis was used to unveil four themes: security awareness and training, infrastructure management, defense-in-depth, and security governance framework. A key recommendation would be for security managers to craft employee security training and awareness to protect the organizations' data assets. The implications for positive social change include the potential for security managers to mitigate data breaches and protect sensitive customer data from being exposed.",,"Omanwa JM,Case S,Griffith G",,2021,,,,Ph.D. Thesis
"Using Mobile Phones for Secure, Distributed Document Processing in the Developing World","Although paper plays an essential role in many information ecologies in the developing world, paper-based record keeping can be inefficient and inflexible. The CAM document-processing framework, so called because the phoneýs built-in digital camera plays a key role in the user interface, exploits smart mobile phonesý utility, usability, and growing ubiquity to link paper with modern information tools. The CAM interface consists of CamForm augmented documents, which users interact with; the CamBrowser mobile phone application, which interprets these documents; and the CamShell scripting language, which ties the two together. CAM is a cost effective and accessible way of providing information services to remote rural areas.",,Parikh TS,,2005,74–81,10.1109/MPRV.2005.43,https://doi-org.proxy.bnl.lu/10.1109/MPRV.2005.43;http://dx.doi.org/10.1109/MPRV.2005.43,Journal Article
A Study of Static Analysis Tools to Detect Vulnerabilities of Branchless Banking Applications in Developing Countries,"The ubiquity of smart phones and their prevalence among the underprivileged has enabled the delivery of financial services to previously unbanked through digital means. At the same time it has exposed the same people to security vulnerabilities of digital infrastructure. In this paper, we analyze 10 Android Digital Financial Services (DFS) applications using static analysis tools and present results to show that off-the-shelf static bug checking tools, can be useful in finding many critical security bugs in DFS applications. Our findings also show that DFS applications from developing countries have more vulnerabilities in application specific code compared with DFS applications from developed countries. However, we observe that general purpose static analysis tools have low specificity for DFS specific bugs, such as the vulnerabilities in the use of cryptography and networking, and there is a need to develop better bug detection tools.",,"Ibrar F,Saleem H,Castle S,Malik MZ",,2017,,10.1145/3136560.3136595,https://doi-org.proxy.bnl.lu/10.1145/3136560.3136595;http://dx.doi.org/10.1145/3136560.3136595,Conference Paper
"Exploring the Influence of Security/Privacy, Trialability, Output Quality and Anxiety on the Adoption of Mobile Decision Support Systems among Nurses: A Developing Country Context","Nursing staff need to be highly mobile in executing their routine work. Therefore, they may need to catch, deliver and/or receive critical information, orders or alerts via mobile devices at any point of care to help them take immediate decisions/actions or orders to accomplish their tasks quickly. This paper investigates the factors that affect the adoption of mobile decision support systems among nurses in Jordan. Experience and voluntariness as moderators in the proposed model were also investigated. The model was analysed and tested using WarpPLS 5.0 software. The findings of this study have demonstrated that perceived usefulness, perceived ease-of-use, security/privacy, trialability, output quality, and anxiety are important constructs in predicting and affecting intentional behaviour to adopt decision support systems among nurses in Jordan. The model has explained 65% of the variance in behavioural intention. Theoretical contributions and practical implications are outlined. Limitations and suggestions for future studies are discussed.",,Jaradat MI,,2021,251–281,10.1504/ijmlo.2021.116508,https://doi-org.proxy.bnl.lu/10.1504/ijmlo.2021.116508;http://dx.doi.org/10.1504/ijmlo.2021.116508,Journal Article
Let's Talk Money: Evaluating the Security Challenges of Mobile Money in the Developing World,"Digital money drives modern economies, and the global adoption of mobile phones has enabled a wide range of digital financial services in the developing world. Where there is money, there must be security, yet prior work on mobile money has identified discouraging vulnerabilities in the current ecosystem. We begin by arguing that the situation is not as dire as it may seem---many reported issues can be resolved by security best practices and updated mobile software. To support this argument, we diagnose the problems from two directions: (1) a large-scale analysis of existing financial service products and (2) a series of interviews with 7 developers and designers in Africa and South America. We frame this assessment within a novel, systematic threat model. In our large-scale analysis, we evaluate 197 Android apps and take a deeper look at 71 products to assess specific organizational practices. We conclude that although attack vectors are present in many apps, service providers are generally making intentional, security-conscious decisions. The developer interviews support these findings, as most participants demonstrated technical competency and experience, and all worked within established organizations with regimented code review processes and dedicated security teams.",,"Castle S,Pervaiz F,Weld G,Roesner F,Anderson R",,2016,,10.1145/3001913.3001919,https://doi-org.proxy.bnl.lu/10.1145/3001913.3001919;http://dx.doi.org/10.1145/3001913.3001919,Conference Paper
Evaluating a Mobile Tablet Project in Rural South Africa against Criteria to Comply with Being an Innovative Educational Ecosystem,The purpose of this paper is to evaluate a specific mobile technology rural schools project known as ICT4E in South Africa to determine if it complies to be regarded as an innovative educational ecosystem. The criteria that were used were sourced from the literature. The project was explained and why it is regarded as a rural school mobile tablet project in education. Some high-level results and lessons learnt from this project are also provided that was used to evaluate whether it can be regarded as an instantiation of an innovative educational ecosystem. The evaluation methodology was applied to do the evaluation. The main results were that based on the criteria and the findings from the ICT4E project it could be regarded as an instantiation of an innovative educational ecosystem as it complied to most of the criteria except for two where it only partially complied. The project was also seen as a good example of how a country's system of innovation can be supported from a socio-cultural context-specific perspective.,,"Herselman M,Botha A,Maremi K",,2019,215–220,10.1145/3345120.3355422,https://doi-org.proxy.bnl.lu/10.1145/3345120.3355422;http://dx.doi.org/10.1145/3345120.3355422,Conference Paper
Exploiting Mobile Phone Data for Multi-Category Land Use Classification in Africa,"In the context of Smart Africa Initiative, we present a method to infer multiple land use in Africa. Such information is usually scarce in developing countries due to the constrained resources. Timely land use information is a critical input to smart urban planning that improves efficiency for the public to access to resources. The mobile phone usage is almost universal, which creates a valuable data source for land use inference. In this paper, we demonstrate that the temporal mobile phone call pattern and call network features can be combined to infer ten-category land use including residential, commercial-industrial/office, commercial-business/retail/leisure, high- and low- density commercial, high- and low- density residential, mixed land use areas as well as commercial and residential hubs of the city. In low income countries where land use surveys are rare, our approach create an alternative for measuring land use.",,"Mao H,Thakur G,Bhaduri B",,2016,,10.1145/3007540.3007549,https://doi-org.proxy.bnl.lu/10.1145/3007540.3007549;http://dx.doi.org/10.1145/3007540.3007549,Conference Paper
A High-Frequency Mobile Phone Data Collection Approach for Research in Social-Environmental Systems: Applications in Climate Variability and Food Security in Sub-Saharan Africa,,,"Giroux SA,Kouper I,Estes LD,Schumacher J,Waldman K,Greenshields JT,Dickinson SL,Caylor KK,Evans TP",,2019,57–69,10.1016/j.envsoft.2019.05.011,https://doi-org.proxy.bnl.lu/10.1016/j.envsoft.2019.05.011;http://dx.doi.org/10.1016/j.envsoft.2019.05.011,Journal Article
Mobile Network Infrastructure Security in Developing Countries – A Kenya Case Study,"The usage of mobile network infrastructure to access internet resources for organizations is getting higher year by year in sub-Saharan Africa. However, there was an increase in malicious attacks on mobile networks and devices accessing mobile network infrastructure, targeting organizations' private information. Grounded in Bandura's social cognitive theory, the purpose of this multiple case study was to explore strategies security managers used to secure mobile network infrastructures from cyberattacks. Participants comprised four security managers in Kenya in two major cities who successfully implemented strategies to mitigate cyberattacks on the mobile network infrastructures. Data were gathered from video conference, face-to-face, semi-structured interviews, and review of organizations' documents comprised of policies and procedures, internal reports, and training procedures. Thematic analysis was used to unveil four themes: security awareness and training, infrastructure management, defense-in-depth, and security governance framework. A key recommendation would be for security managers to craft employee security training and awareness to protect the organizations' data assets. The implications for positive social change include the potential for security managers to mitigate data breaches and protect sensitive customer data from being exposed.",,"Omanwa JM,Case S,Griffith G",,2021,,,,Ph.D. Thesis
Applying Intrusion Detection and Response Systems for Securing the Client Data Signals in the Egyptian Optical Network,,,"Rahouma K,Ali A",,2019,538–549,10.1016/j.procs.2019.12.136,https://doi-org.proxy.bnl.lu/10.1016/j.procs.2019.12.136;http://dx.doi.org/10.1016/j.procs.2019.12.136,Journal Article
"Evaluation and Comparison of Satellite-Based Rainfall Products in Burkina Faso, West Africa","The performance of seven operational high-resolution satellite-based rainfall products – Africa Rainfall Estimate Climatology ARC 2.0, Climate Hazards Group InfraRed Precipitation with Stations CHIRPS, Precipitation Estimation from Remotely Sensed Information using Artificial Neural Networks PERSIANN, African Rainfall Estimation RFE 2.0, Tropical Applications of Meteorology using SATellite TAMSAT, African Rainfall Climatology and Time-series TARCAT, and Tropical Rainfall Measuring Mission TRMM daily and monthly estimates – was investigated for Burkina Faso. These were compared to ground data for 2001–2014 on a point-to-pixel basis at daily to annual time steps. Continuous statistics was used to assess their performance in estimating and reproducing rainfall amounts, and categorical statistics to evaluate rain detection capabilities. The north–south gradient of rainfall was captured by all products, which generally detected heavy rainfall events, but showed low correlation for rainfall amounts. At daily scale they performed poorly. As the time step increased, the performance improved. All except TARCAT provided excellent scores for Bias and Nash–Sutcliffe Efficiency coefficients, and overestimated rainfall amounts at the annual scale. RFE performed the best, whereas TARCAT was the weakest. Choice of product depends on the specific application: ARC, RFE, and TARCAT for drought monitoring, and PERSIANN, CHIRPS, and TRMM daily for flood monitoring in Burkina Faso.",,"Dembélé M,Zwart SJ",,2016,3995–4014,10.1080/01431161.2016.1207258,https://doi-org.proxy.bnl.lu/10.1080/01431161.2016.1207258;http://dx.doi.org/10.1080/01431161.2016.1207258,Journal Article
Proposal for a Platform for the Continuity of Distance Learning in African Schools and Universities at the End of the Politico-Military Crisis in the Face of Covid-19: Case of the Central African Republic,"Following the perpetual political-military crises, most of the rural areas of the Central African Republic (CAR) are occupied by armed groups. This leads to human insecurity in these areas. Children, adolescents, and youth are out of school. Primary and secondary school teachers are unable to travel to unsafe areas. Due to the problem of human insecurity in rural areas and especially poverty in several rural areas of CAR, parents are unable to finance the education of their children who have taken the baccalaureate exams to travel to the capital Bangui to study at Bangui University alone. In this article, we propose a platform for the continuity of educational activities in the Central African Republic. Our initially proposed platform solution allows the creation of a distance primary and secondary school in bimodal mode in the rural areas of the CAR. In a second step, it allows the creation of a complete distance university training coupled with traditional education for young people from all rural areas of the Central African Republic. This platform has been tested at the Higher Institute of Technology (Department of Computer Science and Telecommunications) and the Faculty of Science of the University of Bangui and has enabled the partial resumption of pedagogical activities in these institutions. It has been applied in the field of STEM (science, technology, engineering and mathematics) and can be extended to other disciplines. Access to resources is efficient thanks to the coupling of the WireGuard VPN server and the Apache Guacamole server which is a gateway using standard protocols via a browser. It also uses VXLAN technology which moves the WireGuard VPN server subnet from OSI Layer 3 to Layer 2 and allows the organization of practical work that requires being in the same local subnet. Access to this platform provides learners in the Central African Republic with a complete and secure distance learning environment for courses, assignments and tutorials.",,"Mervyl Saint-Juste Kossingou G,Dégboé B,Gladys Gladys Ndassimba N,Ouya S,Mendy G",,2021,,10.1145/3454127.3456603,https://doi-org.proxy.bnl.lu/10.1145/3454127.3456603;http://dx.doi.org/10.1145/3454127.3456603,Conference Paper
Transportation Planning Based on GSM Traces: A Case Study on Ivory Coast,"In this work we present an analysis process that exploits mobile phone transaction trajectory data to infer a transport demand model for the territory under monitoring. In particular, long-term analysis of individual call traces are performed to reconstruct systematic movements, and to infer an origin-destination matrix. We will show a case study on Ivory Coast, with emphasis on its major urbanization Abidjan. The case study includes the exploitation of the inferred mobility demand model in the construction of a transport model that projects the demand onto the transportation network obtained from open data, and thus allows an understanding of current and future infrastructure requirements of the country.",,"Nanni M,Trasarti R,Furletti B,Gabrielli L,Mede P,Bruijn J,Romph E,Bruil G",,2013,15–25,10.1007/978-3-319-04178-0_2,https://doi-org.proxy.bnl.lu/10.1007/978-3-319-04178-0_2;http://dx.doi.org/10.1007/978-3-319-04178-0_2,Conference Paper
Beyond the Baseline: Establishing the Value in Mobile Phone Based Poverty Estimates,"Within the remit of `Data for Development' there have been a number of promising recent works that investigate the use of mobile phone Call Detail Records (CDRs) to estimate the spatial distribution of poverty or socio-economic status. The methods being developed have the potential to offer immense value to organisations and agencies who currently struggle to identify the poorest parts of a country, due to the lack of reliable and up to date survey data in certain parts of the world. However, the results of this research have thus far only been presented in isolation rather than in comparison to any alternative approach or benchmark. Consequently, the true practical value of these methods remains unknown. Here, we seek to allay this shortcoming, by proposing two baseline poverty estimators grounded on concrete usage scenarios: one that exploits correlation with population density only, to be used when no poverty data exists at all; and one that also exploits spatial autocorrelation, to be used when poverty data has been collected for a few regions within a country. We then compare the predictive performance of these baseline models with models that also include features derived from CDRs, so to establish their real added value. We present extensive analysis of the performance of all these models on data acquired for two developing countries -- Senegal and Ivory Coast. Our results reveal that CDR-based models do provide more accurate estimates in most cases; however, the improvement is modest and more significant when estimating (extreme) poverty intensity rates rather than mean wealth.",,"Smith-Clarke C,Capra L",,2016,425–434,10.1145/2872427.2883076,https://doi-org.proxy.bnl.lu/10.1145/2872427.2883076;http://dx.doi.org/10.1145/2872427.2883076,Conference Paper
Econometric Methods for Program Evaluation and Policy Choice,"This dissertation presents two essays on econometric methods for program evaluation and policy choice.In Chapter 1, I develop a statistically optimal way of using data to make policy decisions when the performance of counterfactual policies is only partially identified. Specifically, I consider a class of statistical decision problems in which the policy maker must decide between two alternative policies to maximize social welfare (e.g., the population mean of an outcome) based on a finite sample. The central assumption is that the underlying, possibly infinite-dimensional parameter, lies in a known convex set, potentially leading to partial identification of the welfare effect. An example of such restrictions is the smoothness of counterfactual outcome functions. As the main theoretical result, I obtain a finite-sample decision rule (i.e., a function that maps data to a decision) that is optimal under the minimax regret criterion. This rule is easy to compute, yet achieves optimality among all decision rules; no ad hoc restrictions are imposed on the class of decision rules. I then apply my results to the problem of whether to change a policy eligibility cutoff in a regression discontinuity setup. I illustrate my approach in an empirical application to the Burkinabé Response to Improve Girls' Chances to Succeed program, a school construction program in Burkina Faso, where villages were selected to receive schools based on scores computed from their characteristics. Under reasonable restrictions on the smoothness of the counterfactual outcome function, the optimal decision rule implies that it is not cost-effective to expand the program. I empirically compare the performance of the optimal decision rule with alternative decision rules.In Chapter 2, joint with Yusuke Narita, we show how to use data obtained from algorithmic decision making for impact evaluation. Machine learning and other algorithms produce a growing portion of decisions and recommendations both in policy and in business. This chapter first highlights a valuable aspect of such algorithmic decisions. That is, algorithmic decisions are natural experiments (conditionally quasi-randomly assigned instruments) since the algorithms make decisions based only on observable input variables. We then use this observation to develop a treatment-effect estimator for a class of stochastic and deterministic decision-making algorithms. Our estimator is shown to be consistent and asymptotically normal for well-defined causal effects. A key special case of our estimator is a multidimensional regression discontinuity design. We apply our estimator to evaluate the effect of the Coronavirus Aid, Relief, and Economic Security (CARES) Act, where hundreds of billions of dollars worth of relief funding were allocated to hospitals via an algorithmic rule. Our estimates suggest that the relief funding has little effect on COVID-19-related hospital activity levels. Naive OLS and IV estimates exhibit substantial selection bias.",,"Yata K,B. Armstrong T,Yusuke Narita",,2022,,,,Ph.D. Thesis
A Structured Demonstration of Five Program Comprehension Tools: Lessons Learnt,"The purpose of this panel is to report on a structured demonstration for comparing program comprehension tools. Five teams of program comprehension tool designers applied their tools to a set of maintenance tasks on a common subject system. By applying a variety of reverse engineering techniques to a predefined set of tasks, the tools can be compared using a common playing field. A secondary topic of discussion will address the development of guinea pig systems and how to use them in a structured demonstration for evaluating software tools.",,"Sim SE,Storey MA,Winter A",,2000,210,,,Conference Paper
Secure Mobile Code Execution Service,"Mobile code refers to programs that come into a host computer over the network and start to execute with or without a user's knowledge or consent. Because these programs run in the execution context of the user that downloads them, they can issue any system calls that the user is allowed to make, and thus pose a serious security threat when they are malicious. Although many solutions have been proposed to solve the malicious mobile code problem, none of them are truly effective at striking a good balance between defeating zero-day attacks and minimizing disruption to the execution of legitimate applications.This paper describes a commercial system called SEES that secures the execution of mobile code that comes into a host computer as an email attachment or as a web document downloaded through an anchor link by running them on a separate guinea pig machine rather than on the user machine. Effectively, it takes an isolation approach to the secure mobile code execution problem. As a result, SEES guarantees that no malicious email attachments or web documents that act on behalf of the user that downloads them, can damage the resources of the user machine, or can leak any confidential information. In particular, even zero-day virus cannot cause any harms. We present the design, implementation and evaluation of SEES on the Windows platform, and contrast it with other existing approaches to the same problem.",,"Lam LC,Yu Y,Chiueh TC",,2006,5,,,Conference Paper
Cisco Nac Appliance: Enforcing Host Security with Clean Access,"Cisco NAC ApplianceEnforcing Host Security with Clean AccessAuthenticate, inspect, remediate, and authorize end-point devices using Cisco NAC ApplianceJamey Heary, CCIE® No. 7680Contributing authors: Jerry Lin, CCIE No. 6469,Chad Sullivan, CCIE No. 6493, and Alok AgrawalWith today's security challenges and threats growing more sophisticated, perimeter defense alone is no longer sufficient. Few organizations are closed entities with well-defined security perimeters, which has led to the creation of perimeterless networks with ubiquitous access. Organizations need to have internal security systems that are more comprehensive, pervasive, and tightly integrated than in the past.Cisco® Network Admission Control (NAC) Appliance, formerly known as Cisco Clean Access, provides a powerful host security policy inspection, enforcement, and remediation solution that is designed to meet these new challenges. Cisco NAC Appliance allows you to enforce host security policies on all hosts (managed and unmanaged) as they enter the interior of the network, regardless of their access method, ownership, device type, application set, or operating system. Cisco NAC Appliance provides proactive protection at the network entry point.Cisco NAC Appliance provides you with all the information needed to understand, design, configure, deploy, and troubleshoot the Cisco NAC Appliance solution. You will learn about all aspects of the NAC Appliance solution including configuration and best practices for design, implementation, troubleshooting, and creating a host security policy.Jamey Heary, CCIE® No. 7680, is a security consulting systems engineer at Cisco, where he works with its largest customers in the northwest United States. Jamey joined Cisco in 2000 and currently leads its Western Security Asset team and is a field advisor for its U.S. Security Virtual team. His areas of expertise include network and host security design and implementation, security regulatory compliance, and routing and switching. His other certifications include CISSP, CCSP®, and Microsoft MCSE. He is also a Certified HIPAA Security Professional. He has been working in the IT field for 13 years and in IT security for 9 years. Understand why network attacks and intellectual property losses can originate from internal network hosts Examine different NAC Appliance design options Build host security policies and assign the appropriate network access privileges for various user roles Streamline the enforcement of existing security policies with the concrete measures NAC Appliance can provide Set up and configure the NAC Appliance solution Learn best practices for the deployment of NAC Appliance Monitor, maintain, and troubleshoot the Cisco NAC Appliance solutionThis security book is part of the Cisco Press® Networking Technology Series. Security titles from Cisco Press help networking professionals secure critical data and resources, prevent and mitigate network attacks, and build end-to-end self-defending networks.Category: Cisco Pressï SecurityCovers: End-Point Security",,"Heary J,Lin J,Sullivan C,Agrawal A",,2007,,,,Book
Mobile Network Infrastructure Security in Developing Countries – A Kenya Case Study,"The usage of mobile network infrastructure to access internet resources for organizations is getting higher year by year in sub-Saharan Africa. However, there was an increase in malicious attacks on mobile networks and devices accessing mobile network infrastructure, targeting organizations' private information. Grounded in Bandura's social cognitive theory, the purpose of this multiple case study was to explore strategies security managers used to secure mobile network infrastructures from cyberattacks. Participants comprised four security managers in Kenya in two major cities who successfully implemented strategies to mitigate cyberattacks on the mobile network infrastructures. Data were gathered from video conference, face-to-face, semi-structured interviews, and review of organizations' documents comprised of policies and procedures, internal reports, and training procedures. Thematic analysis was used to unveil four themes: security awareness and training, infrastructure management, defense-in-depth, and security governance framework. A key recommendation would be for security managers to craft employee security training and awareness to protect the organizations' data assets. The implications for positive social change include the potential for security managers to mitigate data breaches and protect sensitive customer data from being exposed.",,"Omanwa JM,Case S,Griffith G",,2021,,,,Ph.D. Thesis
"Survey on DNS Configurations, Interdependencies, Resilience and Security for *.Ke Domains","Statistics and research work show that the Legacy DNS as used today is slow, vulnerable to denial of service attacks, and does not support fast updates. To further compound this problem, configuring the DNS is complex and most of its implementations in use on many web servers are insecure. Consequently, Internet resources hosted on such servers have been subject to attacks of every kind. The *.ke domains have had a good share of such attacks, for example, 103 Government of Kenya's websites (.go.ke) were recently (January 2012) hacked in one night. In this paper, we present results of a survey for the *.ke domains whose main objective was to establish whether the DNS configurations for the *.ke domains met minimum setup configurations for security, resilience and interdependencies. Our focus on the three aspects was informed by the fact that these aspects are responsible for most DNS implementation shortcomings and by extension, responsible for most of the vulnerabilities and consequent attacks. To achieve this objective, 2,000 *.ke domains were collected through newspapers and magazines, posters and billboards, Internet, email directories and the main *.ke domain registrant KENIC. Dig and NSLOOKUP utilities were then used to drill down their configuration aspects such as primary and DNS servers, DNS application running on them, the dependencies among the DNS server, geographical location, MX records and web servers.The results indicated a very low compliance to the standard DNS configuration requirements making *.ke domains non-resilient to failure, vulnerable (over 60%) and overly insecure. Other findings were that 40% of the domains were hosted by 2 name servers and a further 46% of the domains interrogated were hosted a paltry 8 name servers. Of the 768 servers queried for their DNS applications 574 responded with the DNS application type and version; displaying such private information predisposes the server to attacks. it was also found out that on average, a *.ke domain DNS server depends on an average of 234 DNS servers and that some domains had only one DNS server.The study revealed major gaps in the way the DNS servers for *.ke domains are configured and questioned the capacity of those tasked with configuring these servers. Crypto graphical solutions like IPSEC and NSIG were recommended to secure the DNS servers. Awareness campaigns and capacity building on importance of DNS and security issues surrounding it on the technicians tasked with configuring the servers was also recommended. These findings were then used to inform the development of a web-based step-by-step DNS Configuration Tool. The latter is an online highly technical guide that the administrators can use to check if their DNS server(s) are properly set up to take care of configurations, resilience and interdependencies issues that may render the domain insecure and unavailable.",,"Kagwe JG,Masinde M",,2012,,10.1145/2160601.2160632,https://doi-org.proxy.bnl.lu/10.1145/2160601.2160632;http://dx.doi.org/10.1145/2160601.2160632,Conference Paper
Investigating the Use of M-Health for Learning and Clinical Training by Medical Students in Ghana,"There is a challenge with healthcare access in most developing countries. With the high rate of mobile technology penetration in these countries, there is a strong belief that mobile technology can help address this and other health system and education challenges. This study investigated how clinical year medical students in Ghana used m-health and with what outcomes. This was a mixed-methods study to assess what technologies students used, what the impact of use was, what enablers and barriers they encountered, what factors explained m-health adoption and what the attitudes of students, staff and faculty members were towards m-health use. The study was conducted in four out of five medical schools in Ghana with clinical year students, namely, Kwame Nkrumah University of Science and Technology School of Medical Sciences (KNUST-SMS), University of Cape Coast School of Medical Sciences (UCC-SMS), University of Development Studies School of Medicine and Health Sciences (UDS-SMHS) and University of Ghana School of Medicine and Dentistry (UG-SMD). Online and paper questionnaires were distributed to 828 students and 291 questionnaires were returned. Questionnaires from dental students at UG-SMD (n = 5) were excluded from the analysis.Two focus group discussions were held involving seven students while three students, seven faculty members and five staff were interviewed. Qualitative data were analyzed using thematic analysis. Only one student did not own a mobile device. About 78% of students reported using m-health at some point during their medical education. The most popular devices used by students were laptop computers (90.8%), smartphones (66.2%), cellular phones (46.6%) and tablets (44.1%). Over 84% of the students owned Android devices, while 21% owned iPhones and iPads. Majority of students owned three devices or less. Students used mobile technologies in ways that suited their learning needs and contexts. M-health helped students to participate better in lessons and improve their knowledge, skills and efficiency in various contexts. The main drawbacks of m-health use were distraction and time wasting, difficulty in determining credibility of some online information and the risk of using these technologies inappropriately around patients and during assessments. The main facilitating conditions for m-health use were availability, quality and reliability of technological services, technical support, security, price value, technology competence and training, portability, task and goal fit, social influence and organizational factors. Habit and Hedonic Motivation were the only significant factors that explained intention to use m-health and actual m-health use respectively in the UTAUT2 model, in the presence of age, gender and experience. Students, staff and faculty members were open to using m-health in teaching and learning, although they recommended regulation of use through policies and guidelines to ensure effective teaching and learning and ethical m-health use. Considering the benefits offered by m-health, the study encourages medical schools in Ghana to explore mobile learning with the possibility of incorporating it into their curricula. This should be accompanied by development of policies and guidelines to spell out how mobile technologies should be used in order to mitigate most of the drawbacks identified. This study contributed empirical evidence from the Ghanaian context regarding m-health adoption and use in medical education. This evidence will contribute to theory regarding benefits, drawbacks, facilitating conditions and factors that influence m-health adoption among medical students in a developing country context.",,Sulley AM,,2018,,,,Ph.D. Thesis
ThinSIM-Based Attacks on Mobile Money Systems,"Phone-based mobile money is becoming the dominant paradigm for financial services in the developing world. For example, mPesa has a cash flow of over thirty billion USD, equivalent to nearly half of Kenya's GDP. Inside of these markets, competitors have appeared who leverage ThinSIMS, small SIM-card add-ons, to provide alternative mobile money implementations. However, the security implications of ThinSIMs are not well understood.To resolve this, we explore the security of phone-based mobile money systems against attacks via the SIM interface, the 3GPP-defined interface between a SIM card and a phone. Using a ThinSIM to intercept and initiate communication over the SIM interface, we demonstrate that a malicious ThinSIM can steal a user's mPesa credentials and initiate transactions without the user's consent or knowledge. We also demonstrate a similar ThinSIM-based attack against USSD-based mobile money systems that allows for similar transactions without the user's knowledge or participation. Lastly, we propose and implement modifications to both STK and USSD-based mobile money systems to limit the impact of our discovered ThinSIM-based attacks.",,"Phipps R,Mare S,Ney P,Webster J,Heimerl K",,2018,,10.1145/3209811.3209817,https://doi-org.proxy.bnl.lu/10.1145/3209811.3209817;http://dx.doi.org/10.1145/3209811.3209817,Conference Paper
SCC '15: Proceedings of the 3rd International Workshop on Security in Cloud Computing,"It is our great pleasure to welcome you to the 2015 International Workshop on Security in Cloud Computing (SCC).Cloud computing has emerged as today's most exciting computing paradigm shift in information technology, since it promises numerous benefits, including lower costs, rapid scaling, easier maintenance, and ubiquitous availability. Meanwhile, cloud computing also raises many unique security and privacy challenges, especially when data and computation are outsourced to infrastructures which can be shared by multiple tenants, which requires expertise from various domains including access control, accountability, applied cryptography, architecture, auditing, availability, communications, dependability, hardware security, privacy-enhancing technologies, regulatory compliance, storage systems, trusted computing, to name a few. This workshop aims to bring together the research efforts from both the academia and industry in all security aspects related to cloud computing.This is the third year for our SCC workshop and we have witnessed steady growth. We received 25 submissions from fourteen countries covering four continents, including Australia, China, Denmark, Egypt, Finland, France, Germany, India, Indonesia, Israel, Italy, Japan, Russian Federation, and United States. The submissions were reviewed by a Technical Program Committee of 50 experts from eighteen countries or regions (again covering four continents, including Australia, Austria, Canada, China, France, Germany, Greece, Hong Kong, Israel, Italy, Japan, South Korea, Luxembourg, Singapore, Spain, Switzerland, United Kingdom, and United States). Most submissions received at least 4 review reports. The final program contains 8 papers, representing an acceptance rate of 32%. We thank the Program Committee members as well as 19 external reviewers for their volunteer work, EasyChair for providing a user-friendly system for our review process, and all the people who contributed to the success of SCC.This year's SCC workshop also features ""Short Talks on Emerging Areas in Cloud Security"", by Dr. Sherman S.M. Chow from the Chinese University of Hong Kong (CUHK), Hong Kong, on ""Processing Encrypted Data"", and Dr. Cong Wang from City University of Hong Kong (CityU), Hong Kong, on ""Some Challenges on Enabling Encrypted Cloud Media Centre"". This session is organized by Dr. Kui Ren from the State University of New York at Buffalo, USA, who will also give us the keynote on ""Secure Outsourcing Image Feature Extraction: Challenges and Solutions"". We hope that you will find this program interesting and thought-provoking.",,,,2015,,,,Book
"Decrease the Number of Patients Lost to Follow-up in the Monitoring of PLHIV in Cross-Border Areas between The Gambia, Senegal and Guinea Bissau","The fight against AIDS in West Africa is a big challenge of public health. The main difficulties are related to the availability of antiretrovirals (AVR) and mainly to the observance of the treatment. In the cross-borderland areas of the Gambia, Senegal and Guinea Bissau, these difficulties of the observance of the treatment are amplified by the vulnerability, the high mobility of the populations and the problems of communication between the actors of the monitoring of PLHIV who speak three different languages. That's what makes difficult the fight against AIDS in those areas and justify the phenomenon of the patients lost to follow-up, which is a great indicator for the following of the People Living with HIV-virus (PLHIV). This paper presents the decrease of the number of patients lost to follow-up in the monitoring of the PLHIV in the cross-borderland areas of the Gambia, Senegal and Guinea Bissau by a multilingual semantic web platform of reference, counter reference and auto reference.",,"Diop I,Dieng Y,Faye Y,Malack CA,Cisse O,Diouf B",,2019,,10.1145/3361570.3361596,https://doi-org.proxy.bnl.lu/10.1145/3361570.3361596;http://dx.doi.org/10.1145/3361570.3361596,Conference Paper
Recent Advances in Chaotic Systems and Synchronization: From Theory to Real World Applications,"Recent Advances in Chaotic Systems and Synchronization: From Theory to Real World Applications is a major reference for scientists and engineers interested in applying new computational and mathematical tools for solving complex problems related to modeling, analyzing and synchronizing chaotic systems. Furthermore, it offers an array of new, real-world applications in the field. Written by eminent scientists in the field of control theory and nonlinear systems from 19 countries (Cameroon, China, Ethiopia, France, Greece, India, Italia, Iran, Japan, Mexico, and more), this book covers the latest advances in chaos theory, along with the efficiency of novel synchronization approaches. Readers will find the fundamentals and algorithms related to the analysis and synchronization of chaotic systems, along with key applications, including electronic design, text and image encryption, and robot control and tracking. Explores and evaluates the latest real-world applications of chaos across various engineering and biomedical engineering fields Investigates advances in chaos synchronization techniques, including the continuous sliding-mode control approach, hybrid synchronization between chaotic and hyperchaotic systems, and neural network synchronization Presents recent advances in chaotic systems through an overview of new systems and new proprieties",,"Boubaker O,Jafari S",,2018,,,,Book
MM&Sec '05: Proceedings of the 7th Workshop on Multimedia and Security,"It is our great pleasure to welcome you to the 7th ACM Multimedia and Security Workshop - ACM-MM-Sec'05. This year's workshop continues its tradition of being the premier forum for presentation of research results concerning a wide variety of problems in multimedia security, digital rights management, secure media distribution, digital watermarking, embedded biometrics, digital forensic, and steganography. The mission of the workshop is to bring together researchers from academia, industry, and government to discuss current pressing issues in multimedia security, identify new, emerging, technologies, innovative applications, and new theoretical frameworks. This ACM workshop gives researchers and practitioners a unique opportunity to share their perspectives with others interested in the various aspects of multimedia security.The call for papers attracted 33 submissions from Asia, North America, Europe, and Australia. The program committee accepted 18 papers that cover a variety of topics, including new steganalytic techniques and security models, innovative methods for media fingerprinting, watermarking, and authentication, and new cryptographic elements suitable for content authentication and protection, such as robust hashes. In addition, the program includes three invited talks by representatives from academia, government, and industry: Hany Farid's talk on on detection of digital forgeries from inconsistencies in lighting, a speech by Chad Heitzenreiter on importance of efficient solutions to multimedia security for defense agencies, and an invited talk by Neerja Raman from HP Labs on the industry perspective of multimedia security. The workshop also includes a rump session, where researchers are encouraged to present work in progress, demo applications, or initiate group discussions. We hope that these proceedings will serve as a valuable reference for multimedia security researchers and developers.",,,,2005,,,,Book
A Voice in the Crowd: Broader Implications for Crowdsourcing Translation during Crisis,"Both international non-governmental organizations and government actors have embraced the technological union of humans and software, known as crowdsourcing, to manage the flood of information produced during recent crises. However, unlike a business solution, the task of translation is unique during a crisis situation; the costs are human, and the impact is social and political. This paper follows four crises in which different crowdsourcing applications were developed by a range of actors. In each instance, the design approach failed to incorporate the unique circumstances of the conflict context, resulting in a translation application that removed authorship, dissolved intentionality, and shed contextual markers from original sources. This flawed application prevented the original contributors from interacting with the information directly related to their own life-threatening situation, and the information it amassed formed an unsound basis for decision-making by international actors. The associated consequences during: post-earthquake Haiti 2010, Libya and Egypt 2011 and Somalia 2011/12 are intended to provoke process improvement among all stakeholders.",,Sutherlin G,,2013,397–409,10.1177/0165551512471593,https://doi-org.proxy.bnl.lu/10.1177/0165551512471593;http://dx.doi.org/10.1177/0165551512471593,Journal Article
GPRS Security as a QoS in the Telecommunication Industry Case of Vodafone Egypt,"The changes taking place in the world today are largely due to the developments and evolution in a number of industries; one of which is information and communication technology. Focusing on communications solutions with an emphasis on business applications, it is obvious to claim that business applications that are being deployed with 2.5G wireless networks fall into two general categories: (a) horizontal applications ''mobile office'' such as electronic mailing, voice communications, Internet access, short messaging and personal information management (PIM) tools and (b) vertical applications ''sales force automation''(SFA) and field force automation (FFA), fleet management, government communications and public safety, telemetry and remote monitoring, point-of-sale as well as financial services. The new generation of wireless devices being introduced to the market for 2.5G global system for mobile communications (GSM) and general packet radio service (GPRS) services is designed to support these applications, with features ranging from small standard keyboards to high resolutions rich colored screens. The impressive growth of cellular mobile telephony as well as the number of Internet users promises an exciting potential for a market that combines both innovations. The general packet radio service (GPRS) is a new non-voice value-added service that allows information to be sent and received across a mobile telephone network. Users of GPRS benefit from shorter access time and higher data rates. It is important to note that the standard GPRS network itself does not offer a reasonably secure solution for providing mobile access to a corporate local area network (LAN). Although, the air interface ciphering and the GPRS authentication process are secured, the IP traffic is unencrypted all the way from the serving GPRS support node (SGSN) to the corporate LAN gateway. The most feasible solution for secure remote connections would be to use an end-to-end virtual private network (VPN) solution from the mobile station (MS) to the corporate LAN gateway where the traffic is encrypted for the whole connection and the user can slip to the Internet from the nearest access point. It is important to separate the user traffic from the control traffic to guarantee high level of security with a minor impact on quality of service (QoS) which means providing consistent and predictable data delivery service that can lead to customer application requirement satisfaction. However, to achieve that security has to be looked at as the key player in the new GSM data networks when deploying GPRS. This paper demonstrates the case of Vodafone Egypt, one of the mobile operators, in deploying GPRS networks while focusing on exploring the business opportunities and motivating factors to implement GPRS. Moreover, the paper proposes solutions on how to create secure connections over GPRS networks while proposing a security policy for Vodafone Egypt.",,"Kamel S,Wahba K",,2004,5–27,10.1016/j.ijinfomgt.2003.12.004,https://doi-org.proxy.bnl.lu/10.1016/j.ijinfomgt.2003.12.004;http://dx.doi.org/10.1016/j.ijinfomgt.2003.12.004,Journal Article
"""If God Gives Me the Chance i Will Design My Own Phone"": Exploring Mobile Phone Repair and Postcolonial Approaches to Design in Rural Kenya","This article focuses on ""fundi wa simu,"" (mobile phone repairers) in rural Kenya and their ideas about mobile phone design. Our study design and analysis were guided by ideas from postcolonial computing; we use our qualitative findings, and outcomes from a drawing exercise, to show existing flaws in mobile phone design, and to explore how repairers' knowledge can lead to handsets that are better suited for rural Kenyans. Our argument is that, by engaging with repairers ""[on] their own terms,"" technologists can expand conversations around designing for the 'developing' world that go beyond building novel smartphone applications. In fact, such conversations can also include reimagining mobile phones, and supporting local repairers' efforts to manufacture them. We conclude by discussing ways to improve upon postcolonial approaches to technology design.",,"Wyche S,Dillahunt TR,Simiyu N,Alaka S",,2015,463–473,10.1145/2750858.2804249,https://doi-org.proxy.bnl.lu/10.1145/2750858.2804249;http://dx.doi.org/10.1145/2750858.2804249,Conference Paper
A Study on the Determinants of Ethiopian Minibus Taxi Drivers’ Speeding Behaviour: An Application of the ‘Major Theorists’ Model,,,"Mamo WG,Ross V,Alhajyaseen WK,Reinolsmann N,Brijs K",,2022,189–196,10.1016/j.procs.2022.03.027,https://doi-org.proxy.bnl.lu/10.1016/j.procs.2022.03.027;http://dx.doi.org/10.1016/j.procs.2022.03.027,Journal Article
Exploring Methods Cybersecurity Managers Need to Implement to Minimize Cyber-Frauds in Mobile Money Services in Ghana,"Nearly half the adult population in developing nations lacks a formal bank account and other financial services. Ghana is no exception, having a massive community of unbanked adults and among those countries positioned at the bottom of the spectrum of financial inclusion. The advent of mobile financial services (MFS) claims an alternative mode to financial inclusion. MFS, fundamentally implemented using SMS and USSD code, essentially encompasses mobile wallets, cash out, and over-the-counter transactions overgrowing globally, has a vast potential to minimize impediments to financial inclusion. However, mobile financial services' sustainability is seen to be threatened by cybercriminals or fraudsters while reaching a global penetration rate of 91% in 2020, and this figure keeps growing 3% yearly. The purpose of this qualitative exploratory study was to explore security methods cybersecurity managers need to implement to minimize cyber fraud of mobile financial services (MFS) in Ghana. The research purports to identify viable methods leaders of MFS operators need to reduce fraudsters' threats. Exploratory design was used as the lens to explore this phenomenon in-depth. A sample size of seven and 12 semi-structured interview questions were used as a data collection instrument. Lack of proper security methods and internal control processes were identified as the major causes of cyber fraud in MFS. Seven databases (Web of Science, ProQuest, ABI, EBSCO, IEEE, Sage, Google Scholar, Pub/Med, and Scopus) were searched using standard and adapted search syntax. The study also provided recommendations on the ecosystem the processes needed to adopt in Ghana to utilize mobile technology's full potential.",,"Siaw Afriyie B,Kelly Hughes,Abdullah Alshboul",,2022,,,,Ph.D. Thesis
Security Strategies Information Technology Security Mangers Use in Deploying Blockchain Applications,"Blockchain is seen as a potential game-changer in many industries and a transformational technology in the 21st century. However, security concerns have made blockchain technology adoption relatively slow. Massive security breaches in cryptocurrency, an example of blockchain technology, have caused organizations to lose $11.3 billion in illegal transactions, exacerbating these security concerns for information technology (IT) security managers who are worried about the safety of blockchain. Grounded in the routine activity theory, the purpose of this multiple case study was to explore strategies used by IT security managers to deploy blockchain applications securely. The participants were 4 IT security managers from companies in Ghana, the United States, and Europe with experience in implementing blockchain applications securely. Data collection was done using semistructured interviews and a review of organizational documents for triangulation. A thematic analysis produced three themes: (a) cryptographic key management, (b) comprehensive software auditing, and (c) traditional IT security controls. A critical recommendation is for security managers to implement the National Institute of Technology (NIST) key management and cybersecurity frameworks. The implications for positive social change include the potential to alter people's negative perceptions of blockchain security and giving security assurance to individuals and organizations on their digital assets stored in a blockchain system. In addition, a secured blockchain system could improve people's confidence in blockchain applications for an increased adoption rate of this useful technology development.",,"Nkrumah P,Jon McKeeby",,2021,,,,Ph.D. Thesis
CSTST '08: Proceedings of the 5th International Conference on Soft Computing as Transdisciplinary Science and Technology,"Soft Computing (SC) has an evolving collection of methodologies, which is aimed to exploit tolerance for imprecision uncertainty, and partial truth to achieve robustness, tractability, and low cost. SC provides attractive opportunity to represent the ambiguity in human thinking with real life uncertainty. Fuzzy logic (FL), Neural Networks (NN), and Evolutionary Computation (EC) were the core methodologies of soft computing. Later chaos computing, fractal theory, wavelet transformation, cellular automaton, percolation models, and immune network theory were added to enhance soft computing. However, they should not be viewed as competing with each other, but synergistic and complementary, instead. SC was actually the combination or fusion of each methodology which yielded new computational capabilities (hybrid systems). Soft computing is currently causing a paradigm shift (breakthrough) in science and technology.The stage for the Fifth IEEE/ACM International Conference on Soft Computing as Transdisciplinary Science and Technology (CSTST'08) has been set. This edition is dedicated to commemorate the memory of Professor Yasuhiko Dote, Founding Chair of WSTST series of meetings. In essence, CSTST'08 is built on the success of the previous four events held in Muroran, Japan namely the IEEE International Workshop on Neuro Fuzzy Control, in 1993; IEEE International Workshop on Soft Computing in Industry, in 1996, the IEEE International Workshop on Soft Computing in Industry, in 1999 and International Workshop on Soft Computing as Transdisciplinary Science and Technology (WSTST'2005). CSTST'08 is hosted by University of Cergy Pontoise, France and is technically co-sponsored by IEEE Systems Man and Cybernetics Society, ACM SIGAPP (French Chapter), IEEE French Section, World Federation on Soft Computing, European Society for Fuzzy Logic, Technology and International Fuzzy Systems Association, and AFIHM - French Association of Human Computer Interaction. On behalf of the CSTST'08 program committee, we wish to extend a very warm welcome to this edition in Cergy-Pontoise/Paris, France. The conference program committee has organized an exciting and invigorating program comprising presentations from distinguished experts in the field, and important and wide-ranging contributions on state-of-the-art research that provide new insights into current cutting edge results on ""Soft Computing as Transdisciplinary Science and Technology"".This year, we received over 212 regular submissions and we are really gratified by the international diversity of this conference: authors of submitted work hail from no less than 30 countries including Vietnam, Egypt, Bulgaria, Turkey, Russia, Netherlands, Austria, Malaysia, Sweden, Croatia, Kuwait, Cyprus, Belgium, Estonia, Latvia, Lebanon, Macedonia, Singapore, Argentina, United Arab Emirates, Thailand, Ukraine, Hungary, Ireland, Czech, Republic, Spain, Norway, Taiwan, Canada, Libya, Romania, Mexico, Greece, Brazil, Pakistan, Germany, Australia, Tunisia, India, United States of America, Italy, Korea, Poland, Algeria, Japan, United Kingdom, Iran, China, Portugal, and France. The technical program of CSTST'08 conference comprises of 62 papers. The conference program committee had a very challenging task of choosing high quality submissions. Each paper was peer reviewed by at least three or more independent referees of the program committee and the papers were selected based on the referee recommendations. The papers offer stimulating insights into emerging intelligent technologies and their applications in Internet security, chance discovery, humanized computational intelligence, web intelligence, data mining, image processing, swarm intelligence, optimization and so on.",,,,2008,,,,Book
Integrating Value Modeling and Legal Risk Management: An IT Case Study,"Companies need to be able to demonstrate compliance with rules and regulations, especially start-ups who typically do not have the legal expertise to identify, assess and address legal risks of initial business ideas, nor do they have the resources to hire such expertise. Tools could help them identify and deal with legal risk at an early stage. Existing research in BPM focuses on compliance verification of a consolidated business model by checking the ability of a company to comply with the standards. The challenge is to apply a ‘continuous improvement’ by steering the business on values. Moreover, legal choices typically sit at the strategic level, and not only at the operational level. In this paper, we therefore propose an approach to handle legal risks as part of business model development. The approach makes use of Continuous Business Model Planning method, a value-driven modeling approach for strategic planning, and legal argumentation. The suitability and potential usefulness of the approach is illustrated by a study of the Kenyan court case Lipisha & BitPesa vs. Safaricom.",,"Muthuri R,Capecchi S,Sulis E,Amantea IA,Boella G",,2022,27–55,10.1007/s10257-021-00543-2,https://doi-org.proxy.bnl.lu/10.1007/s10257-021-00543-2;http://dx.doi.org/10.1007/s10257-021-00543-2,Journal Article
Safe Mathare: A Mobile System for Women's Safe Commutes in the Slums,"The spread of mobile phone usage to slum areas raises the possibility of using mobile technology to address problems facing the poorest of the world's poor. We present a case study of Safe Mathare, a design project aimed at improving women's safety in Nairobi, Kenya. Safe Mathare provides community patrols with basic smartphone technology to help women commute safely through a slum neighborhood during dusk and dawn hours. The project started as a prototype developed in a university course and has found willing partners with local NGOs and government. During its pilot phase, it has run into many challenges in particular around the issue of vigilantism. This paper explores the development and implementation of Safe Mathare, raising the questions of whether and how design can leverage technology to build a social network for security.",,"Hagan M,Zhang N,Kaye Jjofish",,2012,47–52,10.1145/2371664.2371675,https://doi-org.proxy.bnl.lu/10.1145/2371664.2371675;http://dx.doi.org/10.1145/2371664.2371675,Conference Paper
Key Issues of Information Systems Management in Botswana,Studies on key issues in information systems IS management aim at finding out critical IS issues that are of concern to IS executives and business leaders. The purpose of this paper was to investigate the key IS management issues in Botswana. A questionnaire was administered to collect data from information systems professionals working from the level of a programmer and above. A simple average was used to rank the key factors while factor analysis was carried out to determine the major groups of key issues in IS management. The results of simple average calculation showed that an information system security is the key management issue in Botswana. The results of factor analysis showed that the classification of key issues of IS management put forward by Niederman et al. 1991 has continued to change with new issues such as mobile applications and ecommerce showing up strongly and information systems security becoming a group in its own right. The findings continue to show that the key issues of IS management tend to differ across space and time. Culture and level of economic development are some of the factors which can make key issues differ from one country to another.,,,,2017,70–84,10.1504/IJISCM.2017.086237,https://doi-org.proxy.bnl.lu/10.1504/IJISCM.2017.086237;http://dx.doi.org/10.1504/IJISCM.2017.086237,Journal Article
ICEGOV '13: Proceedings of the 7th International Conference on Theory and Practice of Electronic Governance,"The 7th International Conference on Theory and Practice of Electronic Governance, ICEGOV2013, took place in Seoul, Republic of Korea from 22 to 25 October 2013. The conference was organized under the patronage of the Ministry of Security and Public Administration of the Republic of Korea (MOSPA) by the National Information Society Agency and by Macao-based Center for Electronic Governance at United Nations University International Institute for Software Technology (UNU-IIST) as the founder and organizer of the ICEGOV series. The conference took place under the theme ""Beyond 2015"" Smart Governance, Smart Development"". It was co-located with the Global e-Government Forum, organized by MOSPA in collaboration with United Nations Department of Economic and Social Affairs (UNDESA).The ICEGOV series focuses on the use of technology to transform relationships between government and citizens, businesses, civil society and other arms of government (Electronic Governance). Established in 2007, the series looks beyond the traditional focus on technology-enabled transformation in government (Electronic Government) towards new forms, new paradigms, and new foundations for technology-enabled governance, collaboration and sustainable development. ICEGOV is a platform where researchers, policy-makers and practitioners meet; a platform where theories are tested, insights are shared and experiences are reported; a platform for network- and capacity-building where keynote lectures and paper sessions are complemented by plenary discussions, town hall debates and poster exhibitions; a platform for international dialogue attended by participants from developing, developed and transition countries, from the United Nations system, and from many academic, governmental, non-governmental and private sector organizations. Since its establishment, the series has traveled globally from Macao (ICEGOV2007), through Cairo (ICEGOV2008), Bogota (ICEGOV2009), Beijing (ICEGOV2010), Tallinn (ICEGOV2011) and Albany (ICEGOV2012), to Seoul (ICEGOV2013) all generating significant local interest and stakeholder engagement.The program of ICEGOV2013 was built upon contributions from researchers and practitioners from around the world. In response to the call for papers, the conference received 133 papers from 54 countries and economies. The papers were evaluated in five categories: 1) Completed Research Papers providing the outcomes of complete research in one or more aspects of EGOV, with proven capability to advance the state of research in the field, limited to 10 pages; 2) Ongoing Research Papers providing the outcomes of ongoing research in one or more aspects of EGOV, with potential capability to advance the state of research in the field, limited to 4 pages; 3) Completed Experience Papers describing completed experience concerning EGOV policy or practice innovations, with proven capability to advance the state of practice in the field, including critical success factors and insights on the challenges encountered and how they were addressed, limited to 10 pages; 4) Ongoing Experience Papers describing ongoing experience concerning EGOV policy and practice innovations, with potential capability to advance the state of practice in the field, including critical success factors and insights on the challenges encountered and how they are being addressed, limited to 4 pages; and 5) Poster Papers presenting novel ideas and initiatives with potential to advance the state of research or state of practice in the field, limited to 2 pages. In total, 43 Completed Research Papers, 45 Ongoing Research Papers, 17 Completed Experience Papers, 21 Ongoing Experience Papers and 8 Posters were received. After anonymous peer-review process carried out by the members of the Program Committee at least three independent reviews were obtained for each submission as a basis for acceptance decisions: 13 submissions were accepted as Completed Research Papers, 8 as Completed Experience Papers, 29 as Ongoing Research Papers, 11 as Ongoing Experience Papers and 21 as Poster Papers. All accepted submissions, revised to address review comments, and presented at the conference within 6 paper tracks, 11 thematic sessions and one poster session, are included in this volume. Among them, like the last three ICEGOV conferences, the authors of selected papers were invited to submit extended versions of their papers for possible publication in the special issue of Government Information Quarterly, Elsevier.Based on the submitted and invited contributions and continuing the ICEGOV tradition, ICEGOV2013 featured a rich academic, capacity-building and network-building program comprising keynote lectures, plenary discussions, town hall debates, paper tracks, thematic sessions and the doctoral colloquium and poster exhibition. The program engaged individuals from over 60 countries and economies as authors, reviewers, committee members or resource persons. The details of the program are provided below.The conference included six keynote lectures on various aspects of Electronic Governance (EGOV), conducted by distinguished experts and practitioners in the area: 1) Park Chan Woo, Vice-Minister of Security and Public Administration of the Republic of Korea; 2) Alikhan Baimenov, Chairman of the Agency for Civil Service Affairs of the Republic of Kazakhstan; 3) Moon Suk Ahn, Chair Professor of e- Government, Korea University, Republic of Korea; 4) Mohammed Ali Al, Chief Executive Officer, e-Government Authority, Kingdom of Bahrain; 5) Henk G. Sol, Professor of Business and ICT and Founding Dean, University of Groningen, Netherlands; and 6) Edwin Lau, Head of Division, Reform of the Public Sector, Organization for Economic Co-operation and Development (OECD).Three plenary sessions followed the keynote lectures on the second, third and fourth day of the conference, focusing on specific questions of interest to the EGOV research and policy community:1. Are international EGOV rankings having a mobilizing or distracting influence on development? Chaired by Tomasz Janowski, Head of the Center for Electronic Governance at UNU-IIST and attended by: Vincenzo Aquaro, Chief of E-Government Branch, Division for Public Administration and Development Management, UNDESA; Bikesh Kurmangaliyeva, Deputy Chairwoman of the Board of ""Zerde"" National CT Holding, Kazakhstan; Mohammed Ali Al Qaed, CEO of eGovernment Authority, Kingdom of Bahrain; Mesfin Belachew Tefera, Technical Advisor to the Minister, Ethiopian Ministry of Communication and Information Technology; and Saleem Zoughbi, Former Regional ICT Advisor, UNESCWA and consultant for UNU-IIST.2. Who should drive smart conversations for sustainable development experts, citizens or politicians? Chaired by Marijn Janssen, Professor of ICT and Governance at Technology, Policy and Management Faculty, Delft University of Technology, Netherlands and attended by: Sunil Choenni, Head, Department of Statistical Information Management and Policy Analysis, Research and Documentation Centre (WODC), Dutch Ministry of Security and Justice; Harekrishna Misra, Professor in IT and Systems at the Institute of Rural Management Anand (IRMA), India; Henk G.Sol, Professor of Business and ICT and Founding Dean, Faculty of Economics and Business, University of Groningen, Netherlands; and Evgeny Styrin, Senior Research Analyst and Associate Professor, National Research University Higher School of Economics, Russia.3. Is a common set of e-government principles, applicable to all countries and contexts, possible? Chaired by Samuel Chan, Member of Executive Committee, Macao Science and Technology Development Fund, Macao SAR Government and attended by: Wojciech Cellary, Head of the Department of Information Technology, Poznan University of Economics, Poland; Sharon Dawes, Senior Fellow, Center for Technology in Government, University at Albany, USA; Edwin Lau, Head of Division, Reform of the Public Sector, Public Governance and Territorial Development Directorate, Organization for Economic Co-operation and Development; and Jeremy Millard, Associate Research Fellow, Brunel University, UK.Three town hall debates took place in the afternoons of the first, second and third days of the conference. They focused on three salient questions for the EGOV research and policy community:1. Catalyzing Smart Transformation: What Makes Governments Smarter? Chaired by Samia Melhem, Lead Policy Specialist, Transform Practice, Chair, eDevelopment Community of Practice, Transport, Water and ICT, Sustainable Development Network, World Bank Group; and Oleg Petrov, Senior Program Officer, ICT, World Bank; and attended by: Jabiri Kuwe Bakari, CEO, e-Government Agency, Tanzania; Rajendra Kumar, Senior Officer, Indian Administrative Service and Joint Secretary (e-Governance), Department of Electronics and Information Technology, Government of India; Bikesh Kurmangaliyeva, Deputy Chairwoman of the Board of ""Zerde"" National CT Holding, Kazakhstan; Margareta Petrusevschi, Knowledge and Learning Coordinator, e-Government Centre, Government of the Republic of Moldova; James Saaka, Executive Director, National Information Technology Authority, Uganda; Mesfin Belachew Tefera, Technical Advisor to the Ethiopian Minister of Communication and Information Technology; and Jeongwon Yoon, Executive Director, National Information Society Agency, Korea. This town hall was organized by the World Bank.2. Is Good Governance a Pre-Condition or a Consequence of the Development of Knowledge Societies? Chaired by Andrea Cairola, Adviser for Communication and Information, UNESCO Office Beijing, Cluster Office to the Democratic People's Republic of Korea, Japan, Mongolia, People's Republic of China and Republic of Korea; and attended by: Johanna Ekua Awotwi, Director of Research and ICT Operations, Centre for e-Governance, Accra, Ghana; Antonio Cordella, Lecturer in Information Systems, London School of Economics and Political Sciences, UK; Marco Peres, Director, Observatory for Society, Technology and Government Information, University Externado of Colombia, Colombia; Margareta Petrusevschi, Knowledge and Learning Coordinator, e-Government Centre, Government of the Republic of Moldova, Moldova; and Jeongwon Yoon, Executive Director, National Information Society Agency, Republic of Korea. This town hall was organized by the UNESCO Information for All Programme.3. Striking the Balance of Security, Privacy and Openness: To Open or Not To Open? Chaired by Theresa Pardo, Director of the Center for Technology in Government, University at Albany, USA, and attended by: Sharon Dawes, Senior Fellow, Center for Technology in Government, University at Albany, USA; Ramon Gil-Garcia, Research Director, Center for Technology in Government, University at Albany, USA; Louise Thomasen, independent consultant and expert in EGOV and technology, Denmark; and Lei Zheng, Assistant Professor, Department of Public Administration, Fudan University, China.The program included six paper tracks, chaired by leading international experts in the corresponding areas, comprising presentations of three to six accepted papers: 1) Building Smart Government chaired by Theresa Pardo, Director of the Center for Technology in Government, University at Albany, USA and Gabriel Puron Cid, Professor at the Centre of Research and Teaching in Economic Sciences, Mexico; 2) Governing through Networks chaired by Sehl Mellouli, Associate Professor at Laval University, Canada and Adegboyega Ojo, Research Fellow and Leader of E-Government Group at INSIGHT, National University of Ireland, Ireland; 3) Policy and Governance Innovation chaired by Natalie Helbig, Senior Research Associate at the Center for Technology in Government, University at Albany, USA and Marijn Janssen, Professor in ICT and Governance at the Delft University of Technology, Netherlands; 4) Smart Governance for Smart Industries chaired by Wojciech Cellary, Professor and Head of the Department of Information Technology at the Poznan University of Economics, Poland and Antonio Cordella, Lecturer at the London School of Economics and Political Sciences, UK; 5) Smart Governance for Smart Societies chaired by Jeremy Millard, Associate Research Fellow at the Brunel University, UK; and 6) Ethics, Transparency and Accountability chaired by Jeanne Holm, Chief Knowledge Architect at the NASA Jet Propulsion Laboratory, USA. Each track took place across the whole duration of the conference, with tutorial introduction to the topic of the track organized on the first day, presentations of accepted papers on the second or third day, and workshop-style discussion on the last day.Complementing the paper tracks, 11 thematic sessions were organized and chaired by industrial, academic, government and international organizations active in the theme of the session, comprising presentations of up to four accepted papers: 1) EGOV for Development chaired by Nag Yeon Lee, ICT Consultant and Instructor for e-Government on behalf of the Asia Pacific Center on ICT for Development, United Nations Economic and Social Commission for Asia Pacific; 2) National Data Policies chaired by Zhanat Zhakhmetova, Head of the Office of State Informatization Policy, Department of State Information Technology Policy, on behalf of the Ministry of Transport and Communications, Republic of Kazakhstan; 3) Governing Ageing Society chaired by Toshio Obi, Professor, Institute of e-Government on behalf of Waseda University, Japan; 4) Governing Smart Cities chaired by Yoon Chang So, Smart Cities Country Leader, IBM Korea on behalf of IBM; 5) Open Government Data Impact chaired by Edwin Lau, Head of Division, Reform of the Public Sector, Public Governance and Territorial Development Directorate, on behalf of the Organization for Economic Co-operation and Development; 6) Interoperability Governance chaired by Jung Sik Hwang, Platform Strategy Lead at Microsoft Korea on behalf of Microsoft; 7) Government on Social Media chaired by Jeanne Holm, Evangelist, Data.Gov and Chief Knowledge Architect at the Jet Propulsion Laboratory, NASA on behalf of the World Wide Web Consortium; 8) Innovative EGOV Applications chaired by Oleg Petrov, Senior Program Officer, ICT, World Bank on behalf of the World Bank; 9) Participatory Government chaired by Bernd Friedrich, Head of the Information and Communications Technologies for Development Project at Deutsche Gesellschaft für Internationale Zusammenarbeit (GIZ) GmbH, Germany on behalf of GTZ; 10) Mobile Governance chaired by Nestor Eduardo Fajardo Infante, Advisor for Research, Development and Innovation, Ministry of Information Technology and Communication, on behalf of the Government of Colombia; and 11) Open Data Ecosystem chaired by Jeanne Holm, Evangelist, Data.Gov and Chief Knowledge Architect, Jet Propulsion Laboratory, NASA on behalf of the U.S. Government and Data.Gov.The program also included poster exhibition, organized in the reception style to allow authors to present their ongoing work, receive feedback and engage in discussions and networking; and an interactive doctoral colloquium, jointly organized by the Center for Electronic Governance at UNU-IIST, Macao, University of Groningen, Netherlands and Chuo University, Japan. The colloquium provided doctoral students from different disciplines an opportunity to discuss a variety of EGOV topics and methods related to their research work, dissertations and career plans. The colloquium was co-chaired by Elsa Estevez, Academic Program Officer, United Nations University International Institute for Software Technology, Macao; Hiroko Kudo, Professor of Public Policy and Public Management, Faculty of Law, Chuo University, Japan; and Henk G. Sol, Professor of Business and ICT and Founding Dean, Faculty of Economics and Business, University of Groningen, Netherlands; and attended by Adegboyega Ojo, Research Fellow and Leader of E-Government Group at the INSIGHT Center for Data Analytics, National University of Ireland, Ireland as invited academic.The conference awarded best paper titles in Best Research Paper and Best Experience Paper categories. The selection was carried out jointly by Elsa Estevez as the ICEGOV2013 Awards Chair, and Tomasz Janowski and Jeanne Holm as the ICEGOV2013 Program Chairs. Three papers were nominated to the Best Experience Paper award: 1) A Reputation Based Electronic Government Procurement Model by Hichem Klabi, Sehl Mellouli and Monia Rekik; 2) Government 3.0 in Korea: Fad or Fashion? byTaewoo Nam; and 3) Secure ID Management for Social Security and Tax Number System by Hisao Sakazaki, Dan Yamamoto, Akihiro Sugimoto and Shinji Hirata. The winner in this category was ""A Reputation Based Electronic Government Procurement Model"" by Hichem Klabi, Sehl Mellouli and Monia Rekik. Three papers were also nominated to the Best Research Paper award: 1) Harnessing the Duality of e-Participation Social Software Infrastructure Design by Lukasz Porwol, Adegboyega Ojo and John Breslin; 2) When Food Quality Control in China Meets Mobile and Wireless Technology: Interactions, Potentials and Pitfalls by Shuhua Liu; and 3) Cross-departmental Collaboration in Government One-Stop Center: Factors and Performance by Xinping Liu. The winner in this category was ""Harnessing the Duality of e-Participation Social Software Infrastructure Design"" by Lukasz Porwol, Adegboyega Ojo and John Breslin.Many people and institutions contributed to the organization of ICEGOV2013. We wish to thank the official patron of ICEGOV2013, the Ministry of Security and Public Administration of the Republic of Korea for endorsing and supporting the conference. Our sincere thanks go to the National Information Society Agency, Republic of Korea (NIA) as the local organizer of the conference, particularly to Jeongwon Yoon for his vision and leadership, and to Dohyoon Kim and the whole team in NIA for their hard work and dedication to making the combined ICEGOV2013 and Global e-Government Forum event successful. We wish to express our most sincere thanks to the key sponsors Macao SAR Government and Macao Foundation and the sponsor Electronic Government of the Republic of Kazakhstan whose generous contributions allowed many academics and practitioners from developing countries to attend the conference. Special gratitude is due to Macao SAR Government, its Public Administration and Civil Service Bureau, and Macao Foundation for continuing support to the ICEGOV conference series and the origin of the series e-Macao Program. We also wish to thank ICEGOV2013 partners for their presence, support and in-kind contributions: Brunel University, London, UK; Center for Technology in Government, University at Albany, USA; Data.Gov, U.S. Government; German Cooperation, Deutsche Zusammenarbeit and Deutsche Gesellschaft fur Internationale Zusammenarbeit, Germany; IBM; Information and Communication Technologies, World Bank; Microsoft; Ministry of Information Technology and Communication, Colombia (MINTIC); Organization for Economic Co-operation and Development; Poznan University of Economics, Poland; The Insight Centre for Data Analytics, National University of Ireland, Ireland; The Science and Technology Development Fund, Government of Macao SAR, Macao; UNESCO Information for All Programme; United Nations Asian and Pacific Training Centre for Information and Communication Technology for Development; Vive Digital Programme, MINTIC, Colombia; Waseda University, Japan; and the World Wide Web Consortium. We also wish to express our thanks to ACM Press for publishing the ICEGOV2013 conference proceedings. We are most grateful to the whole Advisory Committee for supporting the conference and to all members of the Program Committee and additional reviewers for their efforts to carry out quality reviews and to help build a strong conference program. We thank keynote speakers; organizers, chairs and moderators of the plenary sessions, town hall debates, paper tracks, thematic sessions, the doctoral colloquium, and the poster session; and all panelists and speakers for their intellectual contributions. Last but not least, we are most thankful to all authors for their efforts in preparing, submitting and presenting papers at ICEGOV2013.We hope that ICEGOV2013 will further contribute to building, growing and connecting global EGOV research, policy and practice communities, able to cross not only national and regional but also institutional and thematic borders, and that the contacts, discussions and ideas initiated in Seoul in October 2013 will continue well after the conference and towards ICEGOV2014 in Guimaraes, Portugal.",,,,2013,,,,Book
FlashPatch: Spreading Software Updates over Flash Drives in Under-Connected Regions,"Computers in developing regions often lack the Internet connectivity and network bandwidth necessary to consistently download and apply software updates and security patches. However, even unconnected computers contract viruses and malware through the sharing of USB flash drives and other removable media. This paper introduces FlashPatch, a system for distributing software updates to computers in such areas by having software updates ""piggy-back"" on the existing flow of flash drives in rural regions. FlashPatch requires no changes in user behavior once the software has been installed. We implemented a proof-of-concept FlashPatch prototype and evaluated it in a field trial in Ghana. We present data on the prevalence and spread of viruses at our study site and offer experimental evidence of FlashPatch's effectiveness from a nine-month field trial. We found that FlashPatch provided additional antivirus protection to 30% of the machines in our study without imposing any tangible burdens on the system owners.",,"Corrigan-Gibbs H,Chen J",,2014,1–10,10.1145/2674377.2674384,https://doi-org.proxy.bnl.lu/10.1145/2674377.2674384;http://dx.doi.org/10.1145/2674377.2674384,Conference Paper
ICT Security Applications: Information Systems Security Application,"ICT Security Application is a books for all those who wish to know the currect awareness level in developing countries as presented by Abanti Cyrus, Onsongo Jane and Lusiba Badru.The book chapters are as follows: Esoteric-based access control of digital medical summeries by Abanti. Information security incident reporting and audit investigations by Abanti Impact of IT on instruction delivery by Prof Jane Onsongo and Abanti Cyber security evaluation framework by Badru Lusiba and Abanti Integrating ICT on E-schooling for digital content by Abanti Biometric security: Medicine for password headache of millenium IT users by Abanti Cyrus Abanti is Lecturing Information Systems Security Audit aand Information systems strategy at Jomo Kenyatta University of Agriculture and Technology (JKUAT Kenya. Lusiba Badru is Computer lecturer at Nkumba University and Prof Jane Osongo works with the Kenya Ant-Corruption Commission of Kenya she has high interest in ICT application in fighting efraud.",,"Makori CA,Onsongo J,Badru L",,2012,,,,Book
Engaging High School Students in Cameroon with Exam Practice Quizzes via SMS and WhatsApp,"We created a quiz-based intervention to help secondary school students in Cameroon with exam practice. We sent regularly-spaced, multiple-choice questions to students' own mobile devices and examined factors which influenced quiz participation. These quizzes were delivered via either SMS or WhatsApp per each student's preference. We conducted a 3-week deployment with 546 students at 3 schools during their month of independent study prior to their graduating exam. We found that participation rates were heavily impacted by trust in the intervening organization and perceptions of personal security in the socio-technical environment. Parents also played a key gate-keeping role on students' digital activities. We describe how this role - along with different perceptions of smartphones versus basic phones - may manifest in lower participation rates among WhatsApp-based users as compared to SMS. Finally, we discuss design implications for future educational interventions that target students' personal cellphones outside of the classroom.",,"Poon A,Giroux S,Eloundou-Enyegue P,Guimbretiere F,Dell N",,2019,1–13,10.1145/3290605.3300712,https://doi-org.proxy.bnl.lu/10.1145/3290605.3300712;http://dx.doi.org/10.1145/3290605.3300712,Conference Paper
Healthcare Providers’ Perspective about the Use of Telemedicine in Egypt: A National Survey,"Incorporation of telemedicine in general clinical practice is becoming a compelling need nowadays in the context of COVID-19 pandemic and its consequent burdens on the healthcare systems. Though telemedicine appears to be appealing and carries a lot of advantages, yet it is still faced by many challenges and barriers especially in developing countries. Our aim was to explore the impression of healthcare providers about telemedicine and its applicability in clinical practice in Egypt. A cross-sectional study was conducted among healthcare providers from different Egyptian governorates through a web-based survey. The survey gathered information about demographic, socioeconomic features of the enrolled healthcare participants; their knowledge, previous experience, impression about telemedicine, advantages of telemedicine over traditional medical services, barriers that may face telemedicine, and additional services that can be provided by telemedicine were also explored. Our study enrolled 642 healthcare providers from all over Egypt, 43.77% were females, of which 55.5% were physicians, 27.3% were nurses, 6.1% were technicians, 7.6% were administrative clerks, and 3.6% were medical directors. Sixty-four percent of participants reported that they have never used telemedicine. Smartphones were the most commonly used mean in the group who used telemedicine (65%), and smartphone applications were the favorable telemedicine service for about 50% of participants. Participants assumed that the use of telemedicine might not have a negative effect on the doctor-patient relationship but raised some concerns regarding the privacy and security of patients’ data. Despite the fact that telemedicine appears to be appealing and widely accepted by healthcare providers, yet still, its implementation is confronted by some obstacles. Precise organizational guidelines need to be developed to clearly figure out the exact role of each healthcare provider to minimize their doubtfulness about telemedicine and to facilitate its adoption.",,"Alboraie M,Abdalgaber M,Youssef N,Moaz I,Abdeen N,Abosheaishaa HM,Shokry MT,El-Raey F,Asfour SS,Abdeldayem WA,Hassan AA,Mahran EE,Tag-Adeen M,Elshaarawy O,Radwan MI,Altonbary A,Fouad Y,Tsiknakis M",,2022,,10.1155/2022/3811068,https://doi-org.proxy.bnl.lu/10.1155/2022/3811068;http://dx.doi.org/10.1155/2022/3811068,Journal Article
"MobiSys '11: Proceedings of the 9th International Conference on Mobile Systems, Applications, and Services","It is our pleasure to welcome you to the 9th International Conference on Mobile Systems, Applications, and Service, or more simply MobiSys 2011. This year's conference continues the tradition of providing a highlyselective forum for cutting-edge research that takes a broad systems perspective of mobile computing. Continuing trends from last year, there is a strong showing for research related to smartphones that reflects their growing momentum as powerful, programmable mobile systems. Research on energy-efficiency in many guises is also very popular. A broad set of other topics is represented in the program, covering aspects of security, sensor networks and wireless protocols, and mobile applications and services.We received 141 submissions, roughly a 10% increase from the previous year, as MobiSys continues to grow as a venue. The papers were reviewed as in past years, with a two round review process. Each paper received three reviews during the first round. Half of the papers then advanced to the second round, where they received two further reviews. All told, our tireless PC members produced over 560 reviews. To select the program, we discussed 55 papers at an all-day face-to-face PC meeting directly following the HotMobile workshop in Phoenix, AZ. The resulting program that you see before you features 25 highly-competitive papers. All of these papers were further shepherded by a PC member to improve their presentation.As well as papers, we are very fortunate to have an excellent line-up of speakers who capitalize on our ""Washington, DC"" location: Dr. Edward Felten will speak in his new role as Chief Technologist for the FTC, and Dr. Douglas Sicker will speak in his role as the Chief Technologist for the FCC. And as is a tradition for MobiSys, the recipient of the SIGMOBILE Outstanding Contributor Award will deliver a keynote; this year we keenly look forward to hearing from Prof. Mahadev ""Satya"" Satyanarayanan. The technical program also includes an eagerly anticipated event, the poster and demonstration session. This year we have 23 demonstrations and 11 posters that highlight new research! We especially want to thank Moustafa Youssef from EJUST, Egypt for putting together this session as the Posters/Demos Chair.",,,,2011,,,,Book
Security Enhanced Linux Syposium-SELinux 2007,"This book contains 14 original papers on SELinux, an operating system program designed to protect against software vulnerabilities. The papers were originally presented at a 2007symposium sponsored by IBM, Red Hat, Tresys, and Hewlett-Packard. The book contains programming and other guidance for those using SELinux to improve the security of computer operating systems. TABLE OF CONTENTS Security-Enhanced Darwin: Porting SELinux to Mac OS X Christopher Vance, Todd C. Miller, Robert Dekelbaum and Andrew Reisse, SPARTA, Inc. Enforcing Security Enhanced Linux Policies in a Networked Policy Domain Joshua Brindle, Karen Vance and Chad Sellers, Tresys Technology Using the Flask Security Architecture to Facilitate Risk Adaptable Access Controls Machon Gregory and Peter Loscocco, National Security Agency Using GConf as an Example of How to Create an Userspace Object Manager James Carter, National Security Agency Application of the Flask Architecture to the X Window System Server Eamon F. Walsh, National Information Assurance Research Laboratory,National Security Agency FCGlob: A New SELinux File Context Syntax Don Miner, University of Maryland, Baltimore County James Athey, Tresys Technology, LLC Towards Intuitive Tools for Managing SELinux: Hiding the Details but Retaining the Power James Athey, Christopher Ashworth, Frank Mayer and Don Minner, Tresys Technology, LLC Madison: A New Approach to Policy Generation Karl MacMillan, Red Hat Setroubleshoot: A User Friendly Tool to Diagnose AVC Denials John Dennis, Red Hat The Design and Implementation of a Guard Installation and Administration Framework Boyd Fletcher, USJFCOM J9 & SPAWAR Systems Center San Diego Chris Roberts, General Dynamics Kurt Risser, Dataline Securing Inter-Process Communications in SELinux Spencer Shimko and Joshua Brindle, Tresys Technology, LLC Integrating SELinux with Security-Typed Languages Boniface Hicks, Sandra Rueda, Trent Jaeger and Patrick McDaniel, Systems and Internet Infrastructure Security Laboratory (SIIS), Computer Science and Engineering, Pennsylvania State University Porting Legacy Multilevel Secure Applications to Security Enhanced Linux Andy Suchoski and Rick Supplee, Hewlett Packard Company Extending Linux for Multi-Level Security George Wilson, IBM Corporation Klaus Weidner, Atsec Information Security Corporation Loulwa Salem, IBM Corporation Author Index",,Mayer F,,2007,,,,Book
Annual Editions: Computers in Society 08/09,"This Fourteenth Edition of ANNUAL EDITIONS: COMPUTERS IN SOCIETY provides convenient, inexpensive access to current articles selected from the best of the public press. Organizational features include: an annotated listing of selected World Wide Web sites; an annotated table of contents; a topic guide; a general introduction; brief overviews for each section; a topical index; and an instructor’s resource guide with testing materials. USING ANNUAL EDITIONS IN THE CLASSROOM is offered as a practical guide for instructors. ANNUAL EDITIONS titles are supported by our student website, . Table of contents UNIT 1. Introduction 1. 34991 Five Things We Need to Know About Technological Change, Neil Postman, Address to New Tech ’98 conference, March 27, 1998 Neil Postman, a well-known cultural critic, suggests that computer technology is too important to be left entirely to the technologists. “Embedded in every technology,” he says, “is a powerful idea….” 2. 46656 Slouching Toward the Ordinary, Susan C. Herring, New Media & Society, February 2004 Contrary to what we read, changes in the ecology of the computing “will continue to make the internet a simpler, safer and—for better or worse—less fascinating communication environment.” 3. 41735 On the Nature of Computing, Jon Crowcroft, Communications of the ACM, February 2005 The author states, “ Occupying a third place in human intellectual culture, computing is not bound by the need to describe what does exist (as in natural science) or what can be built in the real world (as in engineering).” UNIT 2. The Economy 4. 46657 The Subprime Loan Machine, Lynnley Browning, The New York Times, March 23, 2007 “The rise and fall of the subprime market has been told as a story of a flood of Wall Street money and the desire of Americans desperate to be part of the housing boom,” says Lynnley Browning. Yet, the boom was made possible “by a little-noticed tool of automatic underwriting software.” 5. 46658 Click Fraud, Brian Grow and Ben Elgin, Business Week, October 2, 2006 Internet advertisers think they pay only when an interested customer clicks on their ads. Martin Fleischman, an Atlanta businessman, “noticed a growing number of puzzling clicks coming from such places as Botswana, Mongolia, and Syria.” 6. 41736 The Big Band Era, Christopher Swope, Governing, January 2005 Even as cities like Philadelphia are working to transform the entire city into a wireless hot spot—with government as the internet service provider of last resort—communications companies are fighting to keep local governments out of the broadband business. 7. 45257 The Beauty of Simplicity, Linda Tischler, Fast Company, November 2005 A simple tale about simplicity. One company hired an editor from People Magazine to translate accounting lingo into everyday language, “pared back 125 setup screens to three,” and “sold 100,000 units in its first year on the market.” 8. 41740 The Software Wars, Paul De Palma, American Scholar, Winter 2005 The article argues that software development is like military procurement, and suffers many of the same woes, including excessive complexity and cost overruns. 9. 46659 Scan This Book!, Kevin Kelly, The New York Times Magazine, May 14, 2006 What will happen to libraries, books on paper, and copyright protections if Google’s plans to scan the books of five major research libraries succeeds UNIT 3. Work and the Workplace 10. 46660 National ID, Ryan Singel, Wired, May 15, 2007 Immigration is in the news again. One proposal before Congress is to issue American workers tamper-proof biometric Social Security cards. These would replace the text-only design that’s been issued to Americans almost without change for more than 70 years. 11. 34959 Brain Circulation, AnnaLee Saxenian, Brookings Review, Winter 2002 Do immigrants displace native workers Is the United States siphoning off talent from countries that can ill afford to lose it This Berkeley professor argues that high-skill immigration is more complex than that. 12. 41774 The New Face of the Silicon Age, Daniel H. Pink, Wired, February 12, 2004 This piece on Indian programmers should be enough to keep chairs of American computer science departments awake at night. 13. 46661 Computer Software Engineers, Occupational Outlook Handbook, 200607 Edition Here is one official source that acknowledges the effect of shipping high tech jobs abroad, but still predicts that “ software engineers are projected to be one of the fastest-growing occupations from 2004 to 2014.” 14. 41753 The Computer Evolution, Rob Valletta and Geoffrey MacDonald, FRBSF Economic Letter, July 23, 2004 This article uses data from several surveys “to examine two key aspects of the computer evolution: the spread of PCs at work and the evolving wage differentials between individuals who use them and those who do not.” 15. 41754 Making Yourself Understood, Stuart Crainer and Des Dearlove, Across the Board, MayJune 2004 In a business environment where half of surveyed managers report spending more than two hours each day answering e-mail, “it’s never been so easy to be misunderstood.” 16. 46664 Privacy, Legislation, and Surveillance Software, G. Daryl Nord, Tipton F. McCubbins, and Jeretta Horn Nord, Communications of the ACM, August 2006 The authors tell us that the assumption of employee privacy in the workplace “may be naïve.” Constitutional protections against unreasonable search and seizure “usually apply only to state actions.” UNIT 4. Computers, People, and Social Participation 17. 40654 Romance in the Information Age, Christine Rosen, The New Atlantis, Winter 2004 According to Christine Rosen, “our technologies enable and often promote two detrimental forces in modern relationships: the demand for total transparency and a bias toward the over sharing of personal information.” 18. 46665 How Do I Love Thee , Lori Gottlieb, The Atlantic, March 2006 Some Internet dating sites now use social scientists to “develop a new science of attraction.” Says the author, “My matches included a film editor wearing a kilt—and not in an ironic way. Was this the best science could do ” 19. 46666 The Perfect Mark, Mitchell Zuckoff, The New Yorker, May 15, 2006 A cautionary tale about an African scam and two years in prison for bank fraud and money laundering. 20. 41755 Back-to-School Blogging, Brock Read, The Chronicle of Higher Education, September 3, 2004 It should surprise no one that entering freshmen, who grew up using the Internet, should turn to university-sponsored blogs to ease the transition to college life. 21. 46667 E-Mail Is for Old People, Dan Carnevale, The Chronicle of Higher Education, October 6, 2006 Reaching students through email has become more difficult as students turn to text-messaging and social networking sites. UNIT 5. Societal Institutions: Law, Politics, Education, and the Military 22. 37230 The Copyright Paradox, Jonathan Band, Brookings Review, Winter 2001 According to the author, “the problem with piracy is not the inadequacy of existing laws, but the high cost of enforcing any law against the large universe of infringers.” 23. 46668 Piracy, Computer Crime, and IS Misuse at the University, Timothy Paul Cronan, C. Bryan Foltz, and Thomas W. Jones, Communications of the ACM, June 2006 Who are the students who “openly admit to illegally installing software on home computers or otherwise misusing computer information systems ” This article provides some clues. 24. 41764 Facing Down the E-Maelstrom, Jeffrey Selingo, The Chronicle of Higher Education, April 29, 2005 Never an easy job, leading a college in the age of the Internet requires sifting through e-mail, reading blogs, and fending off criticism, the volume of which would be inconceivable without networked computers. 25. 46669 Can Blogs Revolutionize Progressive Politics , Lakshmi Chaudhry, In These Times, February 2006 Liberals have been envious ever since Richard Viguerie’s computer-generated mailing lists contributed to Ronald Reagan’s victory in 1980. At a time when even Senate Majority Leader Harry Reid has a blog, some Democrats hope that the computer is finally on their side. 26. 46670 Center Stage, Carl Sessions Stepp, American Journalism Review, AprilMay 2006 How do a newspaper’s web and print versions differ Unlike the print version of a newspaper, the Web version receives little editing. 27. 46671 The Coming Robot Army, Steve Featherstone, Harper’s Magazine, February 2007 “Within our lifetime,” says Featherstone, “robots will give us the ability to wage war without committing ourselves to the human cost of actually fighting a war.” Sgt. Jason Mero concurs: “These things are amazing…. They don’t complain…. They don’t cry. They’re not scared. This robot here has no fear.” UNIT 6. Risk and Avoiding Risk 28. 41768 Why Spyware Poses Multiple Threats to Security, Roger Thompson, Communications of the ACM, August 2005 Harm caused by spyware ranges from gobbling up computer speed on your PC to enlisting your machine in attacks that can disrupt major businesses or the government. 29. 41769 Terror’s Server, David Talbot, Technology Review, February 2005 “Most experts agree,” says the author, “that the Internet is not just a tool of terrorist organizations, but is central to their operations.” 30. 37238 The Virus Underground, Clive Thompson, The New York Times Magazine, February 8, 2004 Clive Thompson states, “when Mario is bored…he likes to sit at his laptop and create computer viruses and worms.” 31. 46672 Secrets of the Digital Detectives, The Economist, September 23, 2006 It’s nice to learn that the good guys have some tricks of their own. 32. 46673 Data on the Elderly, Marketed to Thieves, Charles Duhigg, The New York Times, May 20, 2007 Thieves purchase lists of the elderly from consumer databases, then pose as government workers trying to update their files on World War II veterans and retired school teachers. Some seniors find themselves with empty bank accounts. 33. 41770 The Fading Memory of the State, David Talbot, Technology Review, July 2005 Government documents, from the 38 million emails generated by the Clinton administration to electronic records of the 1989 invasion of Panama, are on disintegrating electronic media, stored using now-obsolete formats. 34. 41771 False Reporting on the Internet and the Spread of Rumors, Paul Hitlin, Gnovis, April 26, 2004 Internet news sources can sometimes be unreliable. Paul Hitlin examines Internet coverage of the Vince Foster suicide along with other stories to understand why this is so. UNIT 7. International Perspectives and Issues 35. 46675 China’s Tech Generation Finds a New Chairman to Venerate, Kevin Holden, Wired, May 24, 2007 The new China is not a place that would have made Chairman Mao comfortable. One indication is the popularity of Bill Gates. 36. 46677 Is the Crouching Tiger a Threat , Robert L. Glass, Communications of the ACM, March 2006 All indications suggest that the U.S. domination of computing is about to be eclipsed. Here is one commentator who is not quite convinced. 37. 41776 Restoring the Popularity of Computer Science, David A. Patterson, Communications of the ACM, September 2005 While India turns out more and more programmers willing to work for a fraction of their American counterparts, enrollment in computer science classes across the United States is dropping. The author believes that “inaccurate impressions of opportunities” are behind the decline. 38. 41773 China’s Computer Wasteland, Benjamin Joffe-Walt, The Progressive, January 30, 2005 What to do with the detritus of the digital age is a growing problem. Shipping it to China seems to be one solution. 39. 46678 Cat and Mouse, on the Web, The Economist, December 2, 2006 This article examines censorship on the Internet and the extraordinary steps taken by the anti-censorship community to thwart the efforts of censors. 40. 46681 In Search of a PC for the People, Bruce Einhorn, Business Week, June 12, 2006 What features get included in a $200.00 PC marketed to developing nations and “I think of digital access for kids as a human right,” says Nicholas Negroponte of MIT are two issues explored in this article. UNIT 8. The Frontier of Computing 41. 46682 A Nascent Robotics Culture, Sherry Turkle, AAAI Technical Report, July 2006 “What is a robot kind of love ” and “What will we be like, what kind of people are we becoming as we develop increasingly intimate relationships with machines ” MIT’s pioneering sociologist tries to answer both questions 42. 46683 March of the Robolawyers, The Economist, March 11, 2006 Australian researchers have developed a program that helps divorcing couples divide their property. 43. 46684 Best-Kept Secrets, Gary Stix, Scientific American, January 2005 Public-key cryptography keeps e-commerce secure for now. Quantum cryptography might take its place. 44. 46685 Toward Nature-Inspired Computing, Jiming Liu and K.C. Tsui, Communications of the ACM, October 2006 Computer scientists are turning to biology as a source of inspiration for models of complex systems. These biological models change the rules governing systems behavior. 45. 41778 The Intelligent Internet, William E. Halal, The Futurist, MarchApril 2004 The author claims the Internet will be the “main method used in 30% of courses” by 2014. As with all predictions, enjoy, but read critically. 46. 41780 Mind Control, Richard Martin, Wired, March 2005 What does a quadriplegic young man who plays pong have in common with a monkey mentally moving a joy stick and “soldier-controlled killer robots ” The answer: Brain Computer Interface or BCI.",,De Palma P,,2007,,,,Book
Botswana's Lab-In-A-Briefcase: A Position Paper,"Detecting and managing communicable and non-communicable diseases in rural settings of Africa raises numerous structural, syntactical and semantic issues. Further, it has been observed that both Communicable Diseases (CDs) such as TB and Non-Communicable Diseases (NCDs), such as cancer, diabetes, cardiovascular diseases and chronic respiratory disease, are on the rise in sub-Saharan African countries and estimated to account for about 25% of deaths (Bloomfield et al., 2014) [8]. In Botswana, NCDs account for more than a third of all deaths in the country (WHO NCD country profile, 2014) [9]. For cash-poor part of Africa, this additional spend-requirement in healthcare is unfortunately substantial. One approach to reducing death related to CD/NCD is early detection and control through data collection and appropriate intervention. In sub-Saharan Africa countries, most of the population lives in rural areas where access to healthcare facilities is very limited. In such circumstances, a low-cost and mobile healthcare facility along with associated Information and Communication Technologies (ICT) would be of great assistance.This paper investigates the application of the ""lab-in-a-briefcase"" technology for the management of CDs/NCDs in Botswana and other SADC countries. The ""lab-in-a-briefcase is designed to provide a portable laboratory diagnosis toolkit with rapid results (about 15 minutes) that can be used in areas where access to laboratory or healthcare facility is limited and can be used with minimal training. It contains all the necessary tools and chemicals/reagents which are packaged in a briefcase form so that they can easily be carried. In addition, the Lab-in-a-briefcase employs mini-HPLC and smart camera, microphone, credit card-sized ECG and microscope so that a variety of tests can be performed quickly and efficiently in a portable manner.This paper details the design and deployment of ""Lab-in-a-briefcase"" and associated software tools at primary care health facilities so that diagnosis can be quickly carried out and the resulting medical records are automatically generated, converted into appropriate format and securely shared with different health information systems. The deployment of this system requires adaptation of the system in the context of the linguistic, legal, security, and other policy requirements of the participating countries. As part of the demonstration of the applicability, pilot studies will be extended to all participating African countries. Our end-objective is to develop a fully optioned prototype.",,Narasimhan VL,,2019,,10.1145/3290688.3290716,https://doi-org.proxy.bnl.lu/10.1145/3290688.3290716;http://dx.doi.org/10.1145/3290688.3290716,Conference Paper
Txteagle: Mobile Crowdsourcing,"We present txteagle, a system that enables people to earn small amounts of money by completing simple tasks on their mobile phone for corporations who pay them in either airtime or MPESA (mobile money). The system is currently being launched in Kenya and Rwanda in collaboration with the mobile phone service providers Safaricom and MTN Rwanda. Tasks include translation, transcription, and surveys. User studies in Nairobi involving high school students, taxi drivers, and local security guards have been completed and the service has recently launched in Kenya nationwide.",,Eagle N,,2009,447–456,10.1007/978-3-642-02767-3_50,https://doi-org.proxy.bnl.lu/10.1007/978-3-642-02767-3_50;http://dx.doi.org/10.1007/978-3-642-02767-3_50,Conference Paper
Examining Consumer Adoption and Perception of Mobile Money in Ghana,"This paper investigates the consumer perception of a new electronic financial service in Ghana, namely mobile money MM. We analyse the relationships among five MM-related constructs, which are perceived ease of use PEOU, perceived usefulness PU, perceived mobile money security PMMS, attitude ATT, and intention to use IU. Importantly, the impact of age, family income, and gender on the relationships among the five MM constructs has been studied using a multi-group analysis approach. We find that PEOU, PU, and PMMS are significant determinants of ATT in the MM market in Ghana when age, family income, gender are not considered. However, the relationships among the five constructs exhibit significant variations when age, family income, and gender are considered. We also find an evidence that the effects of age, family income, and gender on consumers' perception of, ATT towards, and intention of using MM are significant in Ghana. The results of this study provide more insights into the research on MM, thus helping the development of marketing strategies for the service.",,,,2016,18–41,10.1504/IJEF.2016.083490,https://doi-org.proxy.bnl.lu/10.1504/IJEF.2016.083490;http://dx.doi.org/10.1504/IJEF.2016.083490,Journal Article
Of Milk and Mobiles: Assessing the Potential of Cellphone Applications to Reduce Cattle Milk Yield Gaps in Africa Using a Case Study,,,"A. Bateki C,Daum T,Salvatierra-Rojas A,Müller J,Birner R,Dickhoefer U",,2021,,10.1016/j.compag.2021.106516,https://doi-org.proxy.bnl.lu/10.1016/j.compag.2021.106516;http://dx.doi.org/10.1016/j.compag.2021.106516,Journal Article
"E-Infrastructure and e-Services: 7th International Conference, AFRICOMM 2015, Cotonou, Benin, December 15-16, 2015, Revised Selected Papers","This book constitutes the thoroughly refereed proceedings of the 7th International Conference on e-Infrastructure and e-Services for Developing Countries, AFRICOMM 2015, held in Cotonou, Benin, in December 2015. The 25 papers were carefully selected from 51 submissions and cover topics such as communication infrastructure, access to information, green IT applications and security, health.",,"Glitho R,Zennaro M,Belqasmi F,Agueh M",,2016,,,,Book
Global Information Society Watch 2014: Communications Surveillance in the Digital Age,"Communications surveillance in the digital age Online surveillance, security and privacy are concerns that have been central to human rights activists for years but with the recent revelations by former National Security Agency (NSA) contractor Edward Snowden of United States (US) government spying on citizens, the issues have reached global attention. This Global Information Society Watch (GISWatch) tracks the state of communications surveillance in 59 countries across the world countries as diverse as Hungary, India, Argentina, The Gambia, Lebanon and the United Kingdom. Each country report approaches the issue from a different perspective. Some analyse legal frameworks that allow surveillance, others the role of businesses in collecting data (including marketing data on children), the potential of biometrics to violate rights, or the privacy challenges when implementing a centralised universal health system. The perspectives from long-time internet activists on surveillance are also recorded. Using the 13 International Principles on the Application of Human Rights to Communications Surveillance as a starting point, eight thematic reports frame the key issues at stake. These include discussions on what we mean by digital surveillance, the implications for a human rights agenda on surveillance, the Five Eyes inter-government surveillance network led by the US, cyber security, and the role of intermediaries. These reports are published at a critical time: they show how rampant government surveillance is across the world, and how business is often complicit in this. They suggest action steps that civil society can take to push for a human rights framework for internet governance and to expose what until now has remained hidden.",,"with Developing Countries (Hivos) HI,for Progressive Communications (APC) A",,2014,,,,Book
Mad Dogs and Englishmen : Constructions of Uk State Identity in Internet News Reporting on the 2011 Libyan Civil War,"This thesis asks how UK state identity is constructed by UK internet news reporting of the 2011 Libyan civil war and the events immediately preceding it. Adopting a discourse-theoretic approach, my analysis examines news reporting on Libya between 15 January and 31 December 2011 in a politically balanced range of outlets: the BBC, The Daily Mail, The Guardian, The Telegraph and The Sun. The articles were selected using a variety of broad keywords relating to Libya, the UK and conflict or violence in order to examine the widest possible range of material. I conceptualise the re/production of state identity as a continuous process in which identity is constituted in relation to difference, relations which may be positive, ambivalent or negative. Key to the process of state identity production is the concept of threat. Risk is omnipresent, yet not all risks come to be recognised as ""threats"". In this thesis 'threat' is not taken to be an objective assessment of a situation but rather an interpretation of a particular subject, object or issue as dangerous. The designation of 'threats' is produced by and productive of two elements: the apparent source of the threat and the identity of the subject at risk. Several issues dominate reporting on the Libyan civil war and are presented as a 'threat' by UK news outlets: the past relationship between the UK and Libya, violence in Libya, migration and weapons of mass destruction. Using these 'threats' to structure my empirical chapters, I examine representations of UK identity and other related subject positions. I find that the UK is portrayed in four specific ways: as capable of making difficult decisions for the greater good, as a supporter of global democracy and human rights, as able to provide for its citizens through an established welfare state, and as a great power ready for military operations. The representation of the UK in these positive terms is underpinned by representations of other subject positions such as Muammar Gaddafi, the Libyan people and the Libyan state, which are variously presented as, for example, evil or childlike. This thesis contributes to the field by making use of an understanding of state identity not commonly applied to the United Kingdom and focusing upon how this subject is constructed. This approach is important because it allows us to consider what is taken for granted in discussions about the UK, especially its role in international politics, and illuminates the way in which power disciplines conceptualisations of UK state identity.",,Jester NA,,2020,,,,Ph.D. Thesis
Experimental Application of Machine Learning on Financial Inclusion Data for Governance in Eswatini,"An objectives of good governance is to increase capital base of small scale businesses (SSB) in order to encourage more investments and hence increase employment rate. Embracing good financial inclusion (FI) schemes in a country helps to ensure that entrepreneurs of SSB have access to financial services and hence meet their needs. In this paper we studied FI scheme in Kingdom of Eswatini with the view to establish the extent to which SSB have access to funds in running their businesses such that they could satisfy the target population and meet their desired goals. We got FI dataset for Eswatini for 2018 from Finscope database. Finscope 2018 dataset contains 1385 attributes with 2928 records. This study extracted attributes based on payment channel, registered/unregistered business, usage of commercial banks/insurance/mobile money and source of income for households from the Finscope database. We identified lot of missing data and hence replaced them using Mode method of preprocessing module in WEKA. We split the datasets and carried out cross validation on it. Training data is 80% of the datasets and 20% was used for testing. We carefully classified FI for selected parameters for Hhohho, Manzini, Shiselweni and Lubombo regions of Eswatini using Logistic regression with 80% for training and 10 fold cross-validation. The best 10 fold cross-validation recall rate for Manzini region using support vector machine (SVM) is 69.4% and 63.4% using logistic regression. These results show that veracity of FI dataset is weak and this is due to large number of missing data.",,"Akinnuwesi BA,Fashoto SG,Metfula AS,Akinnuwesi AN",,2020,414–425,10.1007/978-3-030-45002-1_36,https://doi-org.proxy.bnl.lu/10.1007/978-3-030-45002-1_36;http://dx.doi.org/10.1007/978-3-030-45002-1_36,Conference Paper
Strategies to Maintain Profitability When Crude Oil Prices Fluctuate,"Rapid and sustained fluctuations in the crude oil market have remained a threat to the financial performance of national and multinational oil and gas corporations. Such fluctuations can reduce the profitability of national and multinational oil and gas corporations. The purpose of this qualitative descriptive single case study was to explore strategies oil production leaders used to maintain profitability when crude oil prices fluctuate. The participants included 6 senior oil production leaders in a national oil corporation in Ghana who employed successful strategies to maintain profitability. Kraus and Litzenberger's trade-off theory of capital structure served as the conceptual framework for the study. Data collection methods included semistructured interviews, company documents, direct observation, and a reflective journal. Based on the methodological triangulation and the use of thematic data analysis technique, 3 broad themes emerged: relating to enhancing operational efficiency through organizational restructuring and competitive oil price hedging, business portfolio diversification through effective asset management and innovative technologies, and optimization of capital structure through debt restructuring. Oil production leaders would have to embrace the growth of artificial intelligence and the Internet of Things to improve the efficiency of business operations and maintain profitability. Oil production leaders might apply these findings to enhance business continuity, avoid bankruptcy, and maintain profitability during oil price downturns. Maintaining profitability would help ensure employees' job security and flow of income. Sustained income would benefit employees and their families and could have a positive social impact on employees' local communities.",,"Sulemana Y,Campo M,Dusick D",,2020,,,,Ph.D. Thesis
Annual Editions: Computers in Society 09/10,"Annual Editions is a series of over 65 volumes, each designed to provide convenient, inexpensive access to a wide range of current articles from some of the most respected magazines, newspapers, and journals published today. Annual Editions are updated on a regular basis through a continuous monitoring of over 300 periodical sources. The articles selected are authored by prominent scholars, researchers, and commentators writing for a general audience. The Annual Editions volumes have a number of common organizational features designed to make them particularly useful in the classroom: a general introduction; an annotated table of contents; a topic guide; an annotated listing of selected World Wide Web sites; and a brief overview for each section. Each volume also offers an online Instructor's Resource Guide with testing materials. Using Annual Editions in the Classroom is the general instructor's guide for our popular Annual Editions series and is available in print (0073301906) or online. Visit www.mhcls.com for more details. Table of contents AE_Computers in Society 09/10 Preface Correlation Guide Topic Guide Internet References Unit 1: Introduction Unit Overview 1. Five Things We Need to Know about Technological Change, Neil Postman, Address to New Tech '98 Conference, March 27, 1998 Neil Postman, a well-known cultural critic, suggests that computer technology is too important to be left entirely to the technologists. Embedded in every technology, he says, is a powerful idea. . . . 2. On the Nature of Computing, Jon Crowcroft, Communications of the ACM, February 2005 The author states, Occupying a third place in human intellectual culture, computing is not bound by the need to describe what does exist (as in natural science) or what can be built in the real world (as in engineering). 3. A Place for Hype, Edward Tenner, London Review of Books, May 10, 2007 A new golden age of technological hype seems to be dawning, Tenner writes. Despite all predictions, computers still can't tell whether an object is a hat, a chair, or a shoe. Unit 2: The Economy Unit Overview 4. Click Fraud: The Dark Side of Online Advertising, Brian Grow and Ben Elgin, BusinessWeek, October 2, 2006 Internet advertisers think they pay only when an interested customer clicks on their ads. Martin Fleischman, an Atlanta businessman, noticed a growing number of puzzling clicks coming from such places as Botswana, Mongolia, and Syria. 5. Online Salvation , Paul Farhi, American Journalism Review, December 2007/January 2008 In 2003, newspapers earned $1.2 billion through online services. By 2006, the figure had grown to $2.7 billion. Will the Internet save the beleaguered newspaper business 6. The Big Band Era, Christopher Swope, Governing, January 2005 Even as cities like Philadelphiaare working to transform the, entire city into a wireless hot spotwith government as the Internet service provider of last resortcommunications companies are fighting to keep local governments out of the broadband business. 7. The Beauty of Simplicity, Linda Tischler, Fast Company, November 2005 A simple tale about simplicity. One company hired an editor from People Magazine to translate accounting lingo into everyday language, pared back 125 setup screens to three, and sold 100,000 units in its first year on the market. 8. The Software Wars: Why You Can't Understand Your Computer, Paul De Palma, American Scholar, Winter 2005 The article argues that software development is like military procurement, and suffers many of the same woes, including excessive complexity and cost overruns. Unit 3: Work and the Workplace Unit Overview 9. National ID: Biometrics Pinned to Social Security Cards, Ryan Singel, Wired, May 15, 2007 Immigration is in the news again. One proposal before Congress is to issue American workers tamper-proof biometric Social Security cards. These would replace the text-only design that's been issued to Americans almost without change for more than 70 years. 10. Dilberts of the World, Unite!, David Sirota, The Nation, June 23, 2008 Faced with industry giants who outsource work to India on one hand and import lower cost engineers on the other, software developers have begun to organize. 11. Computer Software Engineers, Occupational Outlook Handbook, 2006/07 Edition Here is one official source that acknowledges the effect of shipping high tech jobs abroad, but still predicts that software engineers are projected to be one of the fastest-growing occupations from 2004 to 2014. 12. How Deep Can You Probe , Rita Zeidner, HR Magazine, October 2007 Tales of employers searching MySpace pages notwithstanding, Many states limit the extent to which employers can consider off duty conduct in making a hiring decision. . . . 13. Privacy, Legislation, and Surveillance Software, G. Daryl Nord, Tipton F. McCubbins, and Jeretta Horn Nord, Communications of the ACM, August 2006 The authors tell us that the assumption of employee privacy in the workplace may be naïve. Constitutional protections against unreasonable search and seizure usually apply only to state actions. 14. The Computer Evolution, Rob Valletta and Geoffrey MacDonald, FRBSF Economic Letter, July 23, 2004 This article uses data from several surveys to examine two key aspects of the computer evolution: the spread of PCs at work and the evolving wage differentials between individuals who use them and those who do not. Unit 4: Computers, People, and Social Participation Unit Overview 15. Back-to-School Blogging, Brock Read, The Chronicle of Higher Education, September 3, 2004 It should surprise no one that entering freshmen, who grew up using the Internet, should turn to university-sponsored blogs to ease the transition to college life. 16. Romance in the Information Age, Christine Rosen, The New Atlantis, Winter 2004 According to Rosen, our technologies enable and often promote two detrimental forces in modern relationships: the demand for total transparency and a bias toward the over sharing of personal information. 17. E-Mail Is for Old People, Dan Carnevale, The Chronicle of Higher Education, October 6, 2006 Reaching students through e-mail has become more difficult as students turn to text messaging and social networking sites. 18. Girl Power, Chuck Salter, Fast Company, September 2007 How does a seventeen year old run a million dollar web site 19. Bloggers against Torture, Negar Azimi, The Nation, February 19, 2007 Authoritarian regimes can't always operate in secret, now that bloggers are writing. Unit 5: Societal Institutions: Law, Politics, Education, and the Military Unit Overview 20. Piracy, Computer Crime, and IS Misuse at the University, Timothy Paul Cronan, C. Bryan Foltz, and Thomas W. Jones, Communications of the ACM, June 2006 Who are the students who openly admit to illegally installing software on home computers or otherwise misusing computer information systems This article provides some clues. 21. Can Blogs Revolutionize Progressive Politics , Lakshmi Chaudhry, In These Times, February 2006 Liberals have been envious ever since Richard Viguerie's computer-generated mailing lists contributed to Ronald Reagan's victory in 1980. At a time when even Senate Majority Leader Harry Reid has a blog, some Democrats hope that the computer is finally on their side. 22. Center Stage, Carl Sessions Stepp, American Journalism Review, April/May 2006 How does a newspaper's web version differ from the print version Unlike the print version of a newspaper, the Web version receives little editing. 23. The Coming Robot Army, Steve Featherstone, Harper's Magazine, February 2007 Within our lifetime, says Featherstone, robots will give us the ability to wage war without committing ourselves to the human cost of actually fighting a war. Sgt. Jason Mero concurs: These things are amazing. . . . They don't complain. . . . They don't cry. They're not scared. This robot here has no fear. 24. A Technology Surges, David Talbot, Technology Review, March/April 2008 Real live soldiers still fighting real live wars. A new on-ground reporting system, Google Maps for the Iraq counterinsurgency might help keep these soldiers safe. 25. Wikipedia in the Newsroom, Donna Shaw, American Journalism Review, February/March 2008 Whether professionals can cite a source that is collective and anonymous remains problematic. 26. E-Mail in Academia: Expectations, Use, and Instructional Impact, Meredith Weiss and Dana Hanson-Baldauf, EDUCAUSE Quarterly, January-March 2008 Studies have shown that there is a relationship between a student's success and the quality of one-on-one communication between teacher and student. What happens when you add e-mail to the mix Unit 6: Risk and Avoiding Risk Unit Overview 27. Why Spyware Poses Multiple Threats to Security, Roger Thompson, Communications of the ACM, August 2005 Harm caused by spyware ranges from gobbling up computer speed on your PC to enlisting your machine in attacks that can disrupt major businesses or the government. 28. The Virus Underground, Clive Thompson, The New York Times Magazine, February 8, 2004 Clive Thompson states, when Mario is bored . . . he likes to sit at his laptop and create computer viruses and worms. 29. False Reporting on the Internet and the Spread of Rumors: Three Case Studies, Paul Hitlin, gnovis, April 26, 2004 Internet news sources can sometimes be unreliable. Paul Hitlin examines Internet coverage of the Vince Foster suicide along with other stories to understand why this is so. 30. The New Right-Wing Smear Machine, Christopher Hayes, The Nation, November 12, 2007 Some e-mails that have gone viral in recent political campaigns. 31. A Growing Watch List, Karen DeYoung, The Washington Post National Weekly Edition, April 2-8, 2007 The Terrorist Identities Datamart Environment database contains information on over 450,000 persons, many of them U.S. citizens. What happens if there is an error Unit 7: International Perspectives and Issues Unit Overview 32. China's Tech Generation Finds a New Chairman to Venerate, Kevin Holden, Wired, May 24, 2007 The new China is not a place that would have made Chairman Mao comfortable. One indication is the popularity of Bill Gates. 33. Restoring the Popularity of Computer Science, David A. Patterson, Communications of the ACM, September 2005 While India turns out more and more programmers who are willing to work for a fraction of their American counterparts, enrollment in computer science classes across the United States is dropping. The author believes that inaccurate impressions of opportunities are behind the decline. 34. China's Computer Wasteland, Benjamin Joffe-Walt, The Progressive, January 30, 2005 What to do with the detritus of the digital age is a growing problem. Shipping it to China seems to be one solution. 35. In Search of a PC for the People, Bruce Einhorn, BusinessWeek, June 12, 2006 What features get included in a $200.00 PC marketed to developing nations and Nicholas Negroponte's remark, I think of digital access for kids as a human right, are two issues explored in this article. 36. In Korea, a Boot Camp Cure for Web Obsession, Martin Fackler, The New York Times, November 18, 2007 In a country where online gaming is a professional sport, up to 30% of South Koreans under 18 . . . are at risk of Internet addiction. 37. New Tech, Old Habits, Moon Ihlwan and Kenji Hall, BusinessWeek, March 26, 2007 Japan and South Korea are behind the United States when it comes to the productivity of information technology workers. Why The answer may be as simple as telecommuting. Unit 8: The Frontier of Computing Unit Overview 38. A Nascent Robotics Culture: New Complicities for Companionship, Sherry Turkle, AAAI Technical Report, July 2006 What is a robot kind of love and What will we be like, what kind of people are we becoming as we develop increasingly intimate relationships with machines MIT's pioneering sociologist tries to answer both questions. 39. Toward Nature-Inspired Computing, Jiming Liu and K. C. Tsui, Communications of the ACM, October 2006 Computer scientists are turning to biology as a source of inspiration for models of complex systems. These biological models change the rules governing systems behavior. 40. Google and the Wisdom of Clouds, Stephen Baker, BusinessWeek, December 24, 2007 Google is teaching researchers around the world how to extract patterns using clusters of computers that it calls, the cloud. Test-Your-Knowledge Form Article Rating Form",,De Palma P,,2009,,,,Book
High Performance Java Card Operating System,"Due to the fast evolving of trusted computing environments and internet-of-things an eager need has been established for open platforms which support interchangeable technologies to co-exist without threatening system's security. Certainly, future embedded applications will need high performance operating systems to support the intensive-computing algorithms required for satisfying acceptable response and secure the application inside the vulnerable open environment, hence, new inevitable requirements for embedded operating systems have arisen including hard real-time response, support for native applications, system openness and system scalability. This paper introduces a new design for secure and open smart card operating system, called ESCOS (Egypt Smart Card Operating System), based on the prevalent Java Card technology. The new design provides competitive characteristics in the main three factors of judging smart card platforms, namely, system security, supported technology and system response. In addition, ESCOS is designed to have high degree of modularity and re-configurability to meet fast-changing business needs and diverse hardware platforms.",,"Eletriby MR,Sobh M,Eldin AM,Fahmy HM",,2014,30–39,10.1109/SERE.2014.16,https://doi-org.proxy.bnl.lu/10.1109/SERE.2014.16;http://dx.doi.org/10.1109/SERE.2014.16,Conference Paper
"Information and Communication Technology for Development for Africa: First International Conference, ICT4DA 2017, Bahir Dar, Ethiopia, September 25-27, 2017","This book constitutes the proceedings of the First International Conference on Information and Communication Technology for Development for Africa, ICT4DA 2017, held in Bahir Dar, Ethiopia, in September 2017. The 31 revised full papers presented were carefully reviewed and selected from 72 submissions. The papers address the impact of ICT in fostering economic development in Africa. In detail they cover the following topics: e-services, natural language processing, intelligent systems, mobile and wireless communication, privacy and security.",,"Mekuria F,Nigussie EE,Dargie W,Edward M,Tegegne T",,2018,,,,Book
A Case Study of an African E-Government/e-Governance Development,"It is a widely held belief, that Information and Communication Technology (ICT) is a powerful and timely source of development aid. A development aid which could be misunderstood, especially if its limitations are not taken into consideration in its planning. When it comes to the use of ICTs for good governance, the literature has very little to say on successes and failures of e-Government applications resulting in e-Governance efficiency and effectiveness in developing and transitional countries.This paper studied Ghana's e-Government/e-Governance profile since the adoption of its ICT4AD Policy, 2003. The method used in this study was observation and content analysis of different government departmental websites and documents from development partners such as The World Bank and the United Nations amongst others. The most quoted source is the 2014 edition of the United Nations E-Government Survey, the latest in the research by the Division for Public Administration and Development Management (DPADM) of the United Nations Department of Economic and Social Affairs (DESA), as well as by many valued external experts, researchers and contributors from other organizations [34]. Another sampling used for this paper involved the choice of interviewing individuals who are most advantageously placed or in the best position to provide the information required.Findings indicated that there is a demand for online or e-Government services in Ghana; indeed, a handful of the Ghana Government departments are utilizing websites to provide certain government services to the citizens. However, since internet connectivity is not available to a majority of citizens, one can safely conclude that e-Government/e-Governance presence is somehow stunted. It is also glaringly clear that apart from infrastructural deficit (inadequate broadband connectivity), there is weakness in the enforcement of policy regime, and extremely weak ICT skills capability. It is therefore suggested that the Government of Ghana assists its citizens with available, accessible, and affordable internet services through capacity building, with a dynamic regulatory body and encouragement to use mobile technology instead of total reliance on computers.The value and implications of this study, since the author believes that it is the most up to date and comprehensive study of the country's level of ICT readiness for the delivery of government services on --line, suggest that an understanding of the current status of e-Government/e-Governance in Ghana can help policy makers recognise the importance of Ghana's future growth by pursuing ICT development of both public and private sector organisations with emphasis on mobile telephony.",,"Awotwi J,Amega-Selorm C",,2015,49–58,10.1145/2846012.2846040,https://doi-org.proxy.bnl.lu/10.1145/2846012.2846040;http://dx.doi.org/10.1145/2846012.2846040,Conference Paper
Investigating the Role of Sensor Based Technologies to Support Domestic Activities in Sub-Saharan Africa,"In sub-Saharan Africa (SSA), homes face various challenges including insecurity, unreliable power supply, and extreme weather conditions. While the use of sensor-based technologies is increasing in industrialized countries, it is unclear how they can be used to support domestic activities in SSA. The availability of low-cost sensors and the widespread adoption of mobile phones presents an opportunity to collect real-time data and utilize proactive methods to monitor these challenges. This dissertation presents three studies that build upon each other to explore the role of sensor-based technologies in SSA. I used a technology probes method to develop three sensor-based systems that support domestic security (M-Kulinda), power blackout monitoring (GridAlert) and poultry farming (NkhukuApp). I deployed M-Kulinda in 20 Kenyan homes, GridAlert in 18 Kenyan homes, and NkhukuProbe in 15 Malawian home-based chicken coops for one month. I used interview, observation, diary, and data logging methods to understand participants' experiences using the probes. Findings from these studies suggest that people in Kenya and Malawi want to incorporate sensor-based technologies into their everyday activities, and they quickly find unexpected ways to use them. Participants' interactions with the probes prompted detailed reflections about how they would integrate sensor-based technologies in their homes (e.g., monitoring non-digital tools). These reflections are useful for motivating new design concepts in HCI. I use these findings to motivate a discussion about unexplored areas that could benefit from sensor-based technologies. Further, I discuss recommendations for designing sensor-based technologies that support activities in some Kenyan and Malawian homes. This research contributes to HCI by providing design implications for sensor-based applications in Kenyan and Malawian homes, employing a technology probes method in a non-traditional context, and developing prototypes of three novel systems.",,"Chidziwisano GH,Bree Holtz,B. Jordan S,Eduardo Nakasone,Kurtis Heimerl",,2022,,,,Ph.D. Thesis
The International Conference on Advanced Machine Learning Technologies and Applications (AMLTA2018),"This book presents the refereed proceedings of the third International Conference on Advanced Machine Learning Technologies and Applications, AMLTA 2018, held in Cairo, Egypt, on February 2224, 2018, and organized by the Scientific Research Group in Egypt (SRGE). The papers cover current research in machine learning, big data, Internet of Things, biomedical engineering, fuzzy logic, security, and intelligence swarms and optimization.",,"Hassanien AE,Tolba MF,Elhoseny M,Mostafa M",,2018,,,,Book
Using Information Technology for an Improved Pharmaceutical Care Delivery in Developing Countries. Study Case: Benin,"One of the problems in health care in developing countries is the bad accessibility of medicine in pharmacies for patients. Since this is mainly due to a lack of organization and information, it should be possible to improve the situation by introducing information and communication technology. However, for several reasons, standard solutions are not applicable here. In this paper, we describe a case study in Benin, a West African developing country. We identify the problem and the existing obstacles for applying standard ECommerce solutions. We develop an adapted system approach and describe a practical test which has shown that the approach has the potential of actually improving the pharmaceutical care delivery. Finally, we consider the security aspects of the system and propose an organizational solution for some specific security problems.",,"Edoh TO,Teege G",,2011,1123–1134,10.1007/s10916-011-9717-y,https://doi-org.proxy.bnl.lu/10.1007/s10916-011-9717-y;http://dx.doi.org/10.1007/s10916-011-9717-y,Journal Article
E-Government Initiatives in Ethiopia,"E-Government, considered to be a narrower concept than e-Governance, offers a great potential and opportunity for developing countries to improve their governance and citizen satisfaction. E-Government reduces costs, increases transparency and citizen participation in decision-making processes, strengthens accountability, improves service delivery, etc. In the process of setting up e-Government initiatives and to become successful, there are some prime issues for developing countries to assess and to research. In developing nations where capital is a major scarcity, failing to be successful in the implementation of information communication technology (ICT) initiatives can have a great impact. In case of faller, the leadership commitment, which is already low in some countries, could be greatly affected.This will cause major obstacles to get some other ICT project's approved. ICT is a capital intensive (for example in infrastructure and application development) and requires qualified human resources. Apart from this, the globalization and dynamism associated with the field are making the realization of e-Government in particular in developing countries more challenging. Implementing e-Government principles and functions require a range of standards, guidelines, rules, policies and legislative changes which do not exist in most developing countries. These all add up to the challenges in e-Government implementation. The following are also potential threats in implementing e-Government: low level working culture, high resistance, weak private sector, low level collaboration/partnership between private and public sectors, etc.In this case study, I will assess the potential of e-Government projects and initiatives for developing countries by taking the case of the Federal Democratic Republic of Ethiopia. The opportunities will be widely discussed by taking practical examples. I will also identify and analyze major challenges that may be encountered in implementing e-Government initiatives based on my practical experience in Ethiopia.",,Belachew M,,2010,49–54,10.1145/1930321.1930332,https://doi-org.proxy.bnl.lu/10.1145/1930321.1930332;http://dx.doi.org/10.1145/1930321.1930332,Conference Paper
IWCMC '07: Proceedings of the 2007 International Conference on Wireless Communications and Mobile Computing,"On behalf of the Technical Program Committee, I welcome you all to the ACM International Wireless Communications and Mobile Computing Conference (ACM IWCMC 2007) in Turtle Bay Resort, Honolulu, Hawaii! I'm delighted that this year's ACM IWCMC accomplishes its goal under our conference theme ""The Future is Now---The New Era of Wireless Communications and Mobile Computing and Networking Technologies"" and continues its tradition of providing the premier forum for presentation of research results and experience reporting on the cutting edge research in the general areas of the wireless communications and mobile computing.This year, we received about 300 submissions from 32 countries. All papers received rigorous peer reviews from our Technical Program Committee (TPC), comprised of 41 Symposia Chairs/Co-Chairs and a total of 200 TPC members from academia, government laboratories, and industries. We also invited more than 470 external expert reviewers from all over the world. After carefully examining all the received review reports, the ACM IWCMC 2007 TPC finally selected 119 high-quality papers for presentation at the conference and publication in ACM IWCMC 2007 proceedings. The accepted papers come from United Kingdom, Canada, Germany, Australia, Taiwan, Korea, China, India, Japan, Portugal, Finland, Egypt, France, Ireland, Pakistan, Spain, Brazil, Italy, Iran, Norway, Sweden, Chile, Singapore, and the United States.The conference program starts each day with a keynote speaker given by the world-class leaders in the areas -- Dr. Robert E. Kahn, Professor Vijay K. Bhargava, Professor Aggelos K. Katsaggelos, highlighting the latest research trends in the wireless communications, mobile computing, and networks. A total of 27 technical sessions, organized in three parallel tracks, from the core of the technical program. This year, the technical sessions reflect the continued and growing interests in a wide range of spectrum, including wireless communications and networks, cross-layer design and optimization, mobile computing, wireless sensor networks, network security, information theory and applications.",,,,2007,,,,Book
Proceedings of the International Conference on Advanced Intelligent Systems and Informatics 2018,"This book presents the proceedings of the 4th International Conference on Advanced Intelligent Systems and Informatics 2018 (AISI2018), which took place in Cairo, Egypt from September 1 to 3, 2018. This international and interdisciplinary conference, which highlighted essential research and developments in the field of informatics and intelligent systems, was organized by the Scientific Research Group in Egypt (SRGE). The book is divided into several main sections: Intelligent Systems; Robot Modeling and Control Systems; Intelligent Robotics Systems; Machine Learning Methodology and Applications; Sentiment Analysis and Arabic Text Mining; Swarm Optimizations and Applications; Deep Learning and Cloud Computing; Information Security, Hiding, and Biometric Recognition; and Data Mining, Visualization and E-learning.",,"Hassanien AE,Tolba MF,Shaalan K,Azar AT",,2018,,,,Book
"Advances in Security of Information and Communication Networks: First International Conference, SecNet 2013, Cairo, Egypt, September 3-5, 2013","This book constitutes the refereed proceedings of the International Conference on Advances in Security of Information and Communication Networks, Sec Net 2013, held in Cairo, Egypt, in September 2013. The 21 revised full papers presented were carefully reviewed and selected from 62 submissions. The papers are organized in topical sections on networking security; data and information security; authentication and privacy; security applications.",,"Awad AI,Hassanien AE,Baba K",,2013,,,,Book
Inter-Organizational Study of Access Control Security Measures,"This study assesses the level of implementation and management of access control security measures among organizations. A survey was conducted and 233 responses were received from 56 organizations drawn from 5 major industry sectors of Ghana. This study focuses on the four access control clauses, namely access control policy, user access management, user responsibility and accountability, and system and application access control, which were adopted from ISO/IEC27002 international information systems security management standard. Overall, the results show that the organizations' level of implementation and management of access control measures were approximately 66.6% Level 3-well defined, indicating that access control measures were documented, approved, and implemented organization-wide. Moreover, the results show significant differences in the implementation and management of access control measures among the organizations. For all the access control measures, the financial and health care institutions outperform educational institutions and government public services.",,"Yaokumah W,Okai ES",,2018,60–79,10.4018/IJTHI.2018010104,https://doi-org.proxy.bnl.lu/10.4018/IJTHI.2018010104;http://dx.doi.org/10.4018/IJTHI.2018010104,Journal Article
A Framework for Integrating Geospatial Information Systems and Hybrid Cloud Computing,,,"Helmi AM,Farhan MS,Nasr MM",,2018,145–158,10.1016/j.compeleceng.2018.03.027,https://doi-org.proxy.bnl.lu/10.1016/j.compeleceng.2018.03.027;http://dx.doi.org/10.1016/j.compeleceng.2018.03.027,Journal Article
A LFM-Based Adaptive Wake-up Signal Detection Approach for Underwater Acoustic Communication System,"The paper focuses on wake-up mechanism for underwater acoustic communication (UAC) system. Wake-up mechanisms for UAC terminals play an important role in reducing the power consumption and extending the battery life. Compared with terrestrial wireless counterparts, the wake-up receivers for UAC terminals are challenged by the severe underwater acoustic channels, which are characterized by doubly-selective fading and low signal-to-noise ratio (SNR). Furthermore, the wake-up receiver is with weak processing ability. The paper proposes a wake-up mechanism named as channel-adaptive detection and joint decision (ChAD-JD). ChAD-JD uses linear frequency modulation (LFM) as wake-up signals. In order to increase the detection probability and reduce the probability of false alarm, the novel approach applies channel-adaptive detection and joint decision methods, respectively. Simulation and experimental results show that ChAD-JD is more reliable and effective compared with traditional LFM-based detection methods with a fixed threshold.",,"Li H,Wang D,Xie Y,Hu X",,2018,,10.1145/3291940.3291962,https://doi-org.proxy.bnl.lu/10.1145/3291940.3291962;http://dx.doi.org/10.1145/3291940.3291962,Conference Paper
SPOT Poachers in Action: Augmenting Conservation Drones with Automatic Detection in near Real Time,"The unrelenting threat of poaching has led to increased development of new technologies to combat it. One such example is the use of long wave thermal infrared cameras mounted on unmanned aerial vehicles (UAVs or drones) to spot poachers at night and report them to park rangers before they are able to harm animals. However, monitoring the live video stream from these conservation UAVs all night is an arduous task. Therefore, we build SPOT (Systematic POacher deTector), a novel application that augments conservation drones with the ability to automatically detect poachers and animals in near real time. SPOT illustrates the feasibility of building upon state-of-the-art AI techniques, such as Faster RCNN, to address the challenges of automatically detecting animals and poachers in infrared images. This paper reports (i) the design and architecture of SPOT, (ii) a series of efforts towards more robust and faster processing to make SPOT usable in the field and provide detections in near real time, and (iii) evaluation of SPOT based on both historical videos and a real-world test run by the end users in the field. The promising results from the test in the field have led to a plan for larger-scale deployment in a national park in Botswana. While SPOT is developed for conservation drones, its design and novel techniques have wider application for automated detection from UAV videos.",,"Bondi E,Fang F,Hamilton M,Kar D,Dmello D,Choi J,Hannaford R,Iyer A,Joppa L,Tambe M,Nevatia R",,2018,,,,Conference Paper
Creating Password Security Using Spark Authentication Secret for Data Privacy and Protection,"Most of the passwords created by individuals, business organisations and other institutions using other software application to ensure the privacy and protection of their data/information according to the research can be hacked by hackers which at the end cause a lot of financial loss and other disparities. In this paper we create password security system using Spark Technology for data privacy and protection. Secondly, we test the created password by selecting data from the various departments in Kumasi Technical University, Ghana, for the experiment to detect if the created password using Spark Technology can be hacked in order to have access to the data selected or not. The actual data selected for experiment are first and second semester examination results of 2016 academic. The password security system created is ""spark.authenticate.secret"" using Apache Spark Technology to guarantee data protection and privacy.",,"Boachie E,Li C,Aduamoah M",,2018,402–406,10.1145/3291842.3291843,https://doi-org.proxy.bnl.lu/10.1145/3291842.3291843;http://dx.doi.org/10.1145/3291842.3291843,Conference Paper
New Audio Encryption Package for TV Cloud Computing,"For any cloud computing (ClComp), encryption of multimedia is one of the main applications as cloud tries to maintain it in a good situation and protect from any tampering. This work provides a new technique for audio for TV cloud computing. Encrypting the audio signals is addressed based on chaotic map and the algorithm was tested using an audio tone (AT) to evaluate the performance. The software of encrypt audio using AT based on chaotic map is specially designed to meet the needs of ClComp of Egyptian Radio and Television Union (ERTU). The proposed software of ClComp of ERTU is practical in nature and aims to provide individuals with an understanding of how to create cutting-edge web applications to be deployed distributive across the latest hosting platforms of ClComp of ERTU, including public/hybrid ClComp of ERTU, peer-to-peer networks, clusters, and multi-servers.",,"Eldin SM,Khamis SA,Hassanin AA,Alsharqawy MA",,2015,131–142,10.1007/s10772-014-9253-5,https://doi-org.proxy.bnl.lu/10.1007/s10772-014-9253-5;http://dx.doi.org/10.1007/s10772-014-9253-5,Journal Article
Interoperability in the Heterogeneous Cloud Environment: A Survey of Recent User-Centric Approaches,"Cloud computing provides users the ability to access shared, online computing resources. However, providers often offer their own proprietary applications, interfaces, APIs and infrastructures, resulting in a heterogeneous cloud environment. This heterogeneous environment makes it difficult for users to change cloud service providers; exploring capabilities to support the automated migration from one provider to another is an active, open research area. Many standards bodies (IEEE, NIST, DMTF and SNIA), industry (middleware) and academia have been pursuing approaches to reduce the impact of vendor lock-in by investigating the cloud migration problem at the level of the VM. However, the migration downtime, decoupling VM from underlying systems and security of live channels remain open issues. This paper focuses on analysing recently proposed live, cloud migration approaches for VMs at the infrastructure level in the cloud architecture. The analysis reveals issues with flexibility, performance, and security of the approaches, including additional loads to the CPU and disk I/O drivers of the physical machine where the VM initially resides. The next steps of this research are to develop and evaluate a new approach LibZam (Libya Zamzem) that will work towards addressing the identified limitations.",,"Mansour I,Sahandi R,Cooper K,Warman A",,2016,,10.1145/2896387.2896447,https://doi-org.proxy.bnl.lu/10.1145/2896387.2896447;http://dx.doi.org/10.1145/2896387.2896447,Conference Paper
Adobe Photoshop Cs3 Extended for 3d and Video,"With the introduction of the Adobe Creative Suite 3, Adobe introduces a new flavor of its flagship imaging application: Adobe Photoshop CS3 Extended. Included in Adobe's CS3 Premium suites, this versatile application includes new features that extend Photoshop's reach into new markets, such as scientific imaging. But Photoshop CS3 Extended also includes new tools for the creative community that work with video footage and 3D animation. Author Chad Perkins, an established video presenter and classroom instructor in the fields of Photoshop, video, and 3D, brings the creative potential of these new features into focus with Adobe Photoshop CS3 Extended for Video and 3D. This book targets both Photoshop users who want to learn about video and 3D through the lens of their favorite application as well as video producers and 3D artists who need to see how Photoshop integrates into their workflow. Adobe Photoshop CS3 Extended for Video and 3D will cover such topics as: "" How to use Video Layers for sophisticated special effects "" How to apply camera effects and lighting on 3D objects "" How to exploit Photoshop's updated and improved Vanishing Point feature "" How to make Photoshop work with other applications such as Adobe Premiere Pro CS3 Adobe Photoshop CS3 Extended for Video and 3D will be the first on the market to look exclusively-and extensively-at Photoshop's new tools that address the ever-expanding fields of video production and 3D animation",,Perkins C,,2007,,,,Book
Voice Recognition Package for ERTU's Cloud,"This paper discusses the application of voice recognition for Egyptian Radio and Television Union's cloud. It is used as secure access to the cloud by authorized group (AuthGs). The voice of each member from AuthGs is watermarked using singular value decomposition and then encrypted by Chaotic map. The results are transmitted through channel under several conditions and received at the receiver side, and then the recognition rate is calculated for various extraction of watermarking after decryption method. It is done to find out the suitable technique for AuthGs to access the cloud and insure the security and privacy of the cloud. Many tests are performed to compare between the voice before and after process to grantee the high robustness of the signal from illegal eavesdrops or any abuse behaviour. The feature extraction is performed using artificial neural network to store it in a database to compare with.",,Serag Eldin SM,,2017,51–67,10.1007/s10772-016-9387-8,https://doi-org.proxy.bnl.lu/10.1007/s10772-016-9387-8;http://dx.doi.org/10.1007/s10772-016-9387-8,Journal Article
The Cloud Computing Adoption in Higher Learning Institutions in Kenya: Hindering Factors and Recommendations for the Way Forward,,,"Njenga K,Garg L,Bhardwaj AK,Prakash V,Bawa S",,2019,225–246,10.1016/j.tele.2018.10.007,https://doi-org.proxy.bnl.lu/10.1016/j.tele.2018.10.007;http://dx.doi.org/10.1016/j.tele.2018.10.007,Journal Article
Electronic Payment Adoption in the Banking Sector of Low-Income Countries,"Banks in low-income countries are launching e-banking services such as Internet banking, SMS banking, ATM banking, card banking, point of sales PoS and mobile banking. Among these planned services, ATM is the most matured service in many private and state owned banks in Ethiopia. ATM is a recent phenomenon in low-income countries ;, and is still being introduced in financial sectors in low-income countries Angeli, 2008; making investigation of factors of ICT technology adoption in low income countries timely. The authors test context specific applicability of UTAUT Unified Theory of Acceptance and Use of Technology model. The authors' analysis of primary data suggests general applicability of the modified UTAUT model in explaining factors and antecedents of technology adoption but also identifies significant differences in the moderating factors of gender and age. Depending on whether they are above or below the age of 30, Ethiopian consumers of banking services exhibit highly differentiated levels of service credibility and technology risk acceptance towards ATM banking. This suggests that banking services sector in low income countries may like to clearly delineate and appropriately differentiate their awareness and reach-out strategies to their customers who belong to one or the other age group. Furthermore, women in this study are found to perceive themselves as more susceptible to fraud and other security risks in ATM banking, suggesting that special design considerations be incorporated in the way locations of ATMs are selected and in the way ATM technology features are accessed to ally such fears. The authors' work also shows research directions where other scholars may investigate an otherwise much diffused technology adoption in the low income countries of the world.",,"Alemu T,Bandyopadhyay T,Negash S",,2015,27–47,,,Journal Article
Front Matter,"In the summer of 1956, John McCarthy organized the famous Dartmouth Conference which is now commonly viewed as the founding event for the field of Artificial Intelligence. During the last 50 years, AI has seen a tremendous development and is now a well-established scientific discipline all over the world. Also in Europe AI is in excellent shape, as witnessed by the large number of high quality submissions we received. About 600 papers and posters were registered for ECAI-06, out of which 550 were actually reviewed (501 paper submissions and 49 poster submissions). The program committee decided to accept 131 full papers, which amounts to an acceptance rate of 26.1%, and 75 posters (authors of full papers had the possibility to opt for acceptance as poster in case of rejectance as full paper).We received submissions from 43 different countries, and accepted papers from 25 countries. The following table shows the number of submissions and accepted papers per country, based on the contact author affiliation:Algeria 5 0Australia 19 9Austria 9 5Belgium 4 2Brazil 12 0Bulgaria 1 0Canada 13 4China 4 0Cyprus 1 0Czechia 4 0Egypt 1 0Estonia 2 0France 111 46Finland 3 2Germany 47 19Greece 15 4Hungary 1 0India 1 0Iran 5 1Ireland 14 9Israel 8 2Italy 83 36Japan 9 1Luxemburg 5 3Mexico 2 0Netherlands 26 12New Zealand 3 0Pakistan 1 0Poland 3 0Portugal 1 0Romania 4 1Russia 3 0Singapore 4 4Spain 32 5Slovenia 2 2Slovakia 3 3Sweden 5 5Switzerland 5 3Thailand 2 0Turkey 3 0UK 49 22USA 22 5Venezuela 1 1It is also interesting to look at the areas of the submitted and accepted papers/posters. We show both absolute numbers and percentage. The area information is based on the first two key words chosen by the authors:# submitted % # accepted %Case-Based Reasoning 10.5 1.9 0.5 0.2Cognitive Modelling 33.5 6.1 13 6.3Constraints & Search 54.5 10.0 26 12.6Distributed AI/Agents 107 19.6 36.5 17.7KR & Reasoning 141.5 25.9 55.5 26.9Machine Learning 83 15.2 29 14.1Model-Based Reasoning 32 5.9 8 3.9Natural Language 21.5 3.9 7.5 3.6Perception/Vision 6 1.1 2 1.0Planning and Scheduling 27 4.9 12 5.8Robotics 12.5 2.3 5 2.4PAIS 18 3.3 11 5.3In comparison with ECAI 2004, we see a strong increase in the relative number of submissions from Distributed AI/Agents and Cognitive Modelling. Knowledge Representation & Reasoning is traditionally strong in Europe and remains the biggest area of ECAI-06. One reason the figures for Case-Based Reasoning are rather low is that much of the high quality work in this area has found its way into prestigious applications and is thus represented under the heading of PAIS.The ECAI-06 best paper award, sponsored by Elsevier, goes to a machine learning paper:A Real Generalization of Discrete AdaBoost, by Richard Nock and Frank NielsenCongratulations! The best poster award, sponsored by IOS Press, will be decided after the poster sessions in Riva. The 10 best papers are invited to a fast track of the Artificial Intelligence Journal.A conference of the size of ECAI needs a lot of support from many people. At this point we would like to thank all those who helped to make ECAI-06 a tremendous success: the poster, workshop, PAIS and area chairs faced a heavy workload and they all did an excellent job. The PC members and additional reviewers came up with timely, high quality reviews and made the life of the PC chair as easy as it can be. Thanks also to Alex Nittka who ran the conference management software, to the PC chair's great relief. We also want to thank the many people involved in the local organization of the conference: there would be no conference without you.Our very special thanks go to a person who is no longer with us. Rob Milne, chair of ECAI-06 until he died of a heart attack close to the summit of Mount Everest on June 5th, 2005. He shaped this conference from the beginning, and we did our best to organize ECAI-06 in his spirit.June 2006, Gerhard Brewka, Silvia Coradeschi, Anna Perini, Paolo Traverso",,,,2006,i–xxvi,,,Conference Paper
"Information, Technology, and Innovation: Resources for Growth in a Connected World","A big-picture look at how the latest trends in information management and technology are impacting business models and innovation worldwideWith all of the recent emphasis on ""big data,"" analytics and visualization, and emerging technology architectures such as smartphone networks, social media, and cloud computing, the way we do business is undergoing rapid change. The right business model can create overnight sensationsthink of Groupon, the iPad, or Facebook. At the same time, alternative models for organizing resources such as home schooling, Linux, or Kenya's Ushihidi tool transcend conventional business designs. Timely and visionary, Information, Technology, and the Future of Commerce looks at how the latest technology trends and their impact on human behavior are impacting business practices from recruitment through marketing, supply chains, and customer service.Discusses information economics, human behavior, technology platforms, and other facts of contemporary lifeExamines how humans organize resources and do work in the changing landscapeProvides case studies profiling how competitive advantage can be a direct result of innovative business models that exploit these trendsRevealing why traditional strategy formulation is challenged by the realities of the connected world, Information, Technology, and the Future of Commerce ties technology to business and social environments in an approachable, informed manner with innovative, big-picture analysis of what's taking place now in information strategy and technology.",,Jordan JM,,2012,,,,Book
Can Phones Build Relationships? A Case Study of a Kenyan Wildlife Conservancy's Community Development,"Wildlife conservancies across the globe are increasingly recognizing their need to support their surrounding communities to sustainably operate. Rapidly shifting environmental and sociopolitical climates increasingly stress existing resource and service provisions, forcing wildlife conservancies to co-manage with local communities shared resources like water, wildlife, soil, pollinators, and security. This work presents a case study in Laikipia, Kenya on Ol Pejeta Conservancy's use of text-based technologies to provide services and build relationships with the many widely-dispersed communities on its borders. Through technology deployments, staff interviews, and community focus groups, we investigate a potential role for basic mobile phone services, like SMS and USSD, to help conservancy personnel disseminate accurate and timely information, gather community feedback, address grievances, and improve accountability. Our findings show that communication with locals requires intense and ongoing effort from conservancy staff. Partially successful deployments of phone services provide a proof-of-concept for their utility in community relations but highlight particular design challenges for wildlife conservancies; having critical needs for broad inclusive engagement; clear, deliberate communication; and careful trust-building.",,"Ziegler M,Wack M,Ingutia N,Muiruri I,Njogu N,Muriithi K,Njoroge W,Long J,Heimerl K",,2020,219–230,10.1145/3378393.3402279,https://doi-org.proxy.bnl.lu/10.1145/3378393.3402279;http://dx.doi.org/10.1145/3378393.3402279,Conference Paper
Fusing Open Source Intelligence and Handheld Situational Awareness: Benghazi Case Study,"This paper reports the results and findings of a historical analysis of open source intelligence (OSINT) information (namely Twitter data) surrounding the events of the September 11, 2012 attack on the US Diplomatic mission in Benghazi, Libya. In addition to this historical analysis, two prototype capabilities were combined for a table top exercise to explore the effectiveness of using OSINT combined with a context aware handheld situational awareness framework and application to better inform potential responders as the events unfolded. Our experience shows that the ability to model sentiment, trends, and monitor keywords in streaming social media, coupled with the ability to share that information to edge operators can increase their ability to effectively respond to contingency operations as they unfold.",,"Boleng J,Novakouski M,Cahill G,Simanta S,Morris E",,2014,1421–1426,10.1109/MILCOM.2014.158,https://doi-org.proxy.bnl.lu/10.1109/MILCOM.2014.158;http://dx.doi.org/10.1109/MILCOM.2014.158,Conference Paper
Recognition System for Libyan Vehicle License Plate,"Automatic license plate recognition system plays an essential role in real life applications, especially those related to security and traffic managements. It essentially extracts and recognizes number plate information from videos or captured images of the targeted vehicle. Vehicle license plates differ from one country to another and because of this the effectiveness of implementing any particular method or system varies based on the plate type. In this study, we present an automatic detection, segmentation and recognition system for Libyan vehicle license plates. The main challenge in this work is our determination to use images of real vehicle plates in Libya, and the majority of these plates are not in a good condition because of poor vehicle maintenance. Three different approaches were used in the proposed system as follows: (1) Projection histogram based approach is used to locate the authorized plate license; (2) Connected component analysis based approach is used to segment the plate characters; (3) The template matching based approach is used to recognise the extracted characters. The proposed system was tested on 200 vehicle images varying in illumination conditions and backgrounds. The detection accuracy of the implemented system was 87%, the segmentation accuracy was 90% and the recognition accuracy was 86%.",,"Almabruk TA,Almaghairbe R,Bukewitin T,Roper M",,2021,,10.1145/3492547.3492595,https://doi-org.proxy.bnl.lu/10.1145/3492547.3492595;http://dx.doi.org/10.1145/3492547.3492595,Conference Paper
Architectural Choices for MHealth Services in Finland and Cameroon,"In the developed world health care sector has used for tens of years information systems in their daily activities. Soon after the Internet usage began to spread to the societies mid 1990'ies the sector began to ponder how to offer services over the web to patients. The USA and the EU began to discuss eHealth scenarios, and many services are now offered, including appointments to doctors and nurses, virtual doctors, and electronic prescriptions. Electronic patient records are widely utilized could also be retrieved by patients and other health care personnel, but due to technical, organizational and legal restrictions this is not yet reality. Mobile networks and high-end mobile terminals are as powerful as laptops were a few years ago and could be used to access and store pertinent health information. In this article we discuss architectural choices that would facilitate the usage of the collected personal health care information, and their pros and cons. We mainly base our findings on the situation in Finland. We contrast, though, the findings with the situation in Cameroon, where the ICT infrastructure is weak and the ICT infrastructure in the health care could more strongly rely on mobile networks and emerging smart terminals.",,"Veijalainen J,Hara V,Bisong B",,2011,46–51,10.1109/MDM.2011.32,https://doi-org.proxy.bnl.lu/10.1109/MDM.2011.32;http://dx.doi.org/10.1109/MDM.2011.32,Conference Paper
The Worth of Water: Evaluating Interventions to Achieve Financially and Operationally Sustainable Urban and Rural Water Systems in Low-Income Settings,"""We only know the worth of water when the well is dry,"" said Benjamin Franklin, quoting an old English proverb. He might have been talking about how the world is not on track to achieve the Sustainable Development Goals (SDGs) for clean water, despite clear evidence that investments in sustainable and resilient water and sanitation services pay for themselves by saving lives. Inadequate access to safe water, sanitation, and hygiene (WASH) causes 829,000 annual deaths globally, 1.9% of the global burden of disease, and economic losses estimated at $260 billion annually (WHO 2012, WHO 2016). In East Africa, severe and persistent drought periods interspersed with flooding have plagued the region for years, affecting water access and the health and livelihoods of millions. Unreliable and unsafe surface water increases the dependence of rural populations in Ethiopia on expensive mechanized boreholes to extract groundwater. However, borehole performance is sub-par without adequate resource allocations, effective monitoring of borehole functionality, and reliable maintenance services. Runtime sensors attached to 185 boreholes in the Afar Region of Ethiopia report that breakdowns occur 3-6 times a year, downtimes last for months, and uptimes range from 60 to 80 percent. Piped services managed by water utilities face similar reliability issues. There are significant budget gaps for capital maintenance globally, making it difficult for water utilities to conduct needed upgrades and repairs to reduce non-revenue water and improve service reliability and coverage. In four case water utilities studied representing low-income, middle, and high-income countries, maintenance budgets were insufficient to keep on top of repairs and replace assets at end of life, which will lead to declining service levels. In some cases, water services are partly or mostly subsidized, but not enough to fill funding gaps. To fill funding gaps and sustain what is already built, national governments and international donors should acknowledge that tariffs will not be able to cover the life cycle costs of water services. Subsidizing ongoing operations and maintenance (O&M) will continue to be critical to improving service quality and provider performance. The water sector is a system made up of actors from governments, regulators, and funders. Factors inside the system boundary that support sustainability include governance, financing, policy, regulations, technical capacity, environmental considerations, and accountability mechanisms. Together, these actors and factors form a complex system with non-linear and disproportional feedback relationships. For example, when actors fail to coordinate and plan asset management together, the ad-hoc maintenance systems they develop not only reach sub-par functionality levels but waste resources. In Afar, poor quality construction, maintenance, and management of water schemes made the region a candidate for development interventions in asset management and real-time monitoring. However, implementers failed to consider how merely collecting more monitoring data without understanding the data usage and support requirements would impact under-resourced staff. In this work, system dynamics modeling is used to quantify how increased support for O&M could turn around sub-par service delivery models by prioritizing capacity building and long-term reliability over constantly falling behind on fixing failures. Sub-par water service delivery significantly impacts the most vulnerable and last-mile communities. The rural pastoral population in Afar has been poorly served by policy establishing community management bodies to operate and maintain complex motorized borehole systems. This institutional model does not work without external support or in places where populations are mobile, leading to low functionality levels. A survey of households in Mille Woreda, Afar finds that low borehole reliability and daily usage are correlated with higher water insecurity and emotional distress. This proves that increased safe water consumption makes more of a difference to water security and well-being than does mere access to an unreliable source. We need to shift aid and development funding for WASH from the failed service delivery model of project cycles that build it and forget it to one that makes the best use of funds to build sustainable services. Will systems approaches to sustainability or climate-resilient services for water security become the next casualties in the grave of sector buzzwords? Or will nations significantly step up contributions and pay reparations towards public life-giving services, as promised by the human right to water? All that can be said is that - partly - the approach doesn't matter. Water sector actors understand the problem and want real solutions that address the lack of available resources, not buzzwords. What matters is supporting service providers with the funds to build their capacity according to their needs in the long term.",,"Libey AK,Amy Javernick-Will,Karl Linden,Jeffrey Walters,John Butterworth",,2022,,,,Ph.D. Thesis
Design and Implementation of a Web-Based University Voting Sytem,"Research Paper (undergraduate) from the year 2017 in the subject Computer Science - Software, Moi University (School of Information Sciences), course: Information Sciences (IT Option), language: English, abstract: An Online Voting System is a web based system that facilitates the running of elections online. Most higher learning institutions in Kenya conduct elections regularly in order to elect a student leadership to elect them. The elections conducted are mainly manual hence they are marred with irregularities which usually affect negatively the results of the election. In this era of advanced technology where online systems boosts work speed, reduces mistakes and promote the generation of accurate results, having a manual system like the paper-based version becomes a misfortune. An online system, which involves procedures like registration of voters, vote casting, vote counting, and declaring results etc. would constitute a good solution to replace current system that is in the universities in Kenya. Online systems have the advantage of providing convenience to the voter and reduce the time wasted in the queuing process at election centers and also promote security in the voting process. This paper hence describes the UVS which is a web-based online voting system that helps facilitate voting on the internet by providing a platform students are provided with an online registration form which requires them to register as voters, and then the details filled on the form are submitted in the database which then approves the user who can then login into the UVS and cast their vote. The UVS was developed using the waterfall model due to the adaptive nature of web based applications and the system proved that a computerized solution is possible with elimination of human related faults that are a commonplace in employment of human clerks to manage the election process. This paper has proposed the basic structure of the system and its functionality which can be employed",,Kaguru S,,2018,,,,Book
Agent-Based Simulation of Local Soy Value Chains in Ghana,"The assessment of changes in the relationships between supply chain agents is considered fundamental for market transformation. This paper reports on the application of a Value Chain Lab that supports the measurement of behavioral change in vertically structured supply-chain relationships. A participative gaming approach is used that enables to identify changes in mutual trust, transaction costs and risk behavior that result from value chain support and co-operation. The Value Chain Lab comprises value chain analysis, value chain games and multi-agent simulation. The paper describes the multi-agent simulation of a soy value chain in northern Ghana. The research was conducted in the context of the 2SCALE program, aiming to improve rural livelihoods and food and nutrition security in a number of African countries by developing agricultural supply chains including local smallholder farmers. The study confirms the positive effects of trust and loyalty in value chain relationships. Furthermore, it demonstrates the usefulness of agent-based simulations for exploring potential consequences of alternative interventions.",,"Verwaart T,Dijkxhoorn Y,Plaisier C,van Wagenberg C",,2019,654–666,10.1007/978-3-030-30244-3_54,https://doi-org.proxy.bnl.lu/10.1007/978-3-030-30244-3_54;http://dx.doi.org/10.1007/978-3-030-30244-3_54,Conference Paper
"Simulating Corn Supply, Demand and Consumption in Egypt: A System Dynamics Approach","Modeling the corn supply chain to satisfy population needs is a challenge, since corn is considered an essential food crop for achieving food security in many countries. Egypt is one of the biggest importers of corn in the world, since its domestic production is not sufficient to satisfy its population needs. A simulation model is built to represent the problem of local corn supply in Egypt, through studying the population demand, the capability of purchase from the markets, the production and import of corn. The proposed model provides good basis for showing the real life cycle of corn during the period (1990-2010) and projects the future until year 2030. It indicates gradual increase in population number so as the demand and imports. Finally, set of policies are applied to manage and improve production and import processes.",,"Khodeir MH,Abdelsalam HM",,2016,14–20,10.1145/2908446.2908484,https://doi-org.proxy.bnl.lu/10.1145/2908446.2908484;http://dx.doi.org/10.1145/2908446.2908484,Conference Paper
"New Adaptation Linkages: Perception, Preferences and Obstruction to Banking Technology in Kenya","Web technology is transforming all businesses into information-based activities and the rate of technological change is so high that emerging electronic commerce is already making fundamental changes in the economic landscape, affecting every aspect of how business is and will be conducted. There is substantial evidence to suggest that e-banking is being embraced by financial institutions in developed and emerging markets to the extent that explosive growth is almost at hand. This paper explores the adoption linkages in customer perceptions, preferences, and barriers to the adoption of banking technology in Kenya. The data for this study has been collected from bank customers in using well structured and pre-tested questionnaires. The result indicated that necessity perception is positive and ATM, Mobile and Internet banking were mostly preferred compared to others whilst negative linkage was observed between barriers and adoption as Security, bank lag in adoption and unawareness were identified as the most reported barriers to adoption. It means that positive link enable adoption while negative link distract adoption. The findings implicate banks and other financial institutions to increase the campaigns which may develop positive customer perceptions and preferences at the same time look for alternatives of reducing the possible barriers posing the banking technology.",,"Nyangosi R,Nyariki KO,Nyang'au SN",,2012,62–77,10.4018/jitpm.2012040104,https://doi-org.proxy.bnl.lu/10.4018/jitpm.2012040104;http://dx.doi.org/10.4018/jitpm.2012040104,Journal Article
Web Information Management: A Cross Disciplinary Textbook,"Summary This is a cross-disciplinary text book on web-based information management for students, faculty and practitioners (in business, industry and government). The Web has emerged as a universal space of information, occasioning proliferation of electronic publications. Though efforts have been made in developing tools and methods such as search engines, metadata, portals, subject directories and subject gateways aimed at enhancing the organization of and accessibility to information on Web, more remains to be done. The book addresses gaps in the existing Web-based tools and methods for information management. Key Features 1.Cross disciplinary - e.g. information science, information systems, computer science, business and records management 2.Addresses topical issues in web information management - such as content management, e-records readiness, e-government, portals and intranets, open source software, emerging technologies-WiMax, Bluetooth, etc 3.Targets audience in tertiary education, government, business and industry The Authors Dr Stephen Mutula is a senior lecturer at the Department of Library and Information Studies, University of Botswana.He teaches in the areas of web-based information systems. He won three professional awards of excellence in 2002 and 2005 from the Emerald Literati Club (UK) and another award from the Standing Conference of East and Southern Africa Librarians (SCECSAL) in 2000 for research and scholarly publications. Dr Justus W. Wamukoya is currently senior lecturer, Department of Library and Information Studies, University of Botswana. He teaches in the area of records and archives management. He has carried out a wide range of consultancy and research work in records and archives management, and has widely published on the subject.Readership Students and faculty in tertiary education, practitioners in business and industry as well as in government Contents Internet and World Wide web applications Information and knowledge management Content management E-records management Electronic mail management Digital literacy E-government Electronic publishing Intranets and web portals Web-based services Organising internet resources Open source software Emerging information and communication technologies and applications Information security E-readiness assessment and information assets Managing copyright in a digital environment Disaster management in a digital environment Conclusion Bibliography Index",,"Mutula MS,Wamukoya MJ",,2007,,,,Book
Applications of Zero-One Linear Programming with Multiple-Choice Constraints,"Linear programs in zero-one, or in mixed zero-one and continuous, variables with multiple-choice constraints (i.e., with generalized upper bounding structure or GUB for short) are one of the most frequently used tools of Operations Research. In this dissertation, we investigate algorithms to solve such programs in three fields of application: reliability optimization, pipeline network design and parallel computing. Large reliability optimization problems in series-parallel or in complex system are difficult to solve. A Tabu Search heuristic is provided to determine the optimum or near-optimum number of redundant components in each subsystem. For the case of series-parallel systems redundancy optimization can be expressed as a zero-one linear program with GUB structure. The algorithm of Dantzig and Van Slyke for linear programming with GUB structure is extended to the case of zero-one variables. Several types of penalties, involving simultaneous fixation of basic and non-basic variables are investigated. The resulting algorithm allows to solve rapidly problems with several hundred variables.We also study two-terminal and all-terminal reliability of general networks, with independent probabilities of failure on the arcs, using Boole-Bonferroni inequalities.The oil pipeline network problem (closely related to the one-terminal TELPAK problem in telecommunications) consists in determining the layout and diameters of the pipes of a network which connect a given set of offshore platforms and onshore wells to a port. A specialized implicit enumeration algorithm, extending that one used for reliability optimization, is designed. Two new types of valid inequalities exploiting geometric properties of feasible solutions are defined. Finding those of the first type amounts to solving multiple choice knapsack problems and those of the second type to enumerating spanning trees. Algorithms and implementation for counting and enumerating spanning trees of graphs are included. A large application is made with simulated data from south Gabon oil field.Assignment of the modules of a parallel program to the processors of a multiple computer system has been studied by Bokhari. He proposed algorithms to solve optimally the following problems: (i) partition chain-structured parallel or pipeline programs over chain-connected systems; (ii) partition multiple chain-structured parallel or pipelined programs over single-host multiple-satellite systems; (iii) partition multiple arbitrarily structured serial programs over single-host multiple-satellite system; (iv) partition single tree-structured parallel or pipelined programs over single-host multiple identical satellites systems. Algorithms with reduced computational complexity are provided for all four problems.",,Lih KW,,1993,,,,Ph.D. Thesis
Managing Globally with Information Technology,"From the Publisher:As the world economy becomes more interdepent and competition more global, the information technology management challenges of enabling the global marketplace must be met with innovative solutions. Covering both technological barriers and managerial challenges, this discussion includes international issues such as managerial experiences in Brazilian hotels, competition in the Asian automotive industry, e-business in Thailand, and job security in Egypt. A business-model handbook for the challenges faced by developing nations is also provided. Author Biography: Sherif Kamel is an assistant professor of MIS and associate director of the Management Center at the American University in Cairo. He is the associate editor of the Annals of Cases on Information Technology Applications and Management in Organizations .",,Kamel S,,2003,,,,Book
The Transition to Sustainable Construction in Botswana : A Multi-Level Perspective,"Interest in sustainable construction has grown in developing countries. The Government of Botswana has over the years introduced a number of initiatives with the potential to transform the construction sector to sustainability. It is noted from a socio-technical perspective that transition depends on changes in technologies, infrastructures, institutions, social practices, markets and regulations among others. The multi level perspective (MLP) approach draws attention to ways in which these configurations change to fulfil societal functions. It has been used in the western world in historical case studies to trace socio-technical changes. What is less clear is how a combination of discrete initiatives can support a transition in a less economically developed country. The multi-level perspective (MLP) has been used to study transition from one socio-technical system to another. MLP in this study is applied to a single sector; the construction sector in Botswana, to analyse the contribution of a combination of different initiatives to an ongoing sustainable transition. These include a demonstration project, environmental impact assessment and a new construction board. Opportunities and challenges that have/are being created by the initiatives are examined in the process. A mixture of in-depth interviews and document analysis were used to study the impact of the initiatives on the construction sector, the reasons for these effects and their implications for the transition to sustainable construction. It has been found that the initiatives influenced on-going efforts towards professionalization in the construction sector. Other effects include introduction of new governance techniques from abroad, produced a valuable ecological profiling of the country and transferring knowledge and technology both within and outside of BOTEC. The MLP focus on visions, learning and development of social networks in transitions have been used to help account for these outcomes. The analysis identified a number of opportunities that the initiatives introduced, but which have yet to be fully exploited. The contribution of the thesis is a reflection on the potential of discrete innovations to contribute to a transition and the use of MLP to study an emergent transition in the construction sector in Botswana.",,Ntshwene K,,2019,,,,Ph.D. Thesis
PTZ-Surveillance Coverage Based on Artificial Intelligence for Smart Cities,,,"Eldrandaly KA,Abdel-Basset M,Abdel-Fatah L",,2019,520–532,10.1016/j.ijinfomgt.2019.04.017,https://doi-org.proxy.bnl.lu/10.1016/j.ijinfomgt.2019.04.017;http://dx.doi.org/10.1016/j.ijinfomgt.2019.04.017,Journal Article
"Revolution, Global Development and Disability Politics in Egypt","The Egyptian Revolution and its consequences mainstreamed disabled people's demands into national policies. It opened further opportunities for self-organisations, as well as impacting upon tactics and activism. Egypt's ratification of the UNCRPD three years earlier had been a driving force towards the advocacy for inclusive rights. The global move from the Millennium Development Goals (MDGs) towards the Sustainable Development Goals (SDGs), five years after the Revolution, was a further factor exploited by the disabled people's movement to increase the recognition of disability within Egyptian policies. This thesis examines the influence of global and national events on the disabled people's movement's participation in government policy consultations and considers how this situation differed from the pre-revolutionary period. It also analyses the unique unity established by disabled activists', determining whether this shift was temporary, only lasting the 18 days of the Revolution, or more permanent. Finally, it examines Disabled People's Organisations' (DPOs) satisfaction with the mainstreaming of their demands within post-revolutionary policies, such as the 2014 Constitution. This research ties together literature on the Egyptian Revolution and global disability discourses, to underpin recommendations for Egypt to benefit from the application of an inclusive approach to its Sustainable Development Strategy. It is the first research to utilise the ongoing discussions about SDGs within the post-revolutionary era, with disability in mind. In answering the research question – how, and to what extent, has the Egyptian Revolution affected national conditions for the implementation of international agreements on disability rights (UNCRPD) and sustainable development (SDGs) – this qualitative research relies on document analysis, semi-structured interviews, and focus groups as data generation methods. Research participants include: Egyptian politicians, including disabled MPs, DPOs, and UN representatives. The same methods were applied to analyse Egyptian government and civil society development projects. A major finding of this thesis is the positive influence of the UNCRPD on increasing the number of DPOs, who then had more ability to advocate for their rights. Both the Egyptian Revolution and SDGs contributed to their collective participation within national policies.",,Attia M,,2019,,,,Ph.D. Thesis
Improving Design & Usability of Interactive Vulnerability Mapping Tools for Global Health Preparedness,"The ability of organizations and governments to anticipate disease outbreak risks and respond to emergent threats, commonly known as global health preparedness, presents both a challenging opportunity and an urgent imperative for public health informatics interventions. An example is the need to address the public health risks of vector-borne and zoonotic disease (VBZD) outbreaks, as understanding and preparing for such multifactorial events involves the careful integration of human, animal, entomological, environmental, and infrastructure data. The integration, presentation, and understanding of this data, and associated risks, demands usable tools and technology. Visualization can be a useful way to apply systems thinking to such problems. Unfortunately, existing visualization tools frequently do not assess whether they meet the needs of their users and do not incorporate best practices championed by human centered design (HCD). In my dissertation research, I propose design recommendations for visualization tools to help decision makers in global health preparedness identify spatial areas that are vulnerable to outbreaks, meaning better awareness in areas at a relatively high risk for VBZD outbreaks and a lower capacity to contain spread.Spatial Systems for Decision Support (SSDS) are a type of visualization tool that enable public health practitioners to make critical decisions informed by timely access to pertinent, analyzed data. In my research, I propose a new type of SSDS, interactive vulnerability mapping tools. This new tool can provide critical, rapid support to decision makers and practitioners in global health. Decision makers include epidemiologists, public health planners, vector control specialists, and directors, each of whom might use this information to allocate vaccine resources or plan intervention activities to high-risk regions.In my dissertation research, I have applied principles of human-centered design (HCD) and data visualization to design and evaluate the usability of interactive vulnerability mapping tools for dengue vulnerability in Peru (Aim 1) and Rift Valley fever vulnerability in Kenya (Aim 2). To situate my Aims 1 and 2 in the context of existing literature, I conducted a scoping review of interactive vulnerability mapping tools for VBZD preparedness (Aim 3) that describes current literature by characterizing data, users, technology, and use cases. I then compare findings from Aims 1 and 2 to the existing literature to identify gaps and inform design recommendations for future work. This work contributes: 1) usable interactive vulnerability mapping tools designed with public health decision makers in Peru and Kenya; 2) empirical data on the design, data visualization preferences, usability, and acceptance of interactive vulnerability mapping tools for VBZD vulnerability in global health settings; and 3) design recommendations for interactive mapping tools for VBZD informed by a scoping review of the literature and findings from Aims 1 and 2. This research will advance the fields of global health and pandemic preparedness, human computer interaction, and data visualization. It provides evidence to suggest that interactive vulnerability mapping tools hold the potential to more effectively prepare for and prevent VBZD outbreaks when they are designed and evaluated with purposeful user engagement.",,"Snyder LE,Peter Rabinowitz,Nancy Puttkammer,Uba Backonja,Chris Adolph",,2021,,,,Ph.D. Thesis
The Effect of Leadership on Corporate Governance Through the Integration of Corporate Social Responsibility : Perspectives from the Boards of Directors and Chief Executive Officers of the Libyan Commercial Banks,"Corporate Governance (CG), Corporate Social Responsibility (CSR) and leadership have become such related concepts that each concept is a supplement and/or a complement to the other. CG, for example, has come to govern the principle-agent relationship and ensure the satisfaction of various stakeholders' interests. CSR demands organisations to provide economic, social and environmental justice where they operate. Leadership is the ability of those with power to influence and guide followers to achieve common goals and simultaneously comprehend and respond bravely and constructively to the most critical economic, social and ecological challenges. Thus, once employed altogether effectively, these concepts establish healthy relationships with various stakeholders, gain competitiveness and generate corporate profits. However, these concepts have been recently under investigation in Libyan society. Scholars and private and public institutions have commenced examining the possible resolutions CG, CSR and leadership can arrange to alleviate the economic, social and environmental concerns the war has produced. Thus far, levels and efforts to apply CG codes have varied in Libyan companies. CSR is a new organisational culture that still designs its route within the Libyan commercial banks (LCBs). Leadership in the LCBs requires transformational and transactional styles to detach itself from any notion of dictatorship and autocratic traits. Therefore, this study tried to link the three concepts CG, CSR and leadership by exploring the impact of the board and executive leadership on CG framework to integrate CSR strategies in the LCBs. To achieve its goal, the study relied on three main questions to examine: First, to what extent does the board and executive leadership involve in the formation and application of CG structure? Second, to what extent does the board and executive leadership, through TSL and TFL styles, facilitate the application of CSR activities in the LCBs in light of the current Libyan crisis? Third, how does the board and executive leadership influence CG regulations to include CSR agendas in the LCBs to manage the issues caused by the current Libyan crisis? To answer these main questions, it was necessary to create a group of sub questions that investigated: First, the meanings, the current situations and challenges of CG, CSR and leadership? Second, the nature of relationship between CG and CSR in the LCBs? Third, the position of the TSL and TFL styles in the LCBs? In regard with the methodology, the study adopted the qualitative approach and conducted semi-structured interviews to collect the required data from 21 BMs and CEOs in 12 public and private LCBs. Then, the data was analysed by using grounded theory approach and thematic analysis method. The study provided some findings that answer the sub and main research questions starting with the sub question the findings showed that CG has narrow and wide definitions in the LCBs. Besides, the findings indicate that CSR is mainly encouraged and directed by religion and is centred on the notion of charity, and hardly touch upon achieving sustainable development. CG and CSR have a mutual relationship. Moreover, the findings demonstrate that Leadership features in the LCBs are under the influence of three factors, including the legacy of the previous regime, the Islamic culture and the Western perspective of leadership. Nonetheless, the leadership's attempts to make economic and social changes seem difficult due to some challenges such as the weak of knowledge in regard with CG and CSR in the LCBs, the weak CG Enforcement system in addition to the Confusion between CSR and the notion of philanthropy, the transformation from traditional to Islamic banks and the impact of social and tribal culture. Concerning the main researcher questions, the findings revealed that the BoDs and CEOs involved in the establishment of the Libyan Corporate governance code (LCGC) by compromising the CG international guidelines with national laws and regulations, in addition to their crucial role in the application and development of CG policies within their banks. The findings also indicated that BoDs and CEOs applied TFL and TSL styles. Both styles work together as continuum to enable the implementation of CSR activities within and outside the banks. Finally, the finding confirmed that the BoDs and CEOs influence the CG regulations to include the CSR practices in the LCBs to meet the interests of both shareholders and other stakeholders. The study has some limitations, including the lack of literature regarding to the concepts of CG, CSR and leadership in Libya. Additionally, the study did not include leaders from the middle and lower management and also did not cover other stakeholders' entities. Moreover, this study did not distinguish the differences between the public banks from the private banks. Female participants are absent in this research due to engendered Libyan environment. Finally, the Libyan crisis experienced during the conduct of this research influenced, somehow, the findings of this research.",,Alshaikh KA,,2019,,,,Ph.D. Thesis
Manifold Learning for Multi-Classifier Systems via Ensembles,"Statistical classification of hyperspectral data is challenging because the inputs are high in dimension, while the quantity of labeled data is typically limited. The resulting classifiers are often unstable and have poor generalization. Nonlinear manifold learning algorithms assume that the original high dimensional data actually lie on a low dimensional manifold defined by local geometric differences between samples. Recent research has demonstrated the potential of these approaches for nonlinear dimension reduction and representation of high dimensional observations. Nonlinear scattering phenomena associated with processes observed in remote sensing data suggest that these may be useful for analysis of hyperspectral data. However, computational requirements limit their applicability for classification of remotely sensed data. Multi-classifier systems potentially provide a means to exploit the advantages of manifold learning through decomposition frameworks, while providing improved generalization. This paper reports preliminary results obtained from an ensemble implementation of Landmark Isomap in conjunction with a kNN classifier. The goal is to achieve improved generalization of the classifier in analysis of hyperspectral data in a dynamic environment with limited training data. The new method is implemented and applied to Hyperion hyperspectral data collected over the Okavango Delta of Botswana.",,"Crawford M,Kim W",,2009,519–528,10.1007/978-3-642-02326-2_52,https://doi-org.proxy.bnl.lu/10.1007/978-3-642-02326-2_52;http://dx.doi.org/10.1007/978-3-642-02326-2_52,Conference Paper
"Land Use Analysis Using GIS, Radar and Thematic Mapper in Ethiopia: PhD Showcase","Land degradation, and poverty issues are very common in our world, especially in developing countries in Africa. There are fewer adaptation strategies for climate change in these countries. Ethiopia is a tropical country found in the horn of Africa. The majority of the population live in rural areas and agriculture is the main economic sector. Extensive agriculture has resulted in an unexpected over-exploitation and land degradation. The project locations are Southwestern and Northwestern Ethiopia. The main objectives are to analize the accuracy of land use classification of each sensors, classification algorithms and analyze land use change. Thematic Mapper (TM) and Radar data will be used to classify and monitor land use change. Two consecutive satellite images will be used to see the land use change in the study area (1998, 2008). ERDAS Imagine will be used to resample and spatially register the Radar and TM data. The image classification for this research study is supervised signature extraction. The Maximum likelihood decision rule and C4.5 algorithm will be applied to classify the images. TM and Radar data will be fused by layer staking. The accuracy of the digital classification will be calculated using error matrix. Land change modeler will be used for analyzing and predicting land cover change. The impact of roads, urban and population density on land use change will be analayzed using GIS.",,Tadesse HK,,2010,53–58,10.1145/1869890.1869897,https://doi-org.proxy.bnl.lu/10.1145/1869890.1869897;http://dx.doi.org/10.1145/1869890.1869897,Conference Paper
"BDCAT '17: Proceedings of the Fourth IEEE/ACM International Conference on Big Data Computing, Applications and Technologies","It is with great pleasure, on behalf of the program committee, that we welcome you to the fourth IEEE/ACM International Conference on Big Data Computing, Applications and Technologies (BDCAT2017), to be held in Austin, Texas, USA.BDCAT, as an international conference series, has established itself as the forum for researchers and practitioners in the varied spectrum of human endeavors where data is produced and consumed; from health and personalized medicine, to social services, to industrial processes, to security, to retail business and to high energy physics to identify elementary particle to unlock the secrets of the universe, among many other fields. Big data is an all-encompassing term combining the various characteristics of data that includes their volume, the velocity of data generation and consumption, the variety of data sources and formats, and the variability in their characteristics. The Big Data ecosystem encompasses theoretical and computational frameworks, the applications that deal with such data, and the emerging technologies that ultimately benefit the masses.Since its birth in 2014 in London, UK, BDCAT has become one of the premier forums for sharing of new advances in the methodology, the applications and technologies for big data. Today, BDCAT continues its success. This year we have received 93 submissions from 22 countries. Of these submissions, 27 were accepted for publication, leading to an acceptance rate of 29%.A monumental effort such as BDCAT2017 would not come to fruition without the vision and cooperative and dedicated work of many individuals across the globe. In particular we would like to thank the experts comprising the BDCAT Technical Program Committee for preserving the tradition of rigorous, high-quality peer reviews through their dedication, hard work, and discussions leading up to the selection of the papers. We acknowledge the relentless support that we received from our honorary leadership, Professors Rajkumar Buyya at the University of Melbourne, Australia, Geoffrey Fox at Indian University, USA and Beng Chin OOI of the National University of Singapore, Singapore. We also kindly acknowledge the dedicated support of the local organizing committee chairs: Professors Tim Cockerill of Texas Advanced Computing Center, Jerry Perez, Texas Tech University, Ravi Vadapalli, Texas Tech University and Zhangxi Lin, Texas Tech University, all of the USA. With efforts that spanned almost a year, we also acknowledge the efforts of the publicity chairs, professors David Chiu, University of Puget Sound, USA, Ningfang Mi, Northeastern University, USA, Gleb Radchenko, South Ural State University, Russia, Andrei Tchernykh, CICESE Research Center, Mexico, Yan Tang, Hohai University, China and Iman Elghandour, Alexandria University, Egypt.",,,,2017,,,,Book
Characterizing the Input-Output Function of the Olfactory-Limbic Pathway in the Guinea Pig,"Nowadays the neuroscientific community is taking more and more advantage of the continuous interaction between engineers and computational neuroscientists in order to develop neuroprostheses aimed at replacing damaged brain areas with artificial devices. To this end, a technological effort is required to develop neural network models which can be fed with the recorded electrophysiological patterns to yield the correct brain stimulation to recover the desired functions. In this paper we present a machine learning approach to derive the input-output function of the olfactory-limbic pathway in the in vitro whole brain of guinea pig, less complex and more controllable than an in vivo system. We first experimentally characterized the neuronal pathway by delivering different sets of electrical stimuli from the lateral olfactory tract (LOT) and by recording the corresponding responses in the lateral entorhinal cortex (l-ERC). As a second step, we used information theory to evaluate how much information output features carry about the input. Finally we used the acquired data to learn the LOT-l-ERC ""I/O function,"" by means of the kernel regularized least squares method, able to predict l-ERC responses on the basis of LOT stimulation features. Our modeling approach can be further exploited for brain prostheses applications.",,"Breschi GL,Ciliberto C,Nieus T,Rosasco L,Taverna S,Chiappalone M,Pasquale V",,2015,,10.1155/2015/359590,https://doi-org.proxy.bnl.lu/10.1155/2015/359590;http://dx.doi.org/10.1155/2015/359590,Journal Article
One Digital Day: How the Microchip Is Changing Our World,"From the Publisher: No invention in history has spread so quickly throughout the world, or revolutionized so many aspects of human existence, as the microchip. Little more than a quarter century since its invention, there are now nearly 15 billion microchips in use worldwide -- the equivalent of two powerful computers for every man, woman, and child on the planet. The microprocessor is not only changing the products we use, but also the way we live, and, ultimately, the way we perceive reality. One Digital Day is the result of a unique project designed to make people aware of the thousands of microprocessors we unknowingly encounter every day. Rick Smolan, creator of the award-winning 'Day in the Life' photography books and the bestseller 24 Hours in Cyberspace, sent 100 of the world's most talented photojournalists around the globe on July 11, 1997. Their mission: to depict intimate and emotional stories of how this tiny chip-a square of silicon the size of a fingernail, weighing less than a postage stamp -- has transformed our human culture forever. The book features more than 200 compelling photographs, taken on that single day, revealing a world that only science-fiction writers once dared envision. Thanks to microchips, it is a world where science, entertainment, business, health, sports, education, and countless other fields are progressing faster than we can imagine. How pervasive is the microchip If you took the microchips out of every application in which they are now used, the results would be stunning and frightening. Microwave ovens, dishwashers, and many other kitchen appliances would stop working. Televisions and VCRs would fade to black; stereos would grow mute; and most clocks would stop. Cars wouldn't start, and airplanes would be unable to leave the ground. The phone system would go dead, as would most streetlights, thermostats, and, of course, a half-billion computers. And these are only the most obvious applications. Every factory in the industrial world would also shut down, as would the electrical grid, stock exchanges, and the global banking system. Pacemakers would stop, too, as would surgical equipment and fetal monitoring systems in obstetrics wards. This infinite variety of applications is vividly illustrated by the images captured last July for One Digital Day. A brief sample of what the hundred photographers came back with: Johannesburg, South Africa -- Once on the verge of extinction, cheetahs at the DeWildt Center are implanted with microchips that contain genetic information. This information, read by a scanner, is crucial to the center's efforts to build up the world population, because in-breeding is a big threat to the genetic strength of the cats. Hollywood, California -- The Jurassic Park River Adventure roller coaster is a completely automated ride which was designed with the help of paleontologists and robotics engineers, at a cost of $100 million. This completely automated ride includes ""animatronic"" dinosaurs which roar, lunge and even spit at riders in passing boats. Bury, England -- Ida Schofield, a 69-year-old grandmother, had never touched a computer or thought she had any need for one until she volunteered as a guinea pig for a state-of-the-art desktop system, with video-conferencing. She now uses it to communicate with family members around the world. Lacey, Washington -- Sprinter Tony Volpentest, born with no hands or feet and only partially formed arms and legs, uses ultra-light artificial feet designed with the help of sophisticated computer modeling programs. He now runs the 100-meter dash only 1.5 seconds slower than the world record holder. Singapore -- The foul-smelling but delicious tropical fruit known as durian is adored throughout Asia, but devotees dread carrying it home in their cars or keeping it around the house. Now connoisseurs of the odoriferous delicacy can order it online from 717 Trading Company and have it delivered just when they're ready to eat it. Since 717 launched its Web site in early 1996, about 20 percent of its sales have come from customers shopping online. Fort Bragg, NC and Sarajevo, Bosnia -- U.S. Army Lieutenant Frank Holmes, stationed 5,000 miles from home in Bosnia, gets his first look at his six-week-old daughter, Morgan, by using a PC-based videoconferencing system. The smooth images that reunited Frank, Morgan, and mom Andrea ran over normal phone lines between computers running ProShare Technology. Frank's commanding officer notes that videoconferencing is the single greatest morale boost for my troops in a long time. As Andrew S. Grove, Chairman and CEO of Intel Corporation, writes in his foreword, As you turn these pages, you'll see a world being reshaped by technology in ways previously unthinkable. One Digital Day makes it fascinatingly clear that there is no place on, above, or below the earth, that the microprocessor hasn't touched.",,"Smolan R,Erwitt J,Malone MS",,1998,,,,Book
Wind Energy Resource Prediction and Optimal Storage Sizing to Guarantee Dispatchability: A Case Study in the Kenyan Power Grid,"Kenya is experiencing a fast increase in grid-connected intermittent renewable energy sources (RESs) to meet its increased power demand, and at the same time be able to fulfill its Paris Agreement obligations of abating greenhouse gas emissions. For instance, Kenya has 102 MW of grid-tied solar power and 410 MW of grid-tied wind power. However, these sources are very intermittent with low predictability. Thus, after their installation and integration into the grid, they impose a new challenge for the secure, reliable, and economic operation of the system. To mitigate these and to ensure proper planning of the system operations, accurate and faster prediction of the generation output of the wind energy resources and optimal design and sizing of storage for the large-scale wind energy integration into the grid are of paramount importance. Artificial intelligence (AI) and metaheuristic techniques have proven to be efficient and robust in offering solutions to complex nonlinear prediction and optimization problems. Therefore, this study aims to utilize backpropagation neural network (BPNN) algorithm to conduct hourly prediction of the generation output of Lake Turkana Wind Power Plant (LTWPP), a 310 MW plant connected to the Kenyan power grid, and optimally size its battery energy storage system (BESS) using genetic algorithm (GA) to guarantee its dispatchability. The historical weather data, namely wind speed, ambient temperature, relative humidity, wind direction, and generation output from LTWPP, are employed in the training, testing, and validation of the neural network. LTWPP and BESS are modelled in MATLAB R2016a software. Thereafter, the developed BPNN and GA algorithms are applied to the modelled systems to predict the wind output and optimize the storage system, respectively. BESS optimization with neural prediction reduces the BESS capacity and investment costs by 59.82%, while the overall dispatchability of LTWPP is increased from 73.36% to 90.14%, hence enabling the farm to meet its allowable loss of power supply probability (LPSP) index of 0.1 while guaranteeing its dispatchability.",,"Odero H,Wekesa C,Irungu G,Vallée F",,2022,,10.1155/2022/4044757,https://doi-org.proxy.bnl.lu/10.1155/2022/4044757;http://dx.doi.org/10.1155/2022/4044757,Journal Article
"A Study of Framework Development and Research of Jewelry Design, Based on Pattern Egyptian Culture (Lotus Flower) Used in Culture Product Design","Industrial design has played a crucial role in the integration of cultural elements into products and in increasing their cultural value in the competitive global marketplace. Nowadays there is a shortage of design studies and products that dealt with the Egyptian styles. Even though, the ancient Egyptian civilization is full of patterns and symbols that we can exploit, develop, and manufacture with different products. Lotus flower one of the most important symbol in the antient Egypt, it was associated with Egyptian gods because of the way in which the Lotus emerged from the water. The Ancient Egyptians believed that Lotuses were symbolic of creation, rebirth, strength. This article aims to clarify the true meaning of the Lotus flower pattern and how to transform this cultural feature into innovative jewelry designs under the framework of culture-oriented design. The main contribution of this work is to explore the old Egyptian styles and convert them into designs that are compatible with this era to spread the culture attractively. This process is done firstly through a detailed explanation of this pattern, this phase consists of cultural features, literature reviews, and concepts. Secondly, used some design programs such as Auto CAD and Render software to transform this pattern into jewelry designs. Finally, this paper establishes a cultural product jewelry design model that is meant to provide designers with valuable research that can be applied in many artistic fields like clothing design, fashion, decoration, and modern designs for this pattern.",,"Ramadan E,Wu Y",,2021,630–645,10.1007/978-3-030-90328-2_43,https://doi-org.proxy.bnl.lu/10.1007/978-3-030-90328-2_43;http://dx.doi.org/10.1007/978-3-030-90328-2_43,Conference Paper
Patient Management Systems: The Early Years,"As I scanned through old papers and reports in preparation for these remarks, I became depressed in the “sameness” of those proposals and descriptions with what is happening today. Then I realized there are major differences - today's systems work and are affordable.The health care delivery system is an industry whose magnitude, complexity and pervasiveness are rarely acknowledged. In a few decades, the industry has literally changed from a cottage industry to a multi-billion dollar giant with whom every individual in our society has come into contact. It is a personal industry, yet at the same time, one of our most technically sophisticated industries. It is not surprising that computers are becoming an integral part of that system. This paper discusses some of the experiences in reaching that goal.As a beginning engineer back in the 1960s, I, with many others, felt that the development of computerized patient management systems was not only natural but mandatory. One merely needs to observe the process to realize that keeping track of what was done and charging appropriately, of sending information from one place to another, of storing data and printing it on demand, and of controlling process and flow are tasks which computers perform well. Many medical specialties already used forms for the collection of data. Most medical knowledge was already clearly identified in textbooks, including what questions to ask, what parameters to measure, what tests to order, how to diagnose, and how to treat. “A simple matter of programming” was a phrase often used and believed. It later became a standing joke. Many predicted that the use of computers for medical applications would develop into a multi-million dollar market whose potential would be quickly realized. The actual events proved to be quite different.The development of patient management systems has been influenced by several factors. The first, and perhaps one of the most significant factors, is that of technology - hardware and software. During this development period, computers evolved from single tasking, “untouchable” and “unfriendly” mainframes to highly interactive, multiuser minicomputers.A second factor is that of the people involved - both the developers and the users. The developers had to learn first what to do and how to do it and then learn how to package and sell it to the ultimate user.Economic factors also influenced progress. As computer costs decreased, the cost of delivering patient care increased. Computers seem to be offer one way to reduce and control these costs.Another factor was the tremendous increase in the amount of data generated and the demand for that data by a variety of individuals. For example, both the number of laboratory tests available and the number of tests actually ordered increased exponentially during this period. Estimates on the costs of information handling vary between 25 and 39% of the total cost of health care [1]. With the influx of many research dollars from NIH, actual medical knowledge increased.Finally, the influence of external factors such as the government and third party payers contributed significantly to the development of patient management systems. As one observer commented [2], “I think that just as the Medicare legislation forced hospitals, almost without exception, to use the computer for financial processing, patient accounting, and patient billing, the PSRO type of thing - which will get built on more and more, particularly with national health insurance likely to go in within the next year - will force computerization of the clinical side of the hospital.”The digital computer became available for general use in the late 1950s. These first systems provided few user-oriented features and required considerable knowledge and skill to use. Early systems were batch oriented and supported single tasking only. These computers were large, required specially prepared spaces, and were quite expensive. In addition to machine language, followed by assembly language, only Fortran and Cobol were available as higher level languages. Most programs were written by computer specialists who had only limited interaction with those who would ultimately use the systems. The reliability of early systems left much to be desired. Hardware failures were the norm rather than the exception. Software crashes were commonplace. Perhaps life with these early systems was best described as “working with a machine you couldn't touch; working with a machine that didn't work; working with a machine that you couldn't afford; and working with systems that were not useful.”I shared office space with two cardiology fellows who seemed to spend most of their day making meticulous measurements of amplitudes and time durations of the various waveforms of the ECG. After recording these carefully on paper, they applied a set of rules to interpret the ECG readings. This task seemed to me to be a simple engineering problem which could be solved almost trivially by a computer. Unfortunately there were the problems of noise, wandering baselines, arrhythmias and PVC's, variations in patterns and other factors to solve to produce the same result as the human. Researchers quickly learned that it was difficult to teach the computer to recognize patterns which were easily identified by humans [3, 4, 5].Gordon [6] points out difficulties of attempting to overlay the computer's orderly, pedantic and, indeed, binary world with the softness, variability and “between the lines implications” of medical data under human direction - a point that is still valid. He notes that the adoption of computer technology in practice must be concerned with the customs of 200,000 physicians serving independently or in 7,000 hospitals and clinics. Changes from manual documentation to automated procedures are often bewildering and ineffective.The early development of patient management systems was supported primarily by NIH grants. Since 1968, the National Center for Health Services Research has played a major role in supporting the development, application and evaluation of patient management systems [7]. No hospital could afford a computer. Since the funding came from external sources, developers often did what they wanted to do and how they wanted to do it, rather than interfacing with users who wanted to have nothing to do with the system in the first place.Developers were consistent in their reasons for developing patient care systems. Almost all papers or proposals started with a line, “We are currently in the midst of a health-care crisis. The average cost of a hospital bed has tripled since 1957.” Systems were proposed to reduce the costs of patient care, to reduce length of stay, to improve patient care, to improve nursing care, to improve communication, and to improve decision making. Little evaluation was done. For the most part, we did what we knew how to do and wrote research papers to justify it.Melville H. Hodge sets the stage for this period in the Preface of his book Medical Information Systems [8]. He states that, in the early 1960s, a small group of hospitals became identified with one common goal, that of a commitment to serve as a site for the development of computerized handling of patient information. Some of these early hospitals include Akron Childrens' Hospital in Ohio; El Camino in Mountain View, California; Baptist in Beaumont, Texas; St. Francis in Peoria, Illinois; Charlotte Memorial in North Carolina; Washington Veteran's Administration Hospital; Henry Ford Hospital, Detroit, Michigan; Monmouth Medical Center, Long Branch, N.J.; Mary's Help Hospital, Daly City, California; Deaconess Hospital, Livingston, Indiana; Latter Day Saints Hospital, Salt Lake City, Utah; and Downstate Medical Center, New York City, New York. We owe a debt of gratitude to these early pioneers, and I might say suffering sites.Most major computer companies, such as IBM, Burroughs, Control Data, Honeywell and NCR, seeing the potential of significant sales, were active in their support. Industries experienced in using computers to manage complex systems joined in. Some of these companies include Lockheed, who supported the early development of the Technicon Hospital Information System; McDonnell-Douglas, who is still active in the field; and other companies, such as GE, who later abandoned these efforts. Most of these systems were well reported in the literature (See, for example (9,10]).Many groups in Europe were developing systems at the same time: the Danderyd Hospital [11] and Karolinska Hospital [12] in Sweden; London Hospital [13] and Kings Hospital [14] in England; and the Hanover Hospital [15] in Germany to mention a few.Unfortunately, most of these early systems resulted in resounding failures. The reason primarily for these failures and for the slow progress into the 1970s was largely due to underestimating the complexity of the information requirements of patient management systems. Furthermore, users, as contrasted to developers, were not involved at an adequate level and, in fact, were not ready for computers. Hardware and software tools were inadequate. Hospitals felt that they had been oversold an unattainable product, and, at the loss of millions of dollars, abandoned their efforts in computerization. As Hodge notes, optimism and enthusiasm was replaced by skepticism and then cynicism.Fortunately others persisted. As technology advanced, driven by the space efforts of the '60s, developers learned to appreciate the complexity of the problem and began to address smaller, more easily defined components of the overall system. A few successes appeared, although some projects failed in the transition from carefully nurtured demonstration projects into systems which interfaced with, usually, the least paid, least motivated, and least educated employees of the medical support staff.By the early 1970s, however, some of these early systems, after years of development and many more development dollars than anyone anticipated, became commercially available [16,17]. After a period of overpromise and underachievement, some progress could be noted [18].The Technicon system, begun by Lockheed in the 1960s, was installed at the El Camino Hospital in Mountain View, California and became, perhaps, the best known “successful” application. The “success” of this system in its early years at El Camino can perhaps be measured by an article in the October 1973 issue of DATAMATION [19]. El Camino was truly a guinea pig in the development of the hospital information system and suffered through the many bugs. During the first year of installation, more than 2000 changes were made to the system, many of these major changes which affected the appearance of things such as reports. Each passing day saw improvement in the attitude of doctors and nurses. In mid-1972, 66% of the doctors opposed the system. By the beginning of 1973, the majority of doctors, except for internists, favored the system. The El Camino system is perhaps one of the most thoroughly evaluated systems of any of the early development systems [20, 21]. The results of this evaluation did encourage further development in patient management systems.The ultimate success of the system at El Camino led to the spread of this and other systems into other hospitals.New crises were encountered as reduced funding from the federal government forced hospitals to decide if computerization was worth the cost and then to find the money to do it. Some hospitals were forced to abandon systems even though the systems finally looked promising.Patient management systems tend to be primarily an automation of manual processes. In 1969, Feinstein [22] noted that while computers had been applied effectively in situations where a standard mechanism already exists for dealing with the data, computers had not yet had an important impact on the more inherently clinical features of medical strategy and tactics. Many of the points made in this article are still valid criticisms of patient management systems. Schwartz [23] makes a similar point. He states that “few systems have fully explored the possibility that the computer as an intellectual tool can reshape the present system of health care, fundamentally alter the role of the physician, and profoundly change the nature of medical manpower recruitment and medical education - in short, the possibility that the health-care system by the year 2000 will be basically different from what it is today.” We clearly have some distance to go.The development of many of the components of a patient management system was driven in the late 1960s and early 1970s by interest in automated multiphasic health testing. The work of Dr. Morris Collen and his colleagues at the Kaiser-Permanente Medical Group in California [24,25] contributed to both a high level of interest in this field and in the progress of automation of tests, data collection and analysis. Dr. Collen stressed the need for AMHT systems to provide high quality testing, to provide good service to doctors and patients, and to be economical. In the early 1970s, only the first of these conditions had been met. The same could be said about other components of patient management systems.Barnett, in an article [26] in The New England Journal of Medicine, again argued the cause for computer applications in areas of medical care. He identified seven major areas in patient management systems which had made progress in development. Caceres [27] similarly reviewed the state of the art and stressed that the physician and patient care data must interact via the computer to realize automated patient management system goals.Patient management systems, to be effective, do need to become a part of the physician/patient interface. Early systems were designed partly by the scientist, partly from the business world, and very little by the practicing physician. Systems designed in our computer laboratories often had major flaws which were obvious when we introduced them into the real world. Intelligent use of computers requires an understanding of the things computers do well: quantified information, well-defined vocabulary, great speed, repetition, accuracy, and versatile control. Humans, on the other hand, communicate by speech, vision, and touch, and have an unlimited vocabulary and great adaptability. It is when the computer is applied in areas of human incompetence, that previously impossible results can be achieved [28]. Too few systems take advantage of this fact. Often we fail to realize that the computer is no substitute for intelligence. It is not a magic box which can make gold from straw.One early experience at Duke is typical of the early days. For over two years, Duke had been involved with IBM in the development of a system called Clinical Decision Support Systems (CDSS). Duke had sent several MDs to work with IBM to develop a system in which the doctor would sit down with a computer terminal, describe the patient's history, physical findings, and laboratory data, and the computer would return the diagnosis and recommend a treatment. A remote system was set up at Duke, and the system was to be demonstrated to the faculty and house staff. Before the grand opening, a few doctors sat down and entered data on patient with some “easy” problems, such as influenza or pneumonia. After an hour of conversation with the computer, the computer was no closer to a conclusion than it was at the beginning. It seems that the computer did not know of the more common diseases since they were not well defined in the literature. The decision was made not to demonstrate or implement the system.Instead, Duke then decided to develop a smaller subset of the system - the automation of the initial or screening medical history. A 19-page mark sense form was designed to be completed by the patient, processed by the computer, and be presented to the doctor in narrative form. After three iterations, the form was complete, and actually did an effective job of collecting the initial medical history. Unfortunately, the logistics of processing this form on a large, remotely located main frame computer led to itsfailure. The 19-page history was scanned by a mark sense reader and the results written on a 9-track magnetic tape. The patient's name, address, and free text data was keypunched onto cards, and the tape, cards, and program were submitted for delivery to the Triangle Universities Computation Center (TUCC), located some 12 miles away, for processing. Rarely did the tape, data, and program arrive at TUCC at the same time, and we spent most of our time trying to track down the components and get them together for processing. And when we managed that, the tape, created on one vendor's machine, could not be read on the other vendor's tape unit. The result was the history usually arrived in the doctor's hands a week after the patient had been seen. This problem was ultimately solved with a minicomputer directly interfaced to the scanner which produced the histories immediately.We tried to use what we had learned with the automated histories to develop a computerized medical record for the Division of Obstetrics at Duke. We metwith a group of physicians, argued over what parameters constituted an appropriate data base, and finally compromised by including any parameter any person felt they might use. The result was a 23 page, narrative printout for a new OB workup. Obviously, this computer program was not reducing the paper work nor helping the doctor. A quick redesign with the assistance of only one physician reduced the output to an acceptable amount; in fact, the essence of the output was reduced to approximately ten lines on the first page in a starred box. We learned an important lesson - the difference between “what I might want and what I need”.Technology produced the minicomputer in the mid-60s and removed some of the problems associated with the mainframes. The cost of these computers was around $30,000. The first of these was the LINC or Laboratory Instrumentation Computer developed at MIT and distributed to a number of system developers by NIH. This move by NIH was, in my opinion, one of the most significant events in the field of medical informatics, and really led to the development of the minicomputer industry. The LINC permitted an affordable, hands-on, real-time interaction with a computer. The minicomputer moved into the locations in which the projects were developed. The first minis were single user and had to be programmed in assembly language. The University of Washington in St. Louis developed a popular operating system which solved many of the system problems.The minicomputer opened the door for many new development in patient management including clinical laboratory systems, automated ECG systems, and ambulatory care patient record systems. Octo Barnett, at Mass General, led the way with the development of COSTAR and the programming language MUMPS [29].At Duke, we learned of the power of the minicomputer on a borrowed LINC-8 and designed a system in 1967 to createon-line surface maps of cardiac body potentials - a process which had previously been performed on a mainframe at a much greater expense of time and money. A group of us then became interested in developing a computerized medical record. Our newly-acquired Digital Equipment Corporation PDP-12 was a dream. It had a 4K memory of 12-bit words, a CRT screen which had to be refreshed under program control, two 135 kbyte DEC minitapes, 12 binary control switches, 6 A-to-D channels, and 6 potentiometers A-to-D inputs. Our first system was the Obstetrical Medical Record in which detailed data was retained during the pregnancy of some 1500 women who subsequently delivered at the Duke Medical Center. One tape would contain the records of approximately one month's pregnancies. Near the end of each month, someone was on call to change the tapes as the women came to Duke for delivery. The output was in upper case only on a teletype located just outside the delivery suite. One lesson we learned was that MDs did place value on the ability of a system to deliver information reliably as it was needed.The programs were written originally in assembly language and used the LAP-6 operating system. These assembly language programs were later converted into a programming language called GEMISCH which we use today.The PDP 12 gave way to a PDP 11/20 in the early '70s. The addition of a movable head, 1.2 Mbyte hard disk seemed to offer more storage than we could ever need. This minicomputer had 28 Kwords of 16-bit memory. We wrote a multiuser operating system which supported 7 simultaneous users using a round-robin swapping algorithm.User acceptance of computers played a major role in the development of patient management systems. The success of any innovation in a medical setting depends upon the attitude of the physicians involved. Surveys [30] indicated that physicians were reluctant to touch the keyboard of a CRT. They were doctors and “not typists”. Systems designed and introduced by physicians were more apt to be accepted than one designed by a non-MD.At Duke, we conducted one experiment which demonstrates this attitude. We asked a number of primary care physicians to look at a computer-generated medical history and a hand-written, human-generated history. The physicians overwhelmingly selected the hand-written form. We then reversed the process, taking the computer-generated medical history and coping it by hand, reformatting it slightly. We then took a human-generated history, typed it into the computer, and printed it on a drum printer so that it was obviously computer-generated. We showed these two histories to a number of physicians and again they overwhelmingly selected the hand-written form.Many worried, and perhaps justly, that computers would be over-accepted, andthe computer's “word” would become truth. In an editorial in JAMA [31], M. Southgate compares today's physician with the medicine man of a primitive tribe who consults his spirits for knowledge. To the modern physician, the computer becomes the powerful and all knowing spirit.Patients had little problem in accepting the computer as part of their health care delivery team [32]. Our own experience with using the PDP-12, certainly a rather imposing creature to a unenlightened patient, for collecting headache histories suggested that patients were less intimidated by the computer than the doctor. The adventuresome spirit of our patients was best illustrated by one incident involving a 67 year old lady. While answering questions about her headache, she would occasionally laugh. Not thinking our displays were humorous, we finally asked her what was funny. She replied that she was just waiting until the man hidden in the “computer box” would step out and greet her.The developers of patient management systems were committed to the task. Typical of that attitude is Mel Hodge: “I am a believer. I happen to believe that the problems of health care delivery are susceptible to well-considered, well-executed approaches and that the introduction of information systems technology is among the more powerful approaches available. I have invested more than a decade to my life in this belief [8].” Many of us can now say we have invested a career to this belief.Both of our speakers in this patient management systems section have contributed significantly to the development of this field. Both have been involved from the early years. Melville Hodge headed the development team which was responsible for the Technicon Medical Information System. This system was the first successful HIS which was subsequently implemented in a number of institutions and is today still a leader in the field of patient management systems.Homer Warner, with his colleagues at the Latter Day Saints (LDS) Hospital in Salt Lake City, Utah, developed a number of subsystems over this period which constitute a patient management system called HELP.The HELP system had its beginning in the late 1950s when Dr. Warner and colleagues began exploring the use of computers in the diagnosis of congenital heart disease [33]. The HELP system grew out of a group of subsystems which were designed to directly help the doctor or the nurse with specific data as relates to recognizing and dealing with specific events in a patient's illness [34]. These efforts included the goal of using the computer to enhance the decision making process [35] in the medical arena. Dr. Warner and colleagues dealt early with specific data collection, management [36], and analysis in such areas as the clinical laboratory, patient monitoring [37], and electrocardiographic interpretation by computer [38]. In the early 1970s, theseareas were integrated to use a common database. Warner describes the HELP system in a recent book [39].The Technicon system, and the contributions of Hodge, is important because it was one on the first systems which worked and was accepted. This system primarily dealt with the service-related components of a patient management system - order entry and result reporting. Contributions were made in what was done and how it was done, even though other systems did not necessarily follow exactly the same patterns. The Technicon system represents one milestone in the development of patient management systems.Warner and his group, through years of development, have added and important and necessary component of clinical involvement. By early-on collecting data, Warner and his group were able to develop their own probabilities for diseases and their relationship to signs, symptoms, and findings. Most impressive is that the HELP system is still evolving at even now represents a state of the art approach to automated patient management.These early years of development had to occur. I am always impressed that, as we became smart enough to recognize what we should do next, technology was always just available to enable us to do it. We are now entering a stage in which the tools seem to be adequate, the users seem to be receptive, the results justify the costs, and the applications seem to be useful. Perhaps we have now arrived at the point in which computerized patient management systems can change the way we teach physicians, the way we practice medicine, and the way we do medical research.",,Hammond WE,,1987,153–164,10.1145/41526.41541,https://doi-org.proxy.bnl.lu/10.1145/41526.41541;http://dx.doi.org/10.1145/41526.41541,Conference Paper
How Methods Make Designers,"Through their combination of lifestyle and method, Silicon Valley models for tech production such as design thinking, startup incubators, lean management, etc. are spreading across the globe. These paradigms are positioned by product designers, politicians, investors and corporations alike as replicable routes to individual and national empowerment. They are portrayed as universal templates, portable across national borders and applicable to local needs. We draw from our ethnographic engagements with tech entrepreneurial efforts in Ghana, China, and Jamaica to unpack the stakes involved in their uptake, showing that while local actors produce situated alternatives, their work nevertheless often results in a continued valorization of these seemingly universal methods. We argue that design methods shape not only use practices, but have consequences for the life worlds of professional designers. This includes how they impact personal and national identities, confer legitimacy in transnational innovation circles, and secure access to social and economic resources. Ultimately, we call for an inclusion of these factors in ongoing conversations about design and design methods.",,"Avle S,Lindtner S,Williams K",,2017,472–483,10.1145/3025453.3025864,https://doi-org.proxy.bnl.lu/10.1145/3025453.3025864;http://dx.doi.org/10.1145/3025453.3025864,Conference Paper
Valuing Natural and Cultural Resources for Eco-Cultural Tourism Development : Libya's Green Mountain,"This study focuses on achieving eco-cultural tourism development via identification and evaluation of natural, cultural and then eco-cultural resources (in terms of capital and criticality) for identifying the potential of eco-cultural tourism development and also determining the drivers of and barriers to tourism development. This is achieved through the views and perceptions of different cohorts of stakeholders. The case study is Libya's Green Mountain region (hereafter known as LGM). This study has developed a new framework for the identification and evaluation of 'Eco-Cultural Tourism Capital' (ECTC) and 'Critical Eco-Cultural Tourism Capital' (CECTC), to indicate eco-cultural tourism development potential, and also to determine the drivers of and barriers to tourism development. The framework ultimately aims to achieve eco-cultural tourism development. For the purposes of this study, eco-cultural resources are a combination of natural and cultural resources. 'Potential' means resources that can be exploited for tourism development (in this case eco-cultural resources). 'Capital' refers to resources that are perceived to have touristic value and commoditisation value. 'Critical Capital' refers to the extent to which this capital is perceived to be important, unique and non-substitutable. The study found that many of the eco-cultural resources in LGM can be considered as ECTC because they have touristic value which refers to recreational, cultural, historical, health and other value. Such ECTC are considered CECTC because they are important, unique and non-substitutable. CECTC indicates the potential for eco-cultural tourism development and needs to be more carefully focused as well as managed and developed sustainably. There are many factors that encourage tourism development which are considered drivers of tourism development in LGM. On the other hand, there are barriers facing tourism development regarding the local community and lack of infrastructure, facilities and services. Determining drivers and barriers helps to identify the key elements that should be considered and involved in achieving eco-cultural tourism development. Therefore, for developing eco-cultural tourism in LGM, it is necessary to identify potential, local community involvement, investment, and take advantage of the drivers. The results of the study can be applied in other destinations where eco-cultural tourism is underdeveloped. This study contributes to the literature in regard to eco-cultural tourism development and its potential. It explains the ideas of ECTC and CECTC and how they can be used for developing eco-cultural tourism. It then develops a framework for developing eco-cultural tourism. The study also contributes to the body of knowledge about tourism industry in Libya and LGM and eco-cultural tourism in particular.",,Mohamed AA,,2014,,,,Ph.D. Thesis
Investigation of the Mechanism of Exocytosis Form the Permeabilised Guinea Pig Eosinophils,"The regulatory pathways controlling exocytosis vary among cell types. The work described here concerns the regulation of exocytosis in eosinophils. In order to investigate the late steps regulating exocytosis, and to exclude complications arising from events mediated by cell surface receptors, it is helpful to work with permeabilised cells. The bacterial cytolysin, streptolysin-O generates large lesions in the plasma membranes of cells and so provides a means to access the interior. The cytosol can then be manipulated with precision allowing regulation of such diverse entities as divalent cations, nucleotides and even proteins which leak out, or which can be applied from the exterior, and their effect on exocytosis measured. When permeabilised with streptolysin-O, secretion of hexosaminidase from guinea pig eosinophils can be induced by Ca2+ and GTPγS both of which have been shown to be necessary. Extending this I have measured secretion of aryl sulphatase and peroxidase and in accompanying sub-cellular fractionations, I found that only the peroxidase is an exclusive component of the dense secretory (crystalloid) granules. If the application of the stimulus is delayed after permeabilisation, soluble proteins leak out and the secretory response runs down (over 30-50min). The rate at which the response runs down depends on the composition of the permeabilisation buffer. The inclusion of Ca2+ (10-5M), or soluble proteins derived from brain homogenates slows the rate of the rundown, and when both are applied together the run down is almost abolished. Using this technique as a basis of bio-assay, I have achieved a partial purification of two active factors from porcine brain homogenates. In addition, I have applied recombinant cytosolic proteins, in particular RhoGDI, to test their roles in the regulation of exocytosis.",,Larbi KY,,1997,,,,Ph.D. Thesis
The Internet Connection: System Connectivity and Configuration,"From the Book: This book explains how to connect your computer or network to the world's largest computer network and community of computer users: the Internet. The Internet The Internet is the largest computer network in the world, consisting of more than 13,000 networks and more than 1,776,000 machines as of July 1993. It has been growing approximately 100 percent annually for the last five years. The TCP/IP protocols used in the Internet are very capable, but are not plug and play. The pool of knowledgeable TCP/IP engineers is not growing as fast as the Internet itself. This book addresses that gap in knowledge. There is no Internet, Inc. to call for service like the old telephone monopoly. The Internet is a worldwide decentralized distributed cooperative interconnection of numerous underlying technologies and organizations with no overall goals, management, or pricing structure. Like the current telephone system, there are many suppliers of Internet connectivity and services, some competing, some complementary. Just as the local telephone company usually does not do the wiring inside your house, your Internet connectivity provider will not set up the environment inside your local host machine or your local network. This book tells you how to do it yourself. The Internet is not like television. When you join the Internet, you become a participant, able to post mail and news; to publish files, documents, and software; and to make your machines directly accessible to others, if you wish. Your machine or network becomes a part of the distributed Internet mail system, and canbecome a server of many other kinds of information. This book tells you how to join the Internet community. The Book This book shows how to connect to the Internet, step by step, from finding a connection through registering a domain and a network number, through configuring your TCP/IP protocols, to running your own domain server and setting up your mail and news systems. Security techniques are described, for use either with or without a router. The most common new Internet services, netfind, archie, WAIS, and gopher, are covered. Access information for network connectivity providers, for domain and IP network number registries, and for other books, is included. Much of the material in this book is applicable to any software platform, because it is about the TCP/IP protocols, which were designed to work with any platform. Singleprocess personal computer operating systems such as MS-DOS and MacOS are most frequently used as clients of network services. The book includes information on where to get TCP/IP software packages for IBM compatibles and Macintoshes. Multi-process operating systems such as UNIX commonly run both clients and servers. Most of the detailed information on the book on setting up and configuring network application servers is about UNIX software. This book is about setting up communications between your host or network and the Internet. That is, it is about communications with the outside world. We must address some internal LAN issues in dealing with external connectivity, but we avoid discussion of issues solely related to LANs, just as we avoid discussion of issues of system administration, unless they also are related to external connectivity. The book includes brief overviews of Internet services and protocols, and it briefly describes what the Internet is and is not, and how it differs from other networks. However, we assume the reader already knows about those other networks, knows about Internet services, and already wants to connect to the Internet. This book shows how to do that. Organization The book begins with two overview chapters, about services and networks. Chapter 1, Internet Services gives a motivational overview of what you can do with the Internet, and then describes the size and growth of the Internet. The bulk of the chapter describes specific Internet services, their facilities and advantages, and the TCP/IP protocols that support them on the Internet. Chapter 2, The Internet and Other Networks gives an overview of the history, protocols, and politics of the Internet and other networks, such as FidoNet, UUCP, BITNET, USENET, that together form the global Matrix of computers that exchange electronic mail. These contextual chapters set the stage and define the terms for the rest of the book. If you are already familiar with the Internet, you may want to skip forward to the other chapters, but there is an amazing amount of disinformation about the Internet in circulation, and these chapters are short and, we hope, accurate. Before you can use the Internet you have to decide how to connect, and you may need to register organizational names and network addresses. Chapter 3, Types of Internet Access categorizes types of access to the Internet, ranging from public hosts to direct fiber optic connections at hundreds of megabits per second. The chapter includes very brief refresher on protocol layering models and Internet protocol layers. Chapter 4, Registering Domain Names and IP Numbers tells exactly how to register a domain name and a network number, and where to get the registration forms by electronic mail, or on paper or CD/ROM. The rest of the chapters show how to set up Internet services, and are presented approximately in the order you are likely to need the services they describe. Chapter 5, Setting Up IP Chapter 6, Setting Up the Domain Name System Chapter 7, Setting Up Internet Electronic Mail Chapter 8, Setting Up USENET News Chapter 9, Security Issues Chapter 10, Setting Up Resource Discovery Services These chapters do not attempt to describe all possibilities in great generality (we've already done that in another book, Practical Internetworking with TCP/IP and UNIX). Instead, they give the short and direct path to getting what you're most likely to need set up as quickly and painlessly as possible. The appendices provide names and addresses for sources of information. Appendix A, Internet Providers lists Internet providers, from public login hosts to dialup and direct IP connectivity providers. Appendix B, Registration Templates includes the actual text of example registration templates for domains and IP network numbers, and the addresses to send them to. Appendix C, Software and Other Information tells where to get the software (often over the Internet itself; sometimes for free; sometimes from commercial suppliers). Appendix D, Further Reading is a brief reading list of books about the Internet and other networks. There is a brief glossary, and a brief index. The cover shows a view of the world from above the north pole, with each of four networks glowing in its own color light. Similar maps appear on four of the endpapers,* showing the whole world, Eurasia, and most of Canada and the United States. As the legends indicate, wide orange ellipses are for UUCP, tall violet ellipses are for FidoNet, blue squares are for BITNET, EARN, and other NJE networks, and green circles are for the Internet; these four networks are the largest distributed networks in the Matrix, and they are described briefly in Chapter 2. The size of an icon indicates the number of host computers near the center of the icon. For example, the map of Eurasia shows the Internet green as the most prevalent in the north and west of Europe, and BITNET (or other NJE network) blue as the most widespread in the middle east. In eastern Europe, Internet green fades into FidoNet and UUCP violet and orange in central Asia, until east Asia suddenly shows all four networks again. However, the Internet is following behind those two access networks, and green Internet circles are visible in Talinn, St. Petersburg, Kiev, Moscow, Novosibirsk, New Delhi, Bombay, and Accra, Ghana. The fourth endpaper shows growth rates of the Internet alone in each country of the world. Much of the world is already connected, from Antarctica to Siberia, from Greenland to Ecuador, from Australia to Austria. The newest countries are growing the fastest, but even the longest connected and most densely networked countries are adding new hosts at exponential rates. Readers This book is for readers who know they want a connection to the Internet, not to a different network. It is for anyone who wants to connect a single machine or a network to the Internet. Such a machine might be in someone's house or office, in a company or a university. Such a network might be in a company office or a university department. Managers and executives can use the book to get a good idea of what is really involved in setting up an Internet connection. Technical people can use the book to actually set up a connection. In 1993 more than a million new machines and ten thousand more networks are expected to connect to the Internet. That is more people confused by technology than all the TCP/IP consultants in the world can help directly. This book can assist many of those people in doing their own basic IP connection configuration. In 1994, two million more machines and twenty thousand more networks are expected. These numbers are just for the Internet proper. There are probably at least as many machines in private IP networks inside companies, and more are forming all the time. Many of these enterprise networks then connect to the Internet, either as full participants, or through one of the kinds of firewalls described in this book. Every company or department considering making such a connection has at least one potential reader. Every engineer involved in setting up the connection, the engineer's manager, and many of the engineer's users, are potential readers. The person wanting to connect may already have electronic mail access to some other network, such as UUCP, FidoNet, or BITNET, or may be a complete newcomer to wide area networking. Even if you work in a place with many network experts, it is very easy to spend a lot of time finding the right person to ask for basic information about a variety of topics. This book answers most of the basic questions, and points you at sources for appropriate registrars, software, and vendors. The book is aimed more at newcomers, but will also be of use to engineers familiar with LANs who want the quick path to setting up wide area Internet services. Most specific details are drawn from the Internet in the United States. Most details are the same in other countries. Where there are major differences, we describe them. The Internet itself is still mostly (60%) in the United States, but already reaches at least 50 other countries, and is growing even more rapidly in some of them than the overall 100% annual growth rate. Thus readers of this book may be anywhere in the world. Terminology and Typography This book is written in American English. We have avoided idioms that might be hard for other English readers to understand. We have also avoided overly formal or academic phrasing, while attempting to maintain clarity. Jargon words and important terms are defined in the glossary, and also appear in the index. Glossary definitions are necesarily brief, and isolated. Definitions of the same terms in the text are often longer, and are always given in context, so you may want to use the index to locate these embedded definitions, as well. Defined terms, whether words, phrases, or acronyms, appear in boldface where they are defined in the text, and sometimes in other places where they are important. Most networking concepts depend on other networking concepts, leading to circular definitions. For this reason, many terms are introduced briefly at the outset, and defined more properly later. For example, major network services are introduced in Chapter 1 and many of them are defined or discussed in more detail in later chapters. Similarly, basics of packet switching are explained in the ARPANET section in Chapter 2, leading into the discussion of network protocol layering in Chapter 3, the discussion of routing in Chapter 5, and the discussion of types of gateways in Chapter 9. Acronyms present some interesting issues. Traditionally in English an acronym is a short sequence of letters, pronounceable or not, derived from a sequence of words, perhaps by taking the first letter of each word. In computing and networking jargon acronyms are not always derived from a longer form; often the reverse is true. That is, frequently an interesting, euphonic, or punning acronym is constructed and later an expansion is produced to fit it. Such expansions serve merely as mnemonics and rationalizations, not as derivations. In addition, many network protocols and networking organizations are known almost exclusively by their acronyms, not by a longer name. For these reasons, we do not follow the usual English convention of always introducing an acronym in parentheses after the longer form of the name. Sometimes we instead give the acronym first, followed by the longer form in parentheses. In addition, it is customary in English to introduce an acronym along with its longer form once only in a text. We also do not follow this custom, since readers may not be reading this book in strict page number order, and even readers who do may easily have forgotten a complicated acronym introduced at the beginning of the book by the time it recurs in a later chapter. Instead we reintroduce an acronym whenever we think it might have been forgotten. All acronyms that are given in the text with an expansion appear in the index, and most also appear in the glossary. Network protocol names are usually given in uppercase, whether they are acronyms or not, as in RLOGIN. UNIX commands are given in lowercase and italics, as in rlogin. Pathnames are given in italics, as in /etc/hosts.equiv.",,"Quarterman JS,Mitchell SC,Smoot CM",,1993,,,,Book
Measuring the Big Data Readiness of Developing Countries – Index Development and Its Application to Africa,"The use of big data promises to drive economic growth and development and can therefore be a value-adding factor, but compared to private or public organisations, the country level is rarely investigated, and that is even more evident for developing countries. Another topic hardly ever considered in the big data research field is ‘big data readiness’, which means the level of preparation and willingness to exploit big data. We address these shortcomings in the literature and focus on the big data readiness of developing countries. Thus, the first research question is: what components are required for an index measuring big data readiness, and how can such an index be designed? We use a design science approach to develop the “Big Data Readiness Index” (BDRI), which is then applied to all African countries to answer our second research question: how do African countries perform in terms of the BDRI? Our analysis yields country rankings that show relatively high BDRI scores for coastal countries, such as South Africa, Kenya and Namibia, and for islands, such as Mauritius. Related implications for both research and policy are discussed.",,"Joubert A,Murawski M,Bick M",,2021,327–350,10.1007/s10796-021-10109-9,https://doi-org.proxy.bnl.lu/10.1007/s10796-021-10109-9;http://dx.doi.org/10.1007/s10796-021-10109-9,Journal Article
DyadChurn: Customer Churn Prediction Using Strong Social Ties,"The increase in mobile phone subscriptions in recent years, has led to near market saturation in the telecom industry. As a result, it has become harder for telecom providers to acquire new customers, and the need for retaining existing ones has become of paramount importance. Because of fierce competition between different telecom providers and because the ease of which customers can move from one provider to another, all telecom service providers suffer from customer churn. In this paper, we propose a dyadic based churn prediction model, DyadChurn, where customer churn is modeled through social influence that propagates in the telecom network over strong social ties. We propose a novel method for evaluating social tie strength between telecom customers. We then, incorporate strong social ties in an influence propagation model to predict the set of future potential churners. The evaluation of the proposed dyadic based churn prediction model has been done using a real dataset, from one of the largest telecom companies in Egypt. The experimental results showed that the ""length of calls"" between customers is the most effective attribute in predicting social influence that result in churning. The results also showed that strong social ties (as opposed to weak ties) were the most effective ties in determining churn. Using strong social ties only enhanced the prediction accuracy (in terms of the lift curve) by more than 20%, when compared to a diffusion model.",,"Abd-Allah MN,El-Beltagy SR,Salah A",,2017,253–263,10.1145/3105831.3105832,https://doi-org.proxy.bnl.lu/10.1145/3105831.3105832;http://dx.doi.org/10.1145/3105831.3105832,Conference Paper
Metaphors and Narratives in Exile : Understanding the Experiences of Forced Migrants in Britain,"The research aimed to reach an understanding of the experience of exile by investigating aspects of life in exile and how refugees portray their own experiences of asylum and define what asylum has meant to them in the process of living in and integrating their host society. The study, focusing on 30 refugees from Congo, Kosovo and Somalia, has sought to examine some of the metaphors that refugees associated with the experience of exile and the narration of these experiences in their own words. The search for meaning has dominated the research and throughout the analysis metaphors formulated by the refugees have been used to elucidate the argument. Some key findings have revealed the following: - There are contradictions in what asylum means to different refugees. The perception of exile ranges from 'hell' to 'heaven' and from 'safe haven' to further 'trauma'. - The process to re-socialisation is not straightforward and many different factors, in isolation or in combination, deeply affect the process. These range from the degree of coercion leading to exile, racialisation in the host country to the multiplicity and types of networks available to the exile in the host country and the degree to which the exile exploits them successfully. - The perception of home is difficult to agree. For people torn apart such as the Somali, Congolese and Kosovan refugees studied, home is neither here or there (i. e. neither in the host country or the country of origin); home is nowhere because events in the home country have disconnected them from feeling a sense of belonging and often isolation and racialisation in the host country comes short of enabling them to connect psychologically, socially and culturally with the new place. The metaphors used by the refugees carry a sense of nostalgia for the past and the lost land for many refugees. The nostalgia and loss encompasses the deficit in social status and mobility, the diminishing cultural identity including language as well as mere familiarity with the environment. But the narratives, with all their emotional contents, carry both a sense of hope and despair. The hope resided in the forecasting of better days in the native country which might trigger the refugees' return to their ""natural waters"" as a respondent put it metaphorically to refer to the original socio-cultural milieu the refugees originated from. For the stayers, hope was expressed in their seeing themselves finding 'a place under the sun' in the land of their exile. However, at the same time, the stagnation or worsening of the situation in the native land and often combined with the feeling of or categorisation as outsiders and others in the host country brought a sense of despair. The study has enabled the drawing of the conclusion that life in exile can often be ambiguity, uncertainty, loss but could also be new light, salvation and opportunity. This dialectics seems to be at the heart and the essence of exile, so far as the humans involved are both psychologically and social beings.",,Hack-Polay DD,,2006,,,,Ph.D. Thesis
"Place-Based Assessment of Intersection of Biophysical and Social Vulnerability to Flooding in Accra, Ghana","The relationship between flood hazards and social vulnerability is firmly on the intellectual agenda of geographers in Ghana. In an attempt to theorize and empirically examine this relationship, scholars have commonly followed a one-sided methodological strand. In this article, a triple-helix approach that relies on the application of social vulnerability index; mapping potential flood hazard zones; and examining degree of coincidence between flood hazards and social vulnerability, is used. Situating the analysis within Hazards-of-Place Model of Vulnerability, the study identifies spatial disparities in biophysical and social vulnerability within the City. It emerged that communities in the Ashiedu Keteke sub-metro were the most vulnerable based on the hazards-of-place model. Significantly, while flood risk awareness was very high among community members, the perception of flood risk management was poor. The study argues that understanding place-based vulnerability is crucial in mitigating the effect of hazards and building resilient communities.",,"Aboagye D,Attakora-Amaniampong E,Owusu-Sekyere E",,2020,55–68,10.4018/IJAGR.2020010104,https://doi-org.proxy.bnl.lu/10.4018/IJAGR.2020010104;http://dx.doi.org/10.4018/IJAGR.2020010104,Journal Article
Theories of Interval Arithmetic: Mathematical Foundations and Applications,"Reviews ""This new book by Hend Dawood is a fresh introduction to some of the basics of interval computation. It stops short of discussing the more complicated subdivision methods for converging to ranges of values, however it provides a bit of perspective about complex interval arithmetic, constraint intervals, and modal intervals, and it does go into the design of hardware operations for interval arithmetic, which is something still to be done by computer manufacturers."" - Ramon E. Moore, (The Founder of Interval Computations) Professor Emeritus of Computer and Information Science, Department of Mathematics, The Ohio State University, Columbus, U.S.A. ""I am delighted to see one more Egyptian citizen re-entering the field of interval mathematics invented in this very country thousands years ago."" - Marek W. Gutowski, Institute of Physics, Polish Academy of Sciences, Warszawa, Poland Book Description Scientists are, all the time, in a struggle with uncertainty which is always a threat to a trustworthy scientific knowledge. A very simple and natural idea, to defeat uncertainty, is that of enclosing uncertain measured values in real closed intervals. On the basis of this idea, interval arithmetic is constructed. The idea of calculating with intervals is not completely new in mathematics: the concept has been known since Archimedes, who used guaranteed lower and upper bounds to compute his constant Pi. Interval arithmetic is now a broad field in which rigorous mathematics is associated with scientific computing. This connection makes it possible to solve uncertainty problems that cannot be efficiently solved by floating-point arithmetic. Today, application areas of interval methods include electrical engineering, control theory, remote sensing, experimental and computational physics, chaotic systems, celestial mechanics, signal processing, computer graphics, robotics, and computer-assisted proofs. The purpose of this book is to be a concise but informative introduction to the theories of interval arithmetic as well as to some of their computational and scientific applications.",,Dawood H,,2011,,,,Book
Shadow Aware License Plate Recognition System,"During recent years, license plate recognition have been widely used as a core technology for security or traffic applications such as in traffic surveillance, parking lot access control, and information management. In this paper, Shadow Aware License Plate Recognition (SALPR) system is proposed to recognize Egyptian LP. This system achieves high recognition rate through applying shadow detection and removal, rotation adjustment and using Multilayer perceptron as a powerful tool to perform the recognition process. To show the efficiency of the proposed system, experiments have been done on numerous captured images including various types of vehicles with different lighting and noise effects. The experimental results yield 95.5 % recognition accuracy, the recognition process takes 1.6 s to recognize plate information. Most of the elapsed time used is for the license plate extraction and rotation adjustment. The results show the feasibility of the methodology followed in this paper. Performance comparison between SALPR and other LP recognition techniques shows that for most of the cases, SALPR performs better than other techniques under different lighting conditions and it shows the high robustness of the proposed algorithm.",,El-Said SA,,2015,225–235,10.1007/s00500-014-1245-5,https://doi-org.proxy.bnl.lu/10.1007/s00500-014-1245-5;http://dx.doi.org/10.1007/s00500-014-1245-5,Journal Article
Evaluation of User Experience and Cognitive Load of a Gamified Cognitive Training Application for Children with Learning Disabilities,"This study presents a gamified application for children with learning disabilities, designed to train and improve working memory. The application takes the form of a treasure hunt, and is designed according to a framework incorporating guidelines derived from accessibility, usability and cognitive load theory, and from gamification techniques. The aim is to exploit working memory capacity, motivate and engage the child in working memory training activities. The main focus of this study is the evaluation of the user experience and the cognitive load level of this gamified application. A sample of 12 Egyptian children with learning disabilities completed a five-week training period using the application, followed by an evaluation process. The evaluation took the form of a simple usability survey, an unstructured observation, and a cognitive load measurement scale. The purpose was to evaluate the children's perceived experience and assess the level of cognitive load experienced in each of the activities. The results revealed that all the children enjoyed playing the gamified application, were eager to participate in the daily training, and the cognitive load experienced during the training was found to be generally appropriate - although some areas for improvement were identified. Finally, the experiments identified a correlation between user experience, cognitive load and training performance.",,"Shaban A,Pearson E",,2020,,10.1145/3371300.3383341,https://doi-org.proxy.bnl.lu/10.1145/3371300.3383341;http://dx.doi.org/10.1145/3371300.3383341,Conference Paper
An Improved Marine Predators Algorithm for the Optimal Design of Hybrid Renewable Energy Systems,,,"Houssein EH,Ibrahim IE,Kharrich M,Kamel S",,2022,,10.1016/j.engappai.2022.104722,https://doi-org.proxy.bnl.lu/10.1016/j.engappai.2022.104722;http://dx.doi.org/10.1016/j.engappai.2022.104722,Journal Article
Now I Know Who My Comrades Are: Voices from the Internet Underground,"In China, university students use the Internet to save the life of an attempted murder victim. In Cuba, authorities unsuccessfully try to silence an online critic by sowing seeds of distrust in her marriage. And in Russia, a lone blogger rises to become one of the most prominent opposition figures since the fall of the Soviet Union. Authoritarian governments try to isolate individuals from one another, but in the age of social media freedom of speech is impossible to contain. Online, people discover that they are not alone. As one blogger put it, ""Now I know who my comrades are."" In her groundbreaking book, Now I Know Who My Comrades Are: Voices from the Internet Underground, Emily Parker, formerly a State Department policy advisor, writer at The Wall Street Journal and editor at The New York Times, provides on-the-ground accounts of how the Internet is transforming lives in China, Cuba, and Russia. It's a new phenomenon, but one that's already brought about significant political change. In 2011 ordinary Egyptians, many armed with little more than mobile phones, helped topple a thirty-year-old dictatorship. It was an extraordinary moment in modern history--and Now I Know Who My Comrades Are takes us beyond the Middle East to the next major civil rights battles between the Internet and state control. Star dissidents such as Cuba's Yoani Snchez and China's Ai Weiwei are profiled. Here you'll also find lesser-known bloggers, as well as the back-stories of Internet activism celebrities. Parker charts the rise of Russia's Alexey Navalny from ordinary blogger to one of the greatest threats to Vladimir Putin's regime. This book introduces us to an army of bloggers and tweeters--generals and foot soldiers alike. These activists write in code to outsmart censors and launch online campaigns to get their friends out of jail. They refuse to be intimidated by surveillance cameras or citizen informers. Even as they navigate the risks of authoritarian life, they feel free. Now I Know Who My Comrades Are is their story.",,Parker E,,2015,,,,Book
Now I Know Who My Comrades Are: Voices from the Internet Underground,"In China, university students use the Internet to save the life of an attempted murder victim. In Cuba, authorities unsuccessfully try to silence an online critic by sowing seeds of distrust in her marriage. And in Russia, a lone blogger rises to become one of the most prominent opposition figures since the fall of the Soviet Union. Authoritarian governments try to isolate individuals from one another, but in the age of social media this is impossible to do. Online, people discover that they are not alone. As one blogger put it, ""Now I know who my comrades are.""In her groundbreaking book, Now I Know Who My Comrades Are: Voices from the Internet Underground, Emily Parker, formerly a State Department policy advisor, writer at The Wall Street Journal and editor at The New York Times, provides on-the-ground accounts of how the Internet is transforming lives in China, Cuba, and Russia.Its a new phenomenon, but one thats already brought about significant political change. In 2011 ordinary Egyptians, many armed with little more than mobile phones, helped topple a thirty-year-old dictatorship. It was an extraordinary moment in modern historyand Now I Know Who My Comrades Are takes us beyond the Middle East to the next major battles between the Internet and state control.Star dissidents such as Cubas Yoani Snchez and China's Ai Weiwei are profiled. Here youll also find lesser-known bloggers, as well as the back-stories of Internet celebrities. Parker charts the rise of Russias Alexey Navalny from ordinary blogger to one of the greatest threats to Vladimir Putins regime.This book introduces us to an army of bloggers and tweetersgenerals and foot soldiers alike. They write in code to outsmart censors and launch online campaigns to get their friends out of jail. They refuse to be intimidated by surveillance cameras or citizen informers. Even as they navigate the risks of authoritarian life, they feel free. Now I Know Who My Comrades Are is their story.",,Parker E,,2014,,,,Book
"Identification of Ditches and Furrows Using Remote Sensing: Application to Sediment Modelling in the Tana Watershed, Kenya","Ridge-tillage is an agricultural practice where crops are planted on elevated ridges, with furrows in-between. Ridge-tillage has been shown to significantly reduce erosion from croplands, but data on the presence of ridge-tillage is sparse and challenging to collect at the landscape scale. Thus, water quality models often do not account for ridge-tillage in a spatially-explicit manner, potentially overlooking the important impacts of this practice. We have developed a novel method that exploits the spectral, radiometric and linearity shape characteristics to identify both drainage ditches and ridge-tillage furrows using remote sensing of 0.5 m satellite data. We applied the method to the Sasumua watershed in Kenya, where we had false positives in only 3% of randomly selected polygons, and we detected the majority of ditches in 59% of randomly selected polygons. We then assessed the potential value of including these data in sediment modelling, showing that representing these practices could reduce sediment export in the study area by roughly 80%. Being able to readily identify the presence of ditches and furrows could enable the development of more accurate water quality models, and help identify priority areas for intervention to improve water quality and possibly crop yields through changing agricultural practices or policies.",,"Ayana EK,Fisher JR,Hamel P,Boucher TM",,2017,4611–4630,10.1080/01431161.2017.1327125,https://doi-org.proxy.bnl.lu/10.1080/01431161.2017.1327125;http://dx.doi.org/10.1080/01431161.2017.1327125,Journal Article
"Product Diversification, Product Relationships, and the Ecomic Resilience of Libyan Tourist Destinations","Tourism product diversification becomes important not only to attract wider a range of tourists and increase market-share but also to ensure adaptation and resilience to enable tourist destinations to effectively prepare for crises. This study examines whether tourism product diversification enhances adaptability in order to make tourist destinations in Libya more resilient. The study examines the diverse patterns of tourism product development. It does this in relation to the patterns of use of alternative and mass tourism products, and the patterns of relationships between these products, for the cases of Tripoli and Alkhoms. Three frameworks were developed deductively in order to understand and evaluate these research issues. They were applied then to assess the patterns of development, the relationships between tourism products, and their influence on inherent economic resilience. Primary qualitative data were collected by means of in-depth, semi-structured, face-to-face interviews with respondents from the supply and demand sides of the tourism industry. Observation and various other secondary data sources were also utilized. The collected data were critically analyzed and interpreted in relation to the themes identified in the frameworks. It was found that diverse patterns of development and relationships evolved between the tourism products. Alkhoms depended on promoting two concentrated types of tourism products that are consumed in different seasons by completely separated sectors (domestic and international tourists). By contrast, Tripoli has developed a wider range of tourism products enabling it to attract larger numbers of tourists from both sectors, and thus it was less influenced by seasonality and forces of decline. In addition, spatial proximity and thematic features have encouraged more businesses - related to tourism to agglomerate near to Tripoli's tourist attractions, resulting in stronger linkages of compatibility and complementarity between its tourist attractions. Such agglomeration has led to more job generation and an improved ability to adapt to change, resulting in greater inherent economic resilience. In contrast, in Alkhoms, the spatial proximities between the main two dissimilar attractions have not been properly exploited. This has caused them to be managed and marketed in isolation, and resulted in Alkhoms having less resilience in the face of seasonality effects and other forces of change. It is argued that destinations that enjoy a wider range and scale of tourism products can develop ways of collaborating that could increase the flexibility and adaptability of the tourism offerings. This can mean they are better placed to meet the changeable and sophisticated needs of tourists, thereby nurturing economic resilience.",,Benur AM,,2013,,,,Ph.D. Thesis
Fractional Optimal Control with Fish Consumption to Prevent the Risk of Coronary Heart Disease,"According to the World Health Organization (WHO), Chronic Heart Disease (CHD) is one of the greatest defies currently confronting humankind which is sweeping the whole globe, with an expanding trend in developing countries. In this paper, a mathematical model (MM) was proposed to study the connection between fish consumption and CHD mortality in Egypt, by considering a system of ordinary differential equations (ODEs) involving time-fractional derivative (FD). We considered here the study on Egypt for the ease of obtaining real data, but the method and approach adopted here is not limited to Egypt only and can be applied to any country in the world with the information of the real data related to the subject of the study. Additionally, the control function which represents the metabolic and the behavioural risk factors of CHD that help to reduce the number of mortality due to CHD is incorporated in the proposed MM. A fractional optimal control problem (FOCP) with a proposed control is formulated and studied theoretically using the Pontryagin maximum principle, to minimize the susceptible population and also to decrease the mortality rate of CHD. Moreover, firstly we discussed the positivity and boundedness of solutions; then, the model equilibria are determined and their local stability analysis was investigated; furthermore, we use the improved forward-backward sweep method (FBSM) based on the predictor-corrector method (PCM) in order to obtain the solution of proposed FOCP. In addition, some numerical simulations were performed to show the effect of the proposed optimal control (OC) besides the impact of fish consumption on the mortality of CHD.",,"Ameen I,Hidan M,Mostefaoui Z,Ali HM,Guo X",,2020,,10.1155/2020/9823753,https://doi-org.proxy.bnl.lu/10.1155/2020/9823753;http://dx.doi.org/10.1155/2020/9823753,Journal Article
Human-Giraffe Interactions: Characterizing Poaching and Use of Parts as a Threat to Giraffe in Northern Kenya,"Giraffe (Giraffe spp.) are iconic wildlife species to Africa, yet relatively little conservation funding and research have been directed at protection of giraffe in the wild. A growing number of national governments and conservation organizations are implementing management strategies to address the threats that giraffe face. To inform these plans, there is a need for social science that examines the human pressures associated with decline of giraffe populations, including poaching and the use of giraffe parts. As the large majority of reticulated giraffe (Giraffa reticulata) range occurs outside formally protected areas, conservation plans must be made with pastoralist communities and other actors in northern Kenya where the land is shared between people, their livestock, and wildlife. The research presented in this dissertation was conducted as part of a community-based program focused on reticulated giraffe, called the Twiga Walinzi Initiative (""Giraffe Guards"" in Swahili), and represents the first quantitative study on the human dimensions of giraffe conservation. Goals of the research project were to examine key cognitions to human-giraffe interactions (i.e. attitudes, beliefs, perceptions), assess relationships between certain cognitions within areas that adopt a community-based conservation approach, and understand the extent and drivers of giraffe meat and part usage. Face-to-face interviews were conducted at two study sites over survey periods in 2016/17 (n = 579) and 2019 (n = 680). Results from these studies provide insights to how pastoralist communities view and act toward local giraffe. Factors that significantly influenced support for giraffe conservation differed between study sites, suggesting that local context is important to shaping human-giraffe interactions (Chapter 2). For instance, perceived benefits had stronger influence on normative belief in communities more recently connected with wildlife-based tourism. The linkages between perceived benefits, attitudes, and behaviors were further explored by assessing the relationships between these concepts within a community-based conservation setting (Chapter 3). Findings suggest a positive association between perceived benefits and attitudes toward giraffe, but there was less evidence that perceptions of wildlife-related benefits influenced use of giraffe meat/parts. As human behavior is of central interest to conservation, we also assessed levels of giraffe meat consumption (Chapter 4) and determinants of intention to consume giraffe meat (Chapter 5). Specialized questioning techniques were utilized to estimate prevalence of giraffe meat consumption preceding the two surveys. Estimated prevalence of giraffe meat consumption declined after establishment of the Twiga Walinzi. Perceived behavioral control had stronger relative influence than attitudes and subjective norms on future intention to consume giraffe meat. Collectively, these research findings are relevant for applied giraffe conservation efforts and provide a framework for understanding human-giraffe interactions and associated threats in diverse global settings.",,"Ruppert KA,Glikman J,De Urioste-Stone S,Noblet C,Rickard L",,2020,,,,Ph.D. Thesis
Safety Evaluation and Consideration of 4 Pin Multi-Needle for Meso-Therapy,"This study was conducted according to the method presented in the Republic of Korea Pharmacopoeia 11th Revision, aseptic test method to evaluate the suitability of sterilization for a sterile needle (4 Pin Multi-needle). In this study, four tests were conducted: sterility test, cytotoxicity test, acute toxicity test, skin sensitization test. First, in the aseptic test, the microorganism was not proliferated in the aseptic test of the medium. As a result of the performance test of the medium, it was confirmed that the microorganism developed within 3 days and the fungus was evident within 5 days. Based on this, it was confirmed that the medium was suitable, and as a result of the aseptic test, the development of microorganisms was not observed during the total culture period. Based on these results, tests were conducted which were confirmed to be suitable for aseptic testing because the development of bacteria on the provided samples was not recognized. For cytotoxicity tests ISO10993-5; 2009 (Biological Evaluation of Medical Devices, Part 5: Test for in vitro Cytotoxicity). As a result, the MEM eluate of the test substance caused very slight cytotoxicity to the fibroblasts of the mouse and was judged to be Grade 1 (Slightly cytotoxic) according to the judgment standard of ISO 10993-5. On the other hand, solvent control, negative control and positive control showed the expected results on the test. Acute Toxicity Test Results: It was judged that there was no systemic toxicity change when ICR mice were treated with 50 mL/kg B.W. of the eluate of sterile injectable needle for 72 hours. Skin sensitization test result: The Hartley guinea pig was evaluated as a substance which is evaluated as a substance which does not induce any skin reaction when skin sensitization is applied to the dissected material of the sterile injectable needle and is weak in skin sensitivity. Based on the above tests, we will study the stability and efficacy of more reliable medical devices based on the verification and performance of medical devices.",,"Kim JT,Choi A,Jeong JH,Jo JH,Ryu OS,Kim EJ,Kim KY,Song MH,Song YH,Shin WS,Lee SS,Gómez C,Schwarzacher SP,Zhou H",,2018,291–306,10.3233/THC-174624,https://doi-org.proxy.bnl.lu/10.3233/THC-174624;http://dx.doi.org/10.3233/THC-174624,Journal Article
Cause of and Factors Contributing to Stillbirth in Sub-Saharan Africa,"BackgroundEvery year, an estimated 2.6 million stillbirths occur worldwide, with up to 98% occurring in low- and middle-income countries (LMIC). Most stillbirths are preventable. To develop strategies and take effective actions to end preventable stillbirths, a good understanding of the cause of death and its contributing factors is necessary. There is, however, a paucity of data from most LMIC settings. This study aimed to determine the cause of stillbirth in LMIC using three methods of assessment, and to assess quality of care delivered to mothers who had stillbirth.MethodsThe study involved 1,563 stillbirths which occurred in 12 selected secondary and tertiary hospitals in Kenya, Malawi, Sierra Leone and Zimbabwe. The cause of death was determined by: (1) consensus of healthcare providers (HCPs) through stillbirth review; (2) expert review of cases and; (3) computer algorithms. Cause of death was classified using the classification according to Relevant Condition at Death (ReCoDe) and the International Classification of Diseases for Perinatal Mortality (ICD-PM). Quality of antenatal and intrapartum care and health system factors were reviewed using a set of criteria. ResultsA total of 1,329 cases were reviewed, of which 1,267 (95.3%) stillbirths met the inclusion criteria. By country, the stillbirth rate ranged from 20.3 (Malawi) to 118.1 (Sierra Leone) per 1,000 births. The distribution of the major causes of stillbirth differed by method of assessment: asphyxia (18.5% – 37.4%), placental disorders (8.4% – 15.1%), hypertensive disorders in the mother (5.1% – 13.6%), infection (4.3% – 9.0%), cord problems (3.3% – 6.5%), and ruptured uterus due to obstructed labour (2.6% – 6.1%). Information was insufficient to assign cause of stillbirth in 17.9% - 26.0% of cases. Significant agreement was observed between cause of stillbirth assigned by the expert panel and by HCP (k=0.69; p<0.0005) but there was a weaker agreement between expert panel and when using computer algorithms (k=0.34; p<0.0005).Using ReCoDe, intrapartum events (mainly intrapartum asphyxia) contributed to most of the deaths, followed by maternal diseases (mainly hypertensive disorders and infection), placental and fetal conditions. With application of ICD-PM, 42.0% were antepartum, 50.7% were intrapartum and 7.3% could not be categorised. The major categories accounting for the death were: intrapartum hypoxia and fetal growth restriction. Major contributing maternal conditions in ICD-PM were: M1 (placental, cord and membranes) and M3 (other complications of labour and delivery). Poor quality of care during antenatal care was identified in 97.8% of cases, and only 30.7% of cases of Caesarean section were conducted within one hour of decision. For 414 (37.9%) stillbirths, the outcome could have been different with better care.Conclusion Stillbirth rate was high, with high variations between countries. HCPs should be encouraged to conduct reviews and act upon findings to improve quality of care. Data requirements of computer algorithms need to be balanced between ability to find a cause and the availability of information. The new ICD-PM could work in LMIC, but there is the need for more guidance on how to handle cases of stillbirths whose time of death cannot be determined.",,Aminu M,,2017,,,,Ph.D. Thesis
Decoding the Translation Initiation Mechanism of Maize Chlorotic Mottle Virus / Decodificación Del Mecanismo de Iniciación En La Traducción Del Virus Moteado CloróTico Del Maíz,"Maize chlorotic mottle virus (MCMV) is the key player of Maize Lethal Necrosis Disease (MLND). MLND is caused by the co-infection and synergistic interaction of MCMV and any potyvirus that infects grasses. Recent outbreaks of MLND have ravaged maize fields in Kenya and neighboring regions of East Africa. The catastrophic economic losses brought back the interest of stakeholders to learn more about the synergistic interaction between MCMV and potyviruses and find ways to use this knowledge to create MLND resistant lines.MCMV is a positive-sense single-stranded RNA virus from the Tombusviridae family. Similarly to other members of the Tombusviridae family, MCMV does not have a 5'-cap or a poly (A) tail and must use alternative mechanisms to translate viral proteins. The lack of a 5'-cap in the viral RNA does not prevent it from interacting with host translation factors. Instead of a 5'-cap, most tombusvirids are known to employ unconventional mechanisms to sequester translation factors to the viral RNA. One mechanism of recruiting host's translation factors is 3'-cap-independent translation elements (3'-CITEs). 3'-CITEs are secondary structures located at the 3'-end of the virus genome used to recruit the translation initiation machinery. The work presented in this dissertation revolves around finding out the translation initiation control elements of MCMV RNA.The unearthing of MCMV's translation initiation process began with a series of deletions at the 3'-end of the virus genome. The genome sequence mapping indicated that there was an essential sequence for virus translation between nucleotides 4164 to 4333. Structural RNA probing of this region showed the presence of a panicum mosaic virus 3-CITE (PTE) -like structure located in the 3'-untranslated region (UTR) of the viral RNA. Similar to other PTEs, the MCMV 3'-CITE secondary structure was composed of a hammer-like helix structure that branched out into two side loops connected by a pyrimidine bridge. On the main stem of the hammer-like structure is a single-stranded bulge that is hyper-modified by SHAPE probing reagents in the presence of magnesium. PTEs are characterized by a pyrimidine rich bridge composed of cytosines and a purine-rich bulge that interacts with each other forming a pseudoknot. In contrary to most PTEs, the MTE was predicted to have a weak pseudoknot.The PTE C-G pseudoknot formation enables the virus to interact with the cap-binding pocket of eIF4E. Although the establishment of a suitable pseudoknot between the C-G domains of MTE is questionable, MTE interacts with initiation factor 4E. Similar to other 3'CITEs, the MTE used long-distance base-pairing to bring the translation machinery to the 5' end. The eIF4E-MTE RNA-protein interaction model was investigated by mutating both the RNA and the protein. Comparison of electrophoretic mobility shift assays (EMSA) results using mutated eIF4E with other PTE-like structure indicated that even though MCMV interact with eIF4E, it might use a mechanism that has yet to be characterized.",,"Carino EJ,Whitham S,Lubberstedt T,Moss W,Yang B",,2020,,,,Ph.D. Thesis
Development of a Mobile Application Platform for Self-Management of Obesity Using Artificial Intelligence Techniques,"Obesity is a major global health challenge and a risk factor for the leading causes of death, including heart disease, stroke, diabetes, and several types of cancer. Attempts to manage and regulate obesity have led to the implementation of various dietary regulatory initiatives to provide information on the calorie contents of meals. Although knowledge of the calorie content is useful for meal planning, it is not sufficient as other factors, including health status (diabetes, hypertension, etc.) and level of physical activity, are essential in the decision process for obesity management. In this work, we present an artificial intelligence- (AI-) based application that is driven by a genetic algorithm (GA) as a potential tool for tracking a user’s energy balance and predicting possible calorie intake required to meet daily calorie needs for obesity management. The algorithm takes the users’ input information on desired foods which are selected from a database and extracted records of users on cholesterol level, diabetes status, and level of physical activity, to predict possible meals required to meet the users need. The micro- and macronutrients of food content are used for the computation and prediction of the potential foods required to meet the daily calorie needs. The functionality and performance of the model were tested using a sample of 30 volunteers from the University of Ghana. Results revealed that the model was able to predict both glycemic and non-glycemic foods based on the condition of the user as well as the macro- and micronutrients requirements. Moreover, the system is able to adequately track the progress of the user’s weight loss over time, daily nutritional needs, daily calorie intake, and predictions of meals that must be taken to avoid compromising their health. The proposed system can serve as a useful resource for individuals, dieticians, and other health management personnel for managing obesity, patients, and for training students in fields of dietetics and consumer science.",,"Sefa-Yeboah SM,Osei Annor K,Koomson VJ,Saalia FK,Steiner-Asiedu M,Mills GA,Hu F",,2021,,10.1155/2021/6624057,https://doi-org.proxy.bnl.lu/10.1155/2021/6624057;http://dx.doi.org/10.1155/2021/6624057,Journal Article
Examining Teacher Performance in Ghana,"This study used longitudinal data on 444 teachers and 3,435 students to examine teacher performance in Ghana. The study is divided into two parts. The first part of the study examined factors that mediate the causal effects of a kindergarten teacher training program on classroom quality and student outcomes. Specifically, it examined whether teachers' knowledge of the learning content, teachers' implementation quality of behavioral and instructional practices and teachers' professional well-being were significant mediators of the treatment effect. It utilized a causal mediation approach, which allowed the average causal mediation effects to be parametrically and nonparametrically identified under a set of minimum conditions. The study found that implementation quality was a significant mediator of positive treatment effect on classroom quality across time. This effect persisted even when teacher knowledge and professional well-being were accounted for. The study also found small marginal mediation effects on student outcomes, including a positive mediation effect on literacy and a negative mediation effect on executive functioning in the presence of all mediators. Overall, this study provides empirical evidence to design future interventions that place more emphasis on the influential pathway of implementation quality to yield positive impacts, particularly in early education contexts.The second part of the study examined teacher profiles that provide diagnostic information about teachers' instructional strengths and weaknesses. It applied stage-wise cluster analysis to reveal different subpopulations of teachers and study how they relate to student outcomes. The study found six profiles of teachers with varying professional well-being and classroom practices, including two that were significantly associated with positive student learning across all four domains of numeracy, literacy, socioemotional development and executive functioning. Overall, the results allow easy identification of growth opportunities for each profile of teachers that helps provide formative feedback and targeted support to facilitate high quality teaching and maximize positive student learning outcomes.",,"Fatima SF,Wolf S,McDermott P,Rovine M",,2020,,,,Ph.D. Thesis
"Mathematics in Computing: An Accessible Guide to Historical, Foundational and Application Contexts","This clearly written and enlightening textbook provides a concise, introductory guide to the key mathematical concepts and techniques used by computer scientists. Topics and features: ideal for self-study, offering many pedagogical features such as chapter-opening key topics, chapter introductions and summaries, review questions, and a glossary; places our current state of knowledge within the context of the contributions made by early civilizations, such as the ancient Babylonians, Egyptians and Greeks; examines the building blocks of mathematics, including sets, relations and functions; presents an introduction to logic, formal methods and software engineering; explains the fundamentals of number theory, and its application in cryptography; describes the basics of coding theory, language theory, and graph theory; discusses the concept of computability and decideability; includes concise coverage of calculus, probability and statistics, matrices, complex numbers and quaternions.",,O'Regan G,,2012,,,,Book
"Mathematics in Computing: An Accessible Guide to Historical, Foundational and Application Contexts","This clearly written and enlightening textbook provides a concise, introductory guide to the key mathematical concepts and techniques used by computer scientists. Topics and features: ideal for self-study, offering many pedagogical features such as chapter-opening key topics, chapter introductions and summaries, review questions, and a glossary; places our current state of knowledge within the context of the contributions made by early civilizations, such as the ancient Babylonians, Egyptians and Greeks; examines the building blocks of mathematics, including sets, relations and functions; presents an introduction to logic, formal methods and software engineering; explains the fundamentals of number theory, and its application in cryptography; describes the basics of coding theory, language theory, and graph theory; discusses the concept of computability and decideability; includes concise coverage of calculus, probability and statistics, matrices, complex numbers and quaternions.",,O'Regan G,,2014,,,,Book
Clear P-Wave Arrival of Weak Events and Automatic Onset Determination Using Wavelet Filter Banks,"P-wave arrivals of many weak events cannot be precisely determined manually. Difference in power levels between noise and P-wave in wavelet detail of weak events enables us to determine P-wave arrival manually. Because of this power difference, automatic onset detection and picking algorithm is introduced using the same wavelet detail. Parameter settings are not needed as algorithm will work on data generated by either short or very broad band seismometers. Application of the proposed algorithm on data of three stations of Egyptian National Seismic Network (ENSN) in Cairo region shows a maximum standard deviation of 0.14 seconds of the corresponding manual picks.",,"Hafez AG,Khan MT,Kohda T",,2010,715–723,10.1016/j.dsp.2009.10.002,https://doi-org.proxy.bnl.lu/10.1016/j.dsp.2009.10.002;http://dx.doi.org/10.1016/j.dsp.2009.10.002,Journal Article
HT '09: Proceedings of the 20th ACM Conference on Hypertext and Hypermedia,"Welcome to Hypertext 2009, the 20th ACM Conference on Hypertext and Hypermedia. The conference continues in its tradition of a diverse and multidisciplinary approach to the study of the ""link"" in all its manifestations. Links between any types of objects, from documents and media to people, are at the center of Hypertext from many perspectives: their design, management, applications, semantics, presentation, dynamics, effects on society, and the knowledge that can be derived from their analysis.Hypertext 2009 also continues to be organized around tracks autonomously organized with separate chairs and program committees. This year we have three tracks in the technical program.The Information Structure and Presentation track represents a multitude of topics, which were traditionally represented at ACM Hypertext Conferences. The track program targets formal study of scholarly, structural, sculptural, spatial, open, dynamic and adaptive or any other type of hypertext (or Web-based information system). This track also focuses on how hypertext approaches and technologies can be applied to structure and present information in diverse domains, and how hypertext techniques can be exploited in classical and advanced applications.The People, Resources, and Annotations track explores social annotations, which have rapidly risen as one of the most exciting recent developments in Web science. Users can easily markup other authors' resources via collaborative mechanisms such as tagging, filtering, voting, editing, classification, and rating. These social processes lead to the emergence of many types of links between texts, users, concepts, pages, articles, photos, videos, tags, and so on. The track's focus is on design, analysis, and modeling of information systems driven by social linking.Finally, the Hypertext and Community track examines and reflects upon social cyberculture in electronic media, ranging from literary fiction and creative scholarship to blog and microblog networks, social sites, games, auctions, net art, and markets.The Hypertext 2009 technical program received 117 submissions, a 70% increase from the previous year, despite the global economic downturn. These submissions originated from 36 countries in Europe, Asia, Africa, Australia, North and South America. 37 papers (26 long and 11 short) were selected for presentation at the conference, yielding an acceptance rate of 31.6%. We feel that the papers contained in these proceedings represent a strong, diverse, and exciting program.We are fortunate to feature two stellar keynotes by Lada Adamic of the University of Michigan on The Social Hyperlink and by Ricardo Baeza-Yates of Yahoo! Research on Relating Content by Web Usage. In addition, the program includes a poster & demo session (also in these proceedings), an ACM student research competition, and four workshops: Web 3.0: Merging Semantic Web and Social Web organized by Federica Cena (University of Torino, Italy), Pasquale Lops (University of Bari, Italy), and Rosta Farzan (University of Pittsburgh, USA).Dynamic and Adaptive Hypertext: Generic Frameworks, Approaches and Techniques organized by Paul De Bra and Mykola Pechenizkiy (Eindhoven University of Technology, the Netherlands).New Forms of Xanalogical Storage and Function organized by Fabio Vitali and Angelo Di Iorio (University of Bologna, Italy), and Jamie Blustein (Dalhousie University, Canada).Tagging Dynamics in Online Communities organized by Vittorio Loreto and Andrea Capocci (Sapienza University of Rome, Italy).Finally, Hypertext 2009 attendees have a chance to experiment with applications mixing real-world data and on-line data. Active RFID tags in the badges of volunteers track the real-time relations of physical proximity between the attendees. The data collection and visualization systems is provided by the SocioPatterns.org project and exposes API methods that allow developers to mash up real-world links between the attendees with other types of linking information from the Web.We are proud that Hypertext 2009 takes place in Torino (Turin). The capital of the Piedmont region in northwestern Italy, Torino lies at the foot of the Alps, the majestic mountains that hosted the 2006 Winter Olympics. First capital of the Kingdom of Italy, then one of the European centers of baroque, today Torino is a dynamic city known for its industry (Fiat and Lancia car makers are headquartered here); art and culture (its 40+ museums include the world's largest Egyptian collection outside of Cairo or the National Museum of Cinema located inside the Mole Antonelliana); sports (home of Juventus FC and Torino FC); research and education (including the University of Torino, the Polytechnic, and the Institute for Scientific Interchange Foundation); and cuisine (solid chocolate was born here in the 18th century). We hope that all attendees have an opportunity to enjoy the many cultural, artistic, historic, architectural, and culinary pleasures that Torino has to offer.",,,,2009,,,,Book
Automatic Expandable Large-Scale Sentiment Lexicon of Modern Standard Arabic and Colloquial,"In subjectivity and sentiment analysis (SSA), there are two main requirements are necessary to improve sentiment analysis effectively in any language and genres, first, high coverage sentiment lexicon - where entries are tagged with semantic orientation (positive, negative and neutral) - second, tagged corpora to train the sentiment classifier. Much of research has been conducted in this area during the last decade, but the need of building these resources is still ongoing, especially for morphologically-Rich language (MRL) such as Arabic. In this paper, we present an automatic expandable wide coverage polarity lexicon of Arabic sentiment words, this lexical resource explicitly devised for supporting Arabic sentiment classification and opinion mining applications. The lexicon is built using a seed of gold-standard Arabic sentiment words which are manually collected and annotated with semantic orientation (positive or negative), and automatically expanded with sentiment orientation detection of the new sentiment words by exploiting some lexical information such as part-of-speech (POS) tags and using synset aggregation techniques from free online Arabic lexicons, thesauruses. We report efforts to expand a manually-built our polarity lexicon using different types of data. Finally, we used various tagged data to evaluate the coverage and quality of our polarity lexicon, moreover, to evaluate the lexicon expansion and its effects on the sentiment analysis accuracy. Our data focus on modern standard Arabic (MSA) and Egyptian dialectal Arabic tweets and Arabic microblogs (hotel reservation, product reviews, and TV program comments).",,"Ibrahim HS,Abdou SM,Gheith M",,2015,94–99,10.1109/ACLing.2015.20,https://doi-org.proxy.bnl.lu/10.1109/ACLing.2015.20;http://dx.doi.org/10.1109/ACLing.2015.20,Conference Paper
"Disciplining Public Employees in Kuwait: A Search for Increased Fairness, Coherence, and Protection in the Light of Prevailing and Expanding Employee Duties, Particularly the Duty to Whistleblow","This thesis critically analyses the equity of disciplinary procedures for public employees in Kuwait. In particular, it examines the degree to which current arrangements and legal provisions facilitate and guarantee, natural justice and due process in the context of employment in general, and in relation to whistleblowing in particular. The research evaluates comparative jurisdictions in order to highlight both the strengths and weaknesses of the existing provision in Kuwait.This thesis further examines the relationship between the disciplinary measure for public employees in Kuwait and their duties to whistle-blow when necessary, and to this end it provides an overview of parallel developments in historical and closely linked Arab and Western countries (i.e. UAE, Egypt, UK, and France,) with a view of evaluating the strengths and shortcomings of existing Kuwaiti provisions. Emphasis is placed on the government sector, with the aim of highlighting its deficiencies and limitations, and subsequently proposing a means by which these can be rectified, leading to improved performance within the public services. This study provides a broad outline of the current disciplinary systems, along with the duty of whistleblowing, in order to identify the various legal concepts operating within the public services in the above-mentioned countries.Particular focus is given on the law of Whistleblowing in Kuwait, requiring the disclosure of information in instances of corruption. In particular this thesis examines the potential conflict between the Whistleblowing Law and Article 25(5) of the Kuwait Civil Service law no. 15 of 1979, which prohibited public employees from disclosing confidential information. This current study aims to understand, question, and draw suggestions from both laws, with the overall aim to improve the current disciplinary system in Kuwait. A detailed literature review suggests that disciplinary procedures relating to public employees in Kuwait mirror incremental developments to other linked Arab states which essentially consist of a mix of legal provisions ranging from constitutional provisions, case law, and administrative arrangements. Such provisions, however, relate either to citizens generally or to public employees in particular and are not framed in an employment law context and therefore may not be sufficiently adaptable to respond to a rapidly changing employment law context and may lag behind legitimate employee expectations. This thesis, therefore, considers the benefit of framing disciplinary procedures within a natural law framework and the compatibility of the concept to Islamic principles.The conceptual discussion in this thesis is also supplemented by empirical evidence relevant to the essential issues that are needed to be considered when regulating the disciplinary provisions applicable to public employees. A questionnaire has been administered to gain concrete opinion of employees about the current legal framework in Kuwait. A representative sample of fifty public sector employees in the State of Kuwait were interviewed, and further discussions were undertaken with a number of key and senior employees working at the Ministry of Justice and the Kuwait National Assembly (i.e. the Kuwaiti Parliament). These employees all had substantial experience from the disciplinary system in Kuwait, along with the duty to whistle-blow. The mixed methodology methods utilised in this thesis, deals with the rationale for establishing specific disciplinary rules for public employees, and the main principles of natural justice by which employers should abide. Reform under the Kuwait's civil service law is needed to incorporate principles of justice and fairness to all employees, regardless of rank differences because although employment in the public sector in Kuwait can provide for a very attractive and rewarding career it paradoxically, as highlighted this study, can also lead to individuals feeling trapped and vulnerable because of the pervasive and hierarchical nature of existing disciplinary procedures within the public service. It may also contribute indirectly to the prevalence of public employee lethargy.This thesis concludes with a summary of results, followed by a number of recommendations including the need for a specialist Court, aimed at improving the regulations used in the disciplinary and whistleblowing system of Kuwait. The study highlights the need for reform since it provides evidence to suggest that existing disciplinary procedures are perhaps overused and applied inconsistently.",,Al-Haidar F,,2020,,,,Ph.D. Thesis
Multi-Objective Optimisation of Robotic Active Particle Swarms for Continuous Repair of Large Scale High Value Structures,"The manufacture and creation of large scale high value structures has been done by humans for centuries. Examples include the Egyptian pyramids, Bridges, Modern Skyscrapers to mention a few. These structures are large but also provide a high value in terms of economy, culture, display of prestige to mention a few. With advances in space technology, we are bound to see these large scale high value structures constructed in space. The vacuum of space present us with the challenges of repairing these structures. This is due to the inhospitable and dangerous environment of space. With increasing number of structures in space, there is bound to be more debris created resulting in high impact damages to these high value structures. Inspired by the biological blood clotting process and biological active particles, in this work, we propose the use of a swarm of live on artificial active particles for the purposes of continuous and timely repair of these structures. We tackle one of the challenges of artificial active particles research; the ability to navigate in crowded and obstacle filled environments. This challenge can be viewed from the perspective of a constrained multi-objective optimisation problem in which a balance between exploration of an environment and its exploitation needs to be achieved while taking into consideration the various other constraints that apply to an active particle. In this work, we show how artificial active particles could avoid obstacles in their environment through the use of an exploration mechanism and find damaged sites. Our results show that as the ability to explore increases, the active particles are able to navigate around obstacles and find a damaged site. However, there is a limit to this.",,Oyekan J,,2021,1312–1318,10.1109/CEC45853.2021.9504749,https://doi-org.proxy.bnl.lu/10.1109/CEC45853.2021.9504749;http://dx.doi.org/10.1109/CEC45853.2021.9504749,Conference Paper
Determinants of Stock Prices in the Egyptian Stock Market : Traditional Asset Pricing Models Versus Behavioural Asset Pricing Models,"The aim of this thesis is to determine a valuation model for stocks in the Egyptian stock market by comparing conventional and behavioural asset pricing models. To achieve this aim, this thesis constructs and tests the following extensions of the Fama and French three-factor model over the time period 2004-2016: (i) time-varying factor loadings; (ii) time-varying risk premia; and (iii) introducing a behavioural risk factor. The cross-sectional tests applied on both individual stocks and portfolios double-sorted on size and the book-to-market ratio show that the Fama and French three-factor model that captures time-variation in betas using either the rolling regression approach or the DCC-GARCH model cannot fully capture the cross-sectional variation in stock returns as both specifications have high and significant pricing errors. Similarly, scaling the factor loadings in the Fama and French three-factor model using the Treasury bill rate, size, the book-to-market ratio and sentiment does not enable the model to capture some of the prominent anomalies in financial markets such as turnover and short-term momentum effects. Modelling time-variation in risk premia, based on simple bull and bear regimes identified using a Markov-switching model, along with time-variation in risk using the DCC-GARCH provides a modest improvement to the results of the model that only captures the time-variation in risk. Specifically, although the hypothesis of time-varying risk premia is never rejected, the model is still weakened by the negative weighted average risk premia of the market factor and the high pricing errors. Finally, the results show that augmenting the Fama and French three-factor model with an additional behavioural factor does not lead to major changes in the performance of the model and that the sentiment risk factor is not significantly priced in the Egyptian stock market. However, by investigating the characteristics of stocks that are most sensitive to changes in sentiment, the results reveal that small and highly volatile are the most sensitive stocks which imply that sentiment is a non-diversifiable risk factor.",,Abdou R,,2019,,,,Ph.D. Thesis
Intelligent Interactive Multimedia Systems and Services: Proceedings of 2018 Conference,"This volume presents a series of carefully selected papers on the theme of Intelligent Interactive Multimedia Systems and Services (IIMSS-18), but also including contributions on Innovation in Medicine and Healthcare (InMed-18) and Smart Transportation Systems (STS-18). The papers were presented at the Smart Digital Futures 2018 multi-theme conference, which grouped the AMSTA, IDT, InMed, SEEL, STS and IIMSS conferences in one venue in Gold Coast, Australia in June 2018. IIMSS-18 included sessions on 'Cognitive Systems and Big Data Analytics', 'Data Processing and Secure Systems', 'Innovative Information Services for Advanced Knowledge Activity', 'Autonomous System' and ' Image Processing'. InMed-18 papers cover major areas of 'Digital Architecture for Internet of Things, Big data, Cloud and Mobile IT in Healthcare' and 'Advanced ICT for Medical and Healthcare'. STS-18 papers provide a comprehensive overview of various aspects of current research into intelligent transportation technology.",,"Pietro G,Gallo L,Howlett RJ,Jain LC,Vlacic L",,2018,,,,Book
AusDM '07: Proceedings of the Sixth Australasian Conference on Data Mining and Analytics - Volume 70,"The Australasian Data Mining Conference series AusDM, started in 2002, is the annual flagship meeting for data mining and analytics professionals in Australia. Both scholars and practitioners present the stateof- the-art in the field. Endorsed by the peak professional body, the Institute of Analytics Professionals of Australia, AusDM has developed a unique profile in nurturing this joint community. The conference series has grown in size each year from early workshops held in Canberra (2002, 2003) and Cairns (2004) to conferences in Sydney (2005, 2006). This year we are delighted to be co-hosted with the Twentieth Australian Joint Conference on Artificial Intelligence on the Gold Coast, Queensland, and the Second International Workshop on Integrating AI and Data Mining. This year's event has been supported by• Togaware, again hosting the website and the conference management system, coordinating the review process and other essential expertise;• Griffith University for providing the venue, registration facilities and various other support;• the Institute of Analytic Professionals of Australia (IAPA) for facilitating the contacts with the industry;• the ARC Research Network on Data Mining and Knowledge Discovery, for providing financial support;• the e-Markets Research Group, for providing essential expertise for the event;• the Australian Computer Society, for publishing the conference proceedings;• StatSoft for their support;• data mining postgraduate students from Queensland University of Technology for their local support.This year the Steering Committee and IAPA have recognised the importance of education in data mining and we have included a special panel session devoted to Data Mining Education. Also this year, for the first time, we have presented a Best Paper Award (voted by the peer review) and a Best Presentation Award (voted by conference delegates). We are delighted to expand the social program this year and hope that conference attendees will enjoy this extra time to make new contacts and to trade ""war stories"".The conference program committee reviewed 69 submissions. This was an almost 20% increase in the number of submissions from last year. From these submissions 26 were selected for publication and presentation. This was an acceptance rate of 38%. AusDM follows a rigid double blind peer-review process and ranking-based paper selection process. All papers were extensively reviewed by at least three referees drawn from the program committee. We would like to note that the cut-off threshold has been high (5 on a 7 point scale). This is testament to the high quality of submissions. We would like to thank all those who submitted their work to the conference. We will continue to extend the conference format to be able to accommodate more presentations. We are proud to include in these proceedings the papers from the Second International Workshop on Integrating AI and Data Mining. Papers published in both volumes by the Australian Computer Society are indexed and available for download.Data mining and analytics today have advanced rapidly from the early days of pattern finding in commercial databases. They are now a core part of business intelligence and inform decision making in many areas of human endeavour including science, business, health care and security. Mining of unstructured text, semi-structured web information and multimedia data have continued to receive attention, as have professional challenges to using data mining in industry. Accepted submissions have been grouped into seven sessions reflecting these application areas. Three invited industry keynote sessions put the research into context.",,,,2007,,,,Book
Factors Affecting Cloud Computing Adoption in Sierra Leone: A Quantitative Analysis Based on Technology-Organization-Environment Framework (Toe) & Human-Organization-Technology Fit (Hot-Fit) Theory,"Information Technology (IT) services are now dynamic, movable, and interactive. This represents a paradigm shift in the IT industry, as IT companies and organizations can now rapidly migrate IT infrastructure, data, and software applications to the cloud to optimize the IT environment and reduce high operational costs. This paradigm shift is, however, limited to developed countries. Using Sierra Leone as a case study, this research examined the factors affecting the adoption rate of Cloud Computing in government organizations. It looked into the roles of top management support and self-efficacy as moderators on the relationship between technology readiness and cloud computing adoption and IT knowledge and cloud implementation adoption. This study used both the technology–organization–environment (TOE) framework and the Human-Organization-Technology fit (HOT-fit) theory to develop a model that examined the adoption challenges in SL. The research model was tested with twenty-five government organizations, using four hundred and four questionnaires. Furthermore, the study used SmartPLS software to analyze the collected data quantitatively. The findings showed that internet connectivity, perceived benefits, information security, technology readiness, and IT knowledge are significant factors affecting the adoption of cloud computing in Sierra Leone. . This research contributes to organizational IT adoption literature in Africa. In addition, it serves as a guide to understanding the factors that significantly influence organizations in adopting cloud computing in Sierra Leone. Although this research provides a sound theoretical model that adequately predicts cloud computing adoption in Sierra Leone, some limitations should be considered for future research. One such limitation is that the sample data is limited to only the government employees of Sierra Leone, which implies that this research reflects only the perspective of government employees. Therefore, the findings of this study cannot be used for non-governmental organizations. It should also be noted that only two theories and four factors were considered for testing in this study. I recommend that future research consider developing and testing other theories and hypotheses to obtain new findings for governmental and non-governmental organizations. Keywords: Information Technology, Cloud Computing, Sierra Leone, TOE Framework HOT-fit model, SmartPLS.",,"Cooper JD,Kenneth Cromer,Carole Angolano",,2022,,,,Ph.D. Thesis
Cause of and Factors Contributing to Stillbirth in Sub-Saharan Africa,"BackgroundEvery year, an estimated 2.6 million stillbirths occur worldwide, with up to 98% occurring in low- and middle-income countries (LMIC). Most stillbirths are preventable. To develop strategies and take effective actions to end preventable stillbirths, a good understanding of the cause of death and its contributing factors is necessary. There is, however, a paucity of data from most LMIC settings. This study aimed to determine the cause of stillbirth in LMIC using three methods of assessment, and to assess quality of care delivered to mothers who had stillbirth.MethodsThe study involved 1,563 stillbirths which occurred in 12 selected secondary and tertiary hospitals in Kenya, Malawi, Sierra Leone and Zimbabwe. The cause of death was determined by: (1) consensus of healthcare providers (HCPs) through stillbirth review; (2) expert review of cases and; (3) computer algorithms. Cause of death was classified using the classification according to Relevant Condition at Death (ReCoDe) and the International Classification of Diseases for Perinatal Mortality (ICD-PM). Quality of antenatal and intrapartum care and health system factors were reviewed using a set of criteria. ResultsA total of 1,329 cases were reviewed, of which 1,267 (95.3%) stillbirths met the inclusion criteria. By country, the stillbirth rate ranged from 20.3 (Malawi) to 118.1 (Sierra Leone) per 1,000 births. The distribution of the major causes of stillbirth differed by method of assessment: asphyxia (18.5% – 37.4%), placental disorders (8.4% – 15.1%), hypertensive disorders in the mother (5.1% – 13.6%), infection (4.3% – 9.0%), cord problems (3.3% – 6.5%), and ruptured uterus due to obstructed labour (2.6% – 6.1%). Information was insufficient to assign cause of stillbirth in 17.9% - 26.0% of cases. Significant agreement was observed between cause of stillbirth assigned by the expert panel and by HCP (k=0.69; p<0.0005) but there was a weaker agreement between expert panel and when using computer algorithms (k=0.34; p<0.0005).Using ReCoDe, intrapartum events (mainly intrapartum asphyxia) contributed to most of the deaths, followed by maternal diseases (mainly hypertensive disorders and infection), placental and fetal conditions. With application of ICD-PM, 42.0% were antepartum, 50.7% were intrapartum and 7.3% could not be categorised. The major categories accounting for the death were: intrapartum hypoxia and fetal growth restriction. Major contributing maternal conditions in ICD-PM were: M1 (placental, cord and membranes) and M3 (other complications of labour and delivery). Poor quality of care during antenatal care was identified in 97.8% of cases, and only 30.7% of cases of Caesarean section were conducted within one hour of decision. For 414 (37.9%) stillbirths, the outcome could have been different with better care.Conclusion Stillbirth rate was high, with high variations between countries. HCPs should be encouraged to conduct reviews and act upon findings to improve quality of care. Data requirements of computer algorithms need to be balanced between ability to find a cause and the availability of information. The new ICD-PM could work in LMIC, but there is the need for more guidance on how to handle cases of stillbirths whose time of death cannot be determined.",,Aminu M,,2017,,,,Ph.D. Thesis
A Comparative Study of Smartphone-User Security Perception and Preference towards Redesigned Security Notifications,"In this paper, we conducted a survey of 206 smartphone users of different demographics in Japan and Tanzania, two countries with different security and privacy expectations, to analyse users' cybersecurity knowledge and attitudes. We studied password choices, smartphone lock behaviour, phishing awareness and attitudes towards public Wi-Fi. We also assessed the acceptability of our novel notification alert for smartphone OS security updates. We found that data privacy is equally important to the majority of participants, 70%, in both countries. However, most participants did not know the characteristics of a strong password for web applications despite being highly conscious of physical access security which was characterised by smartphone locking (78% of participants). We also found that phishing awareness in Tanzania is not satisfactory, with the majority of the participants, 78%, likely to open a link from an unknown email source, whereas in Japan only 32% are likely to do so. Participants in Japan were also slightly more likely to read terms and conditions when connecting to public Wi-Fi (36% vs. 27%). Our novel notification design which integrated security updates with other free information services seemed promising for increasing security awareness and update compliance. Participants were more willing to accept update notices that provided guidance on how-to to perform a required task than plain notices.",,"Ndibwile JD,Luhanga ET,Fall D,Miyamoto D,Kadobayashi Y",,2018,,10.1145/3283458.3283486,https://doi-org.proxy.bnl.lu/10.1145/3283458.3283486;http://dx.doi.org/10.1145/3283458.3283486,Conference Paper
Statistical Modeling and Inference for Complex-Structured Count Data with Applications in Genomics and Social Science,"This dissertation describes models, estimation methods, and testing procedures for count data that build upon classic generalized linear models, including Gaussian, Poisson, and negative binomial regression. The methodological extensions proposed in this dissertation are motivated by complex structures for count data arising in three important classes of scientific problems, from both genomics and sociological contexts. Complexities include large scale, temporal dependence, zero-inflation and other mixture features, and group structure.The first class of problems involves count data that are collected from longitudinal RNA sequencing (RNA-seq) experiments, where the data consist of tens of thousands of short time series of counts, with replicate time series under treatment and under control. In order to determine if the time course differs between treatment and control, we consider two questions: 1) whether the treatment affects the geometric attributes of the temporal profiles and 2) whether any treatment effect varies over time. To answer the first question, we determine whether there has been a fundamental change in shape by modeling the transformed count data for genes at each time point using a Gaussian distribution, with the mean temporal profile generated by spline models, and introduce a measurement that quantifies the average minimum squared distance between the locations of peaks (or valleys) of each gene's temporal profile across experimental conditions. We then develop a testing framework based on a permutation procedure. Via simulation studies, we show that the proposed test achieves good power while controlling the false discovery rate. We also apply the test to data collected from a light physiology experiment on maize.To answer the second question, we model the time series of counts for each gene by a Gaussian-Negative Binomial model and introduce a new testing procedure that enjoys the optimality property of maximum average power. The test allows not only identification of traditional differentially expressed genes but also testing of a variety of composite hypotheses of biological interest. We establish the identifiability of the proposed model, implement the proposed method via efficient algorithms, and expose its good performance via simulation studies. The procedure reveals interesting biological insights when applied to data from an experiment that examines the effect of varying light environments on the fundamental physiology of a marine diatom.The second class of problems involves analyzing group-structured sRNA data that consist of independent replicates of counts for each sRNA across experimental conditions. Most existing methods---for both normalization and differential expression---are designed for non-group structured data. These methods may fail to provide correct normalization factors or fail to control FDR. They may lack power and may not be able to make inference on group effects. To address these challenges simultaneously, we introduce an inferential procedure using a group-based negative binomial model and a bootstrap testing method. This procedure not only provides a group-based normalization factor, but also enables group-based differential expression analysis. Our method shows good performance in both simulation studies and analysis of experimental data on roundworm.The last class of problems is motivated by the study of sensitive behaviors. These problems involve mixture-distributed count data that are collected by a quantitative randomized response technique (QRRT) which guarantees respondent anonymity. We propose a Poisson regression method based on maximum likelihood estimation computed via the EM algorithm. This method allows assessment of the importance of potential drivers of different quantities of non-compliant behavior. The method is illustrated with a case study examining potential drivers of non-compliance with hunting regulations in Sierra Leone.",,"Cao M,Estep D,Meyer M,Peers G",,2020,,,,Ph.D. Thesis
Integrating Cognitive Support with CASE-Tools for Design Recovery,"Reverse engineering (RE) activities account for the largest part of current expenses in software maintenance. The RE support provided by existing design tools is limited to simple mappings of idioms in the source code to diagrammatic primitives. Human analysts still have togo through the laborious task of manually detecting patterns and creating higher abstractions. Perhaps the most important challenge in automating RE is to deal with the imperfect knowledge inherently involved in the detection process. Recently, a number of researchershave developed prototypes of design tools with knowledge-based RE capabilities. For several reasons these research prototypes are rarely acceptable for industrial-strength applications. Consequently, innovative technologies often have difficulties reaching their target audience. We try to address this issue by adopting established design tools and extending them with knowledge-based RE functionality. This paper reports on the development of such an extension component and contains a case study that shows the feasibility of this approach.",,Jahnke JH,,2002,145–154,,,Conference Paper
Essays on Mediating Conflict and Improving Institutional Responsiveness to Marginalized Populations in Weak States,"A large literature demonstrates that states with high-functioning institutions tend to perform better across a wide range of outcomes. This dissertation focuses on so-called weak, or fragile, states, categorized as such due to deficiencies in formal state institutions that can lead to lower levels of welfare overall, and in particular for marginalized populations that face bias from customary or informal institutional alternatives. I aim to answer two central questions. First, how can public support for civil war peace settlements -- the successful implementation of which is widely viewed as a first step toward establishing the rule of law and effective governance -- be increased? Second, how can weak states better address the demands of those hurt most by their shortcomings, marginalized populations, and what are the overall welfare implications of such efforts? My findings indicate that reforming state institutions may offer more potential than attempts to reform customary ones, and that addressing individuals' fears of out-group exploitation may be necessary to build public support for peace settlements.The Introduction lays the stakes for the dissertation, reviews my general framework, discusses each of the chapters at greater length, addresses my methodological choices, and reflects on what the three papers might tell us about efforts to mediate conflicts and improve institutions in weak states. I argue that the contexts and questions I study are important due to the large number of countries to which they apply, the dilemmas they pose and their real-world consequences, and what they can add to our knowledge of institutions in weak states and beyond. I also explain my general framework, clarifying the terms that I use and the grounding for the claims that I make -- for instance, that marginalized populations are often hurt most by a weak formal state. Next, I summarize each chapter and my methodological choices. I conclude with a discussion of what we can learn from my dissertation, and how I hope to further extend my research -- most notably by gaining clarity on the conditions under which institutional reforms in weak states succeed, as well as their broader ramifications.Chapter 2 investigates how leader endorsements shape public opinion toward peace settlements during ongoing conflicts. Drawing on research in political psychology which finds that conflict can decrease individuals' trust in out-group members and increase the value they attach to security, we hypothesize that endorsements from out-group leaders will act as a strong negative cue of a settlement's merits. We conducted an endorsement experiment in South Sudan in 2016 wherein nearly 1,000 respondents were asked to state their support for tentative peace deal policies that were either endorsed by one of the two warring leaders (treated groups) or by no one (control group). We found that peace deal support dropped precipitously where policies were first endorsed by an out-group leader, particularly for those from communities targeted most violently by that out-group. Interestingly, in-group leader endorsements did not increase support, which we argue reflects a loss in credibility sourced in in-group leaders' repeated failures to deliver peace. Findings indicate that increasing peace deal support, and with it the likelihood of establishing the rule of law, may require assuaging individuals' fears of out-group exploitation -- solely providing policy information may be insufficient. The paper is joint with Prabin Khadka and is forthcoming at the American Journal of Political Science.Chapters 3 and 4 evaluate effects of reforms in India and South Central Somalia aimed at improving the responsiveness to marginalized populations of, respectively, state and customary institutions. Chapter 3 studies how changing formal state political institutions through electoral quotas mandating descriptive representation for marginalized communities affects development outcomes overall, and across groups. We examine Scheduled Areas in India, which reserve political office for the historically disadvantaged Scheduled Tribes. We apply a new theoretical framework and dataset of 217,000 villages to evaluate the overall impact of affirmative action on development, as well as its distributional consequences for minorities and non-minorities. Examining effects on the world's largest employment program, the National Rural Employment Guarantee Scheme, we find that reservations deliver no worse overall outcomes, that there are large gains for targeted minorities, and that these gains come at the cost of the relatively privileged, not other minorities. We also find broader improvements in other pro-poor policies, including a rural roads program and general public goods. Contrary to the expectations of affirmative action skeptics, our results indicate that affirmative action can redistribute both political and economic power without hindering overall development. This work is joint with Saad Gulzar and Benjamin Pasquale.Chapter 4 presents results from a field experiment we conducted with UN Women and the Somali Ministry of Justice in a context where the state is sufficiently weak that improvements in formal state institutions are predicted to take decades, or longer, to deliver results. Recent studies indicate that reforming traditional systems can produce positive short-term gains in general welfare, but whether there are welfare gains for populations that have historically alleged bias and faced exclusion from those institutions is not clear. We conducted a randomized controlled trial to evaluate a 7-month UN Women's program that randomly assigned women leaders, and male traditional and religious elders who preside over the customary justice system, from 50 villages to receive a type of therapy, nonviolent communication, as well as with training on how to impart lessons to residents in their villages. We found that the training had differential effects on men and women. The training met its goals with male powerbrokers, who became more willing to recognize discrimination against women and more likely to include women in the dispute resolution process. However, the training largely depressed women's demand for customary institutional justice, which we attribute primarily to discouraging information conveyed during mixed-gender sessions about elders' envisioned pace and breadth of change. Our findings may highlight limits of social norms and attitudinal interventions to produce meaningful reforms to customary institutions in weak, conflict-ridden states. We are planning to conduct a follow-up survey to determine longer-term effects in the coming months. The paper is joint with Prabin Khadka.",,"Haas N,McClendon G,Samii C",,2020,,,,Ph.D. Thesis
GADSA: Decision Support App for Antibiotics Prescribing in Nigeria,"GADSA (Gamified Antimicrobial Stewardship Decision Support App) is a decision support tool to improve evidence-based prescribing, designed to be used at the point-of-care to help clinicians comply with guidelines in their everyday practice. The app represents a novel cross-platform, mobile decision support tool, integrating principles from serious games and gamification, to improve compliance with prescription guidelines of Surgical Antibiotic Prophylaxis (SAP) in Nigeria. This paper focuses on the decision support component of the mobile application, integrating the World Health Organisation and Sanford guidelines for SAP prescriptions.",,"Birjovanu G,Wood C,Olufemi O,Ogunsola F,Okonji P,Kpokiri E,Luedtke S,Shallcross L,Soriano D,Lefevre CE,Hayward A,Molnar A,NCube F,Wiseman S,Kostkova P",,2019,9–10,10.1145/3357729.3357734,https://doi-org.proxy.bnl.lu/10.1145/3357729.3357734;http://dx.doi.org/10.1145/3357729.3357734,Conference Paper
Strategic Culture and Violent Non-State Actors : A Comparative Strategic Cultural Analysis of Al-Qaida and Al-Qaida in the Islamic Maghreb,"While strategic culture has traditionally been applied to states, this work adds to the emerging literature applying strategic cultural approaches to VNSAs. This analysis goes beyond these ideational approaches by also incorporating the concept of practices. In contrast to Alastair Johnston's (1995a; 1995b) conception of strategic culture I concur with Colin Gray (1999b) that behaviour cannot be disentangled from culture. Indeed, narrative and behaviour, in the form of practices, are mutually constitutive of strategic culture (Lock 2010; Neumann and Heikka 2005). This study consists of a comparative strategic cultural analysis of two Salafi-Jihadist violent non-state actors: Al-Qaida-central and its franchise, AlQaida in the Islamic Maghreb (AQIM), employing concepts of strategic narrative and strategic practices. AQIM, formerly known as the Salafist Group for Preaching and Combat or GSPC, rebranded as an Al-Qaida franchise in 2007, leading to speculation of a change from its Algeria-centric agenda to an anti-Western agenda. However, this has not been the case. Rather, AQIM has undergone a process of regionalization, expanding its operations beyond Algeria into Mali, Mauritania and Niger. Indeed, the study finds that while their strategic narratives share a number of common themes they are expressed in differing strategic practices. As such, the two organizations have distinct strategic cultures and differing strategic priorities. AlQaida prioritizes the battle against the far enemy, i.e. the West, whereas AQIM prioritizes the struggle against the near enemy, i.e. local regimes, deemed apostate, in the Maghreb-Sahel, primarily Algeria and Mali. Indeed, Al-Qaida primarily employs strategic practices of terrorism against Western civilians, whereas AQIM primarily employs guerrilla practices targeting local security forces.",,Last ED,,2018,,,,Ph.D. Thesis
Uses of Mobile Phones in Post-Conflict Liberia,"Liberia is a country emerging from years of protracted and devastating civil conflict. Left without any fixed line telephone infrastructure, it relies solely on the mobile phone for telephony. This study investigates the usage of mobile phones in this immediate post-conflict setting. In particular, we adopt the uses and gratifications approach to media research, giving focus to both instrumental and intrinsic motivations for use. Mobile phone users in both the capital city of Monrovia and in various rural areas were surveyed using the Q methodology, which identified distinct perspectives within these urban and rural groups. Participants were then sorted into groups where each group contained users with similar perspectives on their mobile phones. These identified groups included sets of users who saw their phones as productivity enhancers, means of connectivity to family and friends, essential business tools, technological curiosities, and sources of personal security. The idea of a phone as a stylish object was markedly rejected, especially in rural areas. We contrast these Q-sort results from Liberia with previous work from Kigali, Rwanda, finding differences especially as related to security.",,"Best ML,Wornyo E,Smyth TN,Etherton J",,2009,468–477,,,Conference Paper
Business Optimization in Logistics and Performance through Technology Implementation in Developing Countries,"While fast-growing countries in terms of technology like Japan, USA, South Korea, China, Finland, etc. are taking advantage of information and communication technology (ICT) to launch mobile apps, set up online payments and improve public security, public and private businesses also use it to improve economic development and problem-solving efficiency. That is not the case in developing countries where IT use is not considered when it comes to solving a problem because of its higher investment cost. This paper intends to highlight technology's role in business economic development, how it can solve problems, and the difference it can bring. We introduce GreenApp: a set of a mobile and a web application developed for a social waste management company in Madagascar called Greentsika, which has solved many of its monitoring issues and improved financial control and health. This technology improved the business by a factor of 7.14 as proven by computing the economic value created before and after application use.",,"Rabefaritra KM,Rabevohitra FH,Andrianarimanana FH",,2019,226–231,10.1145/3322645.3322692,https://doi-org.proxy.bnl.lu/10.1145/3322645.3322692;http://dx.doi.org/10.1145/3322645.3322692,Conference Paper
An Android-Based Application for Convenient Visitation at Windhoek Correctional Facility,"A global outbreak of Coronavirus affected countries' economy, people's lives and businesses around the world due to lockdown restrictions imposed on everyday activities. Like elsewhere, logistics and visiting hours to correctional facilities in Namibia is no exception. Namibian correctional facilities are far located in remote areas, hence, visitation to these facilities tends to be cumbersome, time-consuming, costly and at times disappointing due to the lack of accurate and timely information. Also, the rate of imprisonment quadrupled due to unfortunate acts during the lockdown. In response to this challenges, authors of this paper developed an android-based application to support social distancing, help Namibians in locating their relatives and friends in Namibian rehabilitation centers and reduce recidivism while improving the morale and safety of inmates and workers amid the pandemic. This study adopted a mixed research methodology with a case study and an experimental as a research design. An Agile software development model has been used for the development of an application. Primary data were collected through open-ended questionnaires, direct observations in the prison and group interviews. The study has been also chiefly informed by related literature reviews, as secondary data. A simple random sampling technique has been used to select the respondents from a target population of thirty participants. The study concluded that the application developed was intelligent, innovative, helpful, easy and convenient to use. Mobile technology can drastically change how we can communicate. It has been recommended that the Ministry of Home Affairs, Safety and Security consider funding and fully implement this project in all thirteen correctional facilities across the country. As part of future work, a similar version of iOS is to be developed and possibly implement a remote and/or onsite Inmate video visitation.",,"Haiduwa T,Hashiyana V,Samuel JN",,2020,,10.1145/3415088.3415127,https://doi-org.proxy.bnl.lu/10.1145/3415088.3415127;http://dx.doi.org/10.1145/3415088.3415127,Conference Paper
Mobile Game-Based Learning System for a Local Language,The problem of language endangerment as a result of deficiency among the Bassa natives of Liberia and the prevention of the extinction threat posed on the language motivates the need to explore the use of game-based learning system for its resolution. A bilingual Electronic Dictionary (ED) for automatic translation of English text to equivalent Bassa language text alongside its corresponding audio pronunciation on an Android-based mobile device was developed. The system was designed using the Unified Modelling Language (UML) tools and implemented using the Java programming language in Android studio environment. The system was evaluated using Mean Opinion Score (MOS). Result of evaluation shows that age is directly proportional to the knowledge of Bassa language in Liberia. It was evident that the elderly Bassa natives have better knowledge of the language while the younger ones have less or do not understand at all. The mobile application software developed in this work will aid youth learner of the Bassa language.,,"Soclo MI,Ninan OD,Olufokunbi KC",,2022,52–73,10.1504/ijmlo.2022.119954,https://doi-org.proxy.bnl.lu/10.1504/ijmlo.2022.119954;http://dx.doi.org/10.1504/ijmlo.2022.119954,Journal Article
Measuring the Big Data Readiness of Developing Countries – Index Development and Its Application to Africa,"The use of big data promises to drive economic growth and development and can therefore be a value-adding factor, but compared to private or public organisations, the country level is rarely investigated, and that is even more evident for developing countries. Another topic hardly ever considered in the big data research field is ‘big data readiness’, which means the level of preparation and willingness to exploit big data. We address these shortcomings in the literature and focus on the big data readiness of developing countries. Thus, the first research question is: what components are required for an index measuring big data readiness, and how can such an index be designed? We use a design science approach to develop the “Big Data Readiness Index” (BDRI), which is then applied to all African countries to answer our second research question: how do African countries perform in terms of the BDRI? Our analysis yields country rankings that show relatively high BDRI scores for coastal countries, such as South Africa, Kenya and Namibia, and for islands, such as Mauritius. Related implications for both research and policy are discussed.",,"Joubert A,Murawski M,Bick M",,2021,327–350,10.1007/s10796-021-10109-9,https://doi-org.proxy.bnl.lu/10.1007/s10796-021-10109-9;http://dx.doi.org/10.1007/s10796-021-10109-9,Journal Article
A Confidential Electronic Result Transfer Using a Hybrid XML Security Scheme,"Over the years, XML technology has been a standard for data representation and data exchange in many areas of information technology particularly those with focus on XML-based description standards for data interchange. Most data exchange involves transfer of confidential information, so there is a logical need for a secured means of storing, transmitting and receiving confidential and sensitive information. The security threats imposed on electronic result transfer is overwhelming and hence, the need to have a secure system in place. To obtain a maximum security level from XML Security standards, there is a need to incorporate a hybrid system that combines the features of the needed security measures. This paper presents a hybrid XML schema (XML encryption and XML signature) to ensure confidentiality and non-reputability in exchanging electronic result. The application involves encrypting the XML file using the XML encryption and then digitally sign this document using the XML signature for confidentiality and non-reputability, integrity and authenticity. The XML Security schema presented here is then proposed for adoption in the University of Agriculture Abeokuta, Ogun State Nigeria.",,"Onashoga SA,Sodiya AS",,2011,397–402,10.1109/ITNG.2011.77,https://doi-org.proxy.bnl.lu/10.1109/ITNG.2011.77;http://dx.doi.org/10.1109/ITNG.2011.77,Conference Paper
Mitigating Voting Irregularities with Secure E-Voting in Nigeria,"This study examines the challenges of free and fair elections in Nigeria and other African Nations. Nonfunctional and untrusted electoral democratic institutions in Nigeria and Africa are attributed to stateful and stateless mass voter intimidation, voter fraud, vote-buying, and other non-democratic criminal infractions resulting in dire and poor outcomes in leadership. This has led to an interest in developing solutions with as little human involvement as possible. This study examines the e-voting systems to mitigate such infractions. National electoral government bodies like INEC of Nigeria, have interests in online voter registration portals where registrants fill out a secure online pdf. The solution appears to ease the cumbersome registration process but is not user-friendly and is limited to the ownership of computers with internet access and does not address the ballot casting dilemma. The number of mobile connections in Nigeria in January 2021 was equivalent to 90.0% of the total population, thus study suggests that mobile e-voting is the best choice for e-voting. In addition to deploying a mobile prototype registration app, the research conducted a survey for the possibility of a complete solution to include registration and secure voting with the same group on its feasibility. The study showed that the proposed novel system was a desirable solution. Secondly, using a post-survey qualitative survey designed to explore public perception towards e-voting and its ability to reduce voter fraud and voter intimidation, the study findings reveal that most Nigerians prefer e-voting and find it a more secure and safe method of registration and casting their votes. Data collected from the voters was analyzed using quantitative methods, more specifically, descriptive and inferential statistical analysis. The manipulation check results show that majority of the participants had a significant understanding of the proposed voting systems meaning that the data collected from the respondents had a significant level of reliability. The study then explored the cryptography-based FOO protocol in e-voting and propose a theoretical framework that combines Two-Factor Authentication (2FA) with the FOO protocol to reduce instances of voter fraud and voter intimidation. The research analyzes the advantages and limitations of this framework and advocate for its adoption into the Nigerian electoral system. In addition, the study recommended that the implementation of risk-limiting audits (RLAs) could help mitigate the threat of cyber-attacks on e-voting systems and equipment failures.",,"Melie KO,K. Agerwala T,Zachary Dall",,2021,,,,Ph.D. Thesis
"Emerging Trends in Electrical, Electronic and Communications Engineering: Proceedings of the First International Conference on Electrical, Electronic ..","The book reports on advanced theories and methods in two related engineering fields: electrical and electronic engineering,and communications engineering and computing. It highlights areas of global and growing importance, such as renewable energy, power systems, mobile communications, security and the Internet of Things (IoT). The contributions cover a number of current research issues, including smart grids, photovoltaic systems, wireless power transfer, signal processing, 4G and 5G technologies, IoT applications, mobile cloud computing and many more. Based on the proceedings of the first International Conference on Emerging Trends in Electrical, Electronic and Communications Engineering (ELECOM 2016), held in Voila Bagatelle, Mauritius from November 25 to 27, 2016, the book provides graduate students, researchers and professionals with a snapshot of the state-of-the-art and a source of new ideas for future research and collaborations.",,"Fleming P,Vyas N,Sanei S,Deb K",,2018,,,,Book
Climate Change and Corporations : Regulating Corporate Participation in Climate Change Mitigation in Nigeria,"Corporations contribute over 71 per cent of global carbon emissions and are, therefore, the leading sources of the climate change problem. Nevertheless, they generally have apathy to participating in climate change mitigation by curbing their emissions or investing in low-carbon transition and mitigation projects, particularly in Nigeria and other developing countries. The weak legal and regulatory frameworks in these countries are exploited by corporations to evade their climate change obligations despite their significant contribution to the problem. This thesis critically analyses the prospects of overhauling the legal framework of climate change regulation of corporations in Nigeria through the implementation of the dilute interventionism paradigm. It argues that Nigeria should directly intervene through legislative mechanisms to compel corporations to incorporate climate change mitigation in their business activities. It proposes that this direct intervention should comprise of prescriptive and facilitative mechanisms structured in a dilute interventionism regulatory model with the strictest interventionist instruments at the foundation and de-escalation to milder prescriptive instruments based on increased corporate compliance. Adopting this model will halt the culture of regulatory impunity that corporations have imbibed in Nigeria and other developing countries. The thesis further argues that implementing this model requires the institution of a strong and independent regulator with a veto firewall protection system that guarantees its de facto independence from government and external influences. The thesis adopts a socio-legal research methodology in analysing the various legal mechanisms and alternative regulatory systems for corporate regulation and the regulatory theories developed in the literature for curtailing corporate excesses. It also undertakes a review of the successful implementation of an exemplar of the dilute interventionism model in the banking sector in Nigeria to establish the feasibility of the model for effectively reforming and incentivising corporate participation in a targeted sector - in this case, climate change mitigation. Although focusing on Nigeria, the analysis in this thesis has broader application to other developing countries around the globe with similar legal, regulatory and socio-economic circumstances as Nigeria.",,Kila K,,2020,,,,Ph.D. Thesis
Smart4Gap: Factors That Influence Smartphone Security Decisions in Developing and Developed Countries,"Despite the importance of an up-to-date Operating System (OS) for smartphone security, few users update it whenever it becomes obsolete. We believe intellectual, financial, sociocultural and other factors may highly affect users' behaviour in updating their OS, and these factors might significantly vary among users of different demographics and users in different geographic locations. In this paper, we conducted a survey of 206 participants from different demographics in Japan and Tanzania (two countries with different socio-cultures, per-capita incomes, security and privacy perceptions). We study and analyze in-depth users' privacy and security attitudes to examine our claims. Our results show that in both countries, the majority of users, even those with higher education levels, do not either set their devices into auto-update mode or instantly update their smartphone OS despite the awareness of security issues. Moreover, users in Tanzania are mostly cost-conscious (mobile data is highly-priced), while those in Japan are mostly concerned with preserving phone battery. Furthermore, the majority of users who update their OS in Tanzania are motivated by improved User Interfaces (UI) and better device performance while in Japan, users are more motivated by security features, and they consider OS updates as generally important. Overall, in both countries, income and motivators are the major determinant for auto- and instant- update behavior for smartphone OS.",,"Ndibwile JD,Luhanga ET,Fall D,Miyamoto D,Kadobayashi Y",,2018,5–15,10.1145/3285957.3285980,https://doi-org.proxy.bnl.lu/10.1145/3285957.3285980;http://dx.doi.org/10.1145/3285957.3285980,Conference Paper
Identification and Functional Characterization of Novel Plasma Membrane Carboxylate Transporters / Identificação e Caracterização Funcional de Novos Transportadores de Carboxilatos Da Membrana PlasmáTica,"Organic acids are recognized as versatile chemical compounds with a vast variety of applications in sectors ranging from food and beverages, pharmaceutical, personal care, cosmetic products, consumer goods to the chemical industry. Due to the strong demand for these compounds, alternative approaches to non-sustainable processes, e.g. chemical synthesis from petroleum derivatives, are being developed. Sustainable strategies rely on the utilization of microbial cell factories, where transporter proteins play a crucial role through the control of substrate influx and product efflux. In particular, the expression of suitable carboxylic acid exporters avoids the internal cell toxicity due to the accumulation of these compounds, while facilitating its purification from the culture broth. The biodiversity of the microbial world is an excellent pool to uncover new organic acid transporters. In this study, wild yeast species isolated from acidic environments were explored regarding their ability to utilize organic acids. The yeast Cyberlindnera jadinii was selected for further studies due to its ability to utilize a vast range of organic acids as sole carbon and energy source, to synthesize a variety of valuable compounds for the food and pharmaceutical sectors, its intrinsic robust characteristics, and its capacity to use inexpensive media with high productivity levels. Using an in silico approach, we have explored the predicted transportome of C. jadinii and selected putative carboxylate transporters for functional characterization by heterologous expression in Saccharomyces cerevisiae. A total of 16 plasma membrane carboxylate transporters, members of the AceTr, SHS, SSS, TDT, DASS, and MCT families, were characterized in terms of transport activity and specificity, structural features, and evolutionary relationships. S. cerevisiae was also used as expression host to deepen the knowledge of the citrate exporter CexAp from Aspergillus niger, a member of the DHA1 transporter family. The structural characterization of AceTr family members Ato1 and SatP led to the identification of a conserved motif essential for protein function. In addition, the plasma membrane proteins Gpr1 from Yarrowia lipolytica and AceP from the archaea Methanosarcina acetivorans were functionally characterized as acetate transporters. The role of AceTr members from S. cerevisiae, Ato1, Ato2, and Ato3, in the transport of monocarboxylic acids was evaluated via a combination of directed evolution, whole-genome resequencing, and reverse engineering, leading to the discovery of Ato2 and Ato3 mutants as two novel lactic acid transport proteins. Structural insights were also provided using 3D structure predictions combined with molecular docking.",,Silva MA,,2021,,,,Ph.D. Thesis
Cybersecurity in Nigeria: A Case Study of Surveillance and Prevention of Digital Crime,"This book reviews the use of digital surveillance for detecting, investigating and interpreting fraud associated with critical cyberinfrastructures in Nigeria, as it is well known that the country's cyberspace and cyberinfrastructures are very porous, leaving too much room for cyber-attackers to freely operate. In 2017, there were 3,500 successful cyber-attacks on Nigerian cyberspace, which led to the country losing an estimated 450 million dollars. These cybercrimes are hampering Nigeria's digital economy, and also help to explain why many Nigerians remain skeptical about Internet marketing and online transactions. If sensitive conversations using digital devices are not well monitored, Nigeria will be vulnerable to cyber-warfare, and its digital economy, military intelligence, and related sensitive industries will also suffer. The Nigerian Army Cyber Warfare Command was established in 2018 in order to combat terrorism, banditry, and other attacks by criminal groups in Nigeria. However, there remains an urgent need to produce digital surveillance software to help law enforcement agencies in Nigeria to detect and prevent these digitally facilitated crimes. The monitoring of Nigeria's cyberspace and cyberinfrastructure has become imperative, given that the rate of criminal activities using technology has increased tremendously. In this regard, digital surveillance includes both passive forensic investigations (where an attack has already occurred) and active forensic investigations (real-time investigations that track attackers). In addition to reviewing the latest mobile device forensics, this book covers natural laws (Benford's Law and Zipf's Law) for network traffic analysis, mobile forensic tools, and digital surveillance software (e.g., A-BOT). It offers valuable insights into how digital surveillance software can be used to detect and prevent digitally facilitated crimes in Nigeria, and highlights the benefits of adopting digital surveillance software in Nigeria and other countries facing the same issues.",,Iorliam A,,2019,,,,Book
Institutions and Consumers,"This study on evaluation of the affirmation of ordinary consumer interest in the digital mobile telecommunications market in Nigeria situates within the broader perspective of the public interest and in the context of policy failure (what happens after adopting policy?). It focuses on aspects of compliance, monitoring, and enforcement of policy objective relating to ordinary consumer interest, areas that receive inadequate attention in policy literature.The study is conducted using the new institutional economics framework. It adapts and extends the Saleth and Dinar (1999) institution decomposition model to deconstruct the mobile institution into four major components: policy, law, administration and enforcement for analytical purposes. Using document analysis, supplemented by semi-structured interviews, the study provides insights into how the regulatory framework engages with the ordinary consumer and the implications this has for the delivery of the policy objective of protecting ordinary consumer interest.The main findings reveal that (1) due to weak institutional structures, the regulator and mobile service providers do not hold ordinary consumer's interest at levels consistent with policy and law (2) there is no special intervention to make basic mobile services accessible and affordable for low-income ordinary consumers and (3) the pervasive violation of consumer interest persists due to laxity in enforcement of existing rules.The public interest in telecommunications policy has so far benefitted mobile companies in Nigeria rather than ordinary consumers. The existing policy failure, as discussed in this case study, can provide inspiration for rethinking the place of the ordinary consumer. Pervasive violation of consumer interest persist due to laxity in rules enforcement.There is no intervention to make mobile service affordable for ordinary consumer.Ranking investment over affordability results in high transaction cost for consumer.The Commission is not consumer focused; she is hampered by conflict of interest.",,Onyeajuwa MK,,2017,642–650,10.1016/j.telpol.2017.05.004,https://doi-org.proxy.bnl.lu/10.1016/j.telpol.2017.05.004;http://dx.doi.org/10.1016/j.telpol.2017.05.004,Journal Article
Enterprise Grade Cloud Computing,"Cloud computing and the as-a-service paradigm have gained a lot of interest recently. The separation of service provider from infrastructure provider has made it much easier for new services to be established online quickly and with low financial risk, and to scale those services as demand dictates. They can be built to run directly on top of infrastructure such as Amazon EC2 [1], on application platforms such as Google App Engine [3], or within higher level platforms such as FaceBook [2] or force.com [6], with increasing levels of ease of development and task specialization.It is clear why a startup company might be attracted to the cloud computing model. Equipment is very costly to purchase and can only be amortized over a period of years. Using someone else's infrastructure on a pay-per-use basis converts this fixed cost into a variable cost based on actual consumption; reducing initial investment and risk. Also the demand for online services can be very variable and poor response due to overload can risk losing customers. So the ability to scale the use of cloud compute power also mitigates the risk of failure.The arguments for an established enterprise are not the same. Such a business would have a well understood compute capacity and multi-year investment lifecycles. As the financial risk becomes less significant other issues come into play such as security, legislation, and dependence on the provider. Exactly where data resides is important as it will be accountable to the local legal system, especially where the main line of business is concerned. Security requirements may not be compatible with those offered by existing infrastructure providers.In reality enterprises already use a mix of services, some in-house, some contracted out. They may use a pay-per-use model to access outsourced payroll, travel arrangement, or even legal services. These may be provided by a cloud computing platform and integrated with further services such as credit card payment and courier distributors.For in-house services there may still be a cost advantage in using someone else's infrastructure -- if they are big enough. Studies have shown that the proportional cost of building and running data centers with tens of thousands of machines is significantly lower than one with just a few hundred. Where an enterprise does retain its own internal systems for IT or its main line of business, it is likely to be interested in using a private cloud; its own internal infrastructure and services managed in just the same way as an external cloud provider would do. But they may also be interested expanding out to external clouds to accommodate peak demands. An enterprise is likely to straddle the line between self-owned, on-premises compute facilities and third party cloud infrastructure.The future will bring a world of services interacting securely and running across multiple infrastructures, scaling and distributing as required. These considerations provide drivers for cloud computing research, both at service level and infrastructure level.At HP Labs we have been investigating service provision in a shared compute infrastructure for more than a decade. Past prototypes include SoftUDC [5], Frame Factory/SE3D, and the HP Utility Rendering Service [4] used by DreamWorks to create the Shrek and Madagascar movies among others. During the course of this work we have addressed a variety of challenges. How can clients with low bandwidth connectivity interact effectively with a service that involves very large volumes of data? How can independent services operate, flex and scale within the same shared infrastructure yet achieve sufficient isolation at the data and service levels? How can data and services be managed across multiple geographically distributed data centers?In addition to the technical challenges we also experienced client behaviors during these trials that have both motivated the use of cloud computing for individual users, but at the same time challenged the economics of the whole paradigm.Our current research addresses the challenges of enterprise-grade cloud computing, starting with the question: what would cloud computing need to provide for enterprises and enterprise service providers? In this talk we discuss our view of cloud computing and what we are doing to address the challenges of this new paradigm.",,Murray P,,2009,1,10.1145/1518691.1518692,https://doi-org.proxy.bnl.lu/10.1145/1518691.1518692;http://dx.doi.org/10.1145/1518691.1518692,Conference Paper
Error Correction and the Cryptographic Key,"We will look at a collection of mathematical problems suggested by side-channel attacks against public key cryptosystems, and how the techniques inspired by this work relate to a variety of different applications. First, we discuss the cold boot attack, a side-channel attack against disk encryption systems that uses the phenomenon of DRAM remanence to recover encryption keys from a running computer. In the course of the attack, however, there may be errors introduced in the keys that the attacker obtains. It turns out that the structure of the key data in an AES key schedule can allow an attacker to more efficiently recover the private key in the presence of such errors. We extend this idea to a RSA private keys, and show how the structure of RSA private key data can allow an attacker to recover a key in the presence of random errors from 27% of the bits of the original key. Most previous work on RSA key recovery used the lattice-based techniques introduced by Coppersmith for finding low-degree roots of polynomials mod numbers of unknown factorization. We show how this approach can be extended from the integers to the ring of polynomials, and give a new proof via lattice basis reduction of Guruswami-Sudan list-decoding of Reed-Solomon codes. These theorems are in fact instances of a general approach, which we extend to give an algorithm to find small solutions to polynomials modulo ideals in number fields and a list-decoding algorithm for multi-point algebraic-geometric codes.",,Heninger NA,,2011,,,,Ph.D. Thesis
SPSM '12: Proceedings of the Second ACM Workshop on Security and Privacy in Smartphones and Mobile Devices,"It is our great pleasure to welcome you to the Second ACM Workshop on Security and Privacy in Smartphones and Mobile Devices -- SPSM'12, held in association with the 19th ACM Conference on Computer and Communications Security, October 19th, 2012, in Raleigh, NC (USA).The workshop was created last year to organize and foster discussion of security in the emerging area of smartphone and mobile device computing. As organizers of top security venues, we've observed an increasing number of submissions describing novel approaches to solving the challenges of this area. We wanted to provide a dedicated venue to discuss these challenges and promising approaches for future research directions. SPSM'11 was a great success, with an excellent turnout of 80 registered attendees and in-depth discussion. This year, we will continue the 15 minute back-to-back talks followed by 45 minutes of discussion and hope to meet and exceed the high bar that was set.The call for papers attracted 30 submissions from Canada, China, Germany, Greece, India, Iran, Italy, Japan, Lebanon, Nigeria, South Africa, and the United States. The program committee accepted 11 papers that cover a variety of topics, including permission models, user studies, attacks on smartphones, and methods of defense. We are especially pleased to have a keynote speech by Geir Olsen, a Principle Program Manager in the operating systems group on the Windows Phone team, on Windows Phone 8 Security. We hope that these proceedings will serve as a valuable reference for security researchers and developers.",,,,2012,,,,Book
Towards Cash-Less Economy: Examining Factors Influencing Intention to Use NFC-Based Mobile Payments,"Recently, there has been speedy development of mobile technologies and an increase in diffusion of smartphones among young people. This has provided opportunities for innovative companies to create new payment solutions to their young customers. Although there has been a lot of coverage on consumer acceptance of mobile payments, only limited studies in Tanzania provide guidelines for NFC technology acceptance. This study aimed at examining factors influencing the acceptance of NFC technology in Tanzania using young customers (students). Current research modifies UTAUT framework based on existing literature to achieve the stated purpose. The snowball sampling technique was used to select 405 students from the IFM. SEM was employed in the analysis of collected data. The findings reveal that both security and trust are significant and positively affect adoption of NFC technology in Tanzania. Surprisingly, the results also indicated that the combination of social influence and security explains 84% of the trust. The implication, limitations, and future studies were also discussed.",,"Lashayo DM,Mhina JR",,2022,1–24,10.4018/IJMDWTFE.311432,https://doi-org.proxy.bnl.lu/10.4018/IJMDWTFE.311432;http://dx.doi.org/10.4018/IJMDWTFE.311432,Journal Article
Preventing Illegal Logging: Simultaneous Optimization of Resource Teams and Tactics for Security,"Green security – protection of forests, fish and wildlife – is a critical problem in environmental sustainability. We focus on the problem of optimizing the defense of forests against illegal logging, where often we are faced with the challenge of teaming up many different groups, from national police to forest guards to NGOs, each with differing capabilities and costs. This paper introduces a new, yet fundamental problem: Simultaneous Optimization of Resource Teams and Tactics (SORT). SORT contrasts with most previous game-theoretic research for green security – in particular based on security games – that has solely focused on optimizing patrolling tactics, without consideration of team formation or coordination. We develop new models and scalable algorithms to apply SORT towards illegal logging in large forest areas. We evaluate our methods on a variety of synthetic examples, as well as a real-world case study using data from our on-going collaboration in Madagascar.",,"Carthy SM,Tambe M,Kiekintveld C,Gore ML,Killion A",,2016,3880–3886,,,Conference Paper
How Advances in Semiconductor Technologies Have Adversely Affected Electrical and Electronics Installations in Africa,"This paper presents findings obtained in the Zambian national instrumentation surveys and highlights the little understood adverse effects that advances in semiconductor technology have on electrical and electronic instrumentation applied in all sectors in most African countries in particular Zambia. These effects have been occasioned by the lack of compliance to revised international installation and protection standards, poor power quality and poor planning compounded in many cases by severe weather conditions. These adverse impacts have been mainly caused by very high transistor integration in semiconductor (ICs). This has meant that electrical environment of yester year now requires vast improvements for the newer low dielectric strength component based equipment to operate reliably and safely. This paper finally describes good installation practices in power conditioning, earthing, bonding, surge and lightning protection solutions.",,"Namukolo S,Musonda E",,2018,86–94,10.4108/eai.20-6-2017.2270803,https://doi-org.proxy.bnl.lu/10.4108/eai.20-6-2017.2270803;http://dx.doi.org/10.4108/eai.20-6-2017.2270803,Conference Paper
"Humans and Vulnerability During Times of Change: Computer Security Needs, Practices, Challenges, and Opportunities","This dissertation explores the relationship between *change* and vulnerability to security and privacy harms. I suggest that change causes vulnerability in part due to the nature of change, and in part due to the design of technical and sociopolitical systems. I suggest that this connection between change and vulnerability exists for three reasons. First, when someone experiences change, new or different threats, risks, assets, technologies, and actors arise; if they do not update their personal threat model, it may be incomplete or inaccurate, making them unable to respond to emergent threats. Second, even if they are aware of all threats, they may be unable to prioritize security and privacy, as other needs may be more important. Third, the design of technology and user education is often misaligned with the needs and threat models of those going through change, causing vulnerable populations to become more vulnerable and exacerbating existing systemic inequities.I explore these three themes through four populations experiencing immense change differing in scope, cause, and time frame: (a) refugees who have moved to the United States; (b) activists in Sudan during the 2018-2019 revolution; (c) people considering using contact tracing apps during the first months of the Covid-19 pandemic; and (d) people who experience hurricanes. This dissertation makes contributions at two levels. First, each individual research chapter contributes an understanding of the security and privacy needs, experiences, and challenges of vulnerable populations. In each chapter, I make design, policy, and research recommendations to work towards more equitable technology. Second, taken together, the entirety of this dissertation contributes a deep understanding of the relationship between *change* and *vulnerability* to computer security and privacy harms. While the nature of change itself *does* engender vulnerability, in many ways the vulnerability is constructed---by sociopolitical and historical injustices or by technical design, or both.",,"Simko L,Franziska Roesner,David Kohlbrenner,Yasemin Acar",,2022,,,,Ph.D. Thesis
Closing the Gender Profit Gap?,"We examine the impact of providing access to mobile savings accounts and improving financial management skills on the performance of microenterprises in Mozambique. The effects are highly heterogeneous: Combining both types of support is associated with a large increase in both short- and long-term firm profits and in financial security for female microentrepreneurs. This allowed female-headed microenterprises, particularly those with a higher level of profits at baseline, to close the gender profit gap in performance and skills relative to their male counterparts. The main drivers of improved business performance are improved financial management practices (bookkeeping), an increase in accessible savings, and reduced transfers to friends and relatives. Providing access to mobile money as a tool to save and manage finances also increases long-term profits of female microentrepreneurs, particularly for those with higher profits at baseline. However, neither treatment has any impact on male-led enterprises. Uncovering this heterogeneity in impact across different types of microenterprises can help improve the targeting of these interventions in the future.This paper was accepted by Yan Chen, behavioral economics and decision analysis.Funding: This work was supported by the International Growth Centre and the U.S. Agency for International Development [Grant AIO-OAA-F-12-00015].Supplemental Material: The data files and Online Appendix are available at .",,"Batista C,Sequeira S,Vicente PC",,2022,8553–8567,10.1287/mnsc.2022.4579,https://doi-org.proxy.bnl.lu/10.1287/mnsc.2022.4579;http://dx.doi.org/10.1287/mnsc.2022.4579,Journal Article
An Exploratory Study on Policy Transfer for SIM Card Registration in Malawi,"Majority of African countries have adopted policies for mandatory Subscriber Identifiable Module SIM card registration to mitigate security threats to citizens and society. However, there are few countries that have not yet adopted the mandatory SIM card registration policies. This study investigated the means through which SIM card registration policy may be transferred in countries without the policy. The context of Malawi was analysed which represented an ideal case of an African country without mandatory SIM card registration policy. The findings showed that the mandatory SIM card registration policy may be transferred through: a voluntary transfer to address local challenges related to mobile technologies b coercive transfer in response to meet international agreements. However, lack of national identification documents for mobile phone users and delays in implementing legal framework affected the transfer of the mandatory SIM Card registration in Malawi. It will be necessary to consider of social, economic and political factors when adopting the mandatory SIM card registration policy.",,Makoza F,,2015,33–45,10.4018/IJTD.2015010102,https://doi-org.proxy.bnl.lu/10.4018/IJTD.2015010102;http://dx.doi.org/10.4018/IJTD.2015010102,Journal Article
StrongBox: A GPU TEE on Arm Endpoints,"A wide range of Arm endpoints leverage integrated and discrete GPUs to accelerate computation such as image processing and numerical processing applications. However, in spite of these important use cases, Arm GPU security has yet to be scrutinized by the community. By exploiting vulnerabilities in the kernel, attackers can directly access sensitive data used during GPU computing, such as personally-identifiable image data in computer vision tasks. Existing work has used Trusted Execution Environments (TEEs) to address GPU security concerns on Intel-based platforms, while there are numerous architectural differences that lead to novel technical challenges in deploying TEEs for Arm GPUs. In addition, extant Arm-based GPU defenses are intended for secure machine learning, and lack generality. There is a need for generalizable and efficient Arm-based GPU security mechanisms.To address these problems, we present StrongBox, the first GPU TEE for secured general computation on Arm endpoints. During confidential computation on Arm GPUs, StrongBox provides an isolated execution environment by ensuring exclusive access to the GPU. Our approach is based in part on a dynamic, fine-grained memory protection policy as Arm-based GPUs typically share a unified memory with the CPU, a stark contrast with Intel-based platforms. Furthermore, by characterizing GPU buffers as secure and non-secure, StrongBox reduces redundant security introspection operations to control access to sensitive data used by the GPU, ultimately reducing runtime overhead. Our design leverages the widely-deployed Arm TrustZone and generic Arm features, without hardware modification or architectural changes. We prototype StrongBox using an off-the-shelf Arm Mali GPU and perform an extensive evaluation. Our results show that StrongBox successfully ensures the GPU computing security with a low (4.70% - 15.26%) overhead across several indicative benchmarks.",,"Deng Y,Wang C,Yu S,Liu S,Ning Z,Leach K,Li J,Yan S,He Z,Cao J,Zhang F",,2022,769–783,10.1145/3548606.3560627,https://doi-org.proxy.bnl.lu/10.1145/3548606.3560627;http://dx.doi.org/10.1145/3548606.3560627,Conference Paper
Why Has Brazil Failed to Achieve a Global Market for Ethanol? : Brazil's International Ethanol Strategy from a Neoclassical Realist Perspective,"Brazil, the world's largest sugar producer, supplies 16 per cent of its energy consumption and approximately three quarters of its transport fuels with sugarcane-based ethanol. During Brazil's internationalisation under President Lula da Silva, and to a lesser extent in President Rousseff's first term, the country aimed at creating a global market for ethanol. The time seemed right to steer foreign policy towards this goal due to a benevolent structural environment with global discussions about energy security, climate change, and South-South cooperation. Within a Neoclassical Realist framework, I show how Brazil failed to bring its Ethanol Diplomacy to success and to create a global market for ethanol. Neoclassical Realism sheds light on the area of energy security. This is an approach that so far has mainly been applied to hard security questions. The analysis covers three analytical levels: the bilateral with Brazil in power deficit, the bilateral with Brazil in power surplus, and the multilateral, represented in three empirical chapters, Brazil-US, Brazil-Mozambique, and Brazil's multilateral ethanol diplomacy, respectively. Process tracing based on primary source research and expert interviews, leads to the conclusion that the Brazilian Foreign Policy Complex (FPC) failed to formulate and implement a coherent ethanol strategy. Ethanol Diplomacy was never a top priority for Brazil's Ministry of Foreign Affairs, representing a lack of cohesion in the FPC. In international negotiations, Brazil has deficits in predicting its negotiating partners' preferences and therefore lacks the ability to form long-term alliances with converging interests, rather than merely convening disruptive ad-hoc coalitions. This study shows that Neoclassical Realism can combine foreign policy output with international politics outcome research and is useful to analyse policy outside the hard security realm. It offers a basis for further research towards an understanding of Brazil's overall foreign policy and the foreign policies of other emerging powers.",,Fröhlich T,,2018,,,,Ph.D. Thesis
"Enhancing Lean Interventions Through the Use of Systems Thinking in the Food Production Industry : A Case in the Niger Delta Region, Nigeria","This research discusses how Lean Thinking (Lean), can be enhanced through the use of Systems Thinking (ST) tools and methodologies. While Lean has emerged as a process improvement philosophy aiming to enhance value by identifying and eradicating waste through the use of various tools, Systems Thinking seeks to recognise the impacts of different parts that function together in an operational process, paying attention to boundaries, interrelationships, perspectives and how systems function as whole. However, the extant literature shows that Lean tends to focus on narrow stakeholder input, leaving out the impact of the operational process on other affected stakeholders who may be affected by the system but are not directly involved. Such a narrow view can have an impact on Lean implementation and adoption among practitioners in modern businesses, and on its success in improving processes and sustaining changes. There can be challenging impacts on stakeholders, such as 'end to end' effects, which pose an issue to the general acceptance of the approach by affected stakeholders. To address this gap, the application of Systems Thinking alongside Lean was adopted, as Systems Thinking seeks to explore impacts on the affected. This led to the development of a Systemic Lean Intervention (SLI) methodology, involving the combination of Lean and Systems tools, to form an approach to identify and address the issues of waste and value development from the perspective of wider stakeholders. The research looks at a case of a commercial live-stock farm in the Niger Delta region of Nigeria. Qualitative data was collected from the identified stakeholders who participated in the research process. One of the findings was that SLI can assist in securing wider stakeholder acceptance of Lean and Systems improvements. However, the research also highlighted constraints on the SLI application, including the autocratic leadership style adopted on the farm and boundary rigidities in decision making, which hindered effective team play. Finally, among other limitations highlighted in the research, it was noted that the SLI approach would require significant time to be learned in the particular context of the Niger Delta Region, where the practice of both Lean and Systems were found to be relatively new.",,Ufua DE,,2015,,,,Ph.D. Thesis
Mesh Messaging in Large-Scale Protests: Breaking Bridgefy,"Mesh messaging applications allow users in relative proximity to communicate without the Internet. The most viable offering in this space, Bridgefy, has recently seen increased uptake in areas experiencing large-scale protests (Hong Kong, India, Iran, US, Zimbabwe, Belarus), suggesting its use in these protests. It is also being promoted as a communication tool for use in such situations by its developers and others. In this work, we report on a security analysis of Bridgefy. Our results show that Bridgefy, as analysed, permitted its users to be tracked, offered no authenticity, no effective confidentiality protections and lacked resilience against adversarially crafted messages. We verified these vulnerabilities by demonstrating a series of practical attacks on Bridgefy. Thus, if protesters relied on Bridgefy, an adversary could produce social graphs about them, read their messages, impersonate anyone to anyone and shut down the entire network with a single maliciously crafted message.",,"Albrecht MR,Blasco J,Jensen RB,Mareková L",,2021,375–398,10.1007/978-3-030-75539-3_16,https://doi-org.proxy.bnl.lu/10.1007/978-3-030-75539-3_16;http://dx.doi.org/10.1007/978-3-030-75539-3_16,Conference Paper
An Integrated Mobile Veld Fire Detection and Sharing Platform for Southern Africa,"While there are clear efforts towards managing veld fires, it comes as a concern that in Southern Africa, the role of local communities in fire control has weakened and veld fires have grown to be a major threat. Current systems and technologies to share veld fire information have several challenges. These include; being unable to detect burning fires in the forests, poor to almost missing veld fire local alerting systems, and malfunctioning local veld firefighting communities. Against this background, a mobile veld fire detection and sharing application prototype was developed using a qualitative data approach and experimental design. Weather data and scientific models of different areas were used to create fire-danger indices based on forecasted weather data and weather station information on the ground. These were programmed into the system to trigger alerts for the veld fire prediction component. For the identification of already burning fires, this was linked to the MODIS system of firefighting stakeholders (EMA Zimbabwe). Results revealed that conditions that promote veld fires can be predicted and local residents can thus be warned instantly to avoid activities that cause fires. For already burning fires, the mobile application was able to instantly communicate to users registered to the system.",,"Jere NR,Scott MS,Taruvinga A",,2017,,10.1145/3129416.3129439,https://doi-org.proxy.bnl.lu/10.1145/3129416.3129439;http://dx.doi.org/10.1145/3129416.3129439,Conference Paper
Managing Customary Land Conflicts and Demarcations Using Mobile Applications Tools: A Case Study of Zambia,Zambia has witnessed domestic and international customary land boundary conflicts due to improper land demarcation mechanism and partial documentation of customary land parcels. In this study we recommend utilisation and integration of Information Communication Technology ICT tools such as the Participatory Geographical Information System PGIS and the mobile application to be used in the implementation of the customary land management system. This will enable families and community groups to properly demarcate customary land boundaries thereby reducing land conflicts and providing security of tenure for customary land.,,,,2018,323–334,,,Journal Article
Txteagle: Mobile Crowdsourcing,"We present txteagle, a system that enables people to earn small amounts of money by completing simple tasks on their mobile phone for corporations who pay them in either airtime or MPESA (mobile money). The system is currently being launched in Kenya and Rwanda in collaboration with the mobile phone service providers Safaricom and MTN Rwanda. Tasks include translation, transcription, and surveys. User studies in Nairobi involving high school students, taxi drivers, and local security guards have been completed and the service has recently launched in Kenya nationwide.",,Eagle N,,2009,447–456,10.1007/978-3-642-02767-3_50,https://doi-org.proxy.bnl.lu/10.1007/978-3-642-02767-3_50;http://dx.doi.org/10.1007/978-3-642-02767-3_50,Conference Paper
ICEGOV '13: Proceedings of the 7th International Conference on Theory and Practice of Electronic Governance,"The 7th International Conference on Theory and Practice of Electronic Governance, ICEGOV2013, took place in Seoul, Republic of Korea from 22 to 25 October 2013. The conference was organized under the patronage of the Ministry of Security and Public Administration of the Republic of Korea (MOSPA) by the National Information Society Agency and by Macao-based Center for Electronic Governance at United Nations University International Institute for Software Technology (UNU-IIST) as the founder and organizer of the ICEGOV series. The conference took place under the theme ""Beyond 2015"" Smart Governance, Smart Development"". It was co-located with the Global e-Government Forum, organized by MOSPA in collaboration with United Nations Department of Economic and Social Affairs (UNDESA).The ICEGOV series focuses on the use of technology to transform relationships between government and citizens, businesses, civil society and other arms of government (Electronic Governance). Established in 2007, the series looks beyond the traditional focus on technology-enabled transformation in government (Electronic Government) towards new forms, new paradigms, and new foundations for technology-enabled governance, collaboration and sustainable development. ICEGOV is a platform where researchers, policy-makers and practitioners meet; a platform where theories are tested, insights are shared and experiences are reported; a platform for network- and capacity-building where keynote lectures and paper sessions are complemented by plenary discussions, town hall debates and poster exhibitions; a platform for international dialogue attended by participants from developing, developed and transition countries, from the United Nations system, and from many academic, governmental, non-governmental and private sector organizations. Since its establishment, the series has traveled globally from Macao (ICEGOV2007), through Cairo (ICEGOV2008), Bogota (ICEGOV2009), Beijing (ICEGOV2010), Tallinn (ICEGOV2011) and Albany (ICEGOV2012), to Seoul (ICEGOV2013) all generating significant local interest and stakeholder engagement.The program of ICEGOV2013 was built upon contributions from researchers and practitioners from around the world. In response to the call for papers, the conference received 133 papers from 54 countries and economies. The papers were evaluated in five categories: 1) Completed Research Papers providing the outcomes of complete research in one or more aspects of EGOV, with proven capability to advance the state of research in the field, limited to 10 pages; 2) Ongoing Research Papers providing the outcomes of ongoing research in one or more aspects of EGOV, with potential capability to advance the state of research in the field, limited to 4 pages; 3) Completed Experience Papers describing completed experience concerning EGOV policy or practice innovations, with proven capability to advance the state of practice in the field, including critical success factors and insights on the challenges encountered and how they were addressed, limited to 10 pages; 4) Ongoing Experience Papers describing ongoing experience concerning EGOV policy and practice innovations, with potential capability to advance the state of practice in the field, including critical success factors and insights on the challenges encountered and how they are being addressed, limited to 4 pages; and 5) Poster Papers presenting novel ideas and initiatives with potential to advance the state of research or state of practice in the field, limited to 2 pages. In total, 43 Completed Research Papers, 45 Ongoing Research Papers, 17 Completed Experience Papers, 21 Ongoing Experience Papers and 8 Posters were received. After anonymous peer-review process carried out by the members of the Program Committee at least three independent reviews were obtained for each submission as a basis for acceptance decisions: 13 submissions were accepted as Completed Research Papers, 8 as Completed Experience Papers, 29 as Ongoing Research Papers, 11 as Ongoing Experience Papers and 21 as Poster Papers. All accepted submissions, revised to address review comments, and presented at the conference within 6 paper tracks, 11 thematic sessions and one poster session, are included in this volume. Among them, like the last three ICEGOV conferences, the authors of selected papers were invited to submit extended versions of their papers for possible publication in the special issue of Government Information Quarterly, Elsevier.Based on the submitted and invited contributions and continuing the ICEGOV tradition, ICEGOV2013 featured a rich academic, capacity-building and network-building program comprising keynote lectures, plenary discussions, town hall debates, paper tracks, thematic sessions and the doctoral colloquium and poster exhibition. The program engaged individuals from over 60 countries and economies as authors, reviewers, committee members or resource persons. The details of the program are provided below.The conference included six keynote lectures on various aspects of Electronic Governance (EGOV), conducted by distinguished experts and practitioners in the area: 1) Park Chan Woo, Vice-Minister of Security and Public Administration of the Republic of Korea; 2) Alikhan Baimenov, Chairman of the Agency for Civil Service Affairs of the Republic of Kazakhstan; 3) Moon Suk Ahn, Chair Professor of e- Government, Korea University, Republic of Korea; 4) Mohammed Ali Al, Chief Executive Officer, e-Government Authority, Kingdom of Bahrain; 5) Henk G. Sol, Professor of Business and ICT and Founding Dean, University of Groningen, Netherlands; and 6) Edwin Lau, Head of Division, Reform of the Public Sector, Organization for Economic Co-operation and Development (OECD).Three plenary sessions followed the keynote lectures on the second, third and fourth day of the conference, focusing on specific questions of interest to the EGOV research and policy community:1. Are international EGOV rankings having a mobilizing or distracting influence on development? Chaired by Tomasz Janowski, Head of the Center for Electronic Governance at UNU-IIST and attended by: Vincenzo Aquaro, Chief of E-Government Branch, Division for Public Administration and Development Management, UNDESA; Bikesh Kurmangaliyeva, Deputy Chairwoman of the Board of ""Zerde"" National CT Holding, Kazakhstan; Mohammed Ali Al Qaed, CEO of eGovernment Authority, Kingdom of Bahrain; Mesfin Belachew Tefera, Technical Advisor to the Minister, Ethiopian Ministry of Communication and Information Technology; and Saleem Zoughbi, Former Regional ICT Advisor, UNESCWA and consultant for UNU-IIST.2. Who should drive smart conversations for sustainable development experts, citizens or politicians? Chaired by Marijn Janssen, Professor of ICT and Governance at Technology, Policy and Management Faculty, Delft University of Technology, Netherlands and attended by: Sunil Choenni, Head, Department of Statistical Information Management and Policy Analysis, Research and Documentation Centre (WODC), Dutch Ministry of Security and Justice; Harekrishna Misra, Professor in IT and Systems at the Institute of Rural Management Anand (IRMA), India; Henk G.Sol, Professor of Business and ICT and Founding Dean, Faculty of Economics and Business, University of Groningen, Netherlands; and Evgeny Styrin, Senior Research Analyst and Associate Professor, National Research University Higher School of Economics, Russia.3. Is a common set of e-government principles, applicable to all countries and contexts, possible? Chaired by Samuel Chan, Member of Executive Committee, Macao Science and Technology Development Fund, Macao SAR Government and attended by: Wojciech Cellary, Head of the Department of Information Technology, Poznan University of Economics, Poland; Sharon Dawes, Senior Fellow, Center for Technology in Government, University at Albany, USA; Edwin Lau, Head of Division, Reform of the Public Sector, Public Governance and Territorial Development Directorate, Organization for Economic Co-operation and Development; and Jeremy Millard, Associate Research Fellow, Brunel University, UK.Three town hall debates took place in the afternoons of the first, second and third days of the conference. They focused on three salient questions for the EGOV research and policy community:1. Catalyzing Smart Transformation: What Makes Governments Smarter? Chaired by Samia Melhem, Lead Policy Specialist, Transform Practice, Chair, eDevelopment Community of Practice, Transport, Water and ICT, Sustainable Development Network, World Bank Group; and Oleg Petrov, Senior Program Officer, ICT, World Bank; and attended by: Jabiri Kuwe Bakari, CEO, e-Government Agency, Tanzania; Rajendra Kumar, Senior Officer, Indian Administrative Service and Joint Secretary (e-Governance), Department of Electronics and Information Technology, Government of India; Bikesh Kurmangaliyeva, Deputy Chairwoman of the Board of ""Zerde"" National CT Holding, Kazakhstan; Margareta Petrusevschi, Knowledge and Learning Coordinator, e-Government Centre, Government of the Republic of Moldova; James Saaka, Executive Director, National Information Technology Authority, Uganda; Mesfin Belachew Tefera, Technical Advisor to the Ethiopian Minister of Communication and Information Technology; and Jeongwon Yoon, Executive Director, National Information Society Agency, Korea. This town hall was organized by the World Bank.2. Is Good Governance a Pre-Condition or a Consequence of the Development of Knowledge Societies? Chaired by Andrea Cairola, Adviser for Communication and Information, UNESCO Office Beijing, Cluster Office to the Democratic People's Republic of Korea, Japan, Mongolia, People's Republic of China and Republic of Korea; and attended by: Johanna Ekua Awotwi, Director of Research and ICT Operations, Centre for e-Governance, Accra, Ghana; Antonio Cordella, Lecturer in Information Systems, London School of Economics and Political Sciences, UK; Marco Peres, Director, Observatory for Society, Technology and Government Information, University Externado of Colombia, Colombia; Margareta Petrusevschi, Knowledge and Learning Coordinator, e-Government Centre, Government of the Republic of Moldova, Moldova; and Jeongwon Yoon, Executive Director, National Information Society Agency, Republic of Korea. This town hall was organized by the UNESCO Information for All Programme.3. Striking the Balance of Security, Privacy and Openness: To Open or Not To Open? Chaired by Theresa Pardo, Director of the Center for Technology in Government, University at Albany, USA, and attended by: Sharon Dawes, Senior Fellow, Center for Technology in Government, University at Albany, USA; Ramon Gil-Garcia, Research Director, Center for Technology in Government, University at Albany, USA; Louise Thomasen, independent consultant and expert in EGOV and technology, Denmark; and Lei Zheng, Assistant Professor, Department of Public Administration, Fudan University, China.The program included six paper tracks, chaired by leading international experts in the corresponding areas, comprising presentations of three to six accepted papers: 1) Building Smart Government chaired by Theresa Pardo, Director of the Center for Technology in Government, University at Albany, USA and Gabriel Puron Cid, Professor at the Centre of Research and Teaching in Economic Sciences, Mexico; 2) Governing through Networks chaired by Sehl Mellouli, Associate Professor at Laval University, Canada and Adegboyega Ojo, Research Fellow and Leader of E-Government Group at INSIGHT, National University of Ireland, Ireland; 3) Policy and Governance Innovation chaired by Natalie Helbig, Senior Research Associate at the Center for Technology in Government, University at Albany, USA and Marijn Janssen, Professor in ICT and Governance at the Delft University of Technology, Netherlands; 4) Smart Governance for Smart Industries chaired by Wojciech Cellary, Professor and Head of the Department of Information Technology at the Poznan University of Economics, Poland and Antonio Cordella, Lecturer at the London School of Economics and Political Sciences, UK; 5) Smart Governance for Smart Societies chaired by Jeremy Millard, Associate Research Fellow at the Brunel University, UK; and 6) Ethics, Transparency and Accountability chaired by Jeanne Holm, Chief Knowledge Architect at the NASA Jet Propulsion Laboratory, USA. Each track took place across the whole duration of the conference, with tutorial introduction to the topic of the track organized on the first day, presentations of accepted papers on the second or third day, and workshop-style discussion on the last day.Complementing the paper tracks, 11 thematic sessions were organized and chaired by industrial, academic, government and international organizations active in the theme of the session, comprising presentations of up to four accepted papers: 1) EGOV for Development chaired by Nag Yeon Lee, ICT Consultant and Instructor for e-Government on behalf of the Asia Pacific Center on ICT for Development, United Nations Economic and Social Commission for Asia Pacific; 2) National Data Policies chaired by Zhanat Zhakhmetova, Head of the Office of State Informatization Policy, Department of State Information Technology Policy, on behalf of the Ministry of Transport and Communications, Republic of Kazakhstan; 3) Governing Ageing Society chaired by Toshio Obi, Professor, Institute of e-Government on behalf of Waseda University, Japan; 4) Governing Smart Cities chaired by Yoon Chang So, Smart Cities Country Leader, IBM Korea on behalf of IBM; 5) Open Government Data Impact chaired by Edwin Lau, Head of Division, Reform of the Public Sector, Public Governance and Territorial Development Directorate, on behalf of the Organization for Economic Co-operation and Development; 6) Interoperability Governance chaired by Jung Sik Hwang, Platform Strategy Lead at Microsoft Korea on behalf of Microsoft; 7) Government on Social Media chaired by Jeanne Holm, Evangelist, Data.Gov and Chief Knowledge Architect at the Jet Propulsion Laboratory, NASA on behalf of the World Wide Web Consortium; 8) Innovative EGOV Applications chaired by Oleg Petrov, Senior Program Officer, ICT, World Bank on behalf of the World Bank; 9) Participatory Government chaired by Bernd Friedrich, Head of the Information and Communications Technologies for Development Project at Deutsche Gesellschaft für Internationale Zusammenarbeit (GIZ) GmbH, Germany on behalf of GTZ; 10) Mobile Governance chaired by Nestor Eduardo Fajardo Infante, Advisor for Research, Development and Innovation, Ministry of Information Technology and Communication, on behalf of the Government of Colombia; and 11) Open Data Ecosystem chaired by Jeanne Holm, Evangelist, Data.Gov and Chief Knowledge Architect, Jet Propulsion Laboratory, NASA on behalf of the U.S. Government and Data.Gov.The program also included poster exhibition, organized in the reception style to allow authors to present their ongoing work, receive feedback and engage in discussions and networking; and an interactive doctoral colloquium, jointly organized by the Center for Electronic Governance at UNU-IIST, Macao, University of Groningen, Netherlands and Chuo University, Japan. The colloquium provided doctoral students from different disciplines an opportunity to discuss a variety of EGOV topics and methods related to their research work, dissertations and career plans. The colloquium was co-chaired by Elsa Estevez, Academic Program Officer, United Nations University International Institute for Software Technology, Macao; Hiroko Kudo, Professor of Public Policy and Public Management, Faculty of Law, Chuo University, Japan; and Henk G. Sol, Professor of Business and ICT and Founding Dean, Faculty of Economics and Business, University of Groningen, Netherlands; and attended by Adegboyega Ojo, Research Fellow and Leader of E-Government Group at the INSIGHT Center for Data Analytics, National University of Ireland, Ireland as invited academic.The conference awarded best paper titles in Best Research Paper and Best Experience Paper categories. The selection was carried out jointly by Elsa Estevez as the ICEGOV2013 Awards Chair, and Tomasz Janowski and Jeanne Holm as the ICEGOV2013 Program Chairs. Three papers were nominated to the Best Experience Paper award: 1) A Reputation Based Electronic Government Procurement Model by Hichem Klabi, Sehl Mellouli and Monia Rekik; 2) Government 3.0 in Korea: Fad or Fashion? byTaewoo Nam; and 3) Secure ID Management for Social Security and Tax Number System by Hisao Sakazaki, Dan Yamamoto, Akihiro Sugimoto and Shinji Hirata. The winner in this category was ""A Reputation Based Electronic Government Procurement Model"" by Hichem Klabi, Sehl Mellouli and Monia Rekik. Three papers were also nominated to the Best Research Paper award: 1) Harnessing the Duality of e-Participation Social Software Infrastructure Design by Lukasz Porwol, Adegboyega Ojo and John Breslin; 2) When Food Quality Control in China Meets Mobile and Wireless Technology: Interactions, Potentials and Pitfalls by Shuhua Liu; and 3) Cross-departmental Collaboration in Government One-Stop Center: Factors and Performance by Xinping Liu. The winner in this category was ""Harnessing the Duality of e-Participation Social Software Infrastructure Design"" by Lukasz Porwol, Adegboyega Ojo and John Breslin.Many people and institutions contributed to the organization of ICEGOV2013. We wish to thank the official patron of ICEGOV2013, the Ministry of Security and Public Administration of the Republic of Korea for endorsing and supporting the conference. Our sincere thanks go to the National Information Society Agency, Republic of Korea (NIA) as the local organizer of the conference, particularly to Jeongwon Yoon for his vision and leadership, and to Dohyoon Kim and the whole team in NIA for their hard work and dedication to making the combined ICEGOV2013 and Global e-Government Forum event successful. We wish to express our most sincere thanks to the key sponsors Macao SAR Government and Macao Foundation and the sponsor Electronic Government of the Republic of Kazakhstan whose generous contributions allowed many academics and practitioners from developing countries to attend the conference. Special gratitude is due to Macao SAR Government, its Public Administration and Civil Service Bureau, and Macao Foundation for continuing support to the ICEGOV conference series and the origin of the series e-Macao Program. We also wish to thank ICEGOV2013 partners for their presence, support and in-kind contributions: Brunel University, London, UK; Center for Technology in Government, University at Albany, USA; Data.Gov, U.S. Government; German Cooperation, Deutsche Zusammenarbeit and Deutsche Gesellschaft fur Internationale Zusammenarbeit, Germany; IBM; Information and Communication Technologies, World Bank; Microsoft; Ministry of Information Technology and Communication, Colombia (MINTIC); Organization for Economic Co-operation and Development; Poznan University of Economics, Poland; The Insight Centre for Data Analytics, National University of Ireland, Ireland; The Science and Technology Development Fund, Government of Macao SAR, Macao; UNESCO Information for All Programme; United Nations Asian and Pacific Training Centre for Information and Communication Technology for Development; Vive Digital Programme, MINTIC, Colombia; Waseda University, Japan; and the World Wide Web Consortium. We also wish to express our thanks to ACM Press for publishing the ICEGOV2013 conference proceedings. We are most grateful to the whole Advisory Committee for supporting the conference and to all members of the Program Committee and additional reviewers for their efforts to carry out quality reviews and to help build a strong conference program. We thank keynote speakers; organizers, chairs and moderators of the plenary sessions, town hall debates, paper tracks, thematic sessions, the doctoral colloquium, and the poster session; and all panelists and speakers for their intellectual contributions. Last but not least, we are most thankful to all authors for their efforts in preparing, submitting and presenting papers at ICEGOV2013.We hope that ICEGOV2013 will further contribute to building, growing and connecting global EGOV research, policy and practice communities, able to cross not only national and regional but also institutional and thematic borders, and that the contacts, discussions and ideas initiated in Seoul in October 2013 will continue well after the conference and towards ICEGOV2014 in Guimaraes, Portugal.",,,,2013,,,,Book
Provable Security for the Fuzzy Fingerprint Vault,"We investigate the security of privacy enhancing techniques for biometric applications.The fuzzy vault of Jules and Sudan is a technique that allows error tolerant authentication, while preserving the privacy of the reference data. Several publications have proposed its application to fingerprints in order to implement privacy-enhanced biometric authentication. While the heuristic security estimates given are promising, no rigid security analysis has been presented so far. We explore if and under what circumstances a provably secure fuzzy fingerprint vault can be implemented. Based on bounds on the loss of entropy for the general fuzzy vault and realistic models for minutiae distributions, we deduce lower bounds for attacks that attempt to recover the template. Furthermore, we show how to select optimal parameters and evaluate both, minimum minutiae match rates and minimum number of minutiae needed to obtain an appropriate security level. Our results indicate that a provable secure scheme is hard to achieve with current fingerprint technology.",,"Merkle J,Niesing M,Schwaiger M,Ihmor H,Korte U",,2010,65–73,10.1109/ICIMP.2010.17,https://doi-org.proxy.bnl.lu/10.1109/ICIMP.2010.17;http://dx.doi.org/10.1109/ICIMP.2010.17,Conference Paper
Usability Evaluation of the ETax Portal for Uganda,"The eTax portal/system is a web-based tax filling and payment system designed to reduce cumbersome manual processes, cut evasion and help boost domestic revenue collection. With the electronic system, tax paying individuals and businesses can apply for driving permit renewals, road licences, passport fees and motor vehicle registration online. The system is also used for payment of public services and other remittances such as the payment of traffic fines, court bails and other licenses. Given the public facing nature of eTax portals, their usability is key for members especially the tax paying public to find them easy to use, effective, efficient as well as satisfying to use. This paper presents results of an evaluation of an eTax portal for Uganda using the popularly used Jakob Nielsen's 10 Usability Heuristics for User Interface Design. Based on the results, the paper further presents recommendations on how URA can fix current usability gaps to make the portal more effective, efficient and satisfying to use. This is hoped to attract and retain more tax payers in the country to use it as opposed to continuing to use the traditional manual system which is not only costly, but also reduces the level of compliance. Furthermore, a usable eTax portal will decrease training, support and maintenance costs. The paper also provides general recommendations on how to develop and manage more usable websites.",,Baguma R,,2018,449–458,10.1145/3209415.3209470,https://doi-org.proxy.bnl.lu/10.1145/3209415.3209470;http://dx.doi.org/10.1145/3209415.3209470,Conference Paper
"Saving the Planet, One Handset at a Time: Designing Low-Power, Low-Bandwidth GPUs","GPUs for mobile devices have to deliver ever-increasing performance and capability while living within strict power and memory bandwidth limits. In this talk we'll explore how these limits influence the design of mobile GPUs, and how applications can exploit GPU features to achieve the best power efficiency and performance, using ARM's Mali™ GPU family as a case study.",,Olson TJ,,2012,,10.1145/2341910.2341912,https://doi-org.proxy.bnl.lu/10.1145/2341910.2341912;http://dx.doi.org/10.1145/2341910.2341912,Conference Paper
A Crowd Sourced Pharmacovigilance Approach Using SMS-Based Asymmetric Encryption,"With the explosive international growth in mobile phone adoption, there is an increasing number of text message-based applications providing mission-critical services to mobile phone owners. With such an unexpected leap in the mobile subscriber base, questions have arisen over the reliability of Short Message Service (SMS) as a communication channel in regions of high mobile teledensity growth. This paper provides insight on two points - the architecture of new SMS-based services and an examination of SMS reliability in low resource environments. This is achieved by providing research results detailing the design and implementation of a crowd sourced mobile-based authentication service using asymmetric encryption and data from an in-field view of the reliability of SMS messages using the SMS product authentication service described. A new Mobile Product Authentication (MPA) platform has been designed and implemented using cloud-based distributed computing (server virtualization) and a mobile network reliability server has been built to emulate a quad-SIM cell phone with 2 GB of RAM, 200 GB storage and 2.5 GHz dual-core processing power. To evaluate network throughput, response time and fault-tolerance, time-stamped SMS messages are sent from the network reliability server to the MPA platform on a round-trip flight through Nigeria’s largest mobile telecommunications company over varying time periods and network conditions.",,"Gogo A,Cybenko G,Garmire E",,2010,226–231,10.1109/ICCGI.2010.36,https://doi-org.proxy.bnl.lu/10.1109/ICCGI.2010.36;http://dx.doi.org/10.1109/ICCGI.2010.36,Conference Paper
Traffic Characterization and Internet Usage in Rural Africa,"While Internet connectivity has reached a significant part of the world's population, those living in rural areas of the developing world are still largely disconnected. Recent efforts have provided Internet connectivity to a growing number of remote locations, yet Internet traffic demands cause many of these networks to fail to deliver basic quality of service needed for simple applications. For an in-depth investigation of the problem, we gather and analyze network traces from a rural wireless network in Macha, Zambia. We supplement our analysis with on-site interviews from Macha, Zambia and Dwesa, South Africa, another rural community that hosts a local wireless network. The results reveal that Internet traffic in rural Africa differs significantly from the developed world. We observe dominance of web-based traffic, as opposed to peer-to-peer traffic common in urban areas. Application-wise, online social networks are the most popular, while the majority of bandwidth is consumed by large operating system updates. Our analysis also uncovers numerous network anomalies, such as significant malware traffic. Finally, we find a strong feedback loop between network performance and user behavior. Based on our findings, we conclude with a discussion of new directions in network design that take into account both technical and social factors.",,"Johnson DL,Pejovic V,Belding EM,van Stam G",,2011,493–502,10.1145/1963192.1963363,https://doi-org.proxy.bnl.lu/10.1145/1963192.1963363;http://dx.doi.org/10.1145/1963192.1963363,Conference Paper
Real-Time Large-Scale Map Matching Using Mobile Phone Data,"With the wide spread use of mobile phones, cellular mobile big data is becoming an important resource that provides a wealth of information with almost no cost. However, the data generally suffers from relatively high spatial granularity, limiting the scope of its application. In this article, we consider, for the first time, the utility of actual mobile big data for map matching allowing for “microscopic” level traffic analysis. The state-of-the-art in map matching generally targets GPS data, which provides far denser sampling and higher location resolution than the mobile data. Our approach extends the typical Hidden-Markov model used in map matching to accommodate for highly sparse location trajectories, exploit the large mobile data volume to learn the model parameters, and exploit the sparsity of the data to provide for real-time Viterbi processing. We study an actual, anonymised mobile trajectories data set of the city of Dakar, Senegal, spanning a year, and generate a corresponding road-level traffic density, at an hourly granularity, for each mobile trajectory. We observed a relatively high correlation between the generated traffic intensities and corresponding values obtained by the gravity and equilibrium models typically used in mobility analysis, indicating the utility of the approach as an alternative means for traffic analysis.",,"Algizawy E,Ogawa T,El-Mahdy A",,2017,,10.1145/3046945,https://doi-org.proxy.bnl.lu/10.1145/3046945;http://dx.doi.org/10.1145/3046945,Journal Article
Plagiarism Detection in Armenian Texts Using Intrinsic Stylometric Analysis,,,"Yeshilbashian YM,Asatryan AA,Ghukasyan TG",,2022,435–444,10.1134/S0361768822070039,https://doi-org.proxy.bnl.lu/10.1134/S0361768822070039;http://dx.doi.org/10.1134/S0361768822070039,Journal Article
Code Generation for Embedded Heterogeneous Architectures on Android,"The success of Android is based on its unified Java programming model that allows to write platform-independent programs for a variety of different target platforms. However, this comes at the cost of performance. As a consequence, Google introduced APIs that allow to write native applications and to exploit multiple cores as well as embedded GPUs for compute-intensive parts. This paper proposes code generation techniques in order to target the Renderscript and Filterscript APIs. Renderscript harnesses multi-core CPUs and unified shader GPUs, while the more restricted Filterscript also supports GPUs with earlier shader models. Our techniques focus on image processing applications and allow to target these APIs and OpenCL from a common description. We further supersede memory transfers by sharing the same memory region among different processing elements on HSA platforms. As reference, we use an embedded platform hosting a multi-core ARM CPU and an ARM Mali GPU. We show that our generated source code is faster than native implementations in OpenCV as well as the pre-implemented script intrinsics provided by Google for acceleration on the embedded GPU.",,"Membarth R,Reiche O,Hannig F,Teich J",,2014,,,,Conference Paper
Fundamental Computing Forensics for Africa: A Case Study of the Science in Nigeria,"This book presents a general introduction to the computational aspects of forensic science, covering the different tools needed for forensic investigations, the importance of forensics and biometrics, and the use of Benfords law for biometrics and network traffic analysis. It specifically focuses on the application of these techniques in Africa, and how they can be of benefit in the investigation of crime in Nigeria in particular.",,Iorliam A,,2018,,,,Book
Harnessing Nigeria's Investment in Satellite Technology for Sustainable Agriculture and Food Security,This paper examines the relevance of satellite technology in promoting and sustaining agricultural development and food security in Africa and Nigeria in particular. Some of the common problems facing agricultural development in Nigeria and Africa as a whole are discussed. The authors justify the relevance of Nigeria's investment in satellite technology for improving agricultural production in Nigeria and Africa as a whole. The paper also presents selected applications of NigeriaSat-1 in sustainable agriculture and food security as embarked on by the government of Nigeria through the National Space Research and Development Agency. Policy recommendations were made to further boost agricultural production and food security in Africa and particularly Nigeria.,,"Opeyemi ZA,Akinyede JO",,2012,63–72,10.4018/jagr.2012010106,https://doi-org.proxy.bnl.lu/10.4018/jagr.2012010106;http://dx.doi.org/10.4018/jagr.2012010106,Journal Article
An Improved Fast Correlation Attack on Stream Ciphers,"At Crypto'2000, Johansson and Jönsson proposed a fast correlation attack on stream ciphers based on the Goldreich-Rubinfeld-Sudan algorithm. In this paper we show that a combination of their approach with techniques for substituting keystream and evaluating parity-checks gives us the most efficient fast correlation attack known so far. An application of the new algorithm results in the first-known near-practical key recovery attack on the shrinking generator with the parameters suggested by Krawczyk in 1994, which was verified in the 40-bit data LFSR case for which the only previously known efficient attacks were distinguishing attacks.",,"Zhang B,Feng D",,2009,214–227,,https://doi-org.proxy.bnl.lu/10.1007/978-3-642-04159-4_14,Book Chapter
Towards a Theoretical Understanding of Workarounds Emerging from Use of a Referral Mobile Application: A Developing Country Context,,,"Kapepo MI,Van Belle JP,Weimann E",,2022,533–541,10.1016/j.procs.2021.12.046,https://doi-org.proxy.bnl.lu/10.1016/j.procs.2021.12.046;http://dx.doi.org/10.1016/j.procs.2021.12.046,Journal Article
A Ground-Up Approach to MHealth in Nigeria,"Mobile Health (mHealth) has been piloted in developing countries to transform the delivery of healthcare services. Despite this heightened focus on mHealth, the number of fully operational mHealth solutions implemented in these locations remains surprisingly low. To extend mHealth projects beyond pilot stage it is imperative that the primary end user is positively predisposed to engaging with the mHealth intervention. Through exploring initial perceptions, we can inform later stages of mHealth projects or develop interventions to convert attitudes into commitment or motivation to use mHealth. This qualitative exploratory study aims to understand end users, namely Primary Healthcare (PHC) workers, initial attitudes towards a mHealth project called IMPACT (usIng Mobile Phones for Assessing, Classifying and Treating sick children). We conducted a field study in Enugu State, Nigeria to understand end users perceptions of the relevance, benefits, threats and initial understanding of the technology influencing end users attitudes towards adoption of mHealth. The initial findings indicate that PHC workers expressed positive perceptions regarding the relevance and benefits associated with the IMPACT app. PHC workers focus on how the technology could support them to be more efficient and effective in their roles. However, they advocate the need for community wide education and training to eradicate negative perceptions or misgivings about the potential use of mHealth as part of a patients assessment.",,"Kenny G,OConnor Y,Eze E,Ndibuagu E,Heavin C",,2017,809–816,10.1016/j.procs.2017.11.105,https://doi-org.proxy.bnl.lu/10.1016/j.procs.2017.11.105;http://dx.doi.org/10.1016/j.procs.2017.11.105,Journal Article
The Nature of Dominance: Roman Representations of Parthia and Armenia and the Development of New Narratives of Imperial Victory,"For over three hundred years, Roman foreign policy in its East was dominated by its relations with the Parthian Empire and the kingdom of Greater Armenia. In the midst of these relations, Roman imperial officials and provincial elites performed lavish diplomatic ceremonies that involved Armenian and Parthian participants or produced monuments and coins that depicted them. My dissertation is the first monograph-length, systematic treatment of these visual means of representing Parthia and Armenia. Some scholars have analyzed Roman depictions of these eastern states in literary sources. However, the ceremonial and material evidence has largely gone ignored. A central tension in this evidence contrasts Roman claims to have subjugated Parthia and Armenia via military or diplomatic means. Rome was never able to achieve the kind of complete, triumphal victory in the East that it achieved in nearly every other corner of the empire. The same straightforward narratives that Roman military might had crushed its foes was not often applicable to Armenia or Parthia. In my dissertation, I argue that Rome used ceremony, monumental art, and coinage to deliberately construct new kinds of narratives about Rome's dominance of the East. That dominance was continually reaffirmed, as Rome always saw itself as the superior power in the region. At the same time, what dominance meant was continually re-conceptualized to match the circumstances of Roman relations with Parthia and Armenia. Therefore, Roman officials had to create new ideas about what it meant to dominate a foreign power and how that subjugation could be presented to the Roman people writ large. At times, Parthia and Armenia were depicted using traditional images of military victory: kneeling in mourning, and/or with their hands tied, seated beneath trophies. However, at other times, the eastern states were represented as a political threat. They challenged the emperor's power to rearrange the geopolitical map of the Near East. In those cases, the emperor was shown in his capacity as granter of monarchical power to Parthian and Armenian client kings, who kneeled before him in ritual submission. Both forms of victory could also be combined. My arguments show fundamentally new ways in which Rome approached and understood its imperial mission in the East. I make two broad points. First, most scholarship sees Roman emperors only interested in Parthia and the East for the sake of achieving personal imperial glory, without any concern for larger ""strategic"" goals. I show how Roman imperial officials approached Rome's eastern affairs with a clear idea of establishing dominance over Parthia and Armenia. However, they were able to reformulate the nature of that core goal as circumstances in the East rendered a proper military conquest impossible. Moreover, provincial elites were also calibrating their images of the eastern states as it suited the relationships they sought to establish with the imperial center. However, these officials were far more concerned with showing the East's military defeat, regardless of whether it had been ""actually"" achieved.Second, most narratives of Roman-Eastern relations consider Armenia only in the context of Rome's interactions with Parthia. I show how this bi-polar view of the East does not reflect Roman priorities. In some cases, especially in Augustan period (chapter II) representations focused on either state, giving each individually-tailored iconography. Under the emperor Nero (chapter III) particular geopolitical circumstances forced Rome to see the two states as almost synonymous with each other. But in the second and early third centuries (chapters III-VI), imperial and provincial officials could ignore one or the other state entirely.",,"Clark TF,Bresson A,Payne R",,2020,,,,Ph.D. Thesis
CyberBullet - Share Your Story: An Interactive Game for Stimulating Awareness on the Harm and Negative Effects of the Internet,"Increased internet connectivity across the African continent through mobile phones not only opens numerous opportunities, but also increases cybercrimes such as online child abuse and sexual exploitation. Previous national studies have shown that Namibia has experienced a surge in cybercrimes, which leaves children vulnerable to predators. While a national reporting portal has been launched, children are less likely to report incidents of cyber bullying or online abuse. This study aimed at investigating how an interactive game-based approach can be used for preventing online child abuse and the study also creates a fully functional game prototype. We wanted to gain insight into the current online experiences in Namibia. After administering an online survey and conducting focus group interviews at a local high school, we then conducted two game design workshops with stakeholders namely students, teachers, parents, and game developers. We found that most girls liked storytelling games whereas boys were more drawn to action games. This led to the development of the game called CyberBullet - Share Your Story. The study contribution is in the application of game-based approach to sensitize and prevent children from becoming victims of online abuse.",,"Mikka-Muntuumo J,Peters A,Jazri H",,2018,,10.1145/3283458.3283482,https://doi-org.proxy.bnl.lu/10.1145/3283458.3283482;http://dx.doi.org/10.1145/3283458.3283482,Conference Paper
A Multiple-Control Fuzzy Vault,"We introduce multiple-control fuzzy vaults allowing generalized threshold, compartmented and multilevel access structure. The presented schemes enable many useful applications employing multiple users and/or multiple locking sets. Introducing the original single control fuzzy vault of Juels and Sudan we identify several similarities and differences between their vault and secret sharing schemes which influence how best to obtain working generalizations. We design multiple-control fuzzy vaults suggesting applications using biometric credentials as locking and unlocking values.Furthermore we assess the security of our obtained generalizations for insider/ outsider attacks and examine the access-complexity for legitimate vault owners.",,"Hirschbichler M,Boyd C,Boles W",,2008,36–47,10.1109/PST.2008.23,https://doi-org.proxy.bnl.lu/10.1109/PST.2008.23;http://dx.doi.org/10.1109/PST.2008.23,Conference Paper
Consumer Acceptance of Mobile Payment Services in Nigeria : A Customised Unified Theory of Acceptance & Use Technology (Utaut) Model,"Innovations in technology continue to influence the daily routines of consumers. In the case of mobile devices, their multifunctionality transforms opportunities for consumers, for instance by conducting mobile payments. In spite of the merits of mobile payment services, the number of users of these services is very low among Nigerian consumers. Hence, the aim of this research is to design a conceptual model for measuring the acceptability rate of consumer acceptance of mobile payment in Nigeria. More specifically, this research analyzes the factors influencing consumer acceptance of mobile payment services in Nigeria and also measures the acceptability rate of consumer acceptance. By using a quantitative research approach with structured self-completion questionnaires, 500 questionnaires from Nigerian consumers were gathered and statistically analyzed so as to empirically test the model of mobile payment acceptance. The researcher employed a crosssectional survey across three different universities in Lagos Nigeria for collecting data to design a customised model. Analysis of the investigation data was performed with SPSS, PLS, PCA, Chi-Square and Regression Analysis. The model was validated with the aid of data extracted from the original investigation of the respondents. The results show that out of the examined seven factors, performance expectancy, relevance, culture, technical support, trust, security and awareness have proven to be significant as they influence the behavioural intention of the Nigerian consumers surveyed towards mobile payment services. In addition, an association between the characteristics age and mobile phone usage behaviour and the behaviour intention could be detected. A key finding is that a behavioural intention among Nigerian consumers toward mobile payment occurs according to this research. This study presents recommendations for service users to improve on the identified influential factors for successful innovation of mobile payment. This research contributes to theory by using the less-researched perspective of innovation resistance to the research field of mobile payment. Also, insights into Nigerian consumers were provided to help service providers to design effective marketing strategies that will satisfy the needs of the consumers.",,Ogundega I,,2019,,,,Ph.D. Thesis
Flexicurity for Investment Reimbursement of Micro Renewable Electric Energy Systems,"Even the most affordable renewable energy installation still needs an investment that is significant for local people, so that a co-financing party is often indispensable. This article investigates through field research in Tanzania and a technology survey, whether technology could be able to support such investment schemes. It would secure reimbursements in the same flexible and secure way people now pay for mobile communication services, thereby applying the success factors of mobile communications in Africa to micro renewable electric energy systems. Further areas for investigation are identified.",,"Van Acker B,Van Acker C,Van Acker V",,2012,149–154,10.1109/GHTC.2012.32,https://doi-org.proxy.bnl.lu/10.1109/GHTC.2012.32;http://dx.doi.org/10.1109/GHTC.2012.32,Conference Paper
Optimal Multimedia Transport on the Internet,"Delivering events in a distributed system needs special attention in cases with large numbers of receivers. With traditional solutions, an event producer needs to know all of his event consumers. To deliver an event, the producer has to issue a remote method invocation on a consumer. Alternatively, the consumers periodically have to poll the producer for new events. Both solutions are inefficient; they require the implementation of registration logic, and do not address partial failure adequately. The most efficient approach consists of pushing the event onto the wire just once and all interested remote listeners automatically pick up the event while it passes by. We have developed a quality of service framework where applications only pay for services they need: programmers can request qualities of service such as reliable multicast, virtual synchrony, encrypted communication and a protocol composition framework that extends to incorporate yet unsupported communication protocols and qualities of service. This paper presents the real time wide area network dissemination architecture protocol (RWANDA) which overcomes synchronous limitations by providing an asynchronous group communication model where applications only pay for the required quality of service (QoS) such as multicast, virtual synchrony and encrypted communication. In RWANDA, information sources use channels to disseminate information to a potentially large and changing set of channel subscribers. RWANDA is a Java architecture and it recognises the differing media characteristics and transport requirements of multimedia by providing a protocol composition framework that extends to incorporate yet unsupported communication protocols, qualities of service and optimised multimedia stacks. RWANDA provides an asynchronous foundation necessary for developing large-scale wide-area network continuous media applications.1998 Academic Press",,"Parr G,Curran K",,1998,149–161,10.1006/jnca.1998.0070,https://doi-org.proxy.bnl.lu/10.1006/jnca.1998.0070;http://dx.doi.org/10.1006/jnca.1998.0070,Journal Article
A SWOT Analysis of Mobile Electronic Banking: The Zimbabwe Case,"Zimbabwe is presently undergoing a transformative shift from a prevalently cash-based economy, to one using mobile electronic payments. The cash economy had the unintended consequence of excluding a large portion of the population from financial services, particularly banking services. This shift is significant for the economy because it has enabled the informal sector to tap into banking services. The Zimbabwean informal sector has gradually emerged as a major employer owing to economic challenges that have shrunk the formal sector. This reality has motivated mobile telephony companies to introduce innovative financial products targeting this sector. In this paper we use the SWOT framework to analyse the mobile banking sector in Zimbabwe. Our investigation seeks to inform stakeholders about the sector's potential and to caution about weaknesses that impact on stability and growth. We also characterise the processes involved in mobile banking to enable easy comparison of offerings from different service providers.",,"Nyandoro A,Mahleko B",,2015,218–238,10.1504/IJEF.2015.070531,https://doi-org.proxy.bnl.lu/10.1504/IJEF.2015.070531;http://dx.doi.org/10.1504/IJEF.2015.070531,Journal Article
Resisting Erasure: The Practice of Learning from Maya Mam Narratives of Survivance in Guatemala / Resistiendo La Borradura: La PráCtica de Aprender de Las Narrativas Mayas de Supervivencia En Guatemala,"Through the implementation of innovative research methods and engagement with Indigenous scholarship, scholars of transitional justice and archival studies can learn important lessons from Indigenous scholars and communities who are working to build brighter futures in the shadow of conflict, violence, and genocide. In my doctoral research, here presented in three articles, I make this argument through exploring the case studies of Rwanda and Guatemala, providing methods of analysis, and identifying opportunities for collaborative research. In part one, ""New Documents Shed Light: Why did Peacekeepers Withdraw During Rwanda's 1994 Genocide?"", an article published in 2018 in Genocide Studies and Prevention: an International Journal, I utilize the ""critical oral history method"" and analysis of recently declassified United States government records that shed light on the failed international response to stop the genocide in Rwanda. While methods used in this study elicit important information, the author finds that the study fails to include voices of people who suffered the consequences of these flawed polices.In part two, ""Collaborative Archival Analysis & Co-Producing Knowledge,"" I discuss the use of innovative archival analysis and ""critical oral history"" to address the flaws in the Rwanda project, drawing on important lessons from Indigenous studies and decolonizing methodologies. I apply these lessons in the case study of a collaborative project with the small town of genocide survivors in Nuevo Amanecer in western Guatemala. I argue that by broadening access to documents from the U.S.' ""colonial archive"" about the Guatemalan genocide, we subvert the colonial archive in important ways. While these methods provide promise, future research needs to further develop these approaches.Finally, in part three, ""Beyond Transitional Justice: Learning from Indigenous Maya Resistance in Guatemala,"" I identify concrete lessons scholars and practitioners can learn from Indigenous scholarship when attempting to determine successes and shortcomings of the international community's approach to transitional justice in the case of Guatemala. Through learning from the Indigenous studies concepts of refusal, survivance, and thrivance, as well as ideas of ""damage-centered research"" and ""desire-centered research,"" non-Native scholars can fundamentally reconfigure ideas of how to do their work by centering the voices and projects of communities who are building brighter futures. Ultimately, I conclude that non-Native scholars and practitioners can learn many lessons from Indigenous scholars and communities who are doing innovative future-building work, especially in the field of archival studies and transitional justice.",,"Willard E,Warren A,Lucero J,Rodríguez-Silva I",,2020,,,,Ph.D. Thesis
Intrusion Detection Based on Data Mining,"In this article we discuss our research in developing general and systematic methods for intrusion detection. The key ideas are to use data mining techniques to discover consistent and useful patterns of system features that describe program and user behavior, and use the set of relevant system features to compute (inductively learned) classifiers that can recognize anomalies and known intrusions. The paper also discusses the current level of computer security development in Tanzania with particular interest in IDS application with the fact that approach is easy to implement with less complexity to computer systems architecture, less dependence on operating environment (as compared with other security-based systems) and ability to detect abuse of user privileges easily. The findings are geared towards developing security infrastructure and providing ICT services.",,"Oreku GS,Mtenzi FJ",,2009,696–701,10.1109/DASC.2009.56,https://doi-org.proxy.bnl.lu/10.1109/DASC.2009.56;http://dx.doi.org/10.1109/DASC.2009.56,Conference Paper
Flipping 419 Cybercrime Scams: Targeting the Weak and the Vulnerable,"Most of cyberscam-related studies focus on threats perpetrated against the Western society, with a particular attention to the USA and Europe. Regrettably, no research has been done on scams targeting African countries, especially Nigeria, where the notorious and (in)famous 419 advanced-fee scam, targeted towards other countries, originated. How- ever, as we know, cybercrime is a global problem affecting all parties. In this study, we investigate a form of advance fee fraud scam unique to Nigeria and targeted at Nigerians, but unknown to the Western world. For the study, we rely substantially on almost two years worth of data harvested from an on-line discussion forum used by criminals. We complement this dataset with recent data from three other active forums to consolidate and generalize the research. We apply machine learning to the data to understand the criminals' modus operandi. We show that the criminals exploit the socio-political and economic problems prevalent in the country to craft various fraud schemes to defraud vulnerable groups such as secondary school students and unemployed graduates. The result of our research can help potential victims and policy makers to develop measures to counter the activities of these criminal groups.",,"Mba G,Onaolapo J,Stringhini G,Cavallaro L",,2017,1301–1310,10.1145/3041021.3053892,https://doi-org.proxy.bnl.lu/10.1145/3041021.3053892;http://dx.doi.org/10.1145/3041021.3053892,Conference Paper
Simulating DDoS Attacks on the US Fiber-Optics Internet Infrastructure,"Network-based attacks like the distributed denial-of-service (DDoS) attacks are not new, but we are beginning to see attacks of unprecedented scale. Examples of such attacks include the 2016 attack on DYN INC that crippled a part of the Internet for hours, and the attack on Liberia, which partially brought down the African nation. Limitations in identifying vulnerable Internet infrastructure and testing possible defense strategies are a part of the problem. We need a simulation testbed that can reflect the complexity of the Internet, yet allows to swiftly test attacks, providing insights that can apply to real-world attack scenarios. In this research, we have designed a test-bed that mirrors the Internet infrastructure of the US and can simulate the Internet traffic flow patterns for different attack targets. We also estimate the degradation in the quality-of-service and the number of users impacted in two attack scenarios.",,"Kumar S,Carley KM",,2017,,,,Conference Paper
"Innovation and Interdisciplinary Solutions for Underserved Areas: First International Conference, InterSol 2017 and Sixth Collogue National Sur La ... and Telecommunications Engineering","This book constitutes the refereed post-conference proceedings of the First International Conference on Innovation and Interdisciplinary Solutions for Underserved Areas, InterSol 2017, and the 6th Collogue National sur la Recherche en Informatique et ses Applications (CNRIA), held in Dakar, Senegal, in April 2017. The 15 papers presented at InterSol were selected from 76 submissions and are grouped thematically in science, energy and environment, education, innovation, and healthcare. The proceedings also contain 13 papers from the co-located 6th CNRIA (Collogue National sur la Recherche en Informatique et ses Applications) focusing on network architecture and security, software engineering, data management, and signal processing.",,"Kebe CM,Gueye A,Ndiaye A",,2018,,,,Book
An Evaluation of Ground Pipe Ventilation and Overnight Radiant Cooling to Displace Air Conditioning in Nigeria,"Stimulating minimal energy use and a consequential reduction in CO2 emissions to curb global warming has been a great challenge for mankind particularly when more than half of all prime energy supplied is consumed by the built environment. In Nigeria, 44% of primary energy consumption is used by the built environment with 75% of this used for air conditioning. To compensate for regular power outages in the Nigerian electricity supply system, 80% of households also operate independent electricity generators, primarily to maintain mechanical cooling and food preservation. Energy consumption is expected to rise as most new buildings don ot incorporate any significant energy saving techniques that could reduce cooling demands. Providing a solution to this becomes a major challenge for architects and the building industry in Nigeria. Factors such as pollution, insects, high dust levels and home security, negatively impact on the use of natural ventilation techniques which is a common practice, with most home owners keeping windows permanently closed, increasing their dependence on mechanical means of cooling to maintain indoor comfort. The task is therefore to develop sustainable cooling strategies for buildings in this region without compromising indoor air quality and security. The research assessed and analysed the thermo physical properties and energy performance of existing building types and developed an enhanced 'fabric first' solution followed by a synergised application of ground pipe supply ventilation and cool air supply due to clear night sky radiation from roof mounted black body as passive cooling techniques for buildings in South Eastern Nigeria. Despite the thermal interfaces being less than optimal, the results suggest that the application of such passive cooling strategies in combination with enhanced building fabric using thermal inertia and external insulation, can displace circa 83.5% of energy demand for domestic air conditioning. This could - in the long term - reduce energy consumption in the building sector from 44% to 16.45 %, with a commensurate reduction in the carbon footprint. The energy saving costs in use, were calculated to offset the additional capital costs, in under 8 years. These strategies and techniques are therefore worth further investigation as they represent both an economic and environmental gain.",,Ogbonnaya IO,,2019,,,,Ph.D. Thesis
Protocols That Hide User's Preferences in Electronic Transactions,"The Internet creates many new threats to personal privacy and raises some unique privacy concerns. In this paper we study the problem of how to protect users' privacy in web transactions of digital products. In particular, we introduce a system which (1) allows a user to disclose his/her identity information (such as user account or credit card number) to a web site in exchange for a digital product, but (2) prevents the web site from learning which specific product the user intends to obtain. The problem concerned here is orthogonal to the problem of anonymous transactions [M. Reed, P. Syverson, D. Goldschag, Anonymous connections and Onion Routing, IEEE Journal of Selected Areas in Communication 16 (4) (1998) 482-494; M. Reiter, A. Rubin, Crowds: anonymity for web transactions, ACM Transactions on Information System Security, 1 (1) (1998) 66-92] but commensurate with the general problem of PIR (private information retrieval) [B. Chor, O. Goldreich, E. Kushilevita, M. Sudan, Private information retrieval, in: Proceedings of 36th FOCS, 1995, pp. 41-50; B. Chor, N. Gilboa, Computational private information retrieval, in: Proceedings of 29th STOC, 1997, pp. 304-313]. Most of the existing results in PIR, however, are theoretical in nature and can not be applied in practice due to their huge communication and computational overheads. In the present paper, we introduce two practical solutions that satisfy the above two requirements and analyze their security and performance. Another issue we study in this paper is how to recover sales statistics data in our user privacy-protected system. We present a novel solution to the problem along with its security analysis.",,"Bao F,Deng RH",,2005,503–515,,,Journal Article
Secure Smartcardbased Fingerprint Authentication,"In this paper, the fundamental insecurities hampering a scalable, wide-spread deployment of biometric authentication are examined, and a cryptosystem capable of using fingerprint data as its key is presented. For our application, we focus on situations where a private key stored on a smartcard is used for authentication in a networked environment, and we assume an attacker can launch o -line attacks against a stolen card.Juels and Sudan's fuzzy vault is used as a starting point for building and analyzing a secure authentication scheme using fingerprints and smartcards called a figerprint vault. Fingerprint minutiae coordinates mi are encoded as elements in a nite eld F and the secret key is encoded in a polynomial f(x) over F[x]. The polynomial is evaluated at the minutiae locations, and the pairs (mi, f(mi)) are stored along with random (ci, di) cha points such that di ≠ f(ci). Given a matching fingerprint, a valid user can seperate out enough true points from the cha points to reconstruct f(x), and hence the original secret key.The parameters of the vault are selected such that the attacker's vault unlocking complexity is maximized, subject to zero unlocking complexity with a matching fingerprint and a reasonable amount of error. For a feature location measurement variance of 9 pixels, the optimal vault is 269 times more difficult to unlock for an attacker compared to a user posessing a matching fingerprint, along with approximately a 30% chance of unlocking failure.",,"Clancy TC,Kiyavash N,Lin DJ",,2003,45–52,10.1145/982507.982516,https://doi-org.proxy.bnl.lu/10.1145/982507.982516;http://dx.doi.org/10.1145/982507.982516,Conference Paper
Adversary Models Account for Imperfect Crime Data: Forecasting and Planning against Real-World Poachers,"Poachers are engaged in extinction level wholesale slaughter, so it is critical to harness historical data for predicting poachers' behavior. However, in these domains, data collected about adversarial actions are remarkably imperfect, where reported negative instances of crime may be mislabeled or uncertain. Unfortunately, past attempts to develop predictive and prescriptive models to address this problem suffer from shortcomings from a modeling perspective as well as in the implementability of their techniques. Most notably these models i) neglect the uncertainty in crime data, leading to inaccurate and biased predictions of adversary behavior, ii) use coarse-grained crime analysis and iii) do not provide a convincing evaluation as they only look at a single protected area. Additionally, they iv) proposed time-consuming techniques which cannot be directly integrated into low resource outposts. In this innovative application paper, we (I) introduce iWare-E a novel imperfect-observation aWare Ensemble (iWare-E) techniquefootnoteiWare-E is a predictive model integrated into Protection Assistant for Wildlife Security system (PAWS) developed at the University of Southern California, which is designed to handle the uncertainty in crime information efficiently. This approach leads to superior accuracy for adversary behavior prediction (up to 34% increase in AUC) compared to the previous state-of-the-art. We also demonstrate the country-wide efficiency of the models and are the first to (II) evaluate our adversary behavioral model across different protected areas in Uganda, i.e., Murchison Fall and Queen Elizabeth National Park, (totaling about 7500 square km) as well as (III) on fine-grained temporal resolutions. Lastly, (IV) we provide a scalable planning algorithm to design fine-grained patrol routes for the rangers, which achieves up to 150% improvement in number of predicted attacks detected.",,"Gholami S,Mc Carthy S,Dilkina B,Plumptre A,Tambe M,Driciru M,Wanyama F,Rwetsiba A,Nsubaga M,Mabonga J,Okello T,Enyel E",,2018,823–831,,,Conference Paper
Investigating the Role of Sensor Based Technologies to Support Domestic Activities in Sub-Saharan Africa,"In sub-Saharan Africa (SSA), homes face various challenges including insecurity, unreliable power supply, and extreme weather conditions. While the use of sensor-based technologies is increasing in industrialized countries, it is unclear how they can be used to support domestic activities in SSA. The availability of low-cost sensors and the widespread adoption of mobile phones presents an opportunity to collect real-time data and utilize proactive methods to monitor these challenges. This dissertation presents three studies that build upon each other to explore the role of sensor-based technologies in SSA. I used a technology probes method to develop three sensor-based systems that support domestic security (M-Kulinda), power blackout monitoring (GridAlert) and poultry farming (NkhukuApp). I deployed M-Kulinda in 20 Kenyan homes, GridAlert in 18 Kenyan homes, and NkhukuProbe in 15 Malawian home-based chicken coops for one month. I used interview, observation, diary, and data logging methods to understand participants' experiences using the probes. Findings from these studies suggest that people in Kenya and Malawi want to incorporate sensor-based technologies into their everyday activities, and they quickly find unexpected ways to use them. Participants' interactions with the probes prompted detailed reflections about how they would integrate sensor-based technologies in their homes (e.g., monitoring non-digital tools). These reflections are useful for motivating new design concepts in HCI. I use these findings to motivate a discussion about unexplored areas that could benefit from sensor-based technologies. Further, I discuss recommendations for designing sensor-based technologies that support activities in some Kenyan and Malawian homes. This research contributes to HCI by providing design implications for sensor-based applications in Kenyan and Malawian homes, employing a technology probes method in a non-traditional context, and developing prototypes of three novel systems.",,"Chidziwisano GH,Bree Holtz,B. Jordan S,Eduardo Nakasone,Kurtis Heimerl",,2022,,,,Ph.D. Thesis
Re-Architecting Internet Access and Wireless Networks for Rural Developing Regions,"In this work we focus on unique Internet connectivity challenges and opportunities in rural areas, especially in developing regions. Providing access in rural areas is particularly challenging due to its unique set of social and technical constraints. In order to quantify the challenges being faced by Internet users in rural areas, we conducted on-site and on-line interviews and analysed network traces from a rural network in Macha, Zambia. Our findings reveal severe local and global connectivity limitations that are unique to rural areas. Local wireless networks experience interference and packet loss due to poor network designs and limitations of WiFi in rural areas. Bandwidth-restricted Internet gateways are constantly congested during normal daytime usage periods. Users describe cost, limited availability and unreliability of Internet access as key barriers to on-line interaction. These obstacles prevent leisurely access to the Internet, including, amongst others, users generating and sharing media. This leads to a more transaction-like or ""deliberate interaction"" model for many rural users. Usage analysis reveals unique behaviour for users in rural areas. Most traffic is web-based traffic as opposed to peer-to-peer traffic in developed countries. Social media features even more prominently than in western countries, with Facebook being the most visited website; users are twice as likely to access Facebook than Google search. Further analysis of Facebook traffic shows a high degree of traffic between local users in the village. These unique patterns and constraints form the basis for ICT solutions we propose for rural regions. In order to avoid using congested rural Internet gateways, we propose a set of techniques to localize traffic. VillageShare, a Facebook-based localization application, facilitates file sharing amongst users in the village without the need to send media over the bandwidth-constrained gateway. VillageShare is also capable of time-shifting uploads to off-peak usage periods in order to avoid upload failures. In order to exploit locality of interest in mobile phone usage, we designed VillageCell, an open-source, low-cost pico-cell-like base station, that allows users to make free local cellular calls in a village. It takes advantage of the very high penetration rate of mobile phones in rural Africa, which occurs even though many villages lack cellular coverage. VillageCell is also able to support SMS to instant message client exchange as well as routing calls between VillageCell phones and phones on the public switched telephone network. The low population density and large village diameters in rural areas of Africa imply a need for novel solutions to spread wireless connectivity to individual homes. Current solutions, based on 802.11 require clear line-of-sight and have limited range. Those based on WiMax are not suitable due to high licence and deployment costs. We propose to use the recently freed TV spectrum bands known as ""white spaces"", encompassing frequencies from 52MHz to 698MHz, to cover vast distances in rural areas. Our solution, VillageLink, builds on the existing 802.22 white space standard to optimally utilize white space spectrum. We add a feature that allows base stations to allocate optimal channels using inter-cell probing across all available TV channels. Channel probing is critical as frequency selectivity is more dependent on antenna characteristics and non-linearity of RF components in the system than on the free-space propagation laws in the white space band. In order to evaluate our interventions, we deployed VillageShare and VillageCell in Macha, Zambia and evaluated VillageLink using simulations built on a real 3 km white space link in South Africa. We collected usage logs of these systems both through quantitative and qualitative studies. For qualitative studies we involved users in an iterative design process and made use of on-line interviews to understanduser perception of our solutions. Ultimately, we hope that this research will lead to better penetration of wireless networks and improved network performance for users in rural villages, culminating in a more inclusive and representative Internet that truly reflects all languages and cultures in the world. (Abstract shortened by UMI.)",,Johnson DL,,2013,,,,Ph.D. Thesis
A Voice in the Crowd: Broader Implications for Crowdsourcing Translation during Crisis,"Both international non-governmental organizations and government actors have embraced the technological union of humans and software, known as crowdsourcing, to manage the flood of information produced during recent crises. However, unlike a business solution, the task of translation is unique during a crisis situation; the costs are human, and the impact is social and political. This paper follows four crises in which different crowdsourcing applications were developed by a range of actors. In each instance, the design approach failed to incorporate the unique circumstances of the conflict context, resulting in a translation application that removed authorship, dissolved intentionality, and shed contextual markers from original sources. This flawed application prevented the original contributors from interacting with the information directly related to their own life-threatening situation, and the information it amassed formed an unsound basis for decision-making by international actors. The associated consequences during: post-earthquake Haiti 2010, Libya and Egypt 2011 and Somalia 2011/12 are intended to provoke process improvement among all stakeholders.",,Sutherlin G,,2013,397–409,10.1177/0165551512471593,https://doi-org.proxy.bnl.lu/10.1177/0165551512471593;http://dx.doi.org/10.1177/0165551512471593,Journal Article
Evaluation of Spectrum Occupancy: A Case for Cognitive Radio in Uganda,"In recent years, proliferation of wireless devices has increased wireless access to nearly all of the world's population and use of services like mobile systems, GPS and Wi-Fi. Users are mobile, dynamic and majority prefer the 30MHz - 3000MHz band of the 300GHz spectrum due to propagation and equipment feasibility, rendering spectrum finite and constrained since virtually all radio-frequency (RF) spectrum is licensed. Cognitive radio (CR), a novel approach, sharing unoccupied spectrum by secondary users (SUs) while minimising interference to satisfy service (QoS) of PUs. The paper provides basis for exploiting the Uganda's spectrum sharing guideline and generally CR in Uganda. Maker ere University in Kampala, the capital, is chosen as a representative busy environment. The spectrum usage in shows relatively high utilisation in the FM, TV and mobile bands with high underutilisation of RF spectrum, for the Ugandan indoor and outdoor radio environment over a week. Further measures and statistics including channel occupancy/vacancy statistics, channel utilization from spectrum detected above the power threshold per band, are compared to other cities. Analysis of temporally freed TV bands is also presented.",,"Kagarura GM,Okello DK,Akol RN",,2013,167–174,10.1109/MSN.2013.66,https://doi-org.proxy.bnl.lu/10.1109/MSN.2013.66;http://dx.doi.org/10.1109/MSN.2013.66,Conference Paper
A Scalable Blockchain Implementation Model for Nation-Wide Electronic Voting System,"Blockchain technology adoption rate is fast growing as seen in cryptocurrency and distributed finance (DiFi) domains. It is also getting lots of attention in many other application areas including electronic voting(e-voting) systems. The electronic voting system is an interesting application use case for blockchain because it helps to solve critical problems within that space- the integrity of voting data, the secrecy of the ballot, and single point of failure. This is because of the characteristics that blockchain technology embodies. One of the challenges, however, is with the scalability of the blockchain network, how the blockchain technology can power the scalability of systems built on it. The aim of this paper, therefore, is to present a Blockchain Implementation Model that tackles scalability concerns for E-Voting System. This model can be adaptable in any national election, specifically, Nigeria’s national elections. The resulting model would present a scalable electronic voting framework by leveraging the security and integrity infrastructures that blockchain technology brings to bear.",,"Apeh AJ,Ayo CK,Adebiyi A",,2021,84–100,10.1007/978-3-030-87013-3_7,https://doi-org.proxy.bnl.lu/10.1007/978-3-030-87013-3_7;http://dx.doi.org/10.1007/978-3-030-87013-3_7,Conference Paper
Accessible Platforms for Anomalous Cell Detection Beyond Laboratory Boundaries,"Resource limitations and socioeconomic challenges pose obstacles to equitable healthcare access worldwide. Accurate diagnosis is an important early step in which technical innovation can help to bridge some gaps. Towards this goal, I worked on developing anomalous cell detection tools to enable broader access to health information for resource-constrained clinical settings. In this thesis, I explore portable tools for detecting malaria in blood samples and screening forensic swab samples. Quantifying the malaria-causing parasite Plasmodium falciparum relies on isolating the ring-stage infected red blood cells, which are challenging to differentiate from the majority of uninfected cells. I used magnetic levitation to perform label-free biophysical separation and detection of malaria-infected cells, by cumulatively leveraging density and magnetic susceptibility differences in the cells. Next, I built an accessible platform to process, image, and analyse whole blood patient samples in field settings, in a malaria-endemic region of Uganda. I combined cell lysis, suspension, and machine learning on cellphone images to develop an automated morphology-based classifier for blood samples. I also explored a different application for portable image-based screening, to address a sample processing backlog in sequencing forensic samples in the sexual assault justice workflow. I developed an automated algorithm for differentiating sperm from epithelial cells in sexual assault swab samples, using morphology features captured in cellphone imaging of microchips. Overall, this work demonstrates examples of anomalous cell detection technology built specifically for under-resourced settings. Such interdisciplinary techniques could help to overcome existing gaps in access to biological data, with the aims of supporting accurate diagnosis and surveillance of malaria, or screening of backlogged sexual assault samples.",,Deshmukh S,,2021,,,,Ph.D. Thesis
Market Information Needs Risk Assessment toward ICT Usage for Green Bean Producers in Dakar Region of Senegal,"In Senegal, information and communication technologies (ICTs) have been applied to accelerate the development of horticulture. Farmers can access information about weather, market price, and production volume through the ICT-based information systems. However, little is known about the nature and limitations of actual ICT usage among farmers in rural areas. Green bean is one of the dominant garden crops in Senegal, and market information is crucial to its farm management. Therefore, this study aims to assess the marketing risks, ICT usage, and information needs of green bean producers to further promote the use of ICT-based information systems. A survey was conducted in Dakar Region, the chief production area of green bean in Senegal. From the results of this study, it is found that perishability and competition were the main marketing risks of green bean producers. Mobile phone and telecentre were the most commonly adopted ICT in their daily life. Their key information needs included wholesale, retail, and input prices. Language and cost were the major limiting factors in further usage of ICT. Furthermore, female producers showed vulnerability in price risk. Younger producers appeared to have relatively higher usage of TV and household telephone while older producers had higher usage of radio usage. Similarly, higher education was positively correlated to higher information needs on weather and agricultural policy. Among ethnic groups, Serer and other ethnic minority groups appeared to be more vulnerable to marketing risks. Members of producers' associations seemed to have less concern about marketing risks and higher radio usage. Meanwhile, telecentre users showed higher marketing risks and greater information needs, indicating the telecentre as one of the key media to assist the vast uses. In sum, the findings of this study suggested tailored information requires handy media and proper format to reach rural producers. Based on the results, there is a necessity to develop an information system supported by voice service in local dialects as well as reliable and cost effective power sources. Finally, a research model for horticultural market information systems is also proposed to meet users' needs and enhance growth opportunities for horticulture industry in Senegal.",,"Chang WI,Tuan CL",,2011,235–246,,,Journal Article
The Impact of Information Lifecycle Management Process in the Nigerian Financial Sector,"The main objective of this paper focuses on the theoretical and practical approaches in the banking sector, with emphasis being placed on financial service operations of cheque processing and securities trading, and in managing financial information. The banking sector was identified as it is presently the most vibrant and emerging sector in the financial services in Nigeria, which was mentioned in the Financial Times. In 2007, the banking sector has been the driving force behind Nigeria's equity, which is in excess of $3.3bn in equity capital market transactions. The banks that were investigated in this research paper included banks that incorporate functions in the equity market as part of their daily operations and in the possession of relevant information on securities trading as well as information on cheque processing procedures. The paper concludes by suggesting that, there is above average knowledge and understanding of ILM within the financial sector in Nigeria. Therefore, it was recommended that the application of ILM techniques should be improved upon in the management of information in the Nigerian financial sector and to obtain the best results at every stage of the information lifecycle.",,"Al-Karaghouli W,Fadare EB",,2010,111–132,10.1504/IJBIS.2010.034008,https://doi-org.proxy.bnl.lu/10.1504/IJBIS.2010.034008;http://dx.doi.org/10.1504/IJBIS.2010.034008,Journal Article
Insider Perspectives of Human-Computer Interaction for Development Research: Opportunities and Challenges,"Human-Computer Interaction (HCI) research has gained traction in Africa in recent years. Researchers and designers have exploited the opportunities created by advances in mobile technology and increasing access to internet services in the communities to develop and deploy user-centered digital solutions targeting African audience and for solving Africa's problems. However, only a few researches have investigated the potential opportunities and challenges that face the adoption of HCI projects and mobile technology among indigenous people, especially in Biafra land in Nigeria. Therefore, the goal of this paper is to present an insider perspective of our lived experiences in conducting HCI4D and mobile technology research, the methodological and practical challenges, and design opportunities for advancing social and culturally-sensitive HCI research and mobile application designs in the Global South.",,"Samuel Nkwo M,Orji R,Ugah J",,2021,131–135,10.1145/3448696.3448709,https://doi-org.proxy.bnl.lu/10.1145/3448696.3448709;http://dx.doi.org/10.1145/3448696.3448709,Conference Paper
"A Sub-Regional Information System for Monitoring and Managing PLHIV in Cross-Border Areas between Gambia, Senegal and Guinea Bissau: Information System for Managing PLHIV in Cross-Border Areas","Since aids appeared, it is of common knowledge that geographical spread of HIV is linked to human mobility [1]. However, this relationship between mobility and Aids is both complex and relatively unknown. The FEVE project (Frontiers and Vulnerability to HIV in West Africa), is implemented by ENDA to underline the causal process in order to achieve the UNAIDS 90-90-90 target [2]: by 2020, 90% of the people living with HIV know their HIV status, 90% of the people who know their HIV-positive status are accessing antiretroviral therapy and 90% of the people receiving antiretroviral therapy will have suppressed viral loads. However, as in most African countries, in Senegal health actors generate a large amount of information every day, such as consultation, hospitalization, monitoring infectious diseases, deaths, etc that is recorded in registers. That make difficult their exploitation. This difficulty is compounded when we interested in data related to infectious diseases such as HIV in cross-border areas. The high mobility of the population in these areas poses a great deal of problems in terms of treatment adherence as well as the search for those lost to follow-up. To overcome this problem, we propose a transboundary platform for the monitoring of People Living with HIV (PVVIH). This platform is a web and application which offers not only a sub-regional system of PLHIV management but also a system of communication and capacity building between actors.",,"Dieng Y,Diop I,Faye Y,Malack CA",,2019,,10.1145/3361570.3361576,https://doi-org.proxy.bnl.lu/10.1145/3361570.3361576;http://dx.doi.org/10.1145/3361570.3361576,Conference Paper
"Innovations and Interdisciplinary Solutions for Underserved Areas: Second International Conference, InterSol 2018, Kigali, Rwanda, March 2425, 2018","This book constitutes the refereed post-conference proceedings of the Second International Conference on Innovations and Interdisciplinary Solutions for Underserved Areas, InterSol 2018, and the 7th Collogue National sur la Recherche en Informatique et ses Applications, CNRIA 2018, held in Kigali, Rwanda, in March 2018. The 23 papers presented were selected from 56 submissions and issue the following themes: papers dealing with the evolution of performances of solar systems in Africa, papers addressing the issues is public health, telecom papers studying the business model of telecommunication, math models presenting the climatic phenomenon and finally health papers dealing with medical devices that are suitable to underserved areas. The proceedings also contain 7 papers from the co-located 7th CNRIA (Collogue National sur la Recherche en Informatique et ses Applications) focusing on network architecture and security, software engineering, data management, and signal processing.",,"Kebe CM,Gueye A,Ndiaye A,Garba A",,2018,,,,Book
Legitimacy and Conflict in Areas of Limited Statehood: A Study of Political Violence in Nigeria,"Legitimacy is a ubiquitous social phenomenon that is studied in a variety of social scientific research domains. Yet, the term is inherently complex, multifaceted, and often situated within complex social arrangements. This dissertation examines the empirical complexities of legitimacy within Nigeria, an emergent and rapidly developing sub-Saharan African nation fraught with political violence and instability. The first chapter outlines the historic and contemporary theoretical fields, revealing legitimacy as a multidimensional symbolic commodity that operates within many levels of a given social structure. The dissertation synthesizes these theories and assumes a relational approach that takes into account the structural characteristics within Nigeria's populace and evaluates the dynamics between primary actors of the country's politically contested field: the institutionalized government, armed non-state challengers, and the citizenry as the legitimizing audience. Chapter 2 begins by operationalizing legitimacy's multidimensionality using survey data from two nationally representative samples. Confirmatory factor analysis reveals seven distinct components consisting of both normative and performance-based evaluative sources (where performance pertains to how a political system, institutions, and leaders function in practice). These dimensions are applied to Nigeria and reveal a diverse legitimacy landscape within a state balancing a complex and contentious ethno-political population. Chapter 3 investigates legitimacy relationships within a populace focusing on how neighborhood-level sentiments and community trust associate with individual legitimacy perceptions and attitudes towards authority system preservation. The analysis reveals that community consensus is a necessary condition for the positive influence of neighborhood-level legitimacy orientation on individual-level perceptions. Moreover, community trust is found to enhance the moderating properties of consensus on performance-based endorsement when predicting attitudes concerning community resistance to terrorist organizations. Chapter 4 examines how broader citizenry needs and relational orientations influence legitimacy evaluations of violent state actions against armed non-state organizations. This chapter applies performance-based and dynamic relational legitimacy theories to two complementary sets of analyses that (1) examine national orientations after a year-long surge in state-based violence against rebel groups and (2) investigate differences in regional orientations after an unexpected and disproportionate state-based attack on civilians. The models reveals that both security needs and orientations toward violent non-state actors significantly moderate the associations between state-based violence and legitimacy perceptions. Additionally, outcomes stemming from the moderating effects of rebel groups orientations reveal inconsistent directionality between legitimacy dimensions. This suggests the possibility of legitimacy dilemmas, such that increased legitimacy within some sectors of a given society may come at the expense of decreased legitimacy in other sectors, and also that increased legitimacy on particular dimensions may exist simultaneously with decreased legitimacy on other dimensions. Finally, the analysis reveals the importance of state-populace relations concerning the provision of government services, especially when matched with the needs of the citizenry, which can significantly enhance legitimacy perceptions across the dimensional spectrum. Overall, the analysis offers empirical clarification and theoretical advancements concerning the multidimensional and relational characteristics of legitimacy within a developing country characterized by violent political strife. The outcomes highlight the meaning, construction, sources, processes, and importance of legitimacy for any state concerned with establishing and sustaining central authority amidst internal areas of limited statehood.",,"Lizzol SM,Schwartzman K,Fiel J",,2020,,,,Ph.D. Thesis
Nonparametric Procedures in ICTs-Based Agricultural Market Information Network Pattern Analysis in Western African Regions,"Information and communication technology (ICT) has become an important and essential decision support tool in agricultural products marketability. The high production cost of farms and the increasing demand for food have pushed ICT to the forefront of the food supply chain. In developing world such as Sub-Saharan African (SSA) regions where the farm profitability is generally low, efforts have been focused on agronomy and production technologies for enhancing farm productivity. In today's competitive global and regional marketplaces, however, producing a sound product is not enough to ensure agricultural farm viability. Market information is considered as a prerequisite of farm business to enable the management of the products flow and substantially increase the benefit. Furthermore, ICT can reduce poverty by improving poor people's access to the market information to have better managerial decisions for maximizing their farms profits. Nevertheless, the realistic availability and effectiveness of ICT in current market information network to farm management in Africa have not yet been much explored. Therefore, this study aims to (i) analyse the ICT contribution in agricultural products market information system (MIS) through nonparametric procedures and (ii) identify the main problems for seeking an effective market-oriented information network pattern. For this study, important data for horticultural marketing have been collected in Saint-Louis and Dakar regions located in Senegal, Western Africa. From the results of this study, it is mainly found that there are different market information network patterns in the survey areas. These market information network patterns mostly rely on weak personal social contacts of producers and one-way media. Statistically, it is observed that some widely available ICTs have not been used effectively for market information dissemination in the survey areas. Basically, the major constraints of ICTs application for MIS in Senegal are the poverty of information contents, disadvantages of the remote communities, language incapability and out-of-date information. Therefore, this study proposes an information network scheme as an effective strategy to enhance the existing market information system in Senegal.",,"Chang WI,Tuan CL,Traore S",,2010,81–91,,,Journal Article
Blood Pressure Concerns: Findings from a Usability Study of Culturally Infused MHealth Design,"High blood pressure BP (i.e., hypertension) is a chronic condition and risk factor for cardiovascular disease, stroke, and heart failure, occurring in populations across the globe. Currently, smart phones and applications are developing rapidly, and mobile health applications are being used to manage hypertension. The goal of this study was to understand the usability findings from an iterative cross-cultural mHealth application to identify the perceived usefulness among African migrant adopters in Maryland, United States. Qualitative and quantitative statistical method were used to collect participants’ data. Usability findings reported that the behavioral intention of using the recommended features was influenced by the perceived usefulness of the AfriBP. The cultural dimensions were rated as the most preferred recommended features, followed by the health management feature. The perceived usefulness had a strong significant effect on attitude in adopting the AfriBP. Female participants adopted the AfriBP more than the male participants. The results regarding ethnicity found that the Nigerian participants considered the perceived usefulness of the AfriBP more than the Ghanaian participants. Few participants owned and or publicly used BP machines to monitor their BP. Few number of participants were less likely to use a smartphone health application to monitor their BP for health. The health status of the participants for BP readings and body mass index (BMI) was of great concern which supported prior research on Africans ancestry having the highest concern for BP.",,"Oladapo H,Chakraborty J",,2022,296–305,10.1007/978-3-031-05028-2_20,https://doi-org.proxy.bnl.lu/10.1007/978-3-031-05028-2_20;http://dx.doi.org/10.1007/978-3-031-05028-2_20,Conference Paper
Understanding Strategies for Implementing Integrated Information Systems for Rabies Surveillance,"Rabies continues to be one of the most perilous viral diseases that affect the nervous system and remains a significant threat to public health across the globe. Available data that show that rabies claims about 59,000 human lives annually. Most industrialized countries have eliminated rabies from domestic dog populations. Conversely, in most of the developing countries, rabies remains endemic in domestic dog populations and poorly controlled. One of the challenges in eradicating rabies in developing countries is attributed to ineffective surveillance systems. Different stakeholders have developed solutions to address this problem without tangible outcomes. Estimation of the economic burden particularly in developing countries is difficult because of the inadequacy of update and reliable surveillance data. Certainly, it is very challenging even to obtain basic information on how many human lives are lost due to rabies and the economics behind preventing the disease amongst those exposed. Up-to-date, official reporting of incidence data on rabies and rabies exposures status remains desperately poor in most canine rabies-endemic countries. Consequently, there is increasingly underestimation of the true burden of the diseases. Worse still data from active surveillance studies highlight the disparities between officially reported and recorded and likely occurring rabies deaths. In some cases, it has been shown that there are higher mortality rates than officially reported data, especially in resource deprived areas. This calls for a need to establish an integrated surveillance system, which allows data to be shared openly among different stakeholders dealing with rabies. The paper presents the state of art of rabies in Tanzania and evaluates the application of ICT in surveillance. It also advocates for a need of a comprehensive approach to addressing the problem. Development and adoption of integrated surveillance systems for rabies and other zoonotic diseases remain a nightmare in many developing countries including Tanzania. This paper calls for the development of an integrated standard mechanism for countries to assess their rabies status and measure progress in eliminating the disease. Such a system will fill the missing link between surveillance and control measures.",,"Geofrey A,Kipanyula MJ,Fue K,Sanga C",,2017,13–26,10.4018/IJUDH.2017010102,https://doi-org.proxy.bnl.lu/10.4018/IJUDH.2017010102;http://dx.doi.org/10.4018/IJUDH.2017010102,Journal Article
"IC4E '18: Proceedings of the 9th International Conference on E-Education, E-Business, E-Management and E-Learning","The 9th International Conference on E-Education, E-Business, E-Management and E-Learning (IC4E) was successfully held in San Diego, USA in January 11-13, 2018. IC4E provides a platform for scientists, engineers and technologists who work in all aspects of E-Education, E-Business, E-Management and E-Learning. IC4E 2018 received 43 submissions. Among rigorous peer review, we accepted 24 papers for presentation. These papers are contributed by researchers in many different countries, including Japan, South Korea, China, Taiwan, Hong Kong, Thailand, Philippines, Pakistan, Saudi Arabia, and Nigeria. These papers cover the topics range from knowledge management, personalized learning, micro-lecture construction, security policies, collaborative learning, mobile learning, social media, student behavior tracking, and employment issues.",,,,2018,,,,Book
Three Essays on Household Adaptation,"The chapters in this dissertation provide a way of tracing out a history of sorts; of how households and people coped with negative shocks in the past: through cooperative land tenure arrangements and collectives, and now: through technological platforms that follow the ""economic laws of the market."" The questions I ask are: How do households adapt to new circumstances in developing countries? And how has the landscape of the labor market changed to incorporate new technology for these households to use? How has new technology and new data allowed us to say more about not just households' economic lives, but their political lives as well?In the first chapter, I study how ride-share applications can be used to send money from cities to rural areas in times of bad rural shocks. Rural-urban linkages have long been a topic of study in the developing world. Remittances are often a key driver of these linkages and can act as insurance against rural weather shock risk, in the absence of availability and access to formal insurance products. The emergence of new technologies, such as ride-share and mobile money platforms, can be potentially transformative at allowing remittance flows to adjust more quickly to adverse shocks. I use a dataset of Uber driver labor supply and a rich dataset of weather indicators to estimate the effect of adverse weather shocks in rural areas on Uber drivers in Kampala, Uganda. Since I do not have explicit information on migrant status and rural connection, I leverage an external dataset of Ugandan voter registration and train a gradient boosting classifier on Ugandan surnames to predict drivers' regions of origin. I develop a switching regression estimator to address the misclassification bias from the predictions. I find that a one standard deviation increase in the intensity of agricultural drought is associated with an increase of 5.1 hours online in the month of the event (a 6% increase over average hours), providing suggestive evidence that Uber's flexibility is used to buffer against adverse weather shocks.In the second chapter, I study ethnic voting in Uganda. A large literature on ethnic voting has shown that ethnic identity is a major element in voting preferences for many developing countries, and that higher fractionalization in ethnicity leads to lower public goods provision. It is often difficult to disentangle ethnic identity and shared economic goals that stem from living in the same location. This paper differentiates between the effects of a voter's ethnic identity and their location, on voting behavior. We estimate these effects by pairing voter registration data with election outcomes from polling stations throughout Uganda. We overcome the challenge of measuring ethnic identity by using a machine learning algorithm that exploits variation in surnames across ethnic and linguistic groups on the polling station level. In particular, we use the letter sequences in each individual voter's surname to predict their ethnicity. We then use these predictions as the explanatory variable of interest in a regression model with the outcome being support for the incumbent president in the 2016 general election. We find that ethnic voting preferences are robust to location effects, but that location tends to amplify preferences, especially when in an area with similar views, and with a history of preferences disfavoring the candidate.In the third chapter, I explore an institution that existed before technology began to shape developing economies: the cooperative farm. Even despite there being heterogeneity in ability in cooperative agriculture, these institutional forms have existed for many years, because of their ability to pool costs amongst its members. I look at how the success of recent policies aimed at privatizing these communal farms (where I take as a context, the PROCEDE reform in Mexico) are affected by market power in the supply chain. What I find is that a higher level of monopsony power in the supply chain leads to lower price passthrough, meaning that an increase in the world price of tortillas, for example, will not transmit completely to an increase in the price of corn. This implies less of an increase to privatization of the cooperative farm.",,"Michuda A,Travis Lybbert,Dalia Ghanem",,2021,,,,Ph.D. Thesis
Adaptive Resource Allocation for Wildlife Protection against Illegal Poachers,"Illegal poaching is an international problem that leads to the extinction of species and the destruction of ecosystems. As evidenced by dangerously dwindling populations of endangered species, existing anti-poaching mechanisms are insufficient. This paper introduces the Protection Assistant for Wildlife Security (PAWS) application - a joint deployment effort done with researchers at Uganda's Queen Elizabeth National Park (QENP) with the goal of improving wildlife ranger patrols. While previous works have deployed applications with a game-theoretic approach (specifically Stackelberg Games) for counter-terrorism, wildlife crime is an important domain that promotes a wide range of new deployments. Additionally, this domain presents new research challenges and opportunities related to learning behavioral models from collected poaching data. In addressing these challenges, our first contribution is a behavioral model extension that captures the heterogeneity of poachers' decision making processes. Second, we provide a novel framework, PAWS-Learn, that incrementally improves the behavioral model of the poacher population with more data. Third, we develop a new algorithm, PAWS-Adapt, that adaptively improves the resource allocation strategy against the learned model of poachers. Fourth, we demonstrate PAWS's potential effectiveness when applied to patrols in QENP, where PAWS will be deployed.",,"Yang R,Ford B,Tambe M,Lemieux A",,2014,453–460,,,Conference Paper
Citizen-Centric E-Government Services in Namibia: Myth or Reality?,"Citizen-centric e-government shifts the focus of electronic services delivery from a top-down approach to the end-users. In order to provide better services to the citizens, the Namibian government took the initiative to make services available online and established a national web portal. The Ministry of Home Affairs and Immigration is one of the first Ministries to have rolled out its e-government services to citizens and among others it has an interactive website where citizens can access information, downloadable application forms and a Short Message Service (SMS) notification system for National Identification Systems. Despite such initiatives, there are still challenges affecting the v by the Ministry of Home Affairs and Immigration. The findings include a lack of awareness of services, cost factors, inadequate accessibility, a need for full online services, trust and security considerations. Citizens preferred mobile online services and expressed willingness to embrace online services.",,"Amukugo K,Peters A",,2016,193–197,10.1145/2998581.2998610,https://doi-org.proxy.bnl.lu/10.1145/2998581.2998610;http://dx.doi.org/10.1145/2998581.2998610,Conference Paper
"""Out from Behind This Mask"": Persona in African American Poetry, 1830-1930","""Out from Behind this Mask"": Persona in African American Poetry, 1830-1930 investigates the processes of poetry production and the politics of black identity from the nineteenth century through modernism, offering readings of significant work by George Moses Horton, Adah Isaacs Menken, Paul Laurence Dunbar, and Jean Toomer. These readings are grounded on a re-theorization of the function of the poetic speaker, or persona, which gives a poem its central identity and voice. I claim that persona's lineage as a masking device—in ancient Greek and Roman drama, a change of masks delineated changes in character—represents an enduring problem in lyric theory that conflates the identity of the poet with his/her constructed personae. This conflation has often resulted in an ambiguous binary of poetic identity as ""mimetic"" (the fictional persona) or ""authentic"" (the poet as him/herself). I argue that leveraging and contesting this binary in the lyric tradition can explain how critics have overlooked the formal innovations of black poetic identity by conceiving of masking simply as disguise rather than as a complicated series of transitions among a broad range of poetic identities. While questions of authenticity are applicable to all poets, I show that they have especially significant resonances for African American poets in the slave era and Jim Crow era. After all, these writers not only produced their work in contexts of racial hierarchy, such as the threat of slavery or the popularity of blackface minstrelsy, but they were also limited by racial-literary standards that determined and restricted the creation, dissemination, and interpretation of their poetry. Through original archival work, research on historical context, and close formal analysis, I show how each poet challenged the binary limitations placed around their verse production and reception: enslaved versus free in Horton's work; theatrical artifice versus private exposé in Menken's; black dialect versus formal English in Dunbar's; lyrical sensuousness versus didactic utility in Toomer's. While Horton, Menken, Dunbar, and Toomer attempted to turn mischaracterizations and/or incomplete assessments of their verses into more complex forms of masking (as movements between a diverse range of poetic identities), their efforts have ironically continued to be neglected in criticism and pedagogy even today. Chapter 1 examines George Moses Horton, who is considered the first African American poet to articulate an antislavery poetics while still enslaved, and more significantly, to protest his own bondage in verse. Horton's personae of positionality, as I call it—his constant manipulation of stance due to his status as a slave (and then former slave)—is first contextualized against his valentine acrostics and antebellum cultures of literacy, and then against his first volume, The Hope of Liberty (1829), which was funded by the American Colonization Society on the condition that Horton's freedom would be granted if he emigrate to Liberia. Lastly I look at Horton's replacement of antislavery personae with love verses in The Poetical Works (1845), and at his newly garnered freedom and experiences traveling with the Ninth Michigan Cavalry Volunteers during the Civil War in Naked Genius (1865). I contend that Horton's distinct move was in cultivating an intertextual poetics that reflected the discordant nature of abolitionism. From the American Colonization Society to the divided factions of gradualists and immediatists, which produced multiple positions on the state of the American racial system, slave identity, and emancipation, Horton's verses address the rightful place of African Americans as they imagined moving, and then actually moved, from enslavement to freedom. Chapter 2 explores Adah Isaacs Menken's forgotten work as the first poet besides Walt Whitman and the only female poet before the twentieth century to write an entire volume—titled Infelicia (1868)—in the form of free verse, following the revolution of prosody in Leaves of Grass (1855). I read Menken's poetry as informed and undermined by her performative role in Mazeppa, which brought her international celebrity in the risqué exposure of her body, as well as by her unconventional womanhood and alleged racial ambiguity, which sensationalized her private life. Menken found in Whitman, whom she met through the bohemian literati at Pfaff's in New York, a model for a new poetics and a shared belief in the mutable nature of identity. Not only did she champion his project publicly in one of the first endorsements of Leaves of Grass but her only poetry collection, Infelicia, was also written almost entirely in free verse during the period in which she was in direct contact with Whitman. I argue that Menken adapted a key feature of Whitman's form and theory of language for her own poetry: the democratization of voice and the license to represent others. Yet unlike Whitman, who leveraged lyric universalism to unify Americans fractured by class and racial conflict, Menken's personae, in light of her own experiences as a professional actress and radical woman, posit that the idea of universal lyric expression is irrevocably fraught. Chapter 3 considers how Paul Laurence Dunbar's metapoetic standard English poems in Oak and Ivy (1893), Majors and Minors (1895), Lyrics of Lowly Life (1896) and Lyrics of the Hearthside (1899) responded to the popularity of black dialect poetry during this era and to Dunbar's role as the first African American commercial writer. I propose that his metapoetic standard English poems, in which the speakers act as poets performing or reflecting on failed literary work, are stagings of poetic process that protest the constraints of black dialect verse as opposed to the lyric spontaneity available to his white Romantic counterparts. The perceived notion that Dunbar's ""pure"" blackness imbued his black dialect verse with more racial authenticity than white writers (and other black writers) of dialect brought him notoriety but also resulted in a poetic and financial crisis that only further contributed to the popularity of his dialect work over his standard English personae. By uncovering programs for and descriptions of Dunbar's recitals, for instance, I find that Dunbar attempted to counter many of these problems by beginning his readings with a standard English poem. Looking at his romantic poems can show us how Dunbar debunked the deeply problematic myths about the imaginative possibilities of black writers and bridged the gap between Post-Reconstruction and Modernism. Chapter 4 situates Jean Toomer's later unpublished verses from the 1930s to the 1960s against his first published work, Cane (1923), considered to be a masterpiece of African American modernist lyricism, steeped in sensuous imagery of the slave past and urban black life. Toomer spent most of his life after Cane distancing himself from the book and his African American identity, in turn influencing the critical consensus that his later didactic writings are both steeped in racial denial and aesthetically flawed. I claim, however, that his nearly four decades of writing illuminate a striving for the kind of multiracial identification that he consistently recognized himself and his work to possess. From Toomer's seeming abandonment of his racial identity and modernist lyric for the Eastern mystic George Gurdjieff in the 1930s, to his long association with the Society of Friends (Quakers) until his death, I explore how Toomer's poetry is marked by the desire for functionality. Toomer believed personae not only represented but also resolved the struggle between the ""outer shell"" of social conditioning, and the inner, ""not manifested"" state of self-consciousness and unity. He therefore envisioned lyric as an expansive genre with uniquely serviceable aims that could readjust individual and collective relations to racial and literary identity, historical violence, and psychological trauma.",,Licato AM,,2018,,,,Ph.D. Thesis
The Evolving Braid: How an Organization in Uganda Achieved Reliable Communications,"When engaged in ICTD research, it is often simpler to focus efforts on a single specific technology, whether that entails computers for telecenters, mobile phones for data collection, or text messages for public health education. In practice, however, people and organizations use a variety of technologies together, smoothly interweaving them as they navigate their lives. In this paper we analyze the ways in which a health financing organization in Uganda integrates a variety of communications technologies together to achieve reliable communications with their partnering health facilities distributed throughout Southwest Uganda. Based on four years of participant observation, we describe two communication scenarios in this organization to illustrate braided communications at work. We find that stakeholders work together to develop and maintain effective relationships using many different communications channels together in parallel, a combined channel we describe as braided communications. Braided communications have three primary characteristics. Firstly, they use co-existing channels, employing each as best suits a given set of goals. Secondly, they are co-dependent, or co-reinforcing, with strengths of individual channels reinforcing weaknesses of other channels. Finally, they are co-evolving; as available technologies and the ways in which they are used change, the nature of the braided use changes as well.",,"Densmore M,Bellows B,Chuang J,Brewer E",,2013,257–266,10.1145/2516604.2516620,https://doi-org.proxy.bnl.lu/10.1145/2516604.2516620;http://dx.doi.org/10.1145/2516604.2516620,Conference Paper
"Quest for Fire, Water, Earth and Air: An Interaction Design Bus and Art Installation Reflecting Climate Change Concerns through Human and Elemental Connectedness","The notion of travelling to open doors onto different perspectives is an antidote to living, working and socializing in spaces mediated by technologically designed artifacts. Interfaces have become ubiquitous and relationships and styles of communication have changed in keeping with this ever-present trend. The Bachelors in Technology design students, from the Cape Peninsula University of Technology, have shared their concerns through their design research problems in exactly these areas. Questions regarding water safety, food security, air quality, sewerage management, marginalisation of people with disability, cultural specificity being erased by generic digital content arose; it became clear that the fragile threads connecting the ecosystem to the human system need nurturing. From an internal landscape to an external one, these students apply themselves creatively and intellectually in order to tackle real problems pro-actively; to talk less and do more. To this end, a sizable art installation piece has been conceptualized and constructed which will be unveiled with an accompanying performance piece at the 13th Participatory Design Conference (PDC) in Namibia, in October 2014.",,"Chisin AV,van Niekerk J,M'Rithaa MK",,2014,183–185,10.1145/2662155.2662229,https://doi-org.proxy.bnl.lu/10.1145/2662155.2662229;http://dx.doi.org/10.1145/2662155.2662229,Conference Paper
Filter List Generation for Underserved Regions,"Filter lists play a large and growing role in protecting and assisting web users. The vast majority of popular filter lists are crowd-sourced, where a large number of people manually label resources related to undesirable web resources (e.g. ads, trackers, paywall libraries), so that they can be blocked by browsers and extensions. Because only a small percentage of web users participate in the generation of filter lists, a crowd-sourcing strategy works well for blocking either uncommon resources that appear on “popular” websites, or resources that appear on a large number of “unpopular” websites. A crowd-sourcing strategy will perform poorly for parts of the web with small “crowds”, such as regions of the web serving languages with (relatively) few speakers. This work addresses this problem through the combination of two novel techniques: (i) deep browser instrumentation that allows for the accurate generation of request chains, in a way that is robust in situations that confuse existing measurement techniques, and (ii) an ad classifier that uniquely combines perceptual and page-context features to remain accurate across multiple languages. We apply our unique two-step filter list generation pipeline to three regions of the web that currently have poorly maintained filter lists: Sri Lanka, Hungary, and Albania. We generate new filter lists that complement existing filter lists. Our complementary lists block an additional 3,349 of ad and ad-related resources (1,771 unique) when applied to 6,475 pages targeting these three regions. We hope that this work can be part of an increased effort at ensuring that the security, privacy, and performance benefits of web resource blocking can be shared with all users, and not only those in dominant linguistic or economic regions.",,"Sjösten A,Snyder P,Pastor A,Papadopoulos P,Livshits B",,2020,1682–1692,10.1145/3366423.3380239,https://doi-org.proxy.bnl.lu/10.1145/3366423.3380239;http://dx.doi.org/10.1145/3366423.3380239,Conference Paper
"Vulnerability to Food Insecurity Index : A Multi-Dimensional Model for Measuring Household's Food Security and Vulnerability in Low, Middle-Income Countries","This thesis developed and validate a multidimensional food security indicator called the Vulnerability to Food Insecurity Index (VFII). Currently, there is no standard indicator of vulnerability analysis in food security research and this thesis responds to this challenge. The first research objective was to demonstrate how to develop this indicator and establish its validity through comparison with other traditional food security indicators such as per capita calorie consumption (PCC), food consumption score (FCS) and coping strategy index (CPI). The second objective was to systematically evaluate the effect of some assumptions on the robustness of the VFII. The aim was to examine how data type, weighting scheme, normalization method and excluding/including of variables, affect the output of the index using sensitivity and uncertainty analysis. The third objective was to verify the result of the VFII with real-life experience and to understand why households are vulnerable to food insecurity using qualitative insight. The research applied both quantitative and qualitative method. The study used the World Bank LSMS panel dataset for households in South-South Nigeria to design the index, while fieldwork was used to verify the results of the index. In designing the VFII four steps were used. The first developed a conceptual framework for vulnerability to food insecurity, which helped to select indicators for the index. Structurally, Vulnerability to Food Insecurity Index is a multidimensional index of the probability of covariate shock occurring (exposure), the accumulative experience of food insecurity (sensitivity) and coping ability of households (adaptive capacity). The second step applies equal weight to each component of the index based on the evidence from the sensitivity and uncertainty analysis. In the third step, variables were normalised using the min-max normalization method. In the last step, a linear aggregation method was applied to generate the score of the index. For the uncertainty and sensitivity analysis, the one-at-a-time and global sensitivity approach were applied to examine the robustness of the index. Using the one-at-a-time approach, the research explored how the VFII output responds to different weighting schemes, normalisation method and inclusion/exclusion of variables. For the global approach, Monte Carlo simulation and Sobol first-order index and total-effect index were used to explore the uncertainty and sensitivity of VFII. In the qualitative phase, the results of the index from the quantitative phase were verified in the field using qualitative methods. Food vulnerability maps for households in South-South Nigeria were used to purposively select Akwa Ibom State for the verification exercise.",,Ibok OW,,2019,,,,Ph.D. Thesis
Integrating Web 2.0 into an Academic Library in Tanzania,"Purpose - This paper aims to demonstrate work undertaken by Muhimbili University of Health and Allied Sciences MUHAS Library in an effort to integrate Web 2.0 technologies in its functions to enhance the quality of its services in Tanzania. Design/methodology/approach - The study conducted an exploratory questionnaire survey to assess user requirements among undergraduate medical students at MUHAS, developed Library 2.0 services, conducted training and created awareness. Findings - The paper shows that Web 2.0 technologies can be implemented effectively according to university goals, user's needs, deployment of user friendly tools, and capacity building among librarians and users. Students positively supported the adoption of Library 2.0 services at MUHAS. Library 2.0 services improved the quality of MUHAS library services, despite various challenges related to infrastructure, awareness, literacy, inadequate staff, security and ownership of Web 2.0 services. Research limitations/implications - The study findings may not be widely replicated because this article is based on a case study of the integration of Web 2.0 technologies into the library functions of MUHAS. This study did not examine the use of Library 2.0 applications among library users such as faculty and students which could illuminate further the case study. Practical implications - Most academic libraries in Africa have not yet adopted Web 2.0 technologies to improve their services. The user preferences, technology adoption, and challenges faced from the present study can help other libraries to plan and integrate their Library 2.0 technologies in their services. Originality/value - MUHAS Library offers a practical example of how Web 2.0 services can be adopted to enhance the quality of academic library services in an African context. This paper is of significance to academic libraries that are still considering their options with regard to the application of Web 2.0 technologies.",,Tandi Lwoga E,,2014,183–202,10.1108/EL-06-2012-0058,https://doi-org.proxy.bnl.lu/10.1108/EL-06-2012-0058;http://dx.doi.org/10.1108/EL-06-2012-0058,Journal Article
Public Perception of Mental Illness: Opportunity for Community-Based Collaborative Intervention,"We explore factors contributing to poor mental healthcare, treatments and help-seeking behaviors among communities in Nigeria and across Africa. The findings from the interview of 25 stakeholders reveal some socio-cultural factors such as negative perceptions, stigmatizations, religious beliefs, and absence of automated supports, which hinder mental healthcare and help-seeking. Also, delays in seeking appropriate medical attention and intake of untested local herbs could lead to severe depressive symptoms, suicidal risks, and adversely affect the mental health of clients. Based on our findings, and in collaboration with the stakeholders, we designed ""Gwam-Okwu"" [Talk to Me]; a culturally-appropriate interactive app that is hyper-localized, safe and secured, and tailored to support communication and collaboration between health workers and clients/relations, personalized self-monitoring, and guided self-learning for the clients.",,"Nkwo M,Suruliraj B,Orji R",,2020,1–7,10.1145/3334480.3383023,https://doi-org.proxy.bnl.lu/10.1145/3334480.3383023;http://dx.doi.org/10.1145/3334480.3383023,Conference Paper
"Validation of Land Surface Temperature Derived from MSG/SEVIRI with in Situ Measurements at Gobabeb, Namibia","Land surface temperature LST derived from Meteosat Second Generation/ Spinning-Enhanced Visible and Infrared Imager MSG/SEVIRI data is an operational product of the Land Surface Analysis Satellite Applications Facility LSA SAF. The LST has a temporal resolution of 15 minutes, a sampling distance of 3 km at nadir, and a targeted accuracy of better than 2 K. Gobabeb Namibia is one of Karlsruhe Institute of Technology's KIT's four dedicated stations for LST validation. In March 2010, a field survey was performed to characterize the Gobabeb site more closely. SAF LST and in situ LST obtained over a period of 3 days from additional measurements with a telescopic mast on the Namib gravel plains were in good agreement with each other bias 1.0 K. For the same period, the bias between SAF LST and Gobabeb main station LST was even smaller 0.4 K. A mobile measurement system was set up by fixing the telescopic mast to a four-wheel drive. Around solar noon, LST from in situ measurements along a 40 km track and LST from Gobabeb main station had a bias of 0.4 K and a standard deviation of 1.2 K, which means that in situ LSTs at Gobabeb main station are representative for large parts of the gravel plains. Exploiting this relationship, 2 years of LST from MSG/SEVIRI were compared with in situ LST from Gobabeb main station. The magnitude of the monthly biases between the two data sets was generally less than 1.0 K and root mean square errors were below 1.5 K. Furthermore, the bias appears to exhibit a seasonality, which could be accounted for in future validation work.",,"Göttsche FM,Olesen FS,Bork-Unkelbach A",,2013,3069–3083,10.1080/01431161.2012.716539,https://doi-org.proxy.bnl.lu/10.1080/01431161.2012.716539;http://dx.doi.org/10.1080/01431161.2012.716539,Journal Article
Inconsistent Requirements: An Argumentation View,"In this article, we present a logical framework for reasoning about inconsistent requirements in the context of multi-viewpoint requirements engineering process. In order to analyse the sources of inconsistencies and to reason with inconsistent requirements, we present an argumentation view of the requirements. Intuitively, argumentation is a tool for reasoning with inconsistent knowledge: requirements are defined in terms of arguments (a conclusion with its support); then, a class of acceptable arguments is built (arguments with no counterarguments). We propose to characterize different classes of requirements which are ordered: from weakly confident to strongly confident (i.e. consistent). In the paper, we present inference rules to build intra and inter-viewpoint reasoning. Inference rules are issued from the classes of requirements. We show how this work is useful for the requirements engineers to analyse inconsistent fragments of requirements.A Multi-Agent System to the Common Management of a Renewable Resource: Application to Water Sharing M. Le Bars a,b and J.M. Attonaty b a LAMSADE laboratory Université Paris-Dauphine 75775 Paris Cedex 16. France. bINRA Station d'économie rurale, BP 01 78850 Grignon France. lebarsm@aol.com;attonaty@grigon.inra.fr Abstract. Water sharing has become an important problem in France. A lot of negotiations are taking place at a local level between farmers, water suppliers, public services and environmentalists to allocate water resources between users. The problem is to share water with respect of different criteria like economic (global output) ethical (disparities between actors) environmental (water savings). Different approaches have been already taken using linear programming or game theory, but they are always based on the hypothesis that decision-makers are completely rational, take into account few players and are often monoperiodic. We suggest that an Agent-Based Modelling (ABM) built with a Multi-Agent approach could help negotiations between different players by showing the consequences of water allocation rules and taking in consideration the players 'respective attitudes and their ability to change their behaviour. In this paper we will first present the model structure with the different types of agents modelled, how the model runs over a number of years and the first results of simulations. Keywords. Distributed Artificial Intelligence; Multi-Agent Systems Title: A new hybrid method for solving constraint optimization problems in anytime contexts Authors: Samir Loudni and Patrice Boizumault Affiliation: Ecole Des Mines de Nantes Abstract: In this paper, we present a new hybrid method for solving constraint optimization problems in anytime contexts. We use the Valued Constraint Satisfaction Problem (VCSP) framework to model numerous discrete optimization problems. Our method (VNS/LDS+CP) combines a Variable Neighborhood Search (VNS) scheme with Limited Discrepancy Search (LDS) using Constraint Propagation (CP) to evaluate cost and legality of moves made by VNS. Our experimental results on real-word problem instances demonstrate that our method clearly outperforms both LNS/CP/GR (another hybrid method which also relies on the VCSP framework) and other standard local search methods as Simulated-Annealing. This confirm the benefit of the use, in a local search, of the LDS partial search with constraint propagation. keywords: Anytime Problems, Constraint-Satisfaction, Constraint-Optimization, Local Search Methods, Hybrid Methods. Using Software Agents to avoid Collisions among Multiple Robots Markus Jäger Corporate Technology, Information and Communications Siemens AG 81739 Munich, Germany markus.jaeger@mchp.siemens.de AI Algorithms Collaborative Software Agents Cooperating Robots This paper describes a method where collaborative software agents are used to coordinate the independently planned trajectories of multiple mobile robots to avoid collisions and deadlocks among them. Whenever the distance between two robots drops below a certain value, the agents exchange information about the planned trajectories of the robots and determine whether they are in danger of a collision. If a possible collision is detected, the agents monitor the robots movements and, if necessary, insert idle times between certain segments of the trajectories in order to avoid the collision. Deadlocks among two or more robots occur if a number of robots block each other in a way such that none of them is able to continue along its trajectory without causing a collision. These deadlocks are reliably detected by the agents. After a deadlock is detected, alternative trajectories for each of the involved robots are successively planned until the deadlock is resolved. The agents use a combination of three fully distributed algorithms to reliably solve the task. They do not use any global synchronization. ----------- Authors Affiliations: - Cooperating Software Agents - Cooperating Robots - Collision Avoidance among multiple Robots - Area Partitioning - Area Coverage The title : Successive Search Method for Valued Constraint Satisfaction and Optimization Problems. Authors Names : Mohamed TOUNSI and Philippe DAVID Email Adresses : mohamed.tounsi@emn.fr and philippe.david@emn.fr Affiliation : Computer Science Department, Ecole des Mines de Nantes , 4 Rue Alfred Kastler 44307 Nantes, FRANCE. Abstract : In this paper we introduce a new method based on Russian Doll Search (RDS) for solving optimization problems expressed as Valued Constraint Satisfaction Problems (VCSPs). The RDS method solves problems of size n (where n is the number of variables) by replacing one search by n successive searches on nested subproblems using the results of each search to produce a better lower bound. The main idea of our method is to introduce the variables through the successive searches not one by one but by sets of k variables. We present two variants of our method: the first one where the number k is fixed, noted kfRDS; the second one, kvRDS, where k can be variable. Finally, we show that our method improves RDS on daily management of an earth observation satellite. Keywords : Constraint Satisfaction, VCSP, Optimization Problems. B-Course: A Web Service for Bayesian Data Analysis Petri Myllymaki, Tomi Silander, Henry Tirri, Pekka Uronen Complex Systems Computation Group (CoSCo) P.O.Box 26, Department of Computer Science FIN-00014 University of Helsinki, Finland URL: http://www.cs.Helsinki.FI/research/cosco/ B-Course (http://b-course.cs.helsinki.fi) is a free web-based online data analysis tool, which allows the users to analyze their data for multivariate probabilistic dependencies. These dependencies are represented as Bayesian network models. In addition to this, B-Course also offers facilities for inferring certain type of causal dependencies from the data. The software uses a novel ""tutorial style"" user-friendly interface which intertwines the steps in the data analysis with support material that gives an informal introduction to the Bayesian approach adopted. Although the analysis methods, modeling assumptions and restrictions are totally transparent to the user, this transparency is not achieved at the expense of analysis power: with the restrictions stated in the support material, B-Course is a powerful analysis tool exploiting several theoretically elaborate results developed recently in the fields of Bayesian and causal modeling. B-Course can be used with most web-browsers (even Lynx), and the facilities include features such as automatic missing data handling and discretization, a flexible graphical interface for probabilistic inference on the constructed Bayesian network models (for Java enabled browsers), automatic pretty-printed layout for the networks, exportation of the models, and analysis of the importance of the derived dependencies. In this paper we discuss both the theoretical design principles underlying the B-Course tool, and the pragmatic methods adopted in the implementation of the software. Artificial Neural Networks in Hydrological Watershed Modeling: Surface Flow Contribution from the Ungaged Parts of a Catchment Richard Chibanga1, Jean Berlamont2 and Joos Vandewalle3 1PhD student in the Civil Eng. Dept. and 2Prof. Civil Eng. Dept., and Head of Hydraulics Laboratory, 3 Prof. Head of Electrical Engineering Dept - SISTA/COSIC, Katholieke Universiteit, Kasteelpark Arenberg 40, 3001 Heverlee (Leuven), Belgium Abstract Watershed modeling is often faced with the difficulty of determining the flow contribution from the ungaged sections of the catchment. Where the main concern is making accurate streamflow forecasts at specific watershed locations, it is cost-effective and efficient to implement a simple system theoretic model. In this paper Artificial Neural Networks (ANNs) are used as system theoretic models to model the ungaged flows. Using data from the Kafue River sub-catchment in Zambia and a simple reservoir routing model, an estimate of the flow contribution from the ungaged sections is derived. Inputs: rainfall, evaporation, previous-time-step flow are fed to a series of Feedforward-Backpropagation ANNs with target-output the current derived flow. Selected best- performing ANNs are compared with Autoregressive Moving Average models with exogenous inputs (ARMAX) and they give accurate and more robust forecasts over long term than the best performing ARMAXs thereby making ANNs a viable alternative in time-series forecasting. Keywords: semi conceptual-system theoretic; Artificial neural networks; subsystem; tributary-runoff; forecasting; mapping. Title: Generation of Propagation Rules for Intentionally Defined Constraints Authors: Slim Abdennadher Computer Science Department, University of Munich Oettingenstr. 67, 80538 Munich, Germany Slim.Abdennadher@informatik.uni-muenchen.de Christophe Rigotti Laboratoire d'Ingenierie des Systemes d'Information Batiment 501, INSA Lyon, 69621 Villeurbanne Cedex, France Christophe.Rigotti@insa-lyon.fr Abstract: A general approach to implement propagation and simplification of constraints consists of applying rules over these constraints. However, a difficulty that arises frequently when writing a constraint solver is to determine the constraint propagation algorithm. In previous work, different methods for automatic generation of propagation rules for constraints defined over finite domains have been proposed. In this paper, we present a method for generating propagation rules for constraint predicates defined by means of a constraint logic program. Keywords: Constraint solving, Machine learning, Rule-based programming Title : ""Data Flow Coherence Criteria in ILP Tools"" Authors : Smaranda Muresan Department of Computer Science, Columbia University, New York, USA smara@cs.columbia.edu Tudor Muresan Department of Computer Science, Technical Univ. of Cluj-Napoca, Cluj-Napoca, Romania tmuresan@cs.utcluj.ro Rodica Potolea Department of Computer Science, Technical Univ. of Cluj-Napoca, Cluj-Napoca, Romania potolea@cs.utcluj.ro Keywords : Inductive Logic Programming, Automatic Program Generation, Data Flow Coherence Criteria, Pruning the Search Space Abstract: In this paper we present a new method that uses data-flow coherence criteria in definite logic program generation. We outline three main advantages of these criteria supported by our results: i) drastically pruning the search space (around 90%), ii) reducing the set of positive examples and reducing or even removing the need for the set of negative examples, and iii) allowing the induction of predicates that are difficult or even impossible to generate by other methods. Besides these criteria, the approach takes into consideration the program termination condition for recursive predicates. The paper outlines some theoretical issues and implementation aspects of our system for automatic logic program induction. Title: An Expert Recommendation System using Concept-based Relevance Discernment Authors: Takashi Yukawa NTT Corporation NTT Communication Science Laboratories 2-4 Hikaridai, Seika-cho, Kyoto, Japan yukawa@cslab.kecl.ntt.co.jp Kaname Kasahara NTT Corporation NTT Communication Science Laboratories 2-4 Hikaridai, Seika-cho, Kyoto, Japan kaname@cslab.kecl.ntt.co.jp Tsuneaki Kato The University of Tokyo Graduate School of Arts and Science Dept. of Language and Information Science 3-1-8 Komaba, Meguro-ku, Tokyo, Japan kato@boz.c.u-tokyo.ac.jp Toshiro Kita NTT Communications Corporation Solution Business Division Yamato Seimei Bldg. 1-1-7, Uchisaiwai-cho Chiyoda-ku, Tokyo, Japan toshiro.kita@ntt.com Keywords: information retrieval, recommendation system, vector space model, concept base, knowledge management Abstract: An expert recommendation system using concept-based relevance discernment is proposed. This system processes the description of a technical topic as input and then finds engineers who have a high level of expertise in that area. The technique employed is an extended vector space model that locates both technical topics and engineers in the same multi-dimensional space, and then calculates their relevance. This system can also retrieve engineers or documents that are related to a field matching a given engineer's technical interests. Such a system can be expected to play the role of a person's professional network, and be a valuable tool for knowledge management among several organizations. Paper Title: Combinatorial Optimization through Statistical Instance-Based Learning Keywords: constructive search, heuristics, optimization, instance-based learning Authors: Orestis Telelis, Panagiotis Stamatopoulos Department of Informatics and Telecommunications University of Athens 157 84 Athens, Greece telelis,takis@di.uoa.gr Abstract: Different successful heuristic approaches have been proposed for solving combinatorial optimization problems. Commonly, each of them is specialized to serve a different purpose or address specific difficulties. However, most combinatorial problems that model real world applications have a priori well known measurable properties. Embedded machine learning methods may aid towards the recognition and utilization of these properties for the achievement of satisfactory solutions. In this paper, we present a heuristic methodology which employs the instance-based machine learning paradigm. This methodology can be adequately configured for several types of optimization problems which are known to have certain properties. Experimental results are discussed concerning two well known problems, namely the knapsack problem and the set partitioning problem. These results show that the proposed approach is able to find significantly better solutions compared to intuitive search methods based on heuristics which are usually applied to the specific problems. TITLE: Interleaved Backtracking in Distributed Constraint Networks > AUTHOR: Youssef Hamadi > AFFILIATION: Hewlett Packards Labs Filton road, Stoke Gifford, > Bristol BS34 8QZ, United Kingdom > EMAIL: yh@hplb.hpl.hp.com > > > Abstract > The adaptation of software technology to distributed > environments is an important challenge today. In this > work we combine parallel and distributed search. By > this way we add the potential speed up of a parallel > exploration in the processing of distributed problems. > This paper extends DIBT, a distributed search proce > dure operating in distributed constraint networks [6]. > The extension is twofold. First the procedure is up > dated to face delayed information problems upcoming > in heterogeneous systems. Second, the search is ex > tended to simultaneously explore independent parts of > a distributed search tree. By this way we introduce > parallelism into distributed search, which brings to In > terleaved Distributed Intelligent BackTracking (IDIBT). > Our results show that 1) insoluble problems do not > greatly degrade performance over DIBT and 2) super > linear speed up can be achieved when the distribution > of solution is nonuniform. > > Keywords: Distributed Constraint Satisfaction, Distributed AI, > Collaborative Software Agents, Search",,"Perrussel L,Charrel PJ",,2001,79,,,Conference Paper
Monitoring Short Term Changes of Malaria Incidence in Uganda with Gaussian Processes,"A method to monitor communicable diseases based on health records is proposed. The method is applied to health facility records of malaria incidence in Uganda. This disease represents a threat for approximately 3.3 billion people around the globe. We use Gaussian processes with vector-valued kernels to analyze time series components individually. This method allows not only removing the effect of specific components, but studying the components of interest with more detail. The short term variations of an infection are divided into four cyclical phases. Under this novel approach, the evolution of a disease incidence can be easily analyzed and compared between different districts. The graphical tool provided can help quick response planning and resources allocation.",,"Andrade-Pacheco R,Mubangizi M,Quinn J,Lawrence N",,2015,2–8,,,Conference Paper
"Transnational Regulation, Lenders' Responses and the Needs of Consumer Borrowers in Nigeria","This thesis undertakes an analysis of the developing international paradigm and rationale for regulating consumer credit and their application to Nigeria. The thesis makes an original contribution by problematising the simple application of the transnational model to Nigeria, which currently produces counterproductive outcomes for consumer borrowers. The thesis argues that the emerging transnational paradigm since the Global Recession presents an opportunity for stronger consumer protection and access to credit in Nigeria only if the framework is adapted to the existing institutional structures. To effectively address the research question, the thesis engages with economic neoliberalism as an analytical framework, the influences of psychology (behavioural science) on law, and the historical relationship of Nigeria with transnational institutions. Through a careful analysis of existing literature and a small scale qualitative study, the thesis found: (a) links between transnational regulation and decreased lending by formal sector lenders to consumer borrowers in Nigeria; (b) a weak coalition of change agents at the national level to advance the interests of consumer borrowers, and thus, forsee a role for transnational actors as change agents within the Nigerian credit market; (c) that International Financial Institutions (IFIs) like the World Bank and the International Monetary Fund (IMF) must approach this role from a social protection prism that rests on a new pro-poor 'conditionality', distinct from how conditionality has traditionally been used in Africa. The thesis is a scoping exercise that engages with the research question primarily from the perspective of formal lenders, and designed to facilitate more in-depth studies of the issues in the longer term. It is hoped that the findings of this thesis will stimulate further study. While making policy recommendations, the thesis was careful to avoid broad generalisations.",,Omede PI,,2019,,,,Ph.D. Thesis
People and Pixels: Integrating Remotely-Sensed and Household Survey Data for Food Security and Nutrition,"For several decades now, the study of environmental impacts on human well-being has been informed by what are called ""People and Pixels'' methods: the combining of remotely sensed data about environmental conditions with geolocated data from household surveys about health and nutrition. However, much of this work has been conducted at the scale of individual countries and often relies on only one or two survey waves, which creates substantial issues around spatial autocorrelation and endogeneity. Furthermore, much of this work uses simple linear regression as its analysis technique, which is limited in its ability to describe spatial variation as well as non-linearities in the relationship between the environment and human well-being. Thus, this dissertation uses several insights from the emerging field of data science to advance these methods. First, this analysis draws on large, multinational datasets from dozens of surveys, making it possible to better estimate the non-linear effects of climate extremes on human well-being as well as examine spatial heterogeneities in vulnerability. Secondly, this analysis uses techniques at the boundary between traditional econometric regression models and more complex machine learning models, such as using Generalized Additive Models (GAMs) as well as LASSO estimation. This permits the creation of spatially-varying terms as well as nonlinear effects. Applying these techniques, the dissertation has yielded several insights that could be beneficial to policymakers in governments, non-profits, and multinational organizations. The initial chapters analyze the effects of rainfall anomalies on food security and malnutrition, finding that the effect of an anomaly varies considerably depending on the local socioeconomic and environmental contexts, with low-income, poorly-governed, and arid countries, such as Somalia and Yemen, being the most vulnerable. The latter chapters look at the role of ecosystem services in improving human livelihoods, as well as how land cover is associated with dependence on local provisioning ecosystem services.",,"Cooper MW,Brown M,Sahyoun N,Silva J,Zvoleff A",,2020,,,,Ph.D. Thesis
"A Critical Examination of the Impact That the Violation of the Psychological Contract Has on Employee Intention to Quit, Commitment, Loyalty, and Employee Identification with the Organisation: A Study of Mutual Alliance Mortgage Bank, Nigeria","PURPOSE To purpose of this study was to critically examine the impact the violation of the psychological contract has on employee intention to quit, commitment, loyalty, and employee identification with the bank (Mutual Alliance Mortgage Bank, Nigeria). DESIGN/METHODOLOGY/APPROACH The research adapted both a collaborative action research methodology developed by Pellerin (2011) and a quantitative methodology utilising questionnaire. These research methods adapted, were a fit for the financial services industry. The researcher investigated Mutual Alliance Mortgage Bank, Nigeria. Two focus groups with seven employees each were set up. The focus groups acted as the Collaborative Professional Development Meetings (CPDM) to capture feedbacks and reflections as well as to develop actionable outcomes. Questionnaires were sent to all seventy employees of the bank to measure employee identification with organisational and organisational loyalty. FINDINGS The findings indicate that a positive relationship exists between psychological contract violation and loyalty in bank employees and a negative relationship exists between psychological contract violation and identification with the bank in bank employees. Also, interestingly ""Promotions"" was the most violated item of psychological contracts because employees interpreted and associated ""Promotions"" with 'immediate cash (pay) or financial gain. The results also highlighted that a positive relationship exists between intention to quit behaviour and the Nigerian culture in bank employees. The research further illustrates the moderating role of national cultural values (i.e., Nigerian culture – families/communities) on employee views towards an intention to quit behaviours. ACTIONABLE KNOWLEDGE The Bank implemented new culture-specific policies that tackled economic and socio-emotional breaches; these included the establishment of a loss of breadwinner insurance scheme; giving employee's foodstuffs at Christmas; Christmas parties for employees and families; birthday gifts to spouses of employees and monthly podcast on the bank's activities. On the issue of promotions (""pay""), the banks' payroll was redesigned to include different salary scales within each grade level to accommodate pay increases without necessarily promoting employees to the next grade level; and the introduction of bi-monthly appraisals of employees that could result in salary increment for performing staff. RESEARCH LIMITATIONS This research utilised data solely from Mutual Alliance Mortgage Bank, Nigeria thus its results cannot be generalised to other sectors or countries characterised by different contexts, cultures, and values. However, the approach utilised in this research can be applied in a wide range of situations consequently enabling the examination of its external validity.",,Amasi O,,2017,,,,Ph.D. Thesis
Exploiting Activation Sparsity for Fast CNN Inference on Mobile GPUs,"Over the past several years, the need for on-device deep learning has been rapidly increasing, and efficient CNN inference on mobile platforms has been actively researched. Sparsity exploitation has been one of the most active research themes, but the studies mostly focus on weight sparsity by weight pruning. Activation sparsity, on the contrary, requires compression at runtime for every input tensor. Hence, the research on activation sparsity mainly targets NPUs that can efficiently process this with their own hardware logic. In this paper, we observe that it is difficult to accelerate CNN inference on mobile GPUs with natural activation sparsity and that the widely used CSR-based sparse convolution is not sufficiently effective due to the compression overhead. We propose several novel sparsification methods that can boost activation sparsity without harming accuracy. In particular, we selectively sparsify some layers with an extremely high sparsity and adopt sparse convolution or dense convolution depending on the layers. Further, we present an efficient sparse convolution method without compression and demonstrate that it can be faster than the CSR implementation. With ResNet-50, we achieved 1.88 speedup compared to TFLite on a Mali-G76 GPU.",,"Oh C,So J,Kim S,Yi Y",,2021,,10.1145/3477008,https://doi-org.proxy.bnl.lu/10.1145/3477008;http://dx.doi.org/10.1145/3477008,Journal Article
Learning Adversary Behavior in Security Games: A PAC Model Perspective,"Recent applications of Stackelberg Security Games (SSG), from wildlife crime to urban crime, have employed machine learning tools to learn and predict adversary behavior using available data about defender-adversary interactions. Given these recent developments, this paper commits to an approach of directly learning the response function of the adversary. Using the PAC model, this paper lays a firm theoretical foundation for learning in SSGs and provides utility guarantees when the learned adversary model is used to plan the defender's strategy. The paper also aims to answer practical questions such as how much more data is needed to improve an adversary model's accuracy. Additionally, we explain a recently observed phenomenon that prediction accuracy of learned adversary behavior is not enough to discover the utility maximizing defender strategy. We provide four main contributions: (1) a PAC model of learning adversary response functions in SSGs; (2) PAC-model analysis of the learning of key, existing bounded rationality models in SSGs; (3) an entirely new approach to adversary modeling based on a non-parametric class of response functions with PAC-model analysis and (4) identification of conditions under which computing the best defender strategy against the learned adversary behavior is indeed the optimal strategy. Finally, we conduct experiments with real-world data from a national park in Uganda, showing the benefit of our new adversary modeling approach and verification of our PAC model predictions.",,"Sinha A,Kar D,Tambe M",,2016,214–222,,,Conference Paper
The Digital Turn in Radio: A Critique of Institutional and Organizational Modeling of New Radio Practices and Cultures,"This article conducts a critical analysis of the use of Internet and mobile phone technologies by Capital radio in Malawi. It examines the uses of the Internet, social networking sites and mobile text-messaging by the radio station. Three central questions constitute the major concerns of the article: (a) To what extent do institutional and organizational contexts shape the uptake and uses of the Internet and mobile phones by radio journalists? (b) How do the uses of the Internet and mobile phones in turn influence the institutional cultures and organizational practices? (c) To what extent, if at all, does radio convergence reconfigure traditional radio to create new spaces that augment audience participation? The article argues that any meaningful critique of the technological affordances to the radio institution must critically engage with the complex questions of the dialectical relationship between technology, structure, and agency especially given the seductive myth of the so- called new media. It concludes that digital media technologies on radio are subject to organisational, institutional, and social shaping, and that questions about the emancipatory power of these technologies especially to audiences and citizens are often exaggerated because the question of power relations between actors or interests is often overlooked. The digital turn and the demotic turn on radio therefore must not be seen as synonymous with the participatory turn, especially in African countries where the regulation of corporate power in mass media is weak and where multiple forms of the digital divide that impede on consistent and meaningful use of digital media still persist.",,Moyo L,,2013,214–222,10.1016/j.tele.2012.10.003,https://doi-org.proxy.bnl.lu/10.1016/j.tele.2012.10.003;http://dx.doi.org/10.1016/j.tele.2012.10.003,Journal Article
Private Information Retrieval,"This book deals with Private Information Retrieval (PIR), a technique allowing a user to retrieve an element from a server in possession of a database without revealing to the server which element is retrieved. PIR has been widely applied to protect the privacy of the user in querying a service provider on the Internet. For example, by PIR, one can query a location-based service provider about the nearest car park without revealing his location to the server. The first PIR approach was introduced by Chor, Goldreich, Kushilevitz and Sudan in 1995 in a multi-server setting, where the user retrieves information from multiple database servers, each of which has a copy of the same database. To ensure user privacy in the multi-server setting, the servers must be trusted not to collude. In 1997, Kushilevitz and Ostrovsky constructed the first single-database PIR. Since then, many efficient PIR solutions have been discovered. Beginning with a thorough survey of single-database PIR techniques, this text focuses on the latest technologies and applications in the field of PIR. The main categories are illustrated with recently proposed PIR-based solutions by the authors. Because of the latest treatment of the topic, this text will be highly beneficial to researchers and industry professionals in information security and privacy. Table of Contents: Preface / Acknowledgments / Classic Private Information Retrieval / FHE-Based Private Information Retrieval / Private Data Warehouse Queries / Privacy-Preserving Location-Based Queries / Discussion and Future Work / Bibliography / Authors' Biographies",,"Yi X,Paulet R,Bertino E",,2013,,,,Book
PENCIL: A Platform-Neutral Compute Intermediate Language for Accelerator Programming,"Programming accelerators such as GPUs withlow-level APIs and languages such as OpenCL and CUDAis difficult, error-prone, and not performance-portable. Au-tomatic parallelization and domain specific languages (DSLs)have been proposed to hide complexity and regain performanceportability. We present P ENCIL, a rigorously-defined subset ofGNU C99 -- enriched with additional language constructs -- that enables compilers to exploit parallelism and produce highlyoptimized code when targeting accelerators. P ENCIL aims toserve both as a portable implementation language for libraries, and as a target language for DSL compilers. We implemented a P ENCIL-to-OpenCL backend using astate-of-the-art polyhedral compiler. The polyhedral compiler, extended to handle data-dependent control flow and non-affinearray accesses, generates optimized OpenCL code. To demon-strate the potential and performance portability of P ENCILand the P ENCIL-to-OpenCL compiler, we consider a numberof image processing kernels, a set of benchmarks from theRodinia and SHOC suites, and DSL embedding scenarios forlinear algebra (BLAS) and signal processing radar applications(SpearDE), and present experimental results for four GPUplatforms: AMD Radeon HD 5670 and R9 285, NVIDIAGTX 470, and ARM Mali-T604.",,"Baghdadi R,Beaugnon U,Cohen A,Grosser T,Kruse M,Reddy C,Verdoolaege S,Betts A,Donaldson AF,Ketema J,Absar J,van Haastregt S,Kravets A,Lokhmotov A,David R,Hajiyev E",,2015,138–149,10.1109/PACT.2015.17,https://doi-org.proxy.bnl.lu/10.1109/PACT.2015.17;http://dx.doi.org/10.1109/PACT.2015.17,Conference Paper
Challenges of Housing Finance in Nigeria: The Federal Mortgage Bank of Nigeria (FMBN) Experience,"With a population of 197 million inhabitants, Nigeria is the most populous nation in Africa. The contribution that housing makes to the socio-economic development of an economy is significant. Despite this assertion, there exists a housing deficit of 17 million in the country. The gap between demand and supply for housing is wide owing to several challenges encountered in providing mortgage to low and middle-income earners that form the majority of prospective beneficiaries of the home ownership scheme initiated by Federal Mortgage Bank of Nigeria (FMBN). This study examined the challenges encountered in the Nigerian housing industry by determining the affordability of individuals in terms of access to the National Housing Fund (NHF) loan and further explored a cost-effective means to providing affordable houses to low and middle-income earners. In addition, the work made effort towards determining the effectiveness of the strategies presently used by the bank (FMBN) in creating mortgages and its existing model for housing finance in Nigeria. The need for this research developed because of the high level of housing deficit in the country and the insufficient attention given to empirical research on housing finance in Nigeria. This research used qualitative study approach by conducting eleven semi-structured interviews with key officials and Chief Executive Officers within the Nigerian housing industry supported with archival records of the housing industry in the country to advance ways by which the lending bank can operate efficiently. Results of the study revealed that key challenges facing the industry were classified into three broad themes namely; financial, procedural and structural challenges. The financial challenges highlighted the constraints (low capitalisation) encountered by FMBN in successfully delivering quality and affordable houses to low and middle-income earners. The procedural challenges emphasise the difficulties (stringent processes) that NHF applicants experience in the process of obtaining mortgage loans. While the structural challenges describe the problems associated with the quality and the standard of the buildings provided by estate developers. Another key finding is the high level of political interference in the housing sector which had greatly impacted on the nation's housing delivery. In addition, the institutional structure of FMBN is also flawed with loopholes such as the lack of Information Technology (IT) integration, lack of involvement of major stakeholders in the executive management of the Bank and a need to review the existing regulatory framework. To meet the housing delivery target set by the government, it is recommended that reform of the Housing Act and restructuring the housing industry will help in the needed redress within the industry.",,Barhama BI,,2019,,,,Ph.D. Thesis
"Relating Violence to MODIS Fire Detections in Darfur, Sudan","The Moderate Resolution Imaging Spectroradiometer (MODIS) Thermal Anomalies and Fire Detection data product can provide an input into early warning and rapid response efforts of interest to human rights, humanitarian and security communities. A review of fire detection data during a period of intense violence in Darfur, Sudan, marked by burning campaigns targeted at many settlements in the region, was conducted to test which, if any, of these fires were detected by MODIS. The results indicate that a significant portion of the fires related to violence were detected, and a relationship was found between increased fire detections and reported violence. However, underlying difficulties in the anecdotal reporting introduce uncertainties into the results. While not universally applicable, calculating increases in fire detections on a daily, global basis can possibly provide objective, satellite-based information on certain violent conflicts impacting civilian populations.",,Bromley L,,2010,2277–2292,,,Journal Article
Beyond the Baseline: Establishing the Value in Mobile Phone Based Poverty Estimates,"Within the remit of `Data for Development' there have been a number of promising recent works that investigate the use of mobile phone Call Detail Records (CDRs) to estimate the spatial distribution of poverty or socio-economic status. The methods being developed have the potential to offer immense value to organisations and agencies who currently struggle to identify the poorest parts of a country, due to the lack of reliable and up to date survey data in certain parts of the world. However, the results of this research have thus far only been presented in isolation rather than in comparison to any alternative approach or benchmark. Consequently, the true practical value of these methods remains unknown. Here, we seek to allay this shortcoming, by proposing two baseline poverty estimators grounded on concrete usage scenarios: one that exploits correlation with population density only, to be used when no poverty data exists at all; and one that also exploits spatial autocorrelation, to be used when poverty data has been collected for a few regions within a country. We then compare the predictive performance of these baseline models with models that also include features derived from CDRs, so to establish their real added value. We present extensive analysis of the performance of all these models on data acquired for two developing countries -- Senegal and Ivory Coast. Our results reveal that CDR-based models do provide more accurate estimates in most cases; however, the improvement is modest and more significant when estimating (extreme) poverty intensity rates rather than mean wealth.",,"Smith-Clarke C,Capra L",,2016,425–434,10.1145/2872427.2883076,https://doi-org.proxy.bnl.lu/10.1145/2872427.2883076;http://dx.doi.org/10.1145/2872427.2883076,Conference Paper
"Relating Violence to MODIS Fire Detections in Darfur, Sudan","The Moderate Resolution Imaging Spectroradiometer (MODIS) Thermal Anomalies and Fire Detection data product can provide an input into early warning and rapid response efforts of interest to human rights, humanitarian and security communities. A review of fire detection data during a period of intense violence in Darfur, Sudan, marked by burning campaigns targeted at many settlements in the region, was conducted to test which, if any, of these fires were detected by MODIS. The results indicate that a significant portion of the fires related to violence were detected, and a relationship was found between increased fire detections and reported violence. However, underlying difficulties in the anecdotal reporting introduce uncertainties into the results. While not universally applicable, calculating increases in fire detections on a daily, global basis can possibly provide objective, satellite-based information on certain violent conflicts impacting civilian populations.",,Bromley L,,2010,2277–2292,,,Journal Article
"Decrease the Number of Patients Lost to Follow-up in the Monitoring of PLHIV in Cross-Border Areas between The Gambia, Senegal and Guinea Bissau","The fight against AIDS in West Africa is a big challenge of public health. The main difficulties are related to the availability of antiretrovirals (AVR) and mainly to the observance of the treatment. In the cross-borderland areas of the Gambia, Senegal and Guinea Bissau, these difficulties of the observance of the treatment are amplified by the vulnerability, the high mobility of the populations and the problems of communication between the actors of the monitoring of PLHIV who speak three different languages. That's what makes difficult the fight against AIDS in those areas and justify the phenomenon of the patients lost to follow-up, which is a great indicator for the following of the People Living with HIV-virus (PLHIV). This paper presents the decrease of the number of patients lost to follow-up in the monitoring of the PLHIV in the cross-borderland areas of the Gambia, Senegal and Guinea Bissau by a multilingual semantic web platform of reference, counter reference and auto reference.",,"Diop I,Dieng Y,Faye Y,Malack CA,Cisse O,Diouf B",,2019,,10.1145/3361570.3361596,https://doi-org.proxy.bnl.lu/10.1145/3361570.3361596;http://dx.doi.org/10.1145/3361570.3361596,Conference Paper
"Relating Violence to MODIS Fire Detections in Darfur, Sudan","The Moderate Resolution Imaging Spectroradiometer MODIS Thermal Anomalies and Fire Detection data product can provide an input into early warning and rapid response efforts of interest to human rights, humanitarian and security communities. A review of fire detection data during a period of intense violence in Darfur, Sudan, marked by burning campaigns targeted at many settlements in the region, was conducted to test which, if any, of these fires were detected by MODIS. The results indicate that a significant portion of the fires related to violence were detected, and a relationship was found between increased fire detections and reported violence. However, underlying difficulties in the anecdotal reporting introduce uncertainties into the results. While not universally applicable, calculating increases in fire detections on a daily, global basis can possibly provide objective, satellite-based information on certain violent conflicts impacting civilian populations.",,Bromley L,,2010,2277–2292,,,Journal Article
Empirical Study of Barriers to Electronic Commerce Uptake by SMEs in Developing Economies,"Electronic commerce E-commerce is a technological innovation that enables small to medium enterprises SMEs to compete on the same level with their larger counterparts. And it has the potential to improve efficiency and productivity in many areas and, therefore, has received significant attention in many countries of the world. A thorough analysis of the impact of the internet and e-commerce across firms, industries and economies is necessary to separate hype from reality. However, several researchers have called for the investigation of the association between the perceptions of e-commerce and the barriers to its adoption in developing countries. It is however on record that SMEs the world over are faced with significant challenges that compromise their ability to function and to contribute optimally to the respective economies where they operate. This study was conducted in three states of Nigeria Lagos, Abuja and Enugu states with the use of interviews to gather relevant data; the aim of which was to understand the challenges which serve as barriers to E-Commerce adoption by small and medium scale enterprises in the Nigerian context. Findings indicates that small and medium scale online present is at best unknown. The most common e-Commerce applications used by most SMEs include but not limited to the use of e-mails for communication purposes and a simple website for basic product information-information contained are usually outdated as most of these websites are hardly updated. Findings revealed, among others, that lack of and total absence of a regulatory framework on e-Commerce security, as well as technical skills, and basic infrastructures are some of the barriers to electronic commerce adoption. The findings however, provide a constructive insight to financial practitioners, governments as well as other stakeholders on the need to give e-commerce a place in all aspects of e-commerce activities.",,"Agwu EM,Murray PJ",,2015,1–19,,,Journal Article
Infrastructure Resilience for Climate Adaptation,"Developing and maintaining resilient transportation infrastructure is a key strategy for meeting several UN sustainable development goals in the face of climate change-driven extreme flooding events. We present a framework for performing data-driven vulnerability analysis for flooding on existing transportation networks, and use this analysis to inform decision-making about investments for climate adaptation. We apply this approach to study the potential impacts of severe flooding on regional mobility in Senegal, using a combination of flood hazard maps and a travel demand model based on call detail record data. We use the estimated number of infeasible trips as a direct measure of flooding-induced mobility impacts, as well as an objective for minimizing these impacts. We then compare three alternative road network upgrade strategies to assess the extent to which each strategy would preserve network functionality under a given flooding scenario. We illustrate that strategies driven solely by travel demand can lead to underinvestment in roads that are at risk of flooding, while solely focusing on repairing flooded road segments neglects the criticality of those repairs to mobility. For example, in a 100 year flooding scenario with a fixed budget, our strategy that considers both flooding and mobility data can achieve a 53% reduction in the number of infeasible trips, while a strategy that just considers flooding data achieves only a 38% reduction for the same cost. Our framework can be applied more broadly to integrate information from a variety of sources about climate hazards and potential human impacts to make better informed decisions about investments in critical infrastructure systems.",,"Gupta A,Robinson C,Dilkina B",,2018,,10.1145/3209811.3209859,https://doi-org.proxy.bnl.lu/10.1145/3209811.3209859;http://dx.doi.org/10.1145/3209811.3209859,Conference Paper
Empirical Study of Barriers to Electronic Commerce Adoption by Small and Medium Scale Businesses in Nigeria,"Electronic commerce (E-commerce) is a technological innovation that enables small to medium enterprises (SMEs) to compete on the same level with their larger counterparts. And it has the potential to improve efficiency and productivity in many areas and, therefore, has received significant attention in many countries of the world. A thorough analysis of the impact of the internet and e-commerce across firms, industries and economies is necessary to separate hype from reality. However, several researchers have called for the investigation of the association between the perceptions of e-commerce and the barriers to its adoption in developing countries. It is however on record that SMEs the world over are faced with significant challenges that compromise their ability to function and to contribute optimally to the respective economies where they operate. This study was conducted in three states of Nigeria (Lagos, Abuja and Enugu states) with the use of interviews to gather relevant data; the aim of which was to understand the challenges which serve as barriers to E-Commerce adoption by small and medium scale enterprises in the Nigerian context. Findings indicates that small and medium scale online present is at best unknown. The most common e-Commerce applications used by most SMEs include but not limited to the use of e-mails for communication purposes and a simple website for basic product information — information contained are usually outdated as most of these websites are hardly updated. Findings revealed, among others, that lack of and total absence of a regulatory framework on e-Commerce security, as well as technical skills, and basic infrastructures are some of the barriers to electronic commerce adoption. The findings however, provide a constructive insight to financial practitioners, governments as well as other stakeholders on the need to give e-commerce a place in all aspects of e-commerce activities.",,"Agwu EM,Murray PJ",,2015,1–19,,,Journal Article
A Framework for Monitoring Movements of Pandemic Disease Patients Based on GPS Trajectory Datasets,"The rapid spread of contagious diseases poses a colossal threat to human existence. Presently, the emergence of coronavirus COVID-19 which has rightly been declared a global pandemic resulting in so many deaths, confusion as well as huge economic losses is a challenge. It has been suggested by the World Health Organization (WHO) in conjunction with different Government authorities of the world and non-governmental organizations, that efforts to curtail the COVID-19 pandemic should rely principally on measures such as social distancing, identification of infected persons, tracing of possible contacts as well as effective isolation of such person(s) for subsequent medical treatment. The aim of this study is to provide a framework for monitoring Movements of Pandemic Disease Patients and predicting their next geographical locations given the recent trend of infected COVID-19 patients absconding from isolation centres as evidenced in the Nigerian case. The methodology for this study, proposes a system architecture incorporating GPS (Global Positioning System) and Assisted-GPS technologies for monitoring the geographical movements of COVID-19 patients and recording of their movement Trajectory Datasets on the assumption that they are assigned with GPS-enabled devices such as smartphones. Accordingly, fifteen (15) participants (patients) were selected for this study based on the criteria of residency and business activity location. The ensuing participants movements generated 157, 218 Trajectory datasets during a period of 3 weeks. With this dataset, mining of the movement trace, Stay Points (hot spots), relationships, and the prediction of the next probable geographical location of a COVID-19 patient was realized by the application of Artificial Intelligence (AI) and Data Mining techniques such as supervised Machine Learning (ML) algorithms (i.e., Multiple Linear Regression (MLR), k-Nearest Neighbor (kNN), Decision Tree Regression (DTR), Random Forest Regression (RFR), Gradient Boosting Regression (GBR), and eXtreme Gradient Boosting regression(XGBR) as well as density-based clustering methods (i.e., DBSCAN) for the computation of Stay Points (hot spots) of COVID-19 patient. The result of this study showed clearly that it is possible to determine the Stay Points (hot spots) of a COVID-19 patient. In addition, this study demonstrated the possibility of predicting the next probable geographical location of a COVID-19 patient. Correspondingly, Six Machine Learning models (i.e., MLR, kNN, DTR, RFR, GBR, and XGBR) were compared for efficiency, in determining the next probable location of a COVID-19 patient. The result showed that the DTR model performed better compared to other models (i.e., MLR, kNN, RFR, GBR, XGBR) based on four evaluation matrices (i.e., ACCURACY, MAE, MSE, and R2) used. It is recommended that less developed Countries consider adopting this framework as a policy initiative for implementation at this burgeoning phase of COVID-19 infection and beyond. The same applies to the developed Countries. There is indication that GPS Trajectory dataset and Machine Learning algorithms as applied in this paper, appear to possess the potential of performing optimally in a real-life situation of monitoring a COVID-19 patient. This paper is unique given its ability to predict the next probable location of a COVID-19 patient. In the review of extant literature, prediction of the next probable location of a COVID-19 patient was not in evidence using the same Machine Learning algorithms.",,"Ugwoke PO,Bakpo FS,Udanor CN,Okoronkwo MC",,2022,1–28,10.1007/s11276-021-02819-4,https://doi-org.proxy.bnl.lu/10.1007/s11276-021-02819-4;http://dx.doi.org/10.1007/s11276-021-02819-4,Journal Article
Evaluating Digital Economy in the Covid-19 Pandemic Era: A Review,"The development of the digital economy in Indonesia is increasing, including Micro, Small, and Medium Enterprises (MSMEs). The digital platform is a solution for MSMEs under threat due to the impact of the Covid-19 pandemic. Indonesia's government collaboration with private sectors has initiated some efforts to recover the national economy. These efforts included developing digital platforms such as e-commerce platforms, digital wallets, and various applications that support the distribution of goods and services that business actors have widely used. The digital platforms that MSMEs have widely used need to be further evaluated for their impact on business processes and economic recovery. This paper performed a bibliometric analysis and delivered a narrative review on developing a digital platform that can be used by MSMEs. The narrative reviews reveal some findings: 1) the digital platforms can used by MSME players, and 2) the method's effectiveness in evaluating the social impact of implementing digital platforms. Furthermore, we pointed the highlight of the digital platform for MSMEs development in Indonesia. In comparison, we also exemplified the situation in Nigeria as a benchmark. As a result, this study proposes a method that can be adopted to evaluate the social impact of implementing digital platforms by MSME players. In the next stage, we consider evaluating the social impact of digital platforms implementation by MSME players.",,"Kushadiani SK,Nugroho B,Mardian S,Muhammad-Bello B,Hermawan A",,2021,80–85,10.1145/3479645.3479653,https://doi-org.proxy.bnl.lu/10.1145/3479645.3479653;http://dx.doi.org/10.1145/3479645.3479653,Conference Paper
Information Technology in Nigerian Banks: The Limits of Expectations,"In the last ten years, banks in developed countries have been investing more and more in information technology (IT) as a means to reduce costs and improve operational efficiency. An investigation of the application of IT in Nigerian banks was carried out in order to determine the expectations and success of IT implementations in the sector. The data were generated from a survey of randomly selected branches of 56 banks in Lagos, the commercial capital of Nigeria. Almost all the banks had an IT policy, the main thrusts of which where to achieve full application of IT, to be able to meet organisational goals, to secure competitive advantage, and to be up to date. Only 54.6% of them actually achieved some measure of successful implementations. The expected benefits of investment in IT were realised in only a relatively few number of banks. The consequence was that less than 40% of the banks were poised to maximise the benefits of IT through major investments, especially in the areas of online access and transactions, electronic commerce, and electronic publishing. It is estimated that at least 60% of the branches of these banks are spending less than $150,000 annually on IT. An upsurge of investment is, however, expected, first by the banks that style themselves as progressive and have already made some success in IT implementations, and later by the other banks.",,Ehikhamenor FA,,2003,13–24,10.1002/itdj.1590100103,https://doi-org.proxy.bnl.lu/10.1002/itdj.1590100103;http://dx.doi.org/10.1002/itdj.1590100103,Journal Article
Stock Market Development and Economic Growth in Nigeria : A Time Series Study for the Period 1980-2007,"This research empirically examines the relationship between stock market development and economic growth in the context of Nigeria. The question guiding this study is focused on whether the development of the stock market has had an impact on economic growth in Nigeria. The thesis examines the long run causal relationship between the stock market and economic growth. It uses one bank and three measures of stock market development: the loans to deposit ratio of banks, Market capitalisation ratio, value traded to market capitalisation ratio as well as value traded to GDP ratio. Essentially the study uses the endogenous growth theory as a basis of its theoretical foundation. The study exploits time series analysis techniques to test for the existence of a relationship and, where one is found to exist, the casual nature of that relationship. The study particularly applies Multivariate vector autoregressive models (VAR) and Vector Error Correction Models (VECM) in testing for the existence of a relationship. The evidence obtained from the study shows the existence of co-integration between the stock market development and economic growth in the short as well as the long run. This suggests that stock market development has impacted on economic growth in Nigeria. The Granger causality test findings indicate the presence of a bi-directional relationship between stock market development and economic growth. The findings of the study support the view that stock market development and economic growth in Nigeria are complementary and any improvements in the stock market would have a positive impact on economic growth in Nigeria. The findings also support the hypothesis of endogenous growth models that financial development causes higher economic growth. The contribution of this study lies in the fact that it provides additional evidence on the ongoing debate of the impact stock markets on the economic growth process within a specific country.",,Chizea J,,2012,,,,Ph.D. Thesis
Metaphors and Narratives in Exile : Understanding the Experiences of Forced Migrants in Britain,"The research aimed to reach an understanding of the experience of exile by investigating aspects of life in exile and how refugees portray their own experiences of asylum and define what asylum has meant to them in the process of living in and integrating their host society. The study, focusing on 30 refugees from Congo, Kosovo and Somalia, has sought to examine some of the metaphors that refugees associated with the experience of exile and the narration of these experiences in their own words. The search for meaning has dominated the research and throughout the analysis metaphors formulated by the refugees have been used to elucidate the argument. Some key findings have revealed the following: - There are contradictions in what asylum means to different refugees. The perception of exile ranges from 'hell' to 'heaven' and from 'safe haven' to further 'trauma'. - The process to re-socialisation is not straightforward and many different factors, in isolation or in combination, deeply affect the process. These range from the degree of coercion leading to exile, racialisation in the host country to the multiplicity and types of networks available to the exile in the host country and the degree to which the exile exploits them successfully. - The perception of home is difficult to agree. For people torn apart such as the Somali, Congolese and Kosovan refugees studied, home is neither here or there (i. e. neither in the host country or the country of origin); home is nowhere because events in the home country have disconnected them from feeling a sense of belonging and often isolation and racialisation in the host country comes short of enabling them to connect psychologically, socially and culturally with the new place. The metaphors used by the refugees carry a sense of nostalgia for the past and the lost land for many refugees. The nostalgia and loss encompasses the deficit in social status and mobility, the diminishing cultural identity including language as well as mere familiarity with the environment. But the narratives, with all their emotional contents, carry both a sense of hope and despair. The hope resided in the forecasting of better days in the native country which might trigger the refugees' return to their ""natural waters"" as a respondent put it metaphorically to refer to the original socio-cultural milieu the refugees originated from. For the stayers, hope was expressed in their seeing themselves finding 'a place under the sun' in the land of their exile. However, at the same time, the stagnation or worsening of the situation in the native land and often combined with the feeling of or categorisation as outsiders and others in the host country brought a sense of despair. The study has enabled the drawing of the conclusion that life in exile can often be ambiguity, uncertainty, loss but could also be new light, salvation and opportunity. This dialectics seems to be at the heart and the essence of exile, so far as the humans involved are both psychologically and social beings.",,Hack-Polay DD,,2006,,,,Ph.D. Thesis
Cloudy with a Chance of Poaching: Adversary Behavior Modeling and Forecasting with Real-World Poaching Data,"Wildlife conservation organizations task rangers to deter and capture wildlife poachers. Since rangers are responsible for patrolling vast areas, adversary behavior modeling can help more effectively direct future patrols. In this innovative application track paper, we present an adversary behavior modeling system, INTERCEPT (INTERpretable Classification Ensemble to Protect Threatened species), and provide the most extensive evaluation in the AI literature of one of the largest poaching datasets from Queen Elizabeth National Park (QENP) in Uganda, comparing INTERCEPT with its competitors; we also present results from a month-long test of INTERCEPT in the field. We present three major contributions. First, we present a paradigm shift in modeling and forecasting wildlife poacher behavior. Some of the latest work in the AI literature (and in Conservation) has relied on models similar to the Quantal Response model from Behavioral Game Theory for poacher behavior prediction. In contrast, INTERCEPT presents a behavior model based on an ensemble of decision trees (i) that more effectively predicts poacher attacks and (ii) that is more effectively interpretable and verifiable. We augment this model to account for spatial correlations and construct an ensemble of the best models, significantly improving performance. Second, we conduct an extensive evaluation on the QENP dataset, comparing 41 models in prediction performance over two years. Third, we present the results of deploying INTERCEPT for a one-month field test in QENP - a first for adversary behavior modeling applications in this domain. This field test has led to finding a poached elephant and more than a dozen snares (including a roll of elephant snares) before they were deployed, potentially saving the lives of multiple animals - including elephants.",,"Kar D,Ford B,Gholami S,Fang F,Plumptre A,Tambe M,Driciru M,Wanyama F,Rwetsiba A,Nsubaga M,Mabonga J",,2017,159–167,,,Conference Paper
Limits on the Rate of Locally Testable Affine-Invariant Codes,"Despite its many applications, to program checking, probabilistically checkable proofs, locally testable and locally decodable codes, and cryptography, ""algebraic property testing"" is not well-understood. A significant obstacle to a better understanding, was a lack of a concrete definition that abstracted known testable algebraic properties and reflected their testability. This obstacle was removed by [Kaufman and Sudan, STOC 2008] who considered (linear) ""affine-invariant properties"", i.e., properties that are closed under summation, and under affine transformations of the domain. Kaufman and Sudan showed that these two features (linearity of the property and its affine-invariance) play a central role in the testability of many known algebraic properties. However their work does not give a complete characterization of the testability of affine-invariant properties, and several technical obstacles need to be overcome to obtain such a characterization. Indeed, their work left open the tantalizing possibility that locally testable codes of rate dramatically better than that of the family of Reed-Muller codes (the most popular form of locally testable codes, which also happen to be affine-invariant) could be found by systematically exploring the space of affine-invariant properties.In this work we rule out this possibility and show that general (linear) affine-invariant properties are contained in Reed-Muller codes that are testable with a slightly larger query complexity. A central impediment to proving such results was the limited understanding of the structural restrictions on affine-invariant properties imposed by the existence of local tests. We manage to overcome this limitation and present a clean restriction satisfied by affine-invariant properties that exhibit local tests. We do so by relating the problem to that of studying the set of solutions of a certain nice class of (uniform, homogenous, diagonal) systems of multivariate polynomial equations. Our main technical result completely characterizes (combinatorially) the set of zeroes, or algebraic set, of such systems.",,"Ben-Sasson E,Sudan M",,2011,412–423,,,Conference Paper
"Sociocultural Barriers to Breast and Cervical Cancer Screening Among Women in Kaduna, Northern Nigeria","Breast and cervical cancer account for the highest cancer-related morbidity and mortality among Nigerian women. Late stage diagnoses are common due to lack of National screening programme, although opportunistic screening occurs in some States. Research in Northern Nigeria has predominantly investigated demographic factors influencing breast and cervical cancer screening behaviours, however few studies have assessed the effects of social and cultural factors. This thesis aimed to examine and gain understanding of Kaduna women's and healthcare professionals' perceptions, knowledge, attitude, and practice towards breast and cervical cancer, and screening behaviours. It also aimed to assess sociodemographic and sociocultural factors that may influence screening and develop strategies towards improving services and uptake. A sequential explanatory Mixed Method Methodology was applied, following a critical review of health behaviours, screening uptake, and intervention approaches. First, quantitative surveys investigated cancer related knowledge, attitude, perception, and factors influencing screening behaviours among 250 adult women. Next, qualitative interviews with 12 women, and 6 healthcare professionals further assessed the phenomena. The moderate sample size (250) used due to security unrest in Kaduna during the study, limited generalisability of the findings. Quantitative results revealed gap in cancer knowledge, and low screening uptake. Having professional jobs, tertiary education, and being married were associated with better awareness, positive attitude and perception of cancer and screening. Healthcare professionals stated that low-risk perception, lack of services and trained providers hindered screening uptake. Women expressed that sociocultural norms and beliefs including fatalism, need for support from husbands, and gender of health providers influenced their screening behaviours. A model to guide development and implementation of screening intervention strategies was produced from the findings. This emphasised the need for multi-organisation collaborations, and development of culturally and socially sensitive programmes. Accessible, affordable, and equipped facilities with trained health workers could improve uptake, and subsequently reduce cancer related morbidity and mortality.",,Dodo A,,2018,,,,Ph.D. Thesis
"Understanding Factors That Increase Citizens’ Participation in Community Development Projects in Lagos, Nigeria","Despite billions of dollars spent and hundreds of studies conducted, there seems to have been progress made in meeting the housing and infrastructures shortages in Less Developed Countries (LDCs) (Akinyemi, 1988; Scott, 1998; Easterly, 2006). Theoretical constructs such as culture, geography and institutions (Huntington, 2000; North et al, 2009; Vanessa Watson, 2002, 2013; Acemoglu & Robinson, 2012) have not been able to explain this deficit. Few studies have provided empirical evidence of factors that encourage citizens of LDCs to participate in community development projects. The objective of this study is therefore to identify such key factors and use them to develop policy recommendations that, when applied would give citizens of marginalized communities more autonomy in the design and implementation of fiscally and environmentally sustainable community redevelopment projects. Using an explanatory sequential mixed methods research design—beginning with a quantitative survey of 1,202 residents of Lagos, Nigeria and followed by an in-depth qualitative interview of a subset of these residents and other stakeholders—this study has determined which of eleven independent variables significantly affect the willingness of residents to participate in self-help community development projects. The quantitative study found that when a citizen group is in-charge (hereby termed ""The New Deal""), the following factors are positively correlated with willingness to participate in self-help community development projects: age, home ownership, their home's physical condition, and the inclusion of their homes, drainage systems and road ways in the work being done. The number of years of residence is negatively correlated with willingness to participate. Education level shows no association to willingness to participate in this procurement modality. The study further found that the aforementioned willingnesses to participate is increased when community elders express support for, or corporations donate money to, the project. The level of participation is unchanged when the government encourage citizens to participate. When the government is in-charge (hereby referred to as ""The Status Quo""), the result is similar (though less eagerly, see Table 5.4) for most of the explanatory variables, except education is now negatively correlated; road improvement and age now show no association to willingness to participate. The subsequent qualitative study offered further explanations in support of the above findings, as well as clarification of anomalous findings of the quantitative study. Grounded theory application in the qualitative study found water, electricity, and security provisions as additional factors that affect residents' willingness to participate. The overarching conclusion of this research is that while residents are willing (to varying degrees) to participate in community redevelopment projects in all three procurement modalities examined (i.e. Citizen Group, Government, or NGO in-charge), they are more eager to do so when they are given significant control over the management and ownership of the project (i.e. Citizen Group in-charge). This conclusion and associated findings helped develop a conceptual framework for a transformational approach to community development in LDCs. The two key tenents of this conceptual policy framework are: a) A collaborative planning approach with pivotal roles for community elders; and b) Alternative funding sources that include leveraging residents' buying power. These tenents formed a new paradigm for procuring community development projects nicknamed ""The New Deal Paradigm."" Successful implementation of this study's policy recommendations would empower communities in LDCs to self-initiate, self-fund, and self-implement holistic redevelopment of their homes and surroundings, thereby creating jobs, improving health, and in the process, improving the trajectory of individual lives.",,"Akinyemi SA,Brower R,Coutts C,Felkner J",,2020,,,,Ph.D. Thesis
Proving Sustainability: The International Development Monitoring Initative,"Nearly a billion people in the world lack access to safe drinking water, two billion have inadequate sanitation facilities, three billion use biomass for their daily energy needs and nearly half the world's population live in rural isolation, lacking access to the most basic human services. Combined, these limitations are a leading cause of the perpetuating cycle of poverty and political insecurity. Meanwhile, the majority of international development agencies are responsible for self-reporting project outcomes. At best, expert spot-checks are conducted in the field occasionally. These results tend to show individual project success, while meta-surveys indicate on-going challenges in the sector. This disconnect may be addressed through independent data monitoring technologies that provide objective data on system performance and use and can be used to demonstrate success and identify project weaknesses. By demonstrating which technologies and programs are truly successful, these successes can be targeted for scaling, through savings realized by eliminating unsuccessful approaches. This will benefit developing communities by providing proven and accountable programs. The Sustainable Water, Energy and Environmental Technologies Laboratory, the SWEETLab , at Portland State University is working with partners to demonstrate this concept across several applications and countries. The SWEETSense technology can provide objective, qualitative and continuous operational data on the usage and performance of programs across a range of sectors and communities. The data is then directly integrated into SWEETData , an internet database presenting summary statistics on performance and usage of the monitored technologies to front-end users. The SWEETLab is currently demonstrating this concept in water, sanitation, household energy and rural infrastructure programs with diverse partners including Mercy Corps, the Lemelson Foundation, Bridges to Prosperity, Manna Energy Limited and Vestergaard Frandsen, in several countries including Indonesia, Haiti, Guatemala and Rwanda. Remote monitoring systems are an innovative method to ensure the success of appropriate technology projects. Rather than infrequent engagement, remote monitoring systems ensure that community partnerships are maintained through continuous monitoring. This approach seeks to raise the quality and accountability of these projects internationally.",,"Thomas E,Zumr Z,Barstow C,Linden K",,2011,164–170,10.1109/GHTC.2011.74,https://doi-org.proxy.bnl.lu/10.1109/GHTC.2011.74;http://dx.doi.org/10.1109/GHTC.2011.74,Conference Paper
"Price Fairness, Service Recovery and Customer Satisfaction in the Mobile Telecommunication Industry in Nigeria","The life wire and the bedrock of any business is customer satisfaction. Evidently, the available market is one major considerations for the success of the telecommunication industry in Nigeria. Companies providing telecommunication services have been striving to seek effective pricing strategies and subscribers' perceptions of price fairness. In addition, the rate at which services fail and are subsequently recovered in the Nigerian mobile telecommunication sector is hugely disturbing. The need to understand the relationship between subscribers' perceived price fairness and service recovery on customer satisfaction have necessitated investigation into this study. Four objectives were put forward for consideration and were basically translated into corresponding hypotheses in order to provide answers to the research questions. In order to achieve these objectives, a cross-sectional survey method was adopted using the descriptive research design. A well-structured questionnaire was used also as data collection instrument. The entire subscribers of the four active telecommunication network providers in the five divisions of Lagos State were taken as the study population. Multi-stage sampling technique was employed to draw the sample size of 610 from the target population. The data collected were subjected to analysis by percentages, mean score, standard deviation and frequency tables. The stated hypotheses were tested using the Pearson's Product Moment Correlation, Simple Regression and Multiple Regression analyses, with the aid of the Statistical Package for Social Sciences (SPSS) software version 20.0. The findings revealed that perceived price fairness is significantly related to customer's satisfaction (R = 0.451, P =0.000 < 0.05) and perceived price fairness and service recovery when combined together, have significant effect on customer's satisfaction with multiple R (0.549). It is recommended that Government, while playing its regulatory role, should enact laws to protect subscribers from being exploited through abnormal pricing or provision of poor service.",,Omoera CI,,2017,,,,Ph.D. Thesis
Disrespect and Abuse in Maternity Care : Women's Experiences and Healthcare Providers' Perspectives in Nigeria,"Background: Disrespect and abuse (D&A) in maternity care facilities is a major public health issue affecting women worldwide. There are reports of high prevalence of D&A during facility-based childbirth in Nigeria; however, studies that explore the issue in-depth from women's and maternity care professionals' perspectives are sparse. This study provides an understanding of women's experiences of and healthcare providers' perspectives on D&A during maternity care in health facilities in Benue State, Nigeria. Methods: This is a qualitative phenomenological study conducted in two phases. Using purposive sampling, focus group discussions were conducted with women (n=32) in the first phase, and in the second, semi-structured interviews with women (n=14) and healthcare providers (n=16) from various professional backgrounds, working in two maternity care facilities. All the women received maternity care in facilities and had experienced at least one incident of D&A. The sample size was determined based on data saturation. All data collected were transcribed and analysed in NVivo version 11 using a six-stage thematic analysis. Findings: Women perceived incidents such as being shouted at and the use of abusive language as a common practice and described these incidents as devaluing and dehumanising to their sense of dignity. They also highlighted the importance of accessing facilities for safe childbirth and expressed that the experiences of D&A may not impact negatively on their intended use of but the choice of maternity facilities. However, their accounts reflected a lack of choice and the adverse effect of D&A on their emotional wellbeing. Healthcare providers recognised D&A as a violation of the human rights of women accessing maternity facilities, but usually highlighted components of respectful care with a good awareness about what it encompasses. They often considered the experiences of D&A as subjective to the women and based on their expectations of care. Their views also reflected underlying gender-related notions and societal perceptions of women being considered weaker than men. The professionals recognised several adverse effects of D&A, including its impact on women, newborns and on their own job satisfaction. Both women and professionals perceived some of the disrespectful and abusive actions were not intended to cause harm but to ensure the health and safety of the mother and her child. Additionally, they reported several factors associated with service users, health professionals and the facilities perceived to contribute to the D&A of women. Conclusion: The participants' accounts showed that the application of respectful care in everyday maternity practice is inadequate in Nigeria. The findings reveal the need for policy and practice interventions to address the issue urgently through preventive measures such as empowering women through education to reinforce their right to respectful care. It reflects the need for sensitising and training health professionals on the importance of providing respectful care and how its elements can be incorporated into everyday practice. The underlying gender-related notion highlights the need for interventions at wider socio-political and community levels including the importance of educating family members about their right to respectful care and empowering them to report disrespectful practices.",,Orpin J,,2019,,,,Ph.D. Thesis
Essays on Resilience Measurement,"Robust measurement is key to the design and targeting of resilience-building interventions. Yet, conventional approaches to resilience measurement are often ill-suited to the needs of development and humanitarian stakeholders, proving costly, timeconsuming and difficult to coordinate. In this thesis I explore the use, validity and viability of an alternative suite of approaches: subjective measures of resilience. I start by clarifying the conceptual distinctions between subjectivity and objectivity as they relate to resilience measurement, before introducing a continuum that highlights the strengths and weaknesses of different types of approaches. I then develop a new perception-based measure, coined the Subjectively self-Evaluated Resilience Score (SERS). Using a large household survey in Northern Uganda, I provide like-for-like comparisons between SERS and a conventional objective approach to resilience measurement. While I show that the two measures are moderately correlated, they differ notably in associations with key socio-economic traits. In order to further probe the validity of subjective measures, I examine whether SERS is sensitive to external shocks. Using mobile phones to conduct remote interviews I assemble a novel high-frequency panel survey on resilience. Here I reveal how perceived levels of resilience fluctuate in the aftermath of seasonal flooding in Eastern Myanmar: dropping sharply in the first few months, before slowly converging over the course of a year. I also compare the impact of flood exposure across different socio-economic groups, revealing how female-headed households are hardest hit. Lastly, using the same site in Myanmar, I look more closely at the temporal dynamics of resilience. Insights from an extended panel provide quantitative evidence of intra-annual variation in levels of resilience. Here I find consistent non-linear associations between subjectively-evaluated scores and changes in seasonality and weather. Findings also point to potential resilience thresholds and tipping points. Weighed together, these results: challenge core assumptions in the resilience literature; highlight the potential of subjective measures; and point to the need for greater diversity of resilience evidence.",,Jones L,,2020,,,,Ph.D. Thesis
Quantifying the Strength and Durability of Induced Immunity to HIV Infection in Women Engaging in Unprotected Sexual Contacts with Infected Men,"Evidence is accumulating that exposure to human immunodeficiency virus (HIV) can lead to an increased resistance or immunity to subsequent infection. A multirisk model that permits either induced immunity or infection to develop after heterosexual inoculation with HIV is shown to be compatible with a wide spectrum of disparate male-to-female transmission data. When the model is applied to time-dependent, HIV-seroprevalence data, the probability that an unexposed woman would remain unexposed after an unprotected contact with an infected man was estimated to be greater than 0.95 on the average. Thus, it would require at least 14 unprotected sexual contacts with HIV-infected men for 50% of an unexposed cohort of women to become exposed to the virus. This suggests that there is a low probability that HIV virions will be found to have penetrated the mucosal barriers of the reproductive tract after a contact. The model also predicts, that the average woman whose mucosal barriers have been breached by HIV has a significant probability of developing immunity to the virus rather than infection. Modelling data for a cohort of unexposed Nairobi women leads to the prediction that the probability of acquiring induced immunity per contact is about 60% of the probability of acquiring the disease per contact. The modelling results also predict that those who had developed resistance to HIV run the small, but significant risk of becoming infected nonetheless by continuing high-risk behavior. For the common contact rate of ten per month, the modelling predicts that the HIV-transmission risk per contact for unexposed women in the Nairobi cohort is 1178 while the transmission risk for the cohort's immunized women is 11548. These numbers suggest that HIV infection is difficult to transmit through heterosexual intercourse on the average and that male-to-female HIV-transmission risk per contact for African women lies between 1178 and 11548. Direct confirmation of the predictions in the last paragraph has been subsequently observed in two completely independent studies. The Nairobi research team recently reported that a notable number of Nairobi prostitutes previously identified to be members of the HIV-resistant group became infected nonetheless. Second, in a study of 174 sexually monogamous, discordant couples in Rakai, Uganda reporting contacts rates of nine to ten per month, the male-to-female HIV-transmission risk per contact was found to be 1769 by direct measurement, a value that falls between the above limits of 1178 and 11548 predicted by the modelling. Thus, a second major prediction of this paper has been directly confirmed, and induced immunity to HIV is limited and not absolutely protective. Circumstantial evidence suggests that the induced immunity to HIV predicted by the model could be generated and/or initiated by nonspecific innate immune responses, specific immunological responses, including IgA-mediated mucosal immunity and cytotoxic T lymphocytes (CTL) immunity, or some combination of the above. It is suggested here, that a decrease in the ability of HIV virions to penetrate the protective mucus layer of the reproductive tract may be a prerequisite, cofactor, or the principle cause of the induced immunity or resistance demonstrated to exist in this paper. The value of the probability that induced immunity to HIV will develop after a contact is shown to be a sensitive function of the woman's human leucocyte antigen (HLA) supertype profile.",,"Kramer I,Shearer GM",,2002,1435–1458,10.1016/S0895-7177(02)00299-6,https://doi-org.proxy.bnl.lu/10.1016/S0895-7177(02)00299-6;http://dx.doi.org/10.1016/S0895-7177(02)00299-6,Journal Article
Mechanisms Fostering the Misuse of Information Systems for Corrupt Practices in the Nigerian Public Sector,"As a result of the pervasiveness of corrupt practices in the public sector, researchers and practitioners in the field of information systems (IS) have argued for the implementation and use of IS to mitigate corrupt practices. Consequently, several public sector institutions leverage IS in the form of e-government, as an anticorruption strategy that provides surveillance and transparency in the public service. However, a number of anecdotal evidences revealed how corrupt bureaucrats misuse the e-government anticorruption platforms under their jurisdiction. Little has been written on the subject of the misuse of IS for corrupt practices, especially within the public sector context, where corrupt bureaucrats connive to misuse the implemented IS anticorruption platforms for selfish gains. The study, therefore, investigated why the misuse of IS for corrupt practices occurs in the Nigerian public sector. This study aimed to identify the motivational dimensions that characterize the misuse of IS and the structural mechanisms associated with the misuse of IS for corrupt practices, and to explain how these structural mechanisms interact. To achieve this aim, critical realism (CR) philosophical assumptions were considered. Routine activity, normalization, reintegrative shaming, model of emergent IT use, and Maslow's hierarchy of needs theories were adopted as theoretical lenses to redescribe the phenomenon under study. The Danemark et al's (2002) six-stage explanatory framework embedded with a longitudinal critical realist single case study design was adopted as the methodology. The research also considered a number of sources of qualitative data that include archival documents between 2018 and 2020 and interviews. Two types of data analysis were considered: (1) a semantic thematic coding procedure that hinges on an inductive-deductive logic was used to make meaning out of the participant's verbalizations and linked them to the chosen theoretical structures, and (2) a retroductive latent thematic coding procedure was used to understand the underlying structural mechanisms that explain why the semantic understanding exist. The key finding of the study was summarized and presented as an explanatory mid-range theory, referred to as the model of IS misuse for corrupt practices (MISMCP). Such a model is capable of providing a detailed explanation of how structural mechanisms interact and manifest themselves in a concrete situation when applied in different contexts, although the research did not claim for the generalizability of the model across all contexts. This research contributes to theory by being the first of its kind in the literature of IS security to explain the causes of misuse of IS for corrupt practices in a public sector environment. In practice, anticorruption agencies can use the research findings for policy development and practical intervention purposes. By using CR methodology, this research further complements the literature of IS security on the application of CR methodology.",,"Inuwa I,M. Aliyu A,C Utulu S",,2022,,,,Ph.D. Thesis
Market Structure Paradigms and Institutional Setting in the Nigerian Electricity Industry,"This thesis examines the Market Structure Paradigms and the Institutional Setting in the Nigerian Electricity Industry. The main objective of this study is to examine and propose market structure that can achieve a production frontier that will produce efficient output and optimal price determination. The study also proposes the most appropriate institutional setting market design that can ensure sustainable electricity supply and enhance consumers' welfare in Nigeria under the restructuring scenario. This study, therefore, attempts to answer the following questions:1. To what extent can the problem besetting the Nigerian power sector be attributed to sub-optimal pricing under an inappropriate institutional setting, formal organisational as against the informal organisation?2. What should be the appropriate number of firms in the generation segment required to supply electricity efficiently? That is, what is the ideal market structure that will produce optimum production efficiency?3. What is the optimal bidding strategy required for effective competition among the generators in dispatching electricity in Nigeria?This study employs two complementary methods to accomplish its stated objectives. In the deregulated market, a partial equilibrium model was employed in the style of Coumot game to determine the appropriate market structure. In addition, a bidding game was equally designed within the game theoretical framework to explain the strategic interaction among the generators.Consequent upon the above, this study has made the following contributions to knowledge: First, the result of the simulation shows that as a monopoly, the price that cleared the market was N11.60 per kilo watt hour as against N7.per kilo watt hour ceiling price set by the government.Second, although the federal government has proposed six generating companies, with unspecified number of independent power producers, this study demonstrates that fourteen firms - seven hydro based and seven gas based are feasible.Besides, this study reveals that a mixed technology is more efficient than single technology. This study has thus shown the need for possible merger between a hydro based firm and a thermal based firm in order to operate efficiently under the restructuring scenario.Furthermore this study also proposes the adoption of bidding strategies which make truthful bidding, e.g. bidding at marginal cost, a weakly dominant strategy for the generators, and hence results in least cost of production, or dispatch efficiency that enhances consumers welfare.The use of game theoretic approach and the application of computable partial equilibrium model as the major tool of analysis is also this studys notable contribution to knowledge. It should noted that this study has only considered purely deterministic models reflecting the early stage of the market in order to provide an insight into the likely behaviour of players in the industry. However, when the market matures, there is the need to consider stochastic models where Bayesian game will be more relevant to analyse such problems as expansion, diversification, and environmental related issues.",,Isola WA,,2007,,,,Ph.D. Thesis
Genres of Spam: Expectations and Deceptions,"This paper is a pilot study that explores how the concept of genre can be applied to the massive set of digital documents known as 'spam'. The authors studied 300 spam messages collected over 15 weeks from a university email system. Messages were coded based on content, form and specific features as well as on the manifest relationship to existing genres of communication. The paper argues that spam is not a single genre but many genres. For the most part, the genres evoked in spam are adaptations of print to Internet, including information artifacts, pamphlets, business cards, order forms, bulletins, advertisements, and ""Nigerian letters"". With spam, however, the concept of genre operates at several levels. Often, there is a contradiction between the manifest genre and the underlying purposes. The paper concludes that spam exploits genre by conforming to known forms while at the same time breaching those norms.",,"Cukier WL,Cody S,Nesselroth EJ",,2006,51.1,10.1109/HICSS.2006.195,https://doi-org.proxy.bnl.lu/10.1109/HICSS.2006.195;http://dx.doi.org/10.1109/HICSS.2006.195,Conference Paper
Knowledge-Based Information Extraction: A Case Study of Recognizing Emails of Nigerian Frauds,"This paper describes the methodology, process and results of developing an application ontology as software specification of the semantics of forensics in the email suspicious of Nigerian frauds. Real life examples of fraud emails are analyzed for evidence and red flags to capture the underlying domain semantics with an application ontology of frauds. A model of the natural language structure in regular expressions is developed in the light of the ontology and applied to emails to extract linguistic evidences of frauds. The evaluation of the initial results shows a satisfactory recognition as an automatic fraud alert system. It also demonstrates a methodological significance: the methodical conceptual modeling and specific purpose-driven linguistic modeling are effective in encapsulating and managing their respective needs, perspectives and variability in real life linguistic processing applications.",,"Gao Y,Zhao G",,2005,161–172,10.1007/11428817_15,https://doi-org.proxy.bnl.lu/10.1007/11428817_15;http://dx.doi.org/10.1007/11428817_15,Conference Paper
Tutorial: Private Information Retrieval,"Private information retrieval (PIR) is a cryptographic primitive that facilitates the seemingly impossible task of letting users fetch records from untrusted and remote database servers without revealing to those servers which records are being fetched. The research literature on PIR is vast; in the over two decades since its 1995 introduction by Chor, Goldreich, Kushilevitz, and Sudan, the cryptography, privacy, and theoretical computer science research communities have studied PIR intensively and from a variety of perspectives. Alas, despite a series of significant advances, most privacy practitioners and theoreticians alike fall into one of two camps: (i) those who believe that PIR is so inefficient and abstruse as to make it all-but-useless in practice, and (ii) those who remain blissfully unaware that PIR even exists. Indeed, to date not even one of the numerous PIR-based applications proposed in the research literature has been deployed at scale to protect the privacy of users ""in the wild"". This tutorial targets both of the above camps, presenting a bird's-eye overview of the current state of PIR research. Topics covered will span the spectrum from purely theoretical through imminently applicable and all the high points in between, thereby providing participants with an awareness of what modern PIR techniques have (and do not have) to offer, dispelling the myth of PIR's inherent impracticality, and hopefully inspiring participants to identify practical use cases for PIR within their own niche areas of expertise. This introductory tutorial will be accessible to anyone comfortable with college-level mathematics (basic linear algebra and some elementary probability and number theory).",,Henry R,,2017,2611–2612,10.1145/3133956.3136069,https://doi-org.proxy.bnl.lu/10.1145/3133956.3136069;http://dx.doi.org/10.1145/3133956.3136069,Conference Paper
On the Concrete Efficiency of Probabilistically-Checkable Proofs,"Probabilistically-Checkable Proofs (PCPs) form the algorithmic core that enables fast verification of long computations in many cryptographic constructions. Yet, despite the wonderful asymptotic savings they bring, PCPs are also the infamous computational bottleneck preventing these powerful cryptographic constructions from being used in practice. To address this problem, we present several results about the computational efficiency of PCPs. We construct the first PCP where the prover and verifier time complexities are quasi-optimal (i.e., optimal up to poly-logarithmic factors). The prover and verifier are also higly-parallelizable, and these computational guarantees hold even when proving and verifying the correctness of random-access machine computations. Our construction is explicit and has the requisite properties for being used in the cryptographic applications mentioned above.Next, to better understand the efficiency of our PCP, we propose a new efficiency measure for PCPs (and their major components, locally-testable codes and PCPs of proximity). We define a concrete-efficiency threshold that indicates the smallest problem size beyond which the PCP becomes ""useful"", in the sense that using it is cheaper than performing naive verification (i.e., rerunning the computation); our definition accounts for both the prover and verifier complexity.We then show that our PCP has a finite concrete-efficiency threshold. That such a PCP exists does not follow from existing works on PCPs with polylogarithmic-time verifiers.As in [Ben-Sasson and Sudan, STOC '05], PCPs of proximity for Reed-Solomon (RS) codes are the main component of our PCP. We construct a PCP of proximity that reduces the concrete-efficiency threshold for testing proximity to RS codes from 2683 in their work to 243, which is tantalizingly close to practicality.",,"Ben-Sasson E,Chiesa A,Genkin D,Tromer E",,2013,585–594,10.1145/2488608.2488681,https://doi-org.proxy.bnl.lu/10.1145/2488608.2488681;http://dx.doi.org/10.1145/2488608.2488681,Conference Paper
Rubric-Based Formative Assessment to Support Students’ Learning of Organic Chemistry in the Selected Secondary Schools in Rwanda: A Technology-Based Learning,"A significant number of instructors, researchers and students have claimed that chemistry is a challenging subject to teach and learn at all education levels. Its main learning difficulties are in line with certain sights of its phenomena that are abstract, and some chemistry teachers do not specify what to be learned and assessed in chemistry lesson. The current work investigates the use of formative assessment rubrics for supporting secondary school students’ progressive learning in organic chemistry through a technology-based learning project approach in Rwanda. The investigators used a convergent parallel research design, and quantitative data were gathered by distributing questionnaires to the students, and the answers were statistically analyzed. Qualitative data were obtained through observation and interview and were narratively analyzed. The results from this study showed that rubric-based formative assessment supported students’ learning of organic chemistry via technology-based learning approach. This is accredited to the fact that students were motivated while doing their assessment and they were able to do self-assessment by applying the provided rubrics via technology. The students understood instructors’ expectations, encouraged their learning, sharpened their technology skills, and their knowledge retention was also increased. The instructors were able to grade the students’ tasks fast with the help of an analytic rubric and good formative feedback was availed to students on time. The instructors were also able to diagnose the strengths and weaknesses of the learners and give them quick formative feedback.",,"Nsabayezu E,Mukiza J,Iyamuremye A,Mukamanzi OU,Mbonyiryivuze A",,2022,12251–12271,10.1007/s10639-022-11113-5,https://doi-org.proxy.bnl.lu/10.1007/s10639-022-11113-5;http://dx.doi.org/10.1007/s10639-022-11113-5,Journal Article
Improved List Decoding of Generalized Reed-Solomon and Alternant Codes over Galois Rings,"We present a two-stage list decoder comprising an errors-only Guruswami-Sudan (GS) decoder and an errors-and-erasures GS decoder as component decoders in the first and second stage, respectively. The two stages are coupled via a post-processor which selects a codeword from the output list of the first component decoder, from which erasure locations are obtained for the second stage. When applied to a generalized Reed-Solomon (RS) code over a Galois ring R that maps into a generalized RS code of the same length n and minimum (Hamming) distance d over the corresponding residue field, the proposed decoder exploits the presence of zero divisors in R to correct s errors where w=lceiln-radic(n(n-d))-1rceil<sleslceiln- radic((n-w)(n-d))-1rceil with a probability determined by s, w, and the ratio of the number of nontrivial zero divisors to the number of units in the code alphabet. Focusing primarily on alternant codes over Zopf(2l), an important class of subring subcodes of generalized RS codes over GR(2l,a), we demonstrate that the GS decoding radius w can be exceeded by a substantial margin with significant probability",,Armand MA,,2005,728–733,10.1109/TIT.2004.840901,https://doi-org.proxy.bnl.lu/10.1109/TIT.2004.840901;http://dx.doi.org/10.1109/TIT.2004.840901,Journal Article
Removing Background of Raman Spectrum Based on Wavelet Transform,"Raman spectrum is usually so weak that the noise usually distorts the interesting Raman bands. Especially, when the sample is fuscous, the strong fluorescence background often completely dominates Raman peaks. In this case, it is vital to remove fluorescence background for analyzing of Raman spectrum. Wavelet transform (WT) has proved to be a high-performance signal processing tool, and very efficient in removing low frequency noise. In this paper, WT was used to remove fluorescent background of Raman spectrum of Sudan I. Both the global thresholding method and the interval thresholding method were applied, and their de-noising performances were compared. The results indicated that the interval thresholding method is more efficient than the global thresholding method in removing fluorescence background of Raman spectrum.",,Li G,,2009,198–200,10.1109/FCC.2009.69,https://doi-org.proxy.bnl.lu/10.1109/FCC.2009.69;http://dx.doi.org/10.1109/FCC.2009.69,Conference Paper
Inadequate Training on Employee Productiveness: A Qualitative Exploratory Case Study of Nigeria Consulate New York,"The importance of training has undoubtfully been recognized in many organizations as the key factor for improved productivity. However, employee training in the public sector is threatened by a lack of improved skills needed to result in increasingly effective performance. This lack of training poses a threat to human capital in the Nigerian Consulate in New York in key facets of their positions and disseminating services to the public. This study uses human capital theory as framework of analyses. Data was collected through a well-structured questionnaire circulated to twenty (20) employees of Nigerian Consulate in New York with a population of 50. Simple random sampling was used to administer the questionnaire which included both senior and junior employees. A qualitative exploratory case study was the methodology and design applied in the study. Data was organized using NVivo 12 software to identify patterns and core themes. The findings of this research indicate a significant need for employee training and productiveness in Nigerian Consulate in New York. Based on these findings, the Nigerian Consulate in New York should embrace employee training as management approach to constantly increase the knowledge, skills and abilities required for employees to remain productive in a competitive environment. The study also recommends that employees who are willing to embark on training programs should be provided adequate resources to ensure a successful program. It was concluded that management in the Nigerian Consulate in New York should endeavor to monitor the implementation of training policies to achieve its objectives.",,"Lawal O,Madueke C,Miah M",,2020,,,,Ph.D. Thesis
Domain Enriched Learning for Brain Image Analysis,"Medical imaging techniques such as magnetic resonance imaging (MRI), computed-tomography (CT), X-ray, ultra-sound, positron emission tomography (PET), and mammography have been widely used over the past few decades for diagnosis and treatment of many medical conditions. Often, medical image interpretation is performed by human experts such as neurosurgeons, pathologists, and radiologists. Given the complex nature of these pathological images and the burden on human experts, it is desired to at least partially automate medical image interpretation using computer-aided methods. With recent advance in machine learning techniques, the goal of achieving automated medical image analysis is now practically realizable. In this dissertation we address two important and challenging medical imaging problems: 1) Brain image super-resolution (SR) and 2) Brain image segmentation. Finally, we combine the frameworks of enhancement and segmentation to pave a way for cost-effective diagnosis of infant-hydrocephalic patients. The first part of the thesis addresses the issue of enhancing the resolution of MR brain images via deep learning techniques. High resolution Magnetic Resonance (MR) images are desired for accurate diagnostics. In practice, image resolution is restricted by factors like hardware and processing constraints. Recently, deep learning methods have been shown to produce compelling state-of-the-art results for image enhancement/super-resolution. Paying particular attention to desired hi-resolution MR image structure, we propose a new regularized network that exploits image priors, namely a low-rank structure and a sharpness prior to enhance deep MR image super-resolution (SR). Our contributions are then incorporating these priors in an analytically tractable fashion as well as towards a novel prior guided network architecture that accomplishes the super-resolution task. This is particularly challenging for the low rank prior since the rank is not a differentiable function of the image matrix (and hence the network parameters), an issue we address by pursuing differentiable approximations of the rank. Sharpness is emphasized by the variance of the Laplacian which we show can be implemented by a fixed feedback layer at the output of the network. As a key extension, we modify the fixed feedback (Laplacian) layer by learning a new set of training data driven filters that are optimized for enhanced sharpness. Experiments performed on publicly available MR brain image databases and comparisons against existing state-of-the-art methods show that the proposed prior guided network offers significant practical gains in terms of improved SNR/image quality measures. Because our priors are on output images, the proposed method is versatile and can be combined with a wide variety of existing network architectures to further enhance their performance.The second part of the thesis leverages the discriminative power of sparse signal representation to segment post-operative CT hydrocephalic images. Hydrocephalus is a medical condition in which there is an abnormal accumulation of cerebrospinal fluid (CSF) in the brain. Segmentation of brain imagery into brain tissue and CSF (before and after surgery, i.e. pre-op vs. post-op) plays a crucial role in evaluating surgical treatment. Segmentation of pre-op images is often a relatively straightforward problem and has been well researched. However, segmenting post-operative (post-op) computational tomographic (CT)-scans become more challenging due to distorted anatomy and subdural hematoma collections pressing on the brain. Most intensity and feature based segmentation methods fail to separate subdurals from brain and CSF as subdural geometry varies greatly across different patients and their intensity vary with time. We combat this problem by a learning approach that treats segmentation as supervised classification at the pixel level, i.e. a training set of CT scans with labeled pixel identities is employed. Our contributions include: 1) a dictionary learning framework that learns class (segment) specific dictionaries that can efficiently represent test samples from the same class while poorly represent corresponding samples from other classes, 2) quantification of associated computation and memory footprint, and 3.) a customized training and test procedure for segmenting post-op hydrocephalic CT images. Experiments performed on infant CT brain images acquired from the CURE Children's Hospital of Uganda reveal the success of our method against the state-of-the-art alternatives. We also demonstrate that the proposed algorithm is computationally less burdensome and exhibits a graceful degradation against number of training samples, enhancing its deployment potential.In the final contribution of the thesis, deep-learning frameworks for simultaneous enhancement and segmentation are developed for infant hydrocephalic patients. Post-infectious Hydrocephalus in infants is a major health problem in majority of the sub-Saharan African countries. As described above, segmentation of the affected brain imagery into brain tissue and cerebrospinal fluid (CSF) plays a crucial role in evaluating the surgical treatment. Owing to cost constraints, often computational tomographic (CT) scans are used for analysis and treatment purposes. However, exposing infants to radiation for a longer time is not recommended. To counter these challenges, deployment of low-field Magnetic Resonance (MR) devices is considered as an viable option in the developing nations. However, the scans obtained from these devices often result in poor quality images which are difficult to interpret and hence may lead to inaccurate volumetric analysis. Recently, deep learning frameworks have pushed the boundaries for several image enhancement and segmentation applications. In this work, we propose a deep learning framework that jointly enhances and segments the brain images and extends the range of biological phenomena observed by low field MR image devices. We demonstrate the capability of our proposed framework on our own database of infant hydrocephalic patients and on normal brain images obtained from National Institute of Health (NIH) data base. The results obtained on extremely noisy and un-interpretable images are so revealing that developing a reliable low- field MR imaging is now made a realistic possibility. The thesis is then concluded by pointing towards appropriate future directions wherein domain-enrichment for brain image analysis can be achieved via other sophisticated deep networks that learn important structural properties of the brain.",,"Cherukuri V,Vijaykrishnan Narayanan",,2020,,,,Ph.D. Thesis
Identifying Software Project Risks in the Canadian Financial Services Sector: An International Comparative Study,"Frequent occurrence of software project failures has created two general streams of research. One theme analyzed the common causes of cost overruns, late schedules, and unmet scope. With the belief that project failures are avoidable through proactive means, another group of researchers investigated software project risks. With such intent, comparative studies were conducted in Finland, Hong Kong, and the United States. Subsequent research in Nigeria determined the impact of the socioeconomic context. To further extend the coverage of prior studies, the research in the current study focused specifically on the Canadian financial services sector. Project managers were solicited for input to discover, determine, and rank risk factors in software projects using the same research design that was used in previous comparative research studies---a three-phase Delphi survey that uses nonparametric statistical techniques. In sharp contrast to prior studies, however, this research aimed not for general applications of the results at the country level but for specific collective relevance to software projects in banks, trust companies, insurance companies, mutual fund companies, and similar organizations. The composite rankings of the studies in Hong Kong, Finland, and the United States listed lack of management commitment, inability to get user commitment, and misunderstanding of the requirements as the top three risk items in software projects. Given that only the misunderstanding of requirements made it into the top three risk items in Nigeria, it would be of value to scholars and practitioners to determine how the results would differ if the study was conducted in a specific sector in the industry. Except for one risk factor (lack of dedicated, full-time project resources), this study confirmed that the previous list of risk factors captured the top risk factors in the Canadian financial services sector.",,Estrella JA,,2006,,,,Ph.D. Thesis
Malarial Retinopathy and Neurovascular Injury in Paediatric Cerebral Malaria,"Background Diseases of the brain are difficult to study because this organ is relatively inaccessible. Only one part of the central nervous system is available to direct, non-invasive observation – the retina. The concept of the retina as a window to the brain has created much interest in the retina as a source of potential markers of brain disease. Paediatric cerebral malaria is a severe neurological complication of infection with the parasite Plasmodium falciparum, which is responsible for death and disability in a significant number of children in sub-Saharan Africa. As with many neurological diseases, the precise mechanisms by which this infection causes damage to the brain remain unclear, and this hampers efforts to develop effective treatments. It may be that studying the retina in paediatric cerebral malaria could both illuminate pathogenesis specific to this disease, and also provide an illustration of how to approach retinal biomarkers in a new, and potentially more effective way.Methods I approached the aim of developing retinal features as markers of brain disease in paediatric cerebral malaria via several objectives. I made use of an existing clinical study to collect new retinal data from ophthalmoscopic examinations and fundus fluorescein angiograms from patients over three successive malaria seasons in Malawi, and added these to historical data obtained previously at the same site. I devised a new method for grading retinal images. I reviewed the biological plausibility of associations between retina and brain in cerebral malaria, and then considered analytical methods to interpret my retinal data effectively. Finally I estimated associations between retinal features, outcomes, and a radiological measure of brain swelling using combinations of regression models.Results My review of retinal and cerebral histopathology, vascular anatomy and physiology indicated that certain retinal and brain regions may be similarly prone to damage from sequestration as a result of interactions between aberrant rheology and microvascular geometry, such as branching patterns and arteriole to venule ratios. My review of evaluations of analogy and surrogacy suggested that biological similarities between retina and brain could be used to justify statistical evaluation of the amount of information the subject and object of the inference share about a common outcome, as used to assess surrogate end points for clinical trials. This kind of approach is able to address questions about whether a particular retinal feature is effectively equivalent to an analogous disease manifestation in the brain. I report analyses on three overlapping groups of subjects, all of whom had retinopathy positive cerebral malaria: children with admission ophthalmoscopy (n=817), children with admission fluorescein angiography (n=260), and children with admission angiography and MRI of the brain (n=134). Several retinal features are associated with death and longer time to recover consciousness in paediatric cerebral malaria. Broadly speaking, these features appear to reflect two processes: neurovascular sequestration (e.g. orange vessel discolouration and death), and neurovascular leakage (e.g. >5 sites of punctate leak and death). Respective adjusted odds ratios and 95% confidence intervals for these particular associations are: 2.88 (1.64-5.05); and 6.90 (1.52-31.3). Other related processes may also be important, such as ischaemia, which can be extensive. Associations between retina and brain are less clear, in part because of selection bias in the samples.Conclusions Neurovascular leak is important in fatal paediatric cerebral malaria, suggesting that fatal brain swelling may occur primarily as a result of vasogenic oedema. Other processes are also likely to be involved, particularly neurovascular sequestration, which is visible on retinal imaging as orange vessels or intravascular filling defects. Sequestration may plausibly cause leak through direct damage to tight junctions and by increasing transmural pressure secondary to venous congestion. Several types of retinal leakage are seen and some of these may represent re-perfusion rather than acute injury. Future work to investigate temporal changes in retinal signs may find clearer associations with radiological and clinical outcomes. The steps taken to evaluate retinal markers in cerebral malaria illustrate a more rigorous approach to retinal biomarkers in general, which can be applied to other neurological diseases.",,MacCormick IJ,,2016,,,,Ph.D. Thesis
Enhancing Horizontal Accountability : The Key to Addressing the Conflict of Interest Loopholes in Public Procurement in Nigeria,"This thesis argues that the current dual accountability model in Nigerian public procurement model is one with vertical accountability and horizontal accountability which is weakened by an inefficient horizontal accountability system, and in order to unleash the true benefits of a dual accountability system there needs to be an enhancement of horizontal accountability. It argues that the key to this enhancement lies in improved access to information and enhanced legal empowerment of certain actors in the public procurement process. This thesis tackles one of the most critical sectors in the Nigeria public sector – public procurement, specifically because of the effect that this sector has on the general development of a country. The thesis argues that in order to reduce corruption in the sector, the focus needs to be placed on ensuring accountability in the stage before corruption – conflict of interest. It puts forward the position that in order to ensure accountability at the conflict of interest stage, horizontal accountability can be extremely beneficial, and therefore the thesis creates a theoretical model - the Transparency and Accountability Matrix (TAM) to determine the level of horizontal accountability within certain interactions in the Nigerian public procurement process. Using the TAM as a baseline measurement for horizontal accountability, solutions are proffered on how to enhance the efficacy of horizontal accountability and this is applied to specific conflict of interest scenarios within the Nigerian public procurement process. The key contributions of this thesis are the creation of the TAM – a theory backed model as a horizontal accountability measurement tool; and a very thorough analysis on the accountability framework of the Nigerian public procurement process with a focus on the surrounding access to information legislations, which has been a hitherto unexplored gap in the research on Nigerian public procurement.",,Ibidapo-Obe B,,2020,,,,Ph.D. Thesis
Interactive Machine Vision for Wildlife Conservation,"The loss rate of endangered animal species has reached levels that are critical enough for our time to be called the sixth mass extinction. Families of vertebrates and large mammals, such as Rhinocerotidae, are likely to become extinct in a few years unless countermeasures are taken. Before doing so, however, it is imperative to assess current animal population sizes through wildlife censuses. Furthermore, conservation efforts require animal populations to be monitored over time, which implies conducting census repetitions over multiple years.Recent developments in technology have paved the way for animal census efforts of unprecedented accuracies and scales, predominantly through the employment of Unmanned Aerial Vehicles (UAVs). UAVs allow for acquiring aerial imagery of vast areas over e.g. a wildlife reserve, and thereby provide evidence of the abundance and location of individuals in a safe manner. Hitherto, the main challenge of UAV-enforced animal censuses has been the stage of manual photo-interpretation, in which animals have to be tediously identified and annotated by hand in potentially tens of thousands of aerial images.To this end, automated image understanding through Machine Learning (ML) and Computer Vision (CV) provides exciting potential for accelerating applications that rely on large-scale datasets, such as image-based aerial animal censuses. Employing machines to detect animals could greatly reduce the efforts required by humans, and therefore lead to vastly increased efficiency in the census process overall.This thesis aims at advancing wildlife conservation efforts by means of automated machine vision methodologies. In a first step, this entails finding new ways to optimize CV algorithms for the task of animal detection in UAV imagery. In a second step, it requires procedures to reuse such detection models for new image data in the context of census repetitions for population monitoring. However, the benefit of machine vision reaches beyond a mere automation of photo-interpretation: a recurrent key principle of this thesis is the concept of interactivity, where CV models and humans work hand-in-hand by reinforcing each other. The result is a census monitoring environment for UAV images, in which machine vision technology actively assists humans in the process. Effectively, when all methodologies proposed throughout this thesis are combined, human annotation efforts are reduced to a fraction, and further simplified in complexity.Chapter 2 addresses the challenges of employing state-of-the-art CV models, known as Convolutional Neural Networks (CNNs), for aerial wildlife detection. Multiple heuristics are presented to train such models properly, all of which target different obstacles of the model training process. Experiments show a significant increase in animal prediction quality, if a CNN is optimized in an appropriate way.Chapter 3 employs this CNN for reusage over new data acquisitions, e.g. in a census monitoring setting. Simply running the CNN over a new dataset to predict animals directly is often not possible, due to differences in characteristics between the datasets known as domain shifts. This chapter presents methodologies to adapt CNNs to new datasets with minimal effort possible, and employs humans in the process in an interactive manner to do so. Results show that less than half a percent of the images need to be reviewed by humans to find more than 80% of the animals in the new campaign.In Chapter 4, human annotation efforts themselves are addressed and reduced in complexity. Traditional settings require human annotators to draw bounding boxes around animals, which may become prohibitively expensive for large image datasets. This chapter instead explores the concept of weakly-supervised object detection, where only simple presence/absence information of animals per image is requested from the annotators. Unlike bounding boxes, an image-wide annotation can be provided in a second. It was found that a CNN, trained on this simpler information alone, is already able to localize animals by itself to a certain degree. However, if spatial bounding boxes are added for just three training images, the CNN predicts animals with the same accuracy as its fully-supervised sibling from Chapter 2.Finally, Chapter 5 combines all findings and models into an integrated census software environment, denoted as Annotation Interface that Does Everything (AIDE). To the best of the author's knowledge, AIDE is the first software solution that explicitly integrates machine vision technology into the labeling process in an interactive manner: in AIDE, CNNs are used to predict animals in a large set of unlabeled data, and further learn directly from annotations provided by humans on the images. The result is a positive feedback loop where humans and machine reinforce each other. A conducted user study shows that machine vision support provides a four-fold increase in the number of animals found in a given time, compared to an unassisted annotation setting on the same dataset. At the time of writing, AIDE is actively employed by conservation agencies in Tanzania and under consideration by other forces around the globe for potential usage.This thesis highlights the importance of interactive machine vision for wildlife conservation, and provides solutions that not only advance the field in a scientific context, but also have a direct impact on wildlife conservation through population monitoring.",,Kellenberger B,,2020,,,,Ph.D. Thesis
Introduction to Special Section on Probabilistic Proof Systems,"The study of probabilistically verifiable proofs originated in the mid 1980s with the introduction of Interactive Proof Systems (IPs). The primary focus of research in this area in the '80s has been twofold: the role of zero-knowledge interactive proofs within cryptographic protocols, and characterizing which languages are efficiently interactively provable. In the 1990s, the focus of research on the topic shifted. Extensions of the interactive proof model, such as Multiprover Interactive Proofs (MIPs) and Probabilistically Checkable Proofs (PCPs), were considered with the intention of expanding our notion of what should be considered efficiently verifiable. In addition, researchers have taken a closer look at the exact resources (and tradeoffs amongst them) needed to verify a proof using various proof systems. This culminated in the important discovery that it is possible to verify NP statements (with a constant error probability) by only examining a constant number of bits of a PCP and using logarithmic amount of randomness. Perhaps, however, the most dramatic development has been the connection which was found between probabilistically verifiable proofs and proving hardness of approximation for optimization problems. It has been shown that a large variety of optimization versions of NP-hard problems (e.g., the maximum size of a clique in a graph, the minimum number of colors necessary to color a graph, and the maximum number of clauses satisfiable in a CNF formula) are not only NP-hard to solve exactly but also NP-hard to approximate in a very strong sense. The tools to establish hardness of approximation came directly from results on MIPs and PCPs. Indeed, almost every improvement in the efficiency of these proof systems translates directly into showing larger factors within which these optimization problems are hard to approximate. In 1994--1995 two exciting workshops were held at the Weizmann Institute in Israel on the new developments in probabilistically verifiable proofs and their applications to approximation problems, cryptography, program checking, and complexity theory at large. Over 60 papers were presented in the workshop series, and we are proud to include three of them in this special section. ""On the Power of Finite Automata with Both Nondeterministic and Probabilistic States"" by Anne Condon, Lisa Hellerstein, Samuel Pottle, and Avi Wigderson, considers constant round interactive proof systems where the verifier is restricted to use constant space and public coins. An equivalent characterization is finite automata with both nondeterministic and random states (npfa's), which accept their languages with a small probability of error. The paper shows that npfa's restricted to run in polynomial expected time accept only the regular languages in the case of npfa with 1-way input head, and that if L is a nonregular language, then either L or its complement is not accepted by any npfa with a 2-way input head. ""A Parallel Repetition Theorem"" by Ran Raz, addresses and resolves the Parallel Repetition Conjecture which has eluded researchers for some time. The broader topic is what happens to the error probability of proof systems when they are composed. It has been known for awhile that sequential composition of proof systems (both single and multiprover interactive proofs) reduces the error exponentially, but this increases the number of rounds. For interactive proof systems, parallel repetition is known to reduce the error exponentially, and the Parallel Repetition Conjecture asserts that the same holds in a one-round two-prover proof system. Raz proves a constructive bound on the probability of error which indeed reduces at an exponential rate. The constant in the exponent is logarithmic in the total number of possible answers of the two provers, which means one can achieve two-prover one-round MIPs for NP statements with arbitrarily small constant error probability. This, in turn, has played a crucial role in further developments in the area and in particular in those reported in the next paper. ""Free Bits, PCPs, and Nonapproximability---Towards Tight Results"" by Mihir Bellare, Oded Goldreich, and Madhu Sudan, continues the investigation of PCPs and nonapproximability with emphasis on trying to get closer to tight results. The work consists of three parts. The first part presents several PCP proof systems for NP, based on a new error-correcting code called the Long Code. The second part shows that the connection between PCPs and hardness of approximation is not accidental. In particular, it shows that the transformation of a PCP for NP into hardness results for MaxClique can be reversed. Finally, the third part initiates a systematic investigation of the properties of PCPs as a function of the various parameters: randomness, query complexity, free-bit complexity, amortized free-bit complexity, proof size, etc. Two more papers submitted for this special section were not ready at this time for publication. They will appear in future issues of the SIAM Journal on Computing.",,Goldwasser S,,1998,737–738,10.1137/SMJCAT000027000003000737000001,https://doi-org.proxy.bnl.lu/10.1137/SMJCAT000027000003000737000001;http://dx.doi.org/10.1137/SMJCAT000027000003000737000001,Journal Article
Short PCPs with Polylog Query Complexity,"We give constructions of probabilistically checkable proofs (PCPs) of length $n cdot polylog n$ proving satisfiability of circuits of size $n$ that can be verified by querying $polylog n$ bits of the proof. We also give analogous constructions of locally testable codes (LTCs) mapping $n$ information bits to $ncdot polylog n$ bit long codewords that are testable with $polylog n$ queries. Our constructions rely on new techniques revolving around properties of codes based on relatively high-degree polynomials in one variable, i.e., Reed-Solomon codes. In contrast, previous constructions of short PCPs, beginning with [L. Babai, L. Fortnow, L. Levin, and M. Szegedy, Checking computations in polylogarithmic time, in Proceedings of the 23rd ACM Symposium on Theory of Computing, ACM, New York, 1991, pp. 21-31] and until the recent [E. Ben-Sasson, O. Goldreich, P. Harsha, M. Sudan, and S. Vadhan, Robust PCPs of proximity, shorter PCPs, and applications to coding, in Proceedings of the 36th ACM Symposium on Theory of Computing, ACM, New York, 2004, pp. 13-15], relied extensively on properties of low-degree polynomials in many variables. We show how to convert the problem of verifying the satisfaction of a circuit by a given assignment to the task of verifying that a given function is close to being a Reed-Solomon codeword, i.e., a univariate polynomial of specified degree. This reduction also gives an alternative to using the “sumcheck protocol” [C. Lund, L. Fortnow, H. Karloff, and N. Nisan, J. ACM, 39 (1992), pp. 859-868]. We then give a new PCP for the special task of proving that a function is close to being a Reed-Solomon codeword. The resulting PCPs are not only shorter than previous ones but also arguably simpler. In fact, our constructions are also more natural in that they yield locally testable codes first, which are then converted to PCPs. In contrast, most recent constructions go in the opposite direction of getting locally testable codes from PCPs.",,"Ben-Sasson E,Sudan M",,2008,551–607,10.1137/050646445,https://doi-org.proxy.bnl.lu/10.1137/050646445;http://dx.doi.org/10.1137/050646445,Journal Article
The Method of Simulated Annealing for the Optimal Adjustment of the Nigerian Horizontal Geodetic Network,"The Horizontal Geodetic Network of Nigeria is made up of terrestrially arranged chains of triangles augmented by precise traverses. The work on the network began early 19th century but by 1930 the work was discarded and a re-observation of the network was carried out to the highest possible accuracy then, enhanced with high order geodimeter traverses which linked to other neighboring African networks.The full network consists of 515 stations, with 2411 observations which comprise 2197 angular observations, 40 Laplace azimuths and 174 measured distances, part of which substituted for the sparse triangulation observations especially in the southern part of the country. The added observations contributed to strengthening of the network in the 1977 adjustment which however was not a holistic optimized adjustment, but rather, a phase adjustment. Based on the 1977 state of adjustment of the network, no meaningful distortion monitoring exercise can take place until the network is adjusted by an optimized simultaneous technique in order to ascertain the state and consistency of the network. The use of the simulated annealing method, which has been successfully applied in other fields, is presented for the classical geodetic problem of simultaneous adjustment of the entire triangulation net using the least squares observation equation method. This method is an iterative heuristic technique (a method of solving problems by learning from past experience and investigating practical ways of finding a solution) in operations research. It uses a thermo dynamic analogy (Cooling theory) to adjust a network of unstable stations (changes to gaseous state) through fairly stable station coordinates (liquid state) to a stable station coordinates (solid state) so as to offer a solution that converges in a probabilistic sense (statistically based) to the global optimum. The simulated annealing method of optimization serves to help determine the position of all triangulation stations by means of minimizing the volume of the error hyper ellipsoid inherent in the solution to give an optimal configuration of the geodetic network. Computer programs were developed using Matlab Software and run on an adequately configured Pentium IV computer. Creation of an intelligent database was achieved through the interactive network of the data storage, processing, manipulation, analysis and retrieval of results of the adjustment.The result of the new adjustment produced a generally consistent trend of changes in the distances and azimuths compared to the previous adjustments. Error analysis of all lines were carried out and the respective standard errors in distances and azimuths were determined. Relative and absolute error ellipses of all stations were determined and plotted. Statistical plots and analysis of the error ellipses of the network stations were also determined. The absolute and relative weakness/strength of the network stations coordinates after adjustment were shown and confirmed by the error plots to have the following geometry error distributions. That is, 90.5% of the 515 Network Stations fell within Network Standard deviation of 1- Sigma, 94.2% within 2-Sigma, while 98.3% fell within 3- Sigma The distributions confirmed the high reliability of the Nigerian Horizontal Geodetic Network and its data quality. Re-strengthening exercise would be necessary using either the 1-Sigma or 2-Sigma region of network standard deviation .A data structure for the entire network was developed and necessary conclusions and recommendations are made for further action to update/upgrade the precision of the Nigerian horizontal geodetic network for future study.",,Omogunloye OG,,2010,,,,Ph.D. Thesis
Critical Risk Path Method: A Risk and Contingency-Driven Model for Construction Procurement in Complex and Dynamic Projects,"Existing approaches to risk management in construction procurement primarily dwell on strategies designed for commonly identifiable risk factors in typical project environments. Commonly identifiable risk factors would include too early or late material delivery-a condition typically ameliorated by implementing a Just In Time JIT plan; inferior construction materials typically mitigated by employing trusted vendors; or ineffective contractors primarily avoided by the use of experienced contractors. The purpose of this paper is to present a coherent model for procurement risk management for construction and infrastructure development projects within the context of dynamic project environments-complex, or chaotic. For the purpose of this study, a critical risk path activity is one in which a delay of activity completion not only leads to project delay, but does so in a manner that may be fatal to project or at best, far greater than the actual delay. The study incorporates observations and theory with practical application for improving initiatives by emergency infrastructure development response organizations such as FEMA Federal Emergency Management Agency and USACE US Army Corps of Engineers in the United States, the NEMA National Emergency Management Agency in Nigeria, or ANDMA Afghanistan National Disaster Management Authority etc. This study presents risk response plans aimed at improving the potential occurrence of positive risk aspects while reducing, or eliminating the same for negative risk occurrences. This study explored material, equipment, and skilled labor procurement strategies related to project risk management from the perspectives of scheduling, cost, and quality-three factors often referred to as the triple project constraints. It identified gaps within specific national and multinational organizations' approaches, and provided detailed recommendations for process improvements from the procurement management perspective to ensure the potential for successful project outcomes in unstable project conditions.",,"Howard C,Iromuanya C,Hargiss KM",,2013,61–73,10.4018/jsita.2013040105,https://doi-org.proxy.bnl.lu/10.4018/jsita.2013040105;http://dx.doi.org/10.4018/jsita.2013040105,Journal Article
Socio-Economic Factors in the Application of Information and Communication Technologies in Nigerian Print Media,"Information and communication technologies (ICTs) have opened up new opportunities for the Nigerian print media to improve on their products and services. This study explores the socio-economic factors associated with the adoption and use of ICTs by the media. Of a total of 54 socio-economic factors considered, exactly 50% were found to have significant influence on the adoption and success of ICT applications. The factors that have the greatest positive influence on adoption of ICTs include organizational goal, profitability, organizational image, communication in the organization, productivity, and openness of workers to change. They also constitute success factors in the use of these technologies. The factors that constrained adoption and also successful application include high rate of inflation, unfavourable exchange rate of the naira to the dollar, low wage level, huge costs, low gross national product, inadequate funding, and unstable political situation. These constraining factors are indicators of economic weakness and political uncertainty. It seems that the significance of such factors, which are completely external to a business organization, was often underestimated in studies of organizational performance in developing countries.",,Ehikhamenor FA,,2002,602–611,10.1002/asi.10044,https://doi-org.proxy.bnl.lu/10.1002/asi.10044;http://dx.doi.org/10.1002/asi.10044,Journal Article
Design of Variable Structure Automatic Generation Controllers for Electrical Power Systems,"This thesis considers the automatic generation control (AGC) problem in electrical power systems. The aim is to closely regulate the mismatch between real power generation and consumption so as to maintain the frequency and/or tie-line power interchange within the scheduled values. A review of existing literature on techniques of AGC reveals that the problem of poor transient performance in terms of large overshoots and relatively long settling time is still unresolved. In addition the existing AGC strategies are not robust in the sense that they are sensitive to parameter variations and extraneous disturbances impinging on the system. In this thesis, a new approach to the design of automatic generation control schemes for electrical power systems is proposed. The new approach, which is based on the theory of variable structure systems (VSS), exploits the concepts of generalised eigenstructure assignment and unit vector control to synthesize a controller that guarantees asymptotically stable sliding motion with prescribed transient behaviour. Such a variable structure automatic generation control (VSAGC) scheme is inherently robust, in the sense that, when sliding, the system acquires invariance properties with respect to parameter variations and disturbances. An interactive computer aided design software package is also developed for the implementation of the proposed AGC design approach - on an industry standard Pc-AT microcomputer. The package is designed to provide the software part of an overall computerised scheme for the AGC of electrical power systems. The performance of the proposed AGC strategy is illustrated by application to models of the Nigerian power system. Simulation results indicate that the VSAGC scheme yields better system performance than the existing technique. It is concluded that the proposed VSAGC scheme provides an effective means of controlling the frequency and/or tieline power deviations, and thus, improving the dynamic performance of electrical power systems.",,Okafor FN,,1992,,,,Ph.D. Thesis
A Case History of a Computer Media Event—Introducing a Supercomputer Center,"I've been asked to tell you how Cornell attempted to explain to the rest of the world the establishment here of one of four national centers for advanced computing — the supercomputer facility formally known as the Center for Theory and Simulation in Science and Engineering and nicknamed the Theory Center. This is the center that was founded this spring with part of the 200 million dollars that the National Science Foundation is allocating in the federal government's supercomputer initiative. That's a polite way of saying, we want to “Pearl Harbor” the Japanese before they do it to us in yet another area of technology.In brief, the Cornell Theory Center will be receiving something in the neighborhood of 30 million dollars from the National Science Foundation and another 30 million from IBM, in equipment and support, over the next three years to build and operate a production supercomputer facility — a sort of jumbo jet of supercomputers — and to conduct research in experimental supercomputer configurations — a program that could be thought of as the X-15 of computing. The Theory Center is still seeking additional industrial support; another $100 million would be a nice round number. Even without the industrial support, this is the largest single research program at Cornell.I'd like to describe the preparation — the groundwork — that went into this public information effort, as much as two years in advance. We'll go into the gory detail of what went wrong in our announcement, and some of the things that went right, not so many thanks to us. We'll take a look at how the news media covered an event like this — in particular television news — and I'll tell you why the hardest part of our job, as public information practitioners, is still ahead us.Let's start with who am I and what am I doing at a conference of computer documentation people? In a way, we're in the same business. We're supposed to be explaining computers and computing to people. Your people — your public — can be assumed to be receptive to computing. Or at least they're using it. The general public includes lots of people like myself who are still on the fringes of the computer revolution. They've been involved in a few skirmishes, maybe not even wounded yet, but they're not sure whose side they're on. They know that “user friendly” isn't good enough. They're not ready to learn a new language to speak to a machine. “The damn things are in the U.S. of A. Let them learn to speak American”.Even the millions of folks who have bought personal computers share a healthy suspicion of computing. Big computers are the ones the IRS uses to lose your tax returns. Big computers are the ones in government weapons labs. Big computers still can't predict the weather. Big computers are the ones, when they make mistakes, you can't argue with.That has something to do with why the idea of super computing — large scale computing — is not so easy to sell. “Bigger is better” went out with tailfins on cars. Now, if you're going to be bigger, or bigger just to be faster, there had better be a good reason for it.We didn't realize all of this, however, when ken Wilson won the Nobel Prize for physics in 1982, and immediately began talking about building supercomputers. I guess we were just glad to hear a theoretical physicist talk about something besides “the deep and hitherto unperceived analogies between the phenomena revealed by phase transitions and certain aspects of elementary particle physics.”The public's first inkling that there would be something called a Theory Center began when Ken Wilson stood up at a press conference that October morning, about four hours after being notified he was a Nobel Prize winner, and said: “I'm working at the national policy level to get people to realize the importance of computers as they become very much more powerful than they are today.” He said that just one field alone—theoretical physics — needed computing support to the tune of $100 million a year. He said, “I hope the prestige of the prize will help me get people — not necessarily to give $100 million — but to look carefully at the problems I've been discussing and to see if we can't get them worked out.”And from that day on, Cornell began promoting Kenneth G. Wilson — and I'm not ashamed of that word, promotion — and capitalizing on his fame. After all, you're only the reigning Nobel Prize winner for 365 days, and then someone else's phone starts ringing off the hook.Now Wilson was already serving on government panels to advise on the future of large scale computing and he had been knocking on the doors of executive suites in big business and industry, trying to convince the movers and shakers that American industry needed supercomputers and that the computer industry wasn't going to make very many of them until there was a demonstrated market and the best showroom, if you will, for supercomputers would be the universities where potential customers could come and “kick the tires” of the latest models.Then suddenly, Nobel Prize winner Wilson was the most prominent member of those government panels. Receptionists would say, “Let me show you right in, Dr. Wilson.” We interviewed Wilson for a Cornell publication a month later and he said, “There is nothing that comes close to providing the kind of forum that the Nobel Prize provides. With the kinds of problems I'm dealing with, with the kinds of barriers I face, anything short of the Nobel Prize doesn't mean very much.”We took the text of that interview — which talked about new uses for computer modeling and simulation and some schemes for parallel processing — and we sent it to about a dozen key business writers and science writers and editors around the country with a note saying, “Keep your eye on this guy. He knows more about supercomputing than anyone else in the country.”Now we didn't know whether that was true at the time. We just sort of became convinced of it.In the meantime, Ken Wilson was stepping up his activities in behalf of supercomputers. He was visiting more industries and getting more involved in advising government policy. When a report or a recommendation came out, if his name wasn't on it, people would ask his opinion. When the Japanese moved a little closer to making some big advance, people would ask Ken Wilson what he thought the U.S. should do. And once you get cited in The New York Times as a “leading expert” then you are one, and everyone else wants to know what you think. He was invited to write lots of articles and give talks on supercomputers and “the Japanese challenge.” He became “Mr. Supercomputer.”We don't claim all the credit for his fame. A lot, maybe most, of the effort was on Wilson's part. We just did everything we could to keep him in the public eye. When he and IBM and some other industries and the National Bureau of Standards sponsored a conference on large scale computing in Washington, we promoted it, even though it had next to nothing to do with Cornell. He became one of about a dozen almost-celebrity professors at Cornell. The only person more quoted, day in and out, was Carl Sagan.After Wilson had convinced the Washington establishment and the people holding the purse strings to spend some big money on scientific supercomputing, he had to step back from the role of neutral adviser and apply for some of the money himself. And somewhere along the line, the Theory Center became the Theory and Simulation Center, and it was to be for engineering and not just science. Must be someone figured out that there's a reason why the Fortune 500 doesn't include companies called International Business Theories or General Theoretical Motors. The co-investigators in the proposal to the National Science Foundation were Wilson; Dr. Kenneth King, who is also a physicist by background and is the computer czar for Cornell; and Ravi Sudan, also a physicist and an engineer who runs a lab for plasma fusion studies.During the time the proposal to the federal government was being reviewed — for months — we couldn't say much about the Theory Center. It's considered bad form to discuss something you're certain you will get. And if you don't get it, you look really silly.So instead, we concentrated on one little phase of the Theory Center, one that was already going on. This was the so-called GIBBS Project, an attempt by Wilson and some of the computer scientists here to create an entirely new scientific programming language to replace FORTRAN. We asked the public relations firm that represents the College of Engineering, of which computer science is a part, to push the GIBBS Project and they tried. It got some attention in the trade press and in places Like Science magazine, not too bad for something that wasn't hatched yet. The Theory Center, itself, wasn't real for a long time either. The Cornell faculty had given its consent and so had the University Board of Trustees. But Cornell's President, Frank Rhodes, wouldn't allow it to be established until Wilson could show some evidence of funding. They had an office with a name on the door and some furniture and a couple of people, but they didn't exist as far as Cornell University was concerned. We took to calling them the Theoretical Theory Center.We also started planning how we would announce the center when it was funded, which everyone said it would be, except that was a secret. We started preparing with the National Science Foundation's public information people to make an announcement. They told us they were afraid of a leak, ahead of the official announcement, and it could come from Congress. We thought they meant congressmen from California or Illinois or someplace. Surely, no elected official from New York would engage in something as sleazy as pork barreling, then spill the beans. Remember, I told you something would go wrong…In our brainstorming sessions, Ken Wilson made a demand that caused some snickers and mumblings of “Boy, is he naive.” He wanted to create the impression in the public mind that all of upstate New York was the next up-and-coming high technology region in the country. That all the isolated high tech areas like Rochester and Schenectady and Syracuse could be working together, rather than in competition, and that they could be linked electronically, by the Digital Thruway. The next Silicon Valley! And he wanted that impression created and established before the Theory Center was established, so that it would seem to be another piece fitting into the high tech picture. So we tossed around some names. Everything new has to have a catchy name. If New York City was the Big Apple, upstate could be the Silicon Apple. Then someone pointed out that gallium arsenide was the next hot semiconductor material and maybe we should be the Gallium Arsenide Apple. But that sounded too much like something the wicked witch would give Snow White. We talked about how the state could become involved. We tried to point out that impressions of prosperity and high tech environments aren't created overnight. Nobody knew they loved New York until millions of dollars worth of jingles and bumper stickers and billboards told them so.I guess we sensed a few inferiority complexes showing in these men who were about to pull off an astounding achievement — to persuade the federal government and the biggest computer company in the world to risk tens of millions of dollars. I remember Ken King telling of a telephone call he had just received from an acquaintance at another university who said, “Congratulations, but you got the booby prize.” He meant that Cornell — although it hadn't been announced yet — would be the fourth last-minute center funded by the government, and that we had to team up with a company that didn't even make supercomputers to do it.We tried to point out that Cornell didn't need to apologize for being the odd man in or out or wherever, because we had the element of surprise on our side. Everyone would want to know why the government was designating a private university in the middle of nowhere as a national center. We said that a couple of times, then shut up, We thought we still had three months to prepare for the announcement.At one point, some thought was given to hiring the same public relations firm that represents the manufacturer of the array processors the Theory Center uses, Floating Point Systems, to represent Cornell as well. They talked a Lot about “building understanding” which is something that p.r. people are big on. “Building understanding” is p.r. shorthand for building understanding of my point of view and convincing you of it. The firm wanted $40,000 to make the announcement of the Theory Center, and that was just to the trade press. It occurred to us that for $40,000 we could parachute Ken Wilson to the roof of every one of the top 100 newspapers in the country to personally hand a news release to the editor. We told them we'd think about it. We thought we still had two months to prepare for an announcement, sometime in the middle of April.In the meantime, we began preparing background information on the supercomputer center. We did a story saying that supercomputing will benefit American business, that “the advanced power of supercomputing and the research discoveries it makes possible promise to improve the entire corporate production cycle, from conception of a product through manufacturing to distribution.”We did a story saying that the marriage of supercomputing and three-dimensional, real-time computer graphics would be the greatest advance in communication since cavemen started painting on walls.We prepared a background piece saying that “Cornell University is a promising Location for a national, advanced scientific computing center because of its experience in operating highly successful interdisciplinary centers for the benefit of the scientific research community.” And we took the opportunity to brag about the Cornell Manufacturing Engineering and Productivity Program (COMEPP) and the Cornell High Energy Synchrotron Source (CHESS) and the Materials Science Center and the National Research and Resource Facility for Submicron Structures (which spells NRRFSS) and the Cornell Biotechnology Institute and the Semiconductor Research Corporation Center of Excellence in Microscience and Technology (which doesn't spell anything).We did another story saying the research uses of the supercomputer will range from “the study of galaxies to subatomic particles, from the motion of drifting continents to the movement of toxic wastes.”We wrote a general news release on the announcement, Leaving blanks for the amount of money and the number of years and the actual date of the announcement. We thought we still had a month to get ready.We solicited statement of congratulations from New York Governor Mario Cuomo and from the congressional delegation from this part of the state and from IBM vice president Jack Keuhler.When we were writing our news releases, by the way, we had to be careful not to mention IBM in the same breath — or even the same paragraph — with the word supercomputers. That directive came down from on high at IBM. IBM was not in the supercomputer business. Never had been, never will be. We couldn't even say the 3084QX would be a building block of a supercomputer. IBM was just giving us $30 million because they like us.We prepared biographies of all key personnel involved — all the way from Cornell President Frink Rhodes, who doesn't know anything about computers but who is able, with a little prompting, to speak eloquently on any issue and thank people for giving us money — down through all the vice presidents of: the university and provosts to the principal investigators in the Theory Center grant to the people who will really run the facility.Then we sat back and waited. Until February 20th, a Wednesday, when the NSF told us the announcement would be made the following Monday morning, in Washington. That didn't bother us. We were ready. We decided to schedule not one, not two, but three simultaneous press conferences. We would send Ken Wilson to the NSF press conference in Washington, along with a couple people from our office to straighten his tie and tuck in his shirt tails. We sent President Rhodes and Vice Provost King and Professor Ravi Sudan to New York for a press conference at Cornell Medical College. And we kept one Cornell vice president and one provost and the executive director of Theory Center, Bill Schrader, and the head of Theorynet, Alison Brown, for a simultaneous press conference in Ithaca.When we announce a press conference, we are very cagey. We try not to give away very much of the story — just enough to entice people to turn out. There's a reason for this. If you give away the story and if it's worth anything, the news people — being in a very competitive business — will try to run with it, and spoil your announcement. It happens every time. So we said something Like: Cornell University, the National Science Foundation and a major manufacturer of computing equipment will announce the start of a $60 million cooperative research venture at 10:30 a.m. Monday, February 25, at the following locations: …. Then we swore everybody to secrecy, everybody who might get calls Late at night or even be likely to talk in their sleep. There was one exception to that: We lined up an interview with Ken Wilson and The New York Times for Monday morning, just preceding the scheduled announcement. Then we sat back and waited.Imagine our surprise, on Saturday morning, February 23, to start receiving calls from news media all over New York State: “Could someone comment on the D'Amato announcement?” The D'Amato announcement? They read from a press release: “Senator Alfonse M. D'Amato (R, NY) is pleased to announce that Cornell University will receive at least $30 million and possibly up to $60 million from the National Science Foundation and up to $35 million from International Business Machines Corp. to do fundamental research from America's next generation of supercomputers.” Senator D'Amato said the grants would make Cornell one of the leading institutions in the country.That surprised us a little. We thought we were already a Leading institution. We are among the top four or five or six research universities in the country, and our academic reputation isn't too shabby, either.It was obvious what Sen. D'Amato was up to. He serves on one or two committees that some time in the past had reviewed the NSF proposals, probably voted for an appropriation when it seemed that some of the money might go to his home state, and placed a note in his future file: Break this on a slow news day and take credit for it.We tried to persuade D'Amato and his staff to back off, to correct his misinformation, to join us in a joint announcement on Monday and to shut up in the meantime. No go. He said the NSF had given him the green light to make the announcement. The NSF was furious. They refused to confirm that Cornell would even get a nickel, they started an investigation to determine how the leak occurred, and they blamed Cornell for prompting Sen. D'Amato to jump the gun and spoil the announcement for the other three centers around the country. The congressmen in the House of Representatives who really had gone to bat for Cornell on this one weren't too happy either. But they knew that D'Amato had a reputation for this sort of thing.Of course the news was in the Sunday papers all around New York the next day. That kind of money, even if the figures are incorrect, gets people's attention. The stories said that Cornell officials refused to comment, except to say that Sen. D'Amato was mistaken, and that an announcement would be made on Monday. It looked like we had something to hide. Or at least that we were caught off guard.Which we weren't. We had even given Ken Wilson media training. We had put him in front of our own TV cameras. We had dry runs of press conferences. We asked him the toughest, the stupidest, the most repetitive questions we could think of. We tried to teach him to look at the camera, not to fidget or play with parts of his clothing, not to look to the heavens or into his pocket for answers. In short, to give the impression that every question he gets is the most perceptive and original he has ever heard, deserving of a sensitive, profound answer that he just thought of. We weren't trying to make a Carl Sagan of him, just to help him come across as the intelligent person that he really is.Then we packed everybody off to their respective press conferences. All in all, they went pretty well, considering that practically nobody came to the New York City press conference and that the University of Illinois beat the pants off us in the Washington conference. The problem at the NSF press conference wag Larry Smarr, the Illinois astrophysicist, who was savvy enough to jump up and answer questions that weren't directed to anybody in particular. (We made a note to train Wilson to do that the next time.) Larry Smarr told a little story that any of the supercomputer people could have told but he thought of it first, and it's been quoted everywhere since then. He said that supercomputers are so scarce in this country that in order to do his research he had to go to Germany to find time on a machine — a machine that was made in this country. No big deal, but it was the kind of anecdote that writers love and lots of them used it in their stories.We knew our homework was paying off, however, when we saw that Ken Wilson was quoted on the front page of The New York Times, even before they mentioned the other places that have supercomputer centers. Larry Smarr had a snappier quote, but it was in the second section of the paper, and besides, IBM was quoted as saying that Cornell's approach to building supercomputers was the only one IBM would consider exploring, not that IBM was particularly interested in making supercomputers, of course.You have before you copies of some of the newspaper and magazine clips that appeared over the next few days. There have been lots more since. I'd like to play for you some tapes from local television stations that covered the Ithaca press conference. They go from bad to worse. We in the public information business collect these things to try to figure out how the information that we thought was presented so clearly gets so screwed up on the six o'clock news. Later I'll play a segment from a public affairs show on a Local PBS station, and then a piece that one of the national networks did — a really first rate job. Those of you who live in major media centers will be educated on what small town television is like. Those of you from small towns can sympathize.",,Segelken R,,1986,146–160,10.1145/10563.10588,https://doi-org.proxy.bnl.lu/10.1145/10563.10588;http://dx.doi.org/10.1145/10563.10588,Conference Paper
An Investigation into Supply Chain Risk Factors and Their Impact on Performance of Humanitarian Pharmaceutical Supply Chain in Sub-Sahara Africa - A Case Study of the Supply Chain System for UNICEF Tanzania,"This thesis investigated the presence of supply chain risk factors and their impact on performance of humanitarian health programs in Sub-Saharan Africa, particularly UNICEF Tanzania. Supply Chain Risk management (SCRM) approach has become a major contributor to supply chain performance and to program/business success. The aim of this study was to contribute to professional practice by suggesting risk management approach (prioritisation and mitigation) as the one possible solution to the criticism that; ""Supply chain management is our Achilles heel; we receive the most criticism for this"" (UNICEF 2014). This criticism triggered this research whose aim was to systematically identify, prioritise and mitigate critical risk factors that impact on supply chain performance metrics of time, cost and quality. To achieve this aim, this study addressed two key research questions of risk prioritisation and risk treatment. A number of Supply Chain Risk Management (SCRM) studies available in literature mainly identified risks factors without much focus on prioritisation using Failure Mode and Effect Analysis (FMEA) methodology. This enquiry was abductive in reasoning and mixed methods in approach using process FMEA to quantify and analyse process risks. Besides being industry relevant, the benefit of using FMEA for this investigation included an increased focus on most imminent risks, prioritisation of risks and development of effective risk mitigation strategies.The research findings confirmed that poor risk management is the primary cause of poor supply chain performance. It also found a causal relationship between detection capability and likelihood of occurrence on a few of the risks, and a zero relationship on most risks tested. Overall, the research confirmed the proposition that effective supply chain risk management approach (prioritisation and mitigation/treatment) contributes to an improvement in supply chain performance of health programs in Sub Saharan Africa. The research findings matter in that the established risk profiles by performance metrics of delivery time, cost and quality (the SCRM Iceberg Model) can be used by supply chain managers to anticipate and proactively manage the potential risks found in their operations. The knowledge on the relationship between investment on risk detection capability and the reduction in risk occurrence challenges managers to re-assess the potential benefit of every investment on risk detection. The suggested context specific challenges and opportunities identified in this study, if applied rationally can help effectively manage supply chain risks for humanitarian operations in Sub-Saharan Africa and similar context globally.",,Sheshe F,,2018,,,,Ph.D. Thesis
The Adoption of Security Control Apps among Smartphone Users in Tanzania,"Threats to mobile devices and smartphones, in particular, are on the rise, suggesting that data and information residing in the mobile device such as smartphones are in danger of being attacked. The current study employs an extended TBP as a theoretical framework to investigate the adoption of security control apps (i.e. antivirus) to safeguard against the attacks. A theoretical framework was tested using structural equation modelling (SEM) with data collected from 233 respondents. The study found that social influence, attitude and security awareness have an influence on the intention to adopt antivirus software while perceived behavioral control and individual risk propensity have no influence. Further security awareness has an influence on the attitude of smartphone users towards using antivirus software.",,"Koloseni D,Sedoyeka EM",,2019,1–18,10.4018/IJTD.2019100101,https://doi-org.proxy.bnl.lu/10.4018/IJTD.2019100101;http://dx.doi.org/10.4018/IJTD.2019100101,Journal Article
Diakoptic Assessment of Power System Voltage Variations and Applications in Weak Grids Introduced by Wind Energy : The Nigerian Power System in Perspective,"The electrical power system is a large interconnected system made up of electrical components to generate, transport and utilize electrical power. The size and complexity of the power system have increased significantly in recent years due to the introduction of wind energy and other renewable energy sources. Hence there is an urgent need to search for new or improved analytical tools for the system performance evaluation and assessment. Load flow analysis is the most important method of assessing the steady-state behaviour of all the components of the power network. The common approach in load flow analysis is to study the network as one-piece and this can take a long time for a very large system. An alternative solution is to reduce the size of the network by tearing apart a large system into small subnetworks, thus a cluster of computers can be supplemented to speed-up the process. Then the system can be solved as separate entities after which their solutions are connected together by mathematical modelling in order to obtain the solution of the original system as if it was solved as one-piece. This method in its original conception is known as diakoptics which, though was conceived for power systems analysis, is now widely viewed as a mathematical method rather than a power systems analysis tool. The work presented in this thesis proposes two novel diakoptic tools for the power system analysis; i.e. the branch voltage multiplier technique (BVMT) and slack bus voltage updating diakoptics (SVUD). Various research works so far have shown that the key factor is in the process of obtaining the fundamental equations of diakoptics and the final equation of solution. The BVMT is a variant of the original diakoptic algorithm mainly by the process of obtaining the diakoptic equations of solution which can reduce the number of solution steps and simplify the method considerably. The resultant algorithm is easier to apply and also more effective in load flow analysis by current injection methods where the relationship is linear. The common practice in present load flow analyses is by power injection which yields nonlinear equations. The BVMT technique has been extended by applying various transformations which make it suitable for nonlinear solutions. This yields the SVUD load flow method that incorporates the classical Gauss-Seidel method. The analysis tools produced have been validated by applying to sample systems including IEEE benchmark systems. In one-piece load flow analysis, the usual practice is to choose one slack bus whose voltage remains unchanged throughout the iterative process. In diakoptic analysis, the systems to be analysed are more than one after tearing, so the subnetworks without the original slack bus will require temporary slack buses during the load flow analysis. During iteration, the voltages of these temporary slack buses would also remain unchanged; the SVUD method ensures that their voltages vary to reflect the state they would have been in one-piece solutions. This is achieved by updating the voltages during iteration using given and computed parameters which, in this case, are the complex powers. This has resulted in the improvement of the convergence characteristics of the traditional Gauss-Seidel method. For example, in the analysis of the IEEE 30-bus network, the number of iterations in one-piece Gauss-Seidel solution was 202 while with the SVUD, the numbers of iterations were 17 when cut into two subnetworks, and 13 when cut into three subnetworks. The SVUD also removes some common problems associated with temporary slack buses. This is demonstrated in the analysis of the 14-bus system using the one-piece Gauss-Seidel, SVUD and diakoptics with the temporary slack bus voltage remaining unchanged during iteration. The one-piece method converged after 106 iterations and the SVUD converged after 5 iterations. The diakoptic analysis with constant temporary slack-bus voltage converged after 146 iterations and erroneous results were obtained. The SVUD analysis of the Nigeria 330kV power transmission network without wind power electricity shows a voltage profile with some violations at a number of buses. The analysis with wind power electricity shows voltage rises, especially at buses close to the point of common coupling. This is a generally accepted effect of electricity from wind on an existing power system. In the Nigeria case, the conventional generation capacity is far below what is required and so the voltage profile is seen to improve because the wind farm constitutes an extra generating capacity. For example, at rated wind power, the voltage magnitudes show rises of 0.12% at bus 31 and 0.15% at bus 32. Increase of wind power to 4.1018 pu shows rises of 3.9% at bus 31 and 4.5% at bus 32.",,Olobaniyi F,,2015,,,,Ph.D. Thesis
German Typographers vs. German Grammar: Decomposition of Wikipedia Category Labels into Attribute-Value Pairs,"Given an instance (Julieta Pinto), most methods for open-domain information extraction focus on acquiring knowledge in the form of either class labels (Costa Rican short story writers, Women novelists) referring to concepts to which the instance belongs; or facts (nationality: Costa Rica) connecting the instance (Julieta Pinto) to other instances or concepts (Costa Rica), where the fact and the other instance often take the form of an attribute (nationality) and a value (Costa Rica) respectively. From extraction through internal representation and storage, class labels and facts are treated as if they carved out disconnected slices within the larger space of factual knowledge. This paper argues that class labels and facts pertaining to an instance exist in symbiosis rather than as a dichotomy. A constituent (Costa Rican) within a class label (Costa Rican short story writers) of an instance may be indicative of a fact (nationality: Costa Rica) applicable to the instance and vice-versa. As an illustration of the relationship between class labels and facts, the paper introduces an open-domain method for the better understanding of the semantics of class labels in one of the larger and most widely-used repositories of knowledge, namely the categories in the Wikipedia category network. The method exploits the category network to associate constituents (Costa Rican) within names of Wikipedia categories, with attributes (nationality) that explain their role.",,Paşca M,,2017,315–324,10.1145/3018661.3018662,https://doi-org.proxy.bnl.lu/10.1145/3018661.3018662;http://dx.doi.org/10.1145/3018661.3018662,Conference Paper
Enhancing Dengue Fever Modeling through a Multi-Scale Analysis Framework - a Case Study in the Central Valley of Costa Rica,"Dengue fever is the second most widespread tropical disease after malaria and affects populations of more than 100 countries (Derouich and Boutayeb 2006). It is considered one of the most severe viral diseases in terms of morbidity and mortality (Guzmán and Kourí 2004). Over the last decade, dengue fever has become the most wide-spread vector-borne disease in Costa Rica (CCSS 2008). However, only a few research studies have been conducted in Costa Rica to investigate the factors influencing the rates of dengue fever. While GIS and statistical analysis have been used in research studies, agent-based modeling has not been applied to the study of dengue. This study emphasizes how traditional macro level GIS analysis and the implementation of a microlevel dengue fever agent-based model can be merged into a novel framework for the study of dengue fever in the Central Valley of Costa Rica. One of the main objectives was to develop an agent-based model, which integrated GIS to simulate the spread of dengue fever disease in an urban environment, as a result of an individual’s interactions in a geospatial context. Precipitation, temperature, socio-economic and demographic variables were analyzed using these technologies to identify the factors affecting the rates of dengue fever in the study area. GIS was used to map dengue risk and the spatial distribution and vulnerability of dengue risk in the study area using geographically weighted regression. The Dengue Fever Agent Based Model (DFABM) was developed using the Java programming language and the open-source MASON simulator, a multi-threaded agentbased simulation platform. The DFABM represented daily movements and interactions of people, the environment, and the vector, relative to dengue cases. The simulation examined detailed data about each scenario to identify the significant events occurring during outbreaks. The data employed included the number of susceptible, exposed, and infected people. The locations (described by longitude and latitude) and temporal data describing infected individuals were also collected for analysis. The DFABM tracked the factors affecting dengue fever, including precipitation, temperature, and the most important demographic and socio-economic characteristics of the population in the study area. The research questions guiding this study were: Does a community-level dengue fever agent-based model (DFABM) produce results comparable (agree) to those produced by traditional macro-level GIS analysis? Does a community-based dengue fever agentbased model (DFABM) enhance traditional geographic information system analysis and could it aid in predicting future dengue fever outbreaks? The findings of the communitylevel ABM generated similar results to (they were in agreement with) the traditional GIS analysis technique. Likewise, the DFABM enhanced traditional methods of analysis and could aid in predicting dengue fever outbreaks. Therefore, the coupling of GIS and ABM was the optimal research design for the study of dengue fever in the Central Valley of Costa Rica.",,Campos Rodriguez RR,,2013,,,,Ph.D. Thesis
CyberActivist: Tool for Raising Awareness on Privacy and Security of Social Media Use for Activists,"Bosnia-Herzegovina (BH) and its entity Republika Srpska (RS) are among the most fragile democratic environments in Europe. In the first phase of our long-term participatory design case study, we engaged the some of the main activists in BH/RS, providing a structured picture of their practices in recent years, concrete needs and the various constraints under which they act. Our research highlighted importance and utilization of the social media for the activism in the region, but also problems such as limited budgets and know-how of the activists, intensive outsourcing practices, and a lack of awareness regarding data privacy and cyber security. Due to the perspective of BH/RS, the rising number of threats and impact incidents, and activist experiences from other unstable regions, we propose a more structured approach to privacy and security within activist circles and non-profit organizations. As the initial step in the second phase of our study, we offered a prototype of the free web application “CyberActivist” to BH/RS activists for user tests. Based on their qualitative feedback we defined the functional and non-functional requirements on further improvement of this privacy and security awareness tool. In the next phase, we will technically address their direct feedback, as well as design recommendations from relevant research and user experience literature. We also plan to propose design method improvements, design corresponding privacy and security trainings and to further internationalize the tool.",,"Tadic B,Rohde M,Wulf V",,2018,498–510,10.1007/978-3-319-91521-0_36,https://doi-org.proxy.bnl.lu/10.1007/978-3-319-91521-0_36;http://dx.doi.org/10.1007/978-3-319-91521-0_36,Conference Paper
Nato's Experience of Supporting Security Sector Reform in the Western Balkans (1995-2015),"This thesis has considered the theoretical and practical underpinning of SSR and NATO's role in its application within two countries of the Western Balkans. It began by reviewing the extant literature on SSR and then analysed NATO's evolution and how it developed its role in assisting countries with reform of their security sectors in the aftermath of the Cold War. Unlike organisations such as the UN and EU, NATO does not have a formal policy and conceptual framework for Security Sector Reform (SSR) but uses a range of interlocking programmes that have evolved over time. The thesis examined critically NATO's interventions in both Bosnia-Herzegovina and Kosovo and how that role transformed from being a security provider to one of advising and mentoring on reform of the security sectors. A comparative analysis was then conducted of the cross-case data presented in the two case studies. The research has reinforced the reality that different actors with different agendas will inevitably complicate an already challenging situation in postconflict and post-authoritarian countries. It became evident that national agendas within the North Atlantic Council also influenced the Alliance's ability to support SSR in the two countries studied. Through a combination of both primary and secondary research the study has established that NATO still managed to add considerable value to these reform processes and has the potential for doing so in the future in other countries. There were limitations to its approach and these have been highlighted. At times NATO's contextual understanding of the situation on the ground was weak and its use of political soft power to encourage the reforms in the countries studied was eclectic but, ultimately, it has had a measure of success in its endeavours. The research has generated a framework of factors for NATO to use when considering current and potential SSR engagements. As NATO becomes more deeply involved in projecting stability through SSR support after a decade of war fighting, this list of factors could have international significance.",,Blease D,,2017,,,,Ph.D. Thesis
The Impact of Facebook and Smart Phone Usage on the Leisure Activities and College Adjustment of Students in Serbia,"This study examined the simultaneous impact of Facebook (FB) and smart phone usage (SP) on the leisure activities (LA) and college adjustment (SACQ) of students in Serbia. The moderating effects of gender on the observed relationships were also examined. An exploratory study of students in Serbia (N = 485) revealed that: 1. Students in Serbia spend a daily average of 2.76 h on Facebook, while the average total daily smart phone use is 8:34 h 2. Facebook and smart phone use has no decisive influence on the allocation of time for leisure activities and college adjustment. 3. Facebook and smart phone use still has a certain positive effect on leisure activities, but this influence can become slightly negative if Facebook and smart phones are used too much. 4. In cases of a lack of time, students are more likely to sacrifice academic work, rather than time for Facebook, smart phones, or leisure activities. 5. The moderating effects of students' gender on the observed relationships are weakly expressed. Finally, Facebook and smart phone use has become a common and integral part of life for the majority of Serbian students, and the time for these activities is integrated into their overall time. Presented are the results of the influence of Facebook and smart phone usage.The questionnaires were completed by 485 students.Facebook and smart phone use has no decisive influence o leisure activities.Slightly negative influence if Facebook and smart phones are used too much.Sacrificing academic work, rather than time for Facebook.",,"Janković B,Nikolić M,Vukonjanski J,Terek E",,2016,354–363,10.1016/j.chb.2015.09.022,https://doi-org.proxy.bnl.lu/10.1016/j.chb.2015.09.022;http://dx.doi.org/10.1016/j.chb.2015.09.022,Journal Article
The Nature of Security: A Conceptual Framework for Integral-Comprehensive Modeling of IT Security and Cybersecurity,,,Villalón-Fonseca R,,2022,,10.1016/j.cose.2022.102805,https://doi-org.proxy.bnl.lu/10.1016/j.cose.2022.102805;http://dx.doi.org/10.1016/j.cose.2022.102805,Journal Article
Identifying the Implied: Findings from Three Differentiated Replications on the Use of Security Requirements Templates,"Identifying security requirements early on can lay the foundation for secure software development. Security requirements are often implied by existing functional requirements but are mostly left unspecified. The Security Discoverer (SD) process automatically identifies security implications of individual requirements sentences and suggests applicable security requirements templates. The objective of this research is to support requirements analysts in identifying security requirements by automating the suggestion of security requirements templates that are implied by existing functional requirements. We conducted a controlled experiment in a graduate-level security class at North Carolina State University (NCSU) to evaluate the SD process in eliciting implied security requirements in 2014. We have subsequently conducted three differentiated replications to evaluate the generalizability and applicability of the initial findings. The replications were conducted across three countries at the University of Trento, NCSU, and the University of Costa Rica. We evaluated the responses of the 205 total participants in terms of quality, coverage, relevance and efficiency. We also develop shared insights regarding the impact of context factors such as time, motivation and support, on the study outcomes and provide lessons learned in conducting the replications. Treatment group, using the SD process, performed significantly better than the control group (at p-value <0.05) in terms of the coverage of the identified security requirements and efficiency of the requirements elicitation process in two of the three replications, supporting the findings of the original study. Participants in the treatment group identified 84 % more security requirements in the oracle as compared to the control group on average. Overall, 80 % of the 111 participants in the treatment group were favorable towards the use of templates in identifying security requirements. Our qualitative findings indicate that participants may be able to differentiate between relevant and extraneous templates suggestions and be more inclined to fill in the templates with additional support. Security requirements templates capture the security knowledge of multiple experts and can support the security requirements elicitation process when automatically suggested, making the implied security requirements more evident. However, individual participants may still miss out on identifying a number of security requirements due to empirical constraints as well as potential limitations on knowledge and security expertise.",,"Riaz M,King J,Slankas J,Williams L,Massacci F,Quesada-López C,Jenkins M",,2017,2127–2178,10.1007/s10664-016-9481-1,https://doi-org.proxy.bnl.lu/10.1007/s10664-016-9481-1;http://dx.doi.org/10.1007/s10664-016-9481-1,Journal Article
Global Change Implications of Adaptation to Climatic Variability,"In this thesis I have examined ecology and evolution in a globally changing environment to address how climate change is likely to differentially affect tropical and temperate populations. I have approached the problem theoretically by applying an evolutionary model to global climate data and I have tested specific hypotheses arising from the model and associated theories using checkerspot butterfly populations throughout North and Central America.The study of climatic variability and adaptation has a long history going back to Alexander von Humboldt. I briefly review this history in chapter 1. In chapter 2 I applied an evolutionary model based on the ""jack-of-all-trades is a master of none'' principle to global temperature and precipitation data to derive theoretical tolerance curves throughout the globe. Primarily, this model predicts lower thermal tolerance breadths for tropical organisms because over a given year they experience far less thermal variation than temperate populations. The pattern is more complicated when considering precipitation effects. Using IPCC projections of climate change for 2100 I then examined what the relative fitness effects would be across the globe and found that other than the Mediterranean region, areas of the tropics (e.g. Southeast Asia, Central America and Equatorial Africa) would be among the most heavily impacted regions in the world (Bonebrake and Mastrandrea 2010).In chapters 3 and 4 I built on work within temperate checkerspot butterfly systems to examine climatic heterogeneity impacts on ecological and evolutionary process. The heavily studied Euphydryas editha population of Jasper Ridge at Stanford University went extinct in 1998 partly due to increases in precipitation variability in recent decades and its effects on host plant dynamics. To evaluate the potential of habitat creation to buffer the impacts of climatic changes, I participated in a multidisciplinary effort to experimentally alter soil conditions to mimic the unique properties of serpentine grasslands. Our efforts showed some success in lowering the invasibility (invasive species being another threat to E. editha) of experimental sites but also showed that the results were highly contingent on the amount of rainfall in each year (Bonebrake et al. in review). I also studied the evolutionary relationship between oviposition behavior and offspring performance in closely related Euphydryas gillettii. This study showed large effects of inter-annual variation in temperature in a montane E. gillettii Colorado population and that, for example, warm years and oviposition preference can significantly accelerate larval development time (Bonebrake et al. 2010).Finally, in chapter 5 I examined the biophysical, morphological and physiological properties of adaptation to temperate and tropical climates using populations of the widely distributed butterfly Chlosyne lacinia. First, the biophysical model demonstrated that variation in air temperature poorly predicted variation in butterfly body temperature (Tb) and that diurnal variation in Tb was lower for butterflies in El Salvador than Tb variation in Arizona. Second, as for morphological characteristics relevant to butterfly thermoregulation, thorax size was consistently smaller in tropical populations while variation in other characters (fur length, body length and wing absorptivity) was not consistent across latitude. Third, physiological studies showed that C. lacinia in Arizona began flying at colder temperatures (Tb=24 °C) than C. lacinia in El Salvador (Tb=27 °C). As consequence of these factors combined, the model predicts increased effects of climate change for tropical C. lacinia in the form of greater increases in flight activity time and more prolonged exposure to lethal temperatures (Tb> 45 °C) relative to temperate butterflies (Bonebrake et al. in prep).",,Bonebrake TC,,2010,,,,Ph.D. Thesis
A Flexible Service-Oriented Approach to Address Hydroinformatic Challenges in Large-Scale Hydrologic Predictions,"Water security is defined as a combination of water for achieving our goals as a society, and an acceptable level of water-related risks. Hydrologic modeling can be used to predict streamflow and aid in the decision-making process with the goal of attaining water security.Developed countries usually have their own hydrologic models; however, developing countries often lack hydrologic models due to factors such as the maintenance, computational costs, and technical capacity needed to run models. A global streamflow prediction system (GSPS) would help decrease vulnerabilities in developing countries and fill gaps in areas where no local models exist by providing extensive results that can be filtered for specific locations.The development of a GSPS has been deemed a grand challenge of the hydrologic community. To this end, many scientists and engineers have started to develop large-scale systems to an acceptable degree of success. Renowned models like the Global Flood Awareness System (GloFAS), the US National Water Model (NWM), and NASA's Land Assimilation System (LDAS) are proof that our ability to model large areas has improved remarkably. Even so, during this evolution the hydrologic community has started to realize that having a large-scale forecasting system does not make it immediately useful. New hydroinformatic challenges have surfaced that prevent these models from reaching their full potential. I have divided these challenges in four main categories: big data, data communication, adoption, and validation.I present a description of the background leading to the development of a GSPS including existing models, and the components needed to create an operational system. A case study with the NWM is also presented where I address the big data and data communication challenges by developing cyberinfrastructure and accessibility tools such as web applications and services.Finally, I used the GloFAS-RAPID model to create a forecasting system covering Africa, North America, South America, and South Asia using a service-oriented approach that includes the development of web applications, and services for providing improved data accessibility, and helping address adoption and validation challenges. I have developed customized services in collaboration with countries that include Argentina, Bangladesh, Colombia, Peru, Nepal, and the Dominican Republic. I also conducted validation tests to ensure that results are acceptable. Overall, a model-agnostic approach to operationalize a GSPS and provide meaningful results at the local level is provided with the potential to allow decision makers to focus on solving some of the most pressing water-related issues we face as a society.",,Souffront Alcantara MA,,2018,,,,Ph.D. Thesis
Listening In: Cybersecurity in an Insecure Age,"A cybersecurity expert and former Google privacy analyst's urgent call to protect devices and networks against malicious hackers and misinformed policymakers. New technologies have provided both incredible convenience and new threats. The same kinds of digital networks that allow you to hail a ride using your smartphone let power grid operators control a country's electricity - and these personal, corporate, and government systems are all vulnerable. In Ukraine, unknown hackers shut off electricity to nearly 230,000 people for six hours. North Korean hackers destroyed networks at Sony Pictures in retaliation for a film that mocked Kim Jong-un. And Russian cyberattackers leaked Democratic National Committee emails in an attempt to sway a US presidential election. And yet despite such documented risks, government agencies, whose investigations and surveillance are stymied by encryption, push for a weakening of protections. In this accessible and riveting book, Susan Landau makes a compelling case for the need to secure our data, explaining how we must maintain cybersecurity in an insecure age.",,Landau S,,2018,,,,Book
Listening In: Cybersecurity in an Insecure Age,"A cybersecurity expert and former Google privacy analysts urgent call to protect devices and networks against malicious hackers New technologies have provided both incredible convenience and new threats. The same kinds of digital networks that allow you to hail a ride using your smartphone let power grid operators control a countrys electricityand these personal, corporate, and government systems are all vulnerable. In Ukraine, unknown hackers shut off electricity to nearly 230,000 people for six hours. North Korean hackers destroyed networks at Sony Pictures in retaliation for a film that mocked Kim Jong-un. And Russian cyberattackers leaked Democratic National Committee emails in an attempt to sway a U.S. presidential election. And yet despite such documented risks, government agencies, whose investigations and surveillance are stymied by encryption, push for a weakening of protections. In this accessible and riveting read, Susan Landau makes a compelling case for the need to secure our data, explaining how we must maintain cybersecurity in an insecure age.",,Landau S,,2017,,,,Book
Gabor Filter Bank with Deep Autoencoder Based Face Recognition System,,,"Hammouche R,Attia A,Akhrouf S,Akhtar Z",,2022,,10.1016/j.eswa.2022.116743,https://doi-org.proxy.bnl.lu/10.1016/j.eswa.2022.116743;http://dx.doi.org/10.1016/j.eswa.2022.116743,Journal Article
SOSerbia: Android-Based Software Platform for Sending Emergency Messages,"This paper presents Android-based SOS platform named SOSerbia for sending emergency messages by citizens in Serbia. The heart of the platform is SOS client Android application which is an easy and simple solution for sending SOS messages with unique combination of volume buttons. The proposed platform solves a lot of safety, security, and emergency problems for people who can be in dangerous situations. After a person presses a correct combination of buttons, a message with his or her location is sent to the operating center of the Serbian Police. The platform merges several appropriately combined advanced Android technologies into one complete solution. The proposed solution also uses the Google location API for getting user’s location and Media Player broadcast receiver for reading pressed buttons for volume. This logic can be also customized for any other mobile operating system. In other words, the proposed architecture can be also implemented in iOS or Windows OS. It should be noted that the proposed architecture is optimized for different mobile devices. It is also implemented with simple widget and background process based on location. The proposed platform is experimentally demonstrated as a part of emergency response center at the Ministry of Interior of the Republic of Serbia. This platform overcomes real-life problems that other state-of-the-art solutions introduce and can be applied and integrated easily in any national police and e-government systems.",,"Jovanovic M,Babic I,Cabarkapa M,Misic J,Mijalkovic S,Nikolic V,Randjelovic D,Fuentes M",,2018,,10.1155/2018/8283919,https://doi-org.proxy.bnl.lu/10.1155/2018/8283919;http://dx.doi.org/10.1155/2018/8283919,Journal Article
Making Encryption Work in the Cloud,"Many enterprises are choosing to leverage cloud services because of their simplicity and cost effectiveness. However, concerns over government inspection of data, service provider breaches, and insufficient access controls are driving conversations about information security controls in the cloud. Specifically, enterprise and Software as a Service (SaaS) providers have a particularly high interest in using cryptographic techniques for protecting data.Concerns over government inspection of data, service provider breaches, and insufficient access controls are driving conversations about information security controls in the cloud.Security practitioners are seeking clarity on the relative strengths and weaknesses of various encryption schemes, the trade-offs between security and application functionality, and the adverse impact that selecting the wrong encryption scheme may have on business operations. Alexandra Boldyreva at Georgia Tech and Paul Grubbs of Skyhigh Networks examine the options.",,"Boldyreva A,Grubbs P",,2014,8–10,10.1016/S1353-4858(14)70101-1,https://doi-org.proxy.bnl.lu/10.1016/S1353-4858(14)70101-1;http://dx.doi.org/10.1016/S1353-4858(14)70101-1,Journal Article
Implementing Data Security in Student Lifecycle Management System at the University of Prishtina,"In this paper is presented a novel approach for fulfilling the data security criteria in a Student Lifecycle Management System at the University of Prishtina. The four main criteria of data security such as: privacy, authentication, integrity and non-repudiation are fulfilled through carefully selected security policies. Student data privacy is achieved using the Secure Socket Layer protocol for web communication with web server. Each user, being student, academic or administrative staff is provided with unique user name and initial password in the Student Lifecycle Management System. Data integrity and non-repudiation are fulfilled using digital signatures. The novelty of implemented solution is based on extending the subject name in X.509 digital certificates and using this certificate for securing student grades, which is in full compliance with the Kosovo Law on Information Society. Public Key Infrastructure and X.509 digital certificates have been established as the most trustworthy methods for assuring data security criteria in modern software applications. Security policy enforces that digital certificate and its associated private key shall be stored in a smart card. Access to private key stored in a smart card is protected by Personal Identification Number, known only by smart card holder. This implementation was installed at the Faculty of Electrical and Computer Engineering and has successfully passed a six semester testing period and students were, for the first time in the history of the University of Prishtina, able to apply online to take an exam.",,"Rexha B,Lajqi H,Limani M",,2010,965–974,,,Journal Article
Anomaly Detection of Scada Networks Through Network Measurement Study,"Despite all the increasing research efforts in industrial control systems (ICS), these systems still fail to defend themselves at the time of some high-profile cyber attacks. The most high-profile attack events include but not limited to the Stuxnet attack on an Iranian nuclear power plant in 2010, the Industroyer malware attack on the Ukrainian power grid in 2016, and the recent ransomware attack on the U.S. Colonial pipeline in May 2021, which severely limit the fuel supply to half of the east coast. The Supervisory Control and Data Acquisition (SCADA) networks expose themselves to a broader attack surface after the migration from serial communication network to TCP/IP compatible networks. Therefore, they are susceptible to cyber-attacks. Another main reason why the security and resilience of SCADA networks have limited improvements over the decade is that, the majority of the previous work does not have access to real-world systems or datasets. Because it is not possible to interrupt the production process with penetration tests, and not easy to earn the trust of the operators.In the prelusive chapter of this dissertation, we first introduce the concepts of SCADA and industrial control protocols. Then we review the previous work divided by energy sectors in the critical infrastructure, which enables us to recognize contributions, identify limitations, raise research questions, and discover answers. With network captures from the SCADA networks in operational industrial control systems, specifically the power grid and the natural gas distribution network, we launch our project with the reversal of the SCADA network topology with different levels of system knowledge, and show that even in the least bliss, one can still conduct network discovery to the majority of network nodes. The later characteristics we extract from the communication conversations between substations and control servers challenge the long-term understanding of the SCADA network in the security community. The primary industrial protocol under investigation is IEC 60870-5-104, an application-layer protocol designed to control and monitor the physical processes in federated SCADA networks.With the knowledge base obtained from network characterization, then we propose network flow based anomaly detection method by applying unsupervised clustering of the network flows, and process-based anomaly detection. The anomaly detection is based on profiling process variables, by applying gradient boosting tree algorithm and deep neural networks on time series datasets. Both work flows are experimented with datasets divided by our system knowledge levels range from the system operators help verifying the majority of network topology and hardware devices, to no support at all.Approaching from the perspective of network measurements, our goal is to establish the normal behavior baseline of the anomaly detector by applying deep-packet inspection, and have captured several intriguing outliers and process anomalies that are not available in a simulation/emulation environment. After successfully training of the gradient boosting based detector, we use feature importance analysis to mitigate the existing limitation of black-boxed machine learning applications, and quantify the contribution of features leading to the detection result.The contributions of this dissertation are as follows:- Provide solid testimonies that shred the security community's consensus of SCADA networks being stable and predictable, from the overall network topology to the subtleties in the process variables- Construct the first network characterization for an operational bulk power grid, that offers the first view of the unique difficulties in defending a federated SCADA network- Implement the first process-aware anomaly detector for two operational SCADA networks, one bulk power grid and one gas pipeline network, that successfully identifies the process anomalies and potentially dangerous mis-configuration errors- Present the discussion of the ambiguous understanding of false positives in the anomaly detection for ICS, with the valuable insight from the study of real-world datasets.",,"Qin X,Chen Qian,Yu Zhang",,2022,,,,Ph.D. Thesis
"Digital Educational Environment of a Modern University: Theory, Practice and Administration","The article reveals theoretical and practical aspects of the digital educational environment of a university. The main normative and legal documents of Ukraine regulating the informatization of the sphere of national education are determined. The experience of introduction of the system of electronic educational courses by the leading institutions of higher education of Ukraine is analysed; the concepts of “distance education”, “digital educational environment”, “educational management” are specified. It has been found that education is a social institution with its own laws, principles and regulations, so the ability to manage education is as important and difficult as finding the right vector for development of all mankind. The benefits of education transformation are listed: development of students’ self-determination, ability to concentrate on the most valuable teaching material; increase of mobility of personality, ability to adapt to the dynamic environment; ensuring cooperation with diverse audiences; creating an individualized educational trajectory of the student; comfortable learning environment. An attempt is made to identify the definition of “digital educational environment” as a set of relevant resources that is able to ensure the implementation of educational, scientific, international and managerial activities of higher educational institutions. It was established that higher educational institutions of Ukraine in the conditions of distance learning increase the capacity of the digital educational environment. The conditions and modern vectors of information educational development are considered, and the basic problems, needed to be resolved at the state level, are defined. Strengths (flexible schedule of educational tasks, provision of inclusiveness, control and evaluation of the results of educational activities, individual consultations in remote mode, etc.) and weaknesses revealed of the development of the digital educational environment (the delay in the creation of digital training courses, lack of information literacy of teachers, low level of integration of digital learning environment and teaching disciplines, etc.). Presented the model of digital education environment of the university from the position of organizational and administrative activity. Described four operational modules of the specified model: scientific and technical module (repository, open publication system, digitalization of the library fund); educational module (electronic management system of educational courses, online learning, control of students’ knowledge quality); administrative module (electronic document management, education environment management, digital archive, online questionnaires, operational process management, digital security systems, innovative activities in the education and information environment); informational module (official website of the institution of higher education, personal pages of teachers, 3D-courses, pages of the university in social networks). It is established that the level of compliance of all activities of the designated operational areas is an indicator of the successful functioning of the university under the conditions of digitalization of the educational environment.",,"Vasyliuk TG,Lysokon IO,Shimko IM",,2022,161–168,10.1145/3526242.3526260,https://doi-org.proxy.bnl.lu/10.1145/3526242.3526260;http://dx.doi.org/10.1145/3526242.3526260,Conference Paper
Using Smart Cards and X.509 Digital Certificates for a Student Management Information System at the University of Prishtina,"In this paper is presented a novel software solution for the implemention of a Student Management Information System at the University of Prishtina. The novelty of implemented solution is based on extending the subject name in X.509 digital certificates and using this certificate for securing student grades. The issued X.509 digital certificate is used to digitally sign the student grades, which is in full compliance with the Kosovo Law on Information Society. For security reasons, the certificate and its associated private are stored in a smart card. The access to private key is protected by a personal identification number. The protection of the student grades against misuse was a ""must have"" feature of the software solution for the management of the university. This implementation was installed at the Faculty of Electrical and Computer Engineering and has successfully passed a six semester testing period. Beyond increasing the security of the systems, students were, for the first time in the history of the University of Prishtina, able to apply online to take an exam.",,"Rexha B,Lajqi H,Limani M",,2010,29–33,,,Conference Paper
"Flowtag: A Collaborative Attack-Analysis, Reporting, and Sharing Tool for Security Researchers","Current tools for forensic analysis require many hours to understand novel attacks, causing reports to be terse and untimely. We apply visual filtering and tagging of flows in a novel way to address the current limitations of post-attack analysis, reporting, and sharing. We discuss the benefits of visual filtering and tagging of network flows and introduce FlowTag as our prototype tool for Honeynet researchers. We argue that online collaborative analysis benefits security researchers by organizing attacks, collaborating on analysis, forming attack databases for trend analysis, and in promoting new security research areas. Lastly, we show three attacks on the Georgia Tech Honeynet and describe the analysis process using FlowTag.",,"Lee CP,Copeland JA",,2006,103–108,10.1145/1179576.1179597,https://doi-org.proxy.bnl.lu/10.1145/1179576.1179597;http://dx.doi.org/10.1145/1179576.1179597,Conference Paper
Augmented Reality Based Smart City Services Using Secure IoT Infrastructure,"This paper presents an application of Augmented Reality (AR) within a smart city service to be deployed in the domain of public transport in the city of Novi Sad in Serbia. The described solution is focused on providing a simple and efficient method to citizens for accessing important information such as bus arrival times, bus routes and tourist landmarks using smart phones and AR technology. The AR information is triggered by image and geo-location markers and the data is provided via secure IoT infrastructure. The IoT infrastructure is based on bus-mounted IoT devices which utilize secure CoAP software protocol to transmit the data to the associated cloud servers. Description of the complete end-to-end solution is presented, providing the overall system set-up, user experience aspects and the security of the overall system, focusing on the lightweight encryption used within the low-powered IoT devices.",,"Pokric B,Krco S,Pokric M",,2014,803–808,10.1109/WAINA.2014.127,https://doi-org.proxy.bnl.lu/10.1109/WAINA.2014.127;http://dx.doi.org/10.1109/WAINA.2014.127,Conference Paper
Cyber Conflict: Competing National Perspectives,"Today, cyber security, cyber defense, information warfare and cyber warfare issues are among the most relevant topics both at the national and international level. All the major states of the world are facing cyber threats and trying to understand how cyberspace could be used to increase power.Through an empirical, conceptual and theoretical approach, Cyber Conflict has been written by researchers and experts in the fields of cyber security, cyber defense and information warfare. It aims to analyze the processes of information warfare and cyber warfare through historical, operational and strategic perspectives of cyber attack. It is original in its delivery because of its multidisciplinary approach within an international framework, with studies dedicated to different states Canada, Cuba, France, Greece, Italy, Japan, Singapore, Slovenia and South Africa describing the states application of information warfare principles both in terms of global development and local usage and examples.Contents1. Canadas Cyber Security Policy: a Tortuous Path Toward a Cyber Security Strategy, Hugo Loiseau and Lina Lemay.2. Cuba: Towards an Active Cyber-defense, Daniel Ventre.3. French Perspectives on Cyber-conflict, Daniel Ventre.4. Digital Sparta: Information Operations and Cyber-warfare in Greece, Joseph Fitsanakis.5. Moving Toward an Italian Cyber Defense and Security Strategy, Stefania Ducci.6. Cyberspace in Japans New Defense Strategy, Daniel Ventre.7. Singapores Encounter with Information Warfare: Filtering Electronic Globalization and Military Enhancements, Alan Chong.8. A Slovenian Perspective on Cyber Warfare, Gorazd Praprotnik, Iztok Podbregar, Igor Bernik and Bojan Ticar.9. A South African Perspective on Information Warfare and Cyber Warfare, Brett van Niekerk and Manoj Maharaj.10. Conclusion, Daniel Ventre",,Ventre D,,2012,,,,Book
Live Enrolment for Identity Documents in Europe,"Digital image alterations (morphing) of identity document photos is a major concern and may potentially allow citizens with malicious intent to enrol for identity document(s) later to be used also by another individual. Taking the photo in the application office – live enrolment – can address this issue. However, this is a break with tradition and entails a sizeable overhaul in the public sector, which can be reluctant to change and often lacks the necessary formal methods that ensure a smooth transition. The objective of this paper is to map the main barriers and drivers related to live enrolment based on theoretical research and interviews conducted with high-ranking officers at passport authorities in Estonia, Kosovo, Norway and Sweden. These countries have successfully switched to live enrolment. The main driver for live enrolment has been increased security; for Estonia, user convenience was important and was behind the decision of keeping alternative application processes for the citizens around. The absence of legacy systems makes it easier to implement public sector innovations, such as live enrolment. Behind the successful implementation is proper risk management, covering technological, political and organisational risks. Finally, the research results indicate varying experiences, obstacles, cultural differences and trade-offs, emphasizing the need to understand barriers and drivers in a contextualised way.",,"Kalvet T,Karlzén H,Hunstad A,Tiits M",,2018,29–39,10.1007/978-3-319-98690-6_3,https://doi-org.proxy.bnl.lu/10.1007/978-3-319-98690-6_3;http://dx.doi.org/10.1007/978-3-319-98690-6_3,Conference Paper
The Use of Artificial Intelligence in Migration-Related Procedures in the European Union - Opportunities and Threats,,,Szwed A,,2022,3645–3651,10.1016/j.procs.2022.09.424,https://doi-org.proxy.bnl.lu/10.1016/j.procs.2022.09.424;http://dx.doi.org/10.1016/j.procs.2022.09.424,Journal Article
Improved Meet-in-the-Middle Attacks on Reduced-Round Kalyna-128/256 and Kalyna-256/512,"Kalyna is an SPN-based block cipher that was selected during the Ukrainian National Public Cryptographic Competition (2007---2010) and its slight modification was approved as the new encryption standard of Ukraine. In this paper, we focus on the key-recovery attacks on reduced-round Kalyna-128/256 and Kalyna-256/512 with the meet-in-the-middle method. The differential enumeration technique and key-dependent sieve technique which are popular to analyze AES are used to attack them. Using the key-dependent sieve technique to improve the complexity is not an easy task, we should build some tables to achieve this. Since the encryption procedure of Kalyna employs pre- and post-whitening operations using addition modulo $$2^64$$264 applied on the state columns independently, we carefully study the propagation of this operation and propose an addition plaintext structure to solve this. For Kalyna-128/256, we propose a 6-round distinguisher, and achieve a 9-round (out of total 14-round) attack. For Kalyna-256/512, we propose a 7-round distinguisher, then achieve an 11-round (out of total 18-round) attack. As far as we know, these are currently the best results on Kalyna-128/256 and Kalyna-256/512.",,"Lin L,Wu W",,2018,721–741,10.1007/s10623-017-0353-5,https://doi-org.proxy.bnl.lu/10.1007/s10623-017-0353-5;http://dx.doi.org/10.1007/s10623-017-0353-5,Journal Article
CSTST '08: Proceedings of the 5th International Conference on Soft Computing as Transdisciplinary Science and Technology,"Soft Computing (SC) has an evolving collection of methodologies, which is aimed to exploit tolerance for imprecision uncertainty, and partial truth to achieve robustness, tractability, and low cost. SC provides attractive opportunity to represent the ambiguity in human thinking with real life uncertainty. Fuzzy logic (FL), Neural Networks (NN), and Evolutionary Computation (EC) were the core methodologies of soft computing. Later chaos computing, fractal theory, wavelet transformation, cellular automaton, percolation models, and immune network theory were added to enhance soft computing. However, they should not be viewed as competing with each other, but synergistic and complementary, instead. SC was actually the combination or fusion of each methodology which yielded new computational capabilities (hybrid systems). Soft computing is currently causing a paradigm shift (breakthrough) in science and technology.The stage for the Fifth IEEE/ACM International Conference on Soft Computing as Transdisciplinary Science and Technology (CSTST'08) has been set. This edition is dedicated to commemorate the memory of Professor Yasuhiko Dote, Founding Chair of WSTST series of meetings. In essence, CSTST'08 is built on the success of the previous four events held in Muroran, Japan namely the IEEE International Workshop on Neuro Fuzzy Control, in 1993; IEEE International Workshop on Soft Computing in Industry, in 1996, the IEEE International Workshop on Soft Computing in Industry, in 1999 and International Workshop on Soft Computing as Transdisciplinary Science and Technology (WSTST'2005). CSTST'08 is hosted by University of Cergy Pontoise, France and is technically co-sponsored by IEEE Systems Man and Cybernetics Society, ACM SIGAPP (French Chapter), IEEE French Section, World Federation on Soft Computing, European Society for Fuzzy Logic, Technology and International Fuzzy Systems Association, and AFIHM - French Association of Human Computer Interaction. On behalf of the CSTST'08 program committee, we wish to extend a very warm welcome to this edition in Cergy-Pontoise/Paris, France. The conference program committee has organized an exciting and invigorating program comprising presentations from distinguished experts in the field, and important and wide-ranging contributions on state-of-the-art research that provide new insights into current cutting edge results on ""Soft Computing as Transdisciplinary Science and Technology"".This year, we received over 212 regular submissions and we are really gratified by the international diversity of this conference: authors of submitted work hail from no less than 30 countries including Vietnam, Egypt, Bulgaria, Turkey, Russia, Netherlands, Austria, Malaysia, Sweden, Croatia, Kuwait, Cyprus, Belgium, Estonia, Latvia, Lebanon, Macedonia, Singapore, Argentina, United Arab Emirates, Thailand, Ukraine, Hungary, Ireland, Czech, Republic, Spain, Norway, Taiwan, Canada, Libya, Romania, Mexico, Greece, Brazil, Pakistan, Germany, Australia, Tunisia, India, United States of America, Italy, Korea, Poland, Algeria, Japan, United Kingdom, Iran, China, Portugal, and France. The technical program of CSTST'08 conference comprises of 62 papers. The conference program committee had a very challenging task of choosing high quality submissions. Each paper was peer reviewed by at least three or more independent referees of the program committee and the papers were selected based on the referee recommendations. The papers offer stimulating insights into emerging intelligent technologies and their applications in Internet security, chance discovery, humanized computational intelligence, web intelligence, data mining, image processing, swarm intelligence, optimization and so on.",,,,2008,,,,Book
Mainstreaming Disaster Risk Reduction into Community Development in the Windward Islands,"The Windward Islands are vulnerable to a number of natural hazards. This thesis examines the possibilities for Disaster Risk Reduction (DRR) in the Windward Islands. The Windward Islands offer a special case of ""Island Vulnerability"". Island vulnerability is essentially defined as an increased probability in disaster events against what would be expected if vulnerability were to be measured against international levels of poverty, defined as Gross National Product per capita. There are three reasons for this namely the topography of islands, the site characteristics and the socio-economic setting. The topography is one where islands, largely of volcanic or coral origins, face multi-hazard experience particularly from flooding and storm surge. The site issue is that islands usually have a high ratio of coastline to land mass implying a relatively higher exposure to extreme events. The socio-economic conditions are peculiar to island including isolation, mono-agriculture and mono-industry essentially laid down by colonial experience, an absence of formal employment opportunities and weak capacity in local governance including the absence of NGOs. Though DRR has evolved over the last 20 years, some islands and communities remain more vulnerable than others. This research investigates the mainstreaming of DRR in the Windward Islands of Dominica, Grenada, Saint Lucia and St Vincent and the Grenadines. The key issue researched was whether DRR could be effectively implemented at the community level. To address this issue, the research investigates the vulnerability and capacity of communities to hazards in the Windward Islands and suggests ways to reduce risk and build community resilience. The factors affecting vulnerability and capacity to hazards in the Anglophone Windward Islands were identified as a means of determining how to reduce risks and build resilience to hazards in the Windward Islands. Efforts to enhance community development and build resilience are not effective as they fail to address fully community needs. This research concluded that some communities are more vulnerable than others and a major contributor to their vulnerability is poverty. None of the methods used in this research are unique to island vulnerability analysis as they have been applied elsewhere in DRR. What is unique is the scoping of the application of these methods to gain an overview of DRR possibilities. What emerges as a conclusion is the limited impact of top down interventions, especially those interventions that try to address poverty alleviation to lower risk. This is essentially because the poor themselves barricade their own coping mechanisms against external interventions, thus building a wall against external help. Building on local organisational capacity, including religious groups, can help address this problem. Research in this area is limited for the Anglophone Windward Islands and this thesis on vulnerability of household and communities will contribute to knowledge in this field.",,Ferdinand I,,2013,,,,Ph.D. Thesis
ICT Innovations 2009,"This book is the result of the first International Conference ICT Innovations 2009. The ICT Innovations conference is the primary scientific action of the Macedonian Society on Information and Communication Technologies (ICT-ACT). It promotes the publication of scientific results of the international community related to innovative fundamental and applied research in ICT. Today, ICT has enlarged its horizons and it is practiced under multidisciplinary contexts that introduce new challenges to theore- cal and technical approaches. The ICT Innovations 2009 conference gathered academics, professionals and pr- titioners reporting their valuable experiences in developing solutions and systems in the industrial and business arena especially innovative commercial implementations, novel applications of technology, and experience in applying recent research advances to practical situations, in any ICT areas. The conference focuses on issues concerning a variety of ICT fields like: Multimedia Information Systems Artificial Intelligence Pervasive and Ubiquitous Computing Eco and Bio Informatics Internet and Web Applications and Services Wireless and Mobile Communications and Services Computer Networks, Security and Cryptography Distributed Systems, GRID and Cloud Computing ICT Innovations 2009 Conference was held in Ohrid, Macedonia, in September 28-30, 2009. Local arrangements provided by the members of the Macedonian Society on Information and Communication Technologies ICT-ACT, mainly consisting of teaching and research staff of Computer Science Department at Faculty of Electrical Engineering and Information Technologies and Institute of Informatics at Faculty of Natural Sciences, both at Ss. Cyril and Methodius University in Skopje, Macedonia.",,"Davcev D,Gmez JM",,2014,,,,Book
Mesh Messaging in Large-Scale Protests: Breaking Bridgefy,"Mesh messaging applications allow users in relative proximity to communicate without the Internet. The most viable offering in this space, Bridgefy, has recently seen increased uptake in areas experiencing large-scale protests (Hong Kong, India, Iran, US, Zimbabwe, Belarus), suggesting its use in these protests. It is also being promoted as a communication tool for use in such situations by its developers and others. In this work, we report on a security analysis of Bridgefy. Our results show that Bridgefy, as analysed, permitted its users to be tracked, offered no authenticity, no effective confidentiality protections and lacked resilience against adversarially crafted messages. We verified these vulnerabilities by demonstrating a series of practical attacks on Bridgefy. Thus, if protesters relied on Bridgefy, an adversary could produce social graphs about them, read their messages, impersonate anyone to anyone and shut down the entire network with a single maliciously crafted message.",,"Albrecht MR,Blasco J,Jensen RB,Mareková L",,2021,375–398,10.1007/978-3-030-75539-3_16,https://doi-org.proxy.bnl.lu/10.1007/978-3-030-75539-3_16;http://dx.doi.org/10.1007/978-3-030-75539-3_16,Conference Paper
Business Continuity and Disaster Recovery Planning for IT Professionals,"Increase Your Company's Odds of Surviving a Major DisasterPowerful Earthquake Triggers Tsunami in Pacific. Hurricane Katrina Makes Landfall in the Gulf Coast. Avalanche Buries Highway in Denver. Tornado Touches Down in Georgia. These headlines not only have caught the attention of people around the world, they have had a significant effect on IT professionals as well. As technology continues to become more integral to corporate operations at every level of the organization, the job of IT has expanded to become almost all-encompassing. These days, it's difficult to find corners of a company that technology does not touch. As a result, the need to plan for potential disruptions to technology services has increased exponentially. Business Continuity Planning (BCP) and Disaster Recovery Planning (DRP) are emerging as the 'next big thing' in corporate IT circles. With distributed networks, increasing demands for confidentiality, integrity and availability of data, and the widespread risks to the security of personal, confidential and sensitive data, no organization can afford to ignore the need for disaster planning.The British Standards Institute is releasing a new standard for BCP this year, the Disaster Recovery Institute has developed a certification for DRP/BCP professionals in conjunction with the British Standards Institute, trade shows are popping up on this topic and the news is filled with companies facing disasters from all sides.In this book you will find:* Complete coverage of the 3 categories of disaster: natural hazards, human-caused hazards, and accidental/ technical hazards.* Updated information on risks from cyber attacks, rioting, protests, product tampering, bombs, explosions, and terrorism. * Extensive disaster planning and readiness checklists for IT infrastructure, enterprise applications, servers and desktops.* Clear guidance on developing alternate work and computing sites and emergency facilities.* Actionable advice on emergency readiness and response.* Up-to-date information on the legal implications of data loss following a security breach or disaster.Featuring Case Studies from:Deanna Conn, Partner, Quarles & Brady, LLP, information security expertDebbie Earnest, Disaster Recovery and IT expertPatty Hoenig, Communications and PR expert * Complete coverage of the 3 categories of disaster: natural hazards, human-caused hazards, and accidental and technical hazards.* Only published source of information on the new BCI standards and government requirements.* Up dated information on recovery from cyber attacks, rioting, protests, product tampering, bombs, explosions, and terrorism.",,Snedaker S,,2007,,,,Book
A Multi-Level Cyber-Security Reference Model In Support Of Vulnerability Analysis,"This paper reports on the second engineering cycle of a reference model for end-to-end cyber-security by design in the electricity sector. In our previous work, we proposed a reference model that relies on the integrated consideration of two fragmented, but complementary, reference models: NISTIR 7628 and powerLang. To align these reference models, we rely on multi-level modeling, specifically on the Flexible Meta Modeling and Execution Language (FMMLx), and integrated modeling and programming. Within this paper, we strengthen the bottom-up design of the reference model’s application by integrating a semi-automated threat analysis. This enables the identification of possible points of improvement in the actual architecture design, as well as a future analysis of business-level impact of different threats. To demonstrate our approach, we rely on the well-studied Ukraine scenario from 2016.",,"Hacks S,Kaczmarek-Heß M,de Kinderen S,Töpel D",,2022,19–35,10.1007/978-3-031-17604-3_2,https://doi-org.proxy.bnl.lu/10.1007/978-3-031-17604-3_2;http://dx.doi.org/10.1007/978-3-031-17604-3_2,Conference Paper
Developing Electricity Forecast Web Tool for Kosovo Market,"In this paper is presented a web tool for electricity forecast for Kosovo market for the upcoming ten years. The input data i.e. electricity generation capacities, demand and consume are taken from the document ""Kosovo Energy Strategy 2009-2018"" compiled by Ministry of Energy of Kosovo and approved by the Kosovo Assembly. The web tool enables different settings for national electricity grid, divided in seven electricity consume regions, five interconnection lines and using domestic generation capacities, using existing power plants and those planed to be built until 2018. It enables different scenarios, such as increasing/decreasing electricity demand in different regions, energy import/export and increasing/decreasing generation capacities. The developed web tool was tested especially against boundary condition such as heavy increased/decreased energy consumption, i.e. beyond the planned economic growth of the country and the delay of starting time of new generation capacities. For these boundary conditions are proposed extra measures to be considered in order to fulfill the security of energy supply criteria. This web application is developed using latest ASP.NET platform, C# as programming language and Microsoft SQL as database server. The web tool shall server as unique tool for governmental decision makers and to contribute inputs to future policy and management decision-making in the energy sector.",,"Rexha B,Ahmeti A,Ahmedi L,Komoni V",,2011,55–64,,,Journal Article
MotionTalk: Personalized Home Rehabilitation System for Assisting Patients with Impaired Mobility,"Physical injury, stroke, trauma, traumatic brain injury and spinal cord injury rank among the top causes of disability. There are a total of 54 million people in the US requiring rehabilitative assistance of which 15.3 million people are in the age groups of 18-44. However, the compliance rate for patients performing rehabilitation exercises in the home environment is poor. In this paper, we design and prototype a personalized home rehabilitation system, MotionTalk, for the real time quantitative assessment of mobility. Performance of rehabilitation is designed to be assessed using the changes in mobility, reflected in the exercises performed by patients at home with respect to the same exercises performed in the clinic. Our system is capable of capturing motion using Microsoft Kinect and analyzing the position and rotation information to give scores for assessing rehabilitation progress. In comparison to conventional rehabilitation systems, MotionTalk is an inexpensive ($1000), less intrusive and personalized home rehabilitation system, which was developed and tested using data from able-bodied volunteers at Georgia Institute of Technology.",,"Venugopalan J,Cheng CW,Wang MD",,2014,455–463,10.1145/2649387.2649430,https://doi-org.proxy.bnl.lu/10.1145/2649387.2649430;http://dx.doi.org/10.1145/2649387.2649430,Conference Paper
Secure Telematic Applications for National Scale Projects - Volume 20 NATO Science for Peace and Security Series - D: Information and Communication Security ... and Communitcation Sercurity- Vol. 20),"The NATO Advances Research Workshop (ARW) 'Secure Telematic Applications for National Scale Projects' is organized in the frame of the National Week of Information Technologies and Sixth International Congress 'Scientific and Methodological Facilitation of the Development of Informatization and the System of Scientific and Technical Information in the Republic of Belarus. Secure Telematic Applications in National and International Projects'. The participants of the event came to Minsk from all regions of Europe, as well as from Asia and Africa. Besides the general ARW topic, the participants also discussed the progress in the activity of the High Technologies Park, the creation of the corporate network of libraries of Belarus on the basis of the National Library of Belarus and the newest technologies of e-government. The presentations are also oriented on creation of the National Scientific and Research Computer Network of the Republic of Belarus on the basis of Academic Network BASNET. It is expected that the scientific and practical achievements of the forum will considerably influence both the development of the information technologies and their utilization for advantage of the national economy of Belarus and all participating countries.",,"Fontaine JG,Fontaine JG,Makhaniok M",,2009,,,,Book
Proving Sustainability: The International Development Monitoring Initative,"Nearly a billion people in the world lack access to safe drinking water, two billion have inadequate sanitation facilities, three billion use biomass for their daily energy needs and nearly half the world's population live in rural isolation, lacking access to the most basic human services. Combined, these limitations are a leading cause of the perpetuating cycle of poverty and political insecurity. Meanwhile, the majority of international development agencies are responsible for self-reporting project outcomes. At best, expert spot-checks are conducted in the field occasionally. These results tend to show individual project success, while meta-surveys indicate on-going challenges in the sector. This disconnect may be addressed through independent data monitoring technologies that provide objective data on system performance and use and can be used to demonstrate success and identify project weaknesses. By demonstrating which technologies and programs are truly successful, these successes can be targeted for scaling, through savings realized by eliminating unsuccessful approaches. This will benefit developing communities by providing proven and accountable programs. The Sustainable Water, Energy and Environmental Technologies Laboratory, the SWEETLab , at Portland State University is working with partners to demonstrate this concept across several applications and countries. The SWEETSense technology can provide objective, qualitative and continuous operational data on the usage and performance of programs across a range of sectors and communities. The data is then directly integrated into SWEETData , an internet database presenting summary statistics on performance and usage of the monitored technologies to front-end users. The SWEETLab is currently demonstrating this concept in water, sanitation, household energy and rural infrastructure programs with diverse partners including Mercy Corps, the Lemelson Foundation, Bridges to Prosperity, Manna Energy Limited and Vestergaard Frandsen, in several countries including Indonesia, Haiti, Guatemala and Rwanda. Remote monitoring systems are an innovative method to ensure the success of appropriate technology projects. Rather than infrequent engagement, remote monitoring systems ensure that community partnerships are maintained through continuous monitoring. This approach seeks to raise the quality and accountability of these projects internationally.",,"Thomas E,Zumr Z,Barstow C,Linden K",,2011,164–170,10.1109/GHTC.2011.74,https://doi-org.proxy.bnl.lu/10.1109/GHTC.2011.74;http://dx.doi.org/10.1109/GHTC.2011.74,Conference Paper
Technical and Economic Feasibility for Passive Housing in the Social Sector of Honduras,"In this research, the authors designed an energy efficiency system applied for social housing in Honduras. The design consists of constructing a standardized passive house in areas where communities do not have access to electricity and gas. The social-economic situation in Honduras is very delicate, with almost 1.5 million people homeless. 8% of the population don't have access to electricity. For rural areas, people only rely on a wood fire to cock, which has accelerated deforestation all long the country. In the region, other countries are adopting more energy efficiency politics to counter-attack social-economic difficulties and climate change. Honduras has not yet recovered from the bankrupt of the governmental bureau in charge of the productive chain of electricity. It is urgent to apply relief measures, and passive houses sure are one that can make Honduras take one step toward development. The social housing will need an OFFGRID photovoltaic (PV) system to supply their electrical energy and a pre-fabricated biodigester to produce biogas with biomass found in the community. The authors replaced the conventional concrete block used in Honduras with an ICF block to improve thermic conditions inside the house. Building a house with an ICF block is even cheaper than one of concrete block, and there is a difference in temperature inside the house of 6 degrees Celsius between the ICF and concrete one. The project will have a return on investment period of no more than six years. The inversion for each house is $ 10,836.71. The authors consider the design suggested in this investigation can benefit the social-economic situation in Honduras as energy efficiency and social politic. However, there is still more to be done to perceive mayor results in the Honduran energy efficiency culture.",,"Luis Ordoez-Avila J,Hermida E",,2022,178–183,10.1145/3497701.3497735,https://doi-org.proxy.bnl.lu/10.1145/3497701.3497735;http://dx.doi.org/10.1145/3497701.3497735,Conference Paper
"ICT Innovations 2017: Data-Driven Innovation. 9th International Conference, ICT Innovations 2017, Skopje, Macedonia, September 18-23, 2017","This book constitutes the refereed proceedings of the 9th International Conference on Data-Driven Innovation, ICT Innovations 2017, held in Skopje, Macedonia, in September 2017. The 26 full papers presented were carefully reviewed and selected from 90 submissions. They cover the following topics: big data analytics, cloud computing, data mining, digital signal processing, e-health, embedded systems, emerging mobile technologies, multimedia, Internet of Things (IoT), machine learning, software engineering, security and cryptography, codingtheory, wearable technologies, wireless communication, and sensor networks.",,"Trajanov D,Bakeva V",,2017,,,,Book
Penetration Testing: A Hands-On Introduction to Hacking,"In Penetration Testing, security researcher and trainer Georgia Weidman provides you with a survey of important skills that any aspiring pentester needs. This beginner-friendly book opens with some basics of programming and helps you navigate Kali Linux, an operating system that comes preloaded with useful computer security tools like Wireshark and Metasploit. You'll learn about gathering information on a target, social engineering, capturing network traffic, analyzing vulnerabilities, developing exploits, and more. Hands-on examples discuss even advanced topics like mobile device security and bypassing anti-virus software.",,Weidman G,,2014,,,,Book
SOUPS '05: Proceedings of the 2005 Symposium on Usable Privacy and Security,"Welcome to the Symposium On Usable Privacy and Security! This inaugural event brings together an interdisciplinary group of researchers and practitioners in human computer interaction, security, and privacy. While papers on usable privacy and security have appeared periodically in privacy, security, and human-computer interaction conferences and workshops for many years, until recently there had been no event focusing specifically on this area. Because of the inherently interdisciplinary nature of this area, there are benefits to researchers from these communities meeting together to discuss their work. Successful workshops at CHI 2003 and DIMACS in 2004, and a number of recent journal special issues on related topics demonstrated that there was sufficient interest to organize a symposium featuring refereed papers. We organized a program that provides both a forum for refereed papers as well as opportunities for informal interactions and small group discussions.The program features 10 refereed papers, two tutorials, 22 posters, two panels, four discussion sessions, and an invited talk. We received 39 paper submissions. Each paper was refereed by at least three members of the refereed papers committee, and through an online discussion process the committee selected 10 papers for presentation and publication. The committee also selected the paper ""Developing Privacy Guidelines for Social Location Disclosure Applications and Services"" by Giovanni Iachello (Georgia Institute of Technology), Ian Smith, Sunny Consolvo, Mike Chen (Intel Research ), and Gregory D. Abowd (Georgia Institute of Technology) to receive the best paper award.Our two tutorials are intended to help attendees who have a primary background in either security/privacy or HCI/usability to get up to speed in the other area. Jason I. Hong (Carnegie Mellon University) developed a tutorial on ""User Interface Design, Prototyping, and Evaluation,"" covering the key concepts and techniques in these areas. Simson Garfinkel (MIT) developed a tutorial on ""Introduction to Computer Security and Privacy,"" providing a primer on security and privacy for those with a background in usability. While there is much more to learn in these areas than can be covered in a half-day tutorial, we hope our tutorials provide a good overview of these areas, allowing participants to gain an appreciation for the important issues and techniques.We have lined up two interesting panels. The first panel, organized by Konstantin Beznosov (University of British Columbia), explores, ""Usability of Security Administration vs. Usability of End-user Security."" The second panel, organized by Robert Miller (MIT), examines what happens ""When User Studies Attack: Evaluating Security By Intentionally Attacking Users.""The program also features an invited talk by Bill Cheswick on ""My Dad's Computer, Microsoft, and the Future of Internet Security."" Cheswick uses his father's computer to illustrate why millions of people routinely run dangerous software on badly-infected computers. He discusses the prospects for improved security for home users, and for corporate and government intranets.Finally, the SOUPS 2005 program includes four parallel ""discussion"" sessions, featuring moderated discussion on a topic of interest to attendees. Discussion sessions have been organized around the following topics: ""Usability and Acceptance of Biometrics,"" ""Valuation and Context,"" ""When User Studies Attack: Evaluating Security By Intentionally Attacking Users,"" and ""Usable Interfaces for Anonymous Communication."" We hope the small group format will lead to lively and productive interactions.",,,,2005,,,,Book
Inside Cyber Warfare: Mapping the Cyber Underworld,"What people are saying about Inside Cyber Warfare ""The necessary handbook for the 21st century."" --Lewis Shepherd, Chief Tech Officer and Senior Fellow, Microsoft Institute for Advanced Technology in Governments ""A must-read for policy makers and leaders who need to understand the big-picture landscape of cyber war."" --Jim Stogdill, CTO, Mission Services Accenture You may have heard about ""cyber warfare"" in the news, but do you really know what it is? This book provides fascinating and disturbing details on how nations, groups, and individuals throughout the world are using the Internet as an attack platform to gain military, political, and economic advantages over their adversaries. You'll learn how sophisticated hackers working on behalf of states or organized crime patiently play a high-stakes game that could target anyone, regardless of affiliation or nationality. Inside Cyber Warfare goes beyond the headlines of attention-grabbing DDoS attacks and takes a deep look inside multiple cyber-conflicts that occurred from 2002 through summer 2009. Learn how cyber attacks are waged in open conflicts, including recent hostilities between Russia and Georgia, and Israel and Palestine Discover why Twitter, Facebook, LiveJournal, Vkontakte, and other sites on the social web are mined by the intelligence services of many nations Read about China's commitment to penetrate the networks of its technologically superior adversaries as a matter of national survival Find out why many attacks originate from servers in the United States, and who's responsible Learn how hackers are ""weaponizing"" malware to attack vulnerabilities at the application level",,Carr J,,2009,,,,Book
"Testing of Network Security Systems through DoS, SQL Injection, Reverse TCP and Social Engineering Attacks","Cyber-attacks are happening with an ever-increasing frequency with the goal of gaining access to sensitive information. These attacks can cause huge damage to all kinds of organisations. With web applications becoming a preferred target for attackers through which to try and access sensitive data, it has become of a paramount importance for organisations to implement robust security policies. Measures should be taken to prevent these attacks by testing security systems before attacks happen. The most frequent types of attacks are: SQL injection, DoS, reverse TCP and social engineering. In this paper, we use penetration testing techniques on computer systems and networks. We analyse firewalls and other protective systems and their role through different scenarios. Using penetration testing techniques, we try to find the best solution for protecting sensitive data within the governmental network of Kosovo. We also tackle the issue of social engineering attacks on networks.",,"Maraj A,Rogova E,Jakupi G",,2020,115–133,10.1504/ijguc.2020.103976,https://doi-org.proxy.bnl.lu/10.1504/ijguc.2020.103976;http://dx.doi.org/10.1504/ijguc.2020.103976,Journal Article
ICEGOV '13: Proceedings of the 7th International Conference on Theory and Practice of Electronic Governance,"The 7th International Conference on Theory and Practice of Electronic Governance, ICEGOV2013, took place in Seoul, Republic of Korea from 22 to 25 October 2013. The conference was organized under the patronage of the Ministry of Security and Public Administration of the Republic of Korea (MOSPA) by the National Information Society Agency and by Macao-based Center for Electronic Governance at United Nations University International Institute for Software Technology (UNU-IIST) as the founder and organizer of the ICEGOV series. The conference took place under the theme ""Beyond 2015"" Smart Governance, Smart Development"". It was co-located with the Global e-Government Forum, organized by MOSPA in collaboration with United Nations Department of Economic and Social Affairs (UNDESA).The ICEGOV series focuses on the use of technology to transform relationships between government and citizens, businesses, civil society and other arms of government (Electronic Governance). Established in 2007, the series looks beyond the traditional focus on technology-enabled transformation in government (Electronic Government) towards new forms, new paradigms, and new foundations for technology-enabled governance, collaboration and sustainable development. ICEGOV is a platform where researchers, policy-makers and practitioners meet; a platform where theories are tested, insights are shared and experiences are reported; a platform for network- and capacity-building where keynote lectures and paper sessions are complemented by plenary discussions, town hall debates and poster exhibitions; a platform for international dialogue attended by participants from developing, developed and transition countries, from the United Nations system, and from many academic, governmental, non-governmental and private sector organizations. Since its establishment, the series has traveled globally from Macao (ICEGOV2007), through Cairo (ICEGOV2008), Bogota (ICEGOV2009), Beijing (ICEGOV2010), Tallinn (ICEGOV2011) and Albany (ICEGOV2012), to Seoul (ICEGOV2013) all generating significant local interest and stakeholder engagement.The program of ICEGOV2013 was built upon contributions from researchers and practitioners from around the world. In response to the call for papers, the conference received 133 papers from 54 countries and economies. The papers were evaluated in five categories: 1) Completed Research Papers providing the outcomes of complete research in one or more aspects of EGOV, with proven capability to advance the state of research in the field, limited to 10 pages; 2) Ongoing Research Papers providing the outcomes of ongoing research in one or more aspects of EGOV, with potential capability to advance the state of research in the field, limited to 4 pages; 3) Completed Experience Papers describing completed experience concerning EGOV policy or practice innovations, with proven capability to advance the state of practice in the field, including critical success factors and insights on the challenges encountered and how they were addressed, limited to 10 pages; 4) Ongoing Experience Papers describing ongoing experience concerning EGOV policy and practice innovations, with potential capability to advance the state of practice in the field, including critical success factors and insights on the challenges encountered and how they are being addressed, limited to 4 pages; and 5) Poster Papers presenting novel ideas and initiatives with potential to advance the state of research or state of practice in the field, limited to 2 pages. In total, 43 Completed Research Papers, 45 Ongoing Research Papers, 17 Completed Experience Papers, 21 Ongoing Experience Papers and 8 Posters were received. After anonymous peer-review process carried out by the members of the Program Committee at least three independent reviews were obtained for each submission as a basis for acceptance decisions: 13 submissions were accepted as Completed Research Papers, 8 as Completed Experience Papers, 29 as Ongoing Research Papers, 11 as Ongoing Experience Papers and 21 as Poster Papers. All accepted submissions, revised to address review comments, and presented at the conference within 6 paper tracks, 11 thematic sessions and one poster session, are included in this volume. Among them, like the last three ICEGOV conferences, the authors of selected papers were invited to submit extended versions of their papers for possible publication in the special issue of Government Information Quarterly, Elsevier.Based on the submitted and invited contributions and continuing the ICEGOV tradition, ICEGOV2013 featured a rich academic, capacity-building and network-building program comprising keynote lectures, plenary discussions, town hall debates, paper tracks, thematic sessions and the doctoral colloquium and poster exhibition. The program engaged individuals from over 60 countries and economies as authors, reviewers, committee members or resource persons. The details of the program are provided below.The conference included six keynote lectures on various aspects of Electronic Governance (EGOV), conducted by distinguished experts and practitioners in the area: 1) Park Chan Woo, Vice-Minister of Security and Public Administration of the Republic of Korea; 2) Alikhan Baimenov, Chairman of the Agency for Civil Service Affairs of the Republic of Kazakhstan; 3) Moon Suk Ahn, Chair Professor of e- Government, Korea University, Republic of Korea; 4) Mohammed Ali Al, Chief Executive Officer, e-Government Authority, Kingdom of Bahrain; 5) Henk G. Sol, Professor of Business and ICT and Founding Dean, University of Groningen, Netherlands; and 6) Edwin Lau, Head of Division, Reform of the Public Sector, Organization for Economic Co-operation and Development (OECD).Three plenary sessions followed the keynote lectures on the second, third and fourth day of the conference, focusing on specific questions of interest to the EGOV research and policy community:1. Are international EGOV rankings having a mobilizing or distracting influence on development? Chaired by Tomasz Janowski, Head of the Center for Electronic Governance at UNU-IIST and attended by: Vincenzo Aquaro, Chief of E-Government Branch, Division for Public Administration and Development Management, UNDESA; Bikesh Kurmangaliyeva, Deputy Chairwoman of the Board of ""Zerde"" National CT Holding, Kazakhstan; Mohammed Ali Al Qaed, CEO of eGovernment Authority, Kingdom of Bahrain; Mesfin Belachew Tefera, Technical Advisor to the Minister, Ethiopian Ministry of Communication and Information Technology; and Saleem Zoughbi, Former Regional ICT Advisor, UNESCWA and consultant for UNU-IIST.2. Who should drive smart conversations for sustainable development experts, citizens or politicians? Chaired by Marijn Janssen, Professor of ICT and Governance at Technology, Policy and Management Faculty, Delft University of Technology, Netherlands and attended by: Sunil Choenni, Head, Department of Statistical Information Management and Policy Analysis, Research and Documentation Centre (WODC), Dutch Ministry of Security and Justice; Harekrishna Misra, Professor in IT and Systems at the Institute of Rural Management Anand (IRMA), India; Henk G.Sol, Professor of Business and ICT and Founding Dean, Faculty of Economics and Business, University of Groningen, Netherlands; and Evgeny Styrin, Senior Research Analyst and Associate Professor, National Research University Higher School of Economics, Russia.3. Is a common set of e-government principles, applicable to all countries and contexts, possible? Chaired by Samuel Chan, Member of Executive Committee, Macao Science and Technology Development Fund, Macao SAR Government and attended by: Wojciech Cellary, Head of the Department of Information Technology, Poznan University of Economics, Poland; Sharon Dawes, Senior Fellow, Center for Technology in Government, University at Albany, USA; Edwin Lau, Head of Division, Reform of the Public Sector, Public Governance and Territorial Development Directorate, Organization for Economic Co-operation and Development; and Jeremy Millard, Associate Research Fellow, Brunel University, UK.Three town hall debates took place in the afternoons of the first, second and third days of the conference. They focused on three salient questions for the EGOV research and policy community:1. Catalyzing Smart Transformation: What Makes Governments Smarter? Chaired by Samia Melhem, Lead Policy Specialist, Transform Practice, Chair, eDevelopment Community of Practice, Transport, Water and ICT, Sustainable Development Network, World Bank Group; and Oleg Petrov, Senior Program Officer, ICT, World Bank; and attended by: Jabiri Kuwe Bakari, CEO, e-Government Agency, Tanzania; Rajendra Kumar, Senior Officer, Indian Administrative Service and Joint Secretary (e-Governance), Department of Electronics and Information Technology, Government of India; Bikesh Kurmangaliyeva, Deputy Chairwoman of the Board of ""Zerde"" National CT Holding, Kazakhstan; Margareta Petrusevschi, Knowledge and Learning Coordinator, e-Government Centre, Government of the Republic of Moldova; James Saaka, Executive Director, National Information Technology Authority, Uganda; Mesfin Belachew Tefera, Technical Advisor to the Ethiopian Minister of Communication and Information Technology; and Jeongwon Yoon, Executive Director, National Information Society Agency, Korea. This town hall was organized by the World Bank.2. Is Good Governance a Pre-Condition or a Consequence of the Development of Knowledge Societies? Chaired by Andrea Cairola, Adviser for Communication and Information, UNESCO Office Beijing, Cluster Office to the Democratic People's Republic of Korea, Japan, Mongolia, People's Republic of China and Republic of Korea; and attended by: Johanna Ekua Awotwi, Director of Research and ICT Operations, Centre for e-Governance, Accra, Ghana; Antonio Cordella, Lecturer in Information Systems, London School of Economics and Political Sciences, UK; Marco Peres, Director, Observatory for Society, Technology and Government Information, University Externado of Colombia, Colombia; Margareta Petrusevschi, Knowledge and Learning Coordinator, e-Government Centre, Government of the Republic of Moldova, Moldova; and Jeongwon Yoon, Executive Director, National Information Society Agency, Republic of Korea. This town hall was organized by the UNESCO Information for All Programme.3. Striking the Balance of Security, Privacy and Openness: To Open or Not To Open? Chaired by Theresa Pardo, Director of the Center for Technology in Government, University at Albany, USA, and attended by: Sharon Dawes, Senior Fellow, Center for Technology in Government, University at Albany, USA; Ramon Gil-Garcia, Research Director, Center for Technology in Government, University at Albany, USA; Louise Thomasen, independent consultant and expert in EGOV and technology, Denmark; and Lei Zheng, Assistant Professor, Department of Public Administration, Fudan University, China.The program included six paper tracks, chaired by leading international experts in the corresponding areas, comprising presentations of three to six accepted papers: 1) Building Smart Government chaired by Theresa Pardo, Director of the Center for Technology in Government, University at Albany, USA and Gabriel Puron Cid, Professor at the Centre of Research and Teaching in Economic Sciences, Mexico; 2) Governing through Networks chaired by Sehl Mellouli, Associate Professor at Laval University, Canada and Adegboyega Ojo, Research Fellow and Leader of E-Government Group at INSIGHT, National University of Ireland, Ireland; 3) Policy and Governance Innovation chaired by Natalie Helbig, Senior Research Associate at the Center for Technology in Government, University at Albany, USA and Marijn Janssen, Professor in ICT and Governance at the Delft University of Technology, Netherlands; 4) Smart Governance for Smart Industries chaired by Wojciech Cellary, Professor and Head of the Department of Information Technology at the Poznan University of Economics, Poland and Antonio Cordella, Lecturer at the London School of Economics and Political Sciences, UK; 5) Smart Governance for Smart Societies chaired by Jeremy Millard, Associate Research Fellow at the Brunel University, UK; and 6) Ethics, Transparency and Accountability chaired by Jeanne Holm, Chief Knowledge Architect at the NASA Jet Propulsion Laboratory, USA. Each track took place across the whole duration of the conference, with tutorial introduction to the topic of the track organized on the first day, presentations of accepted papers on the second or third day, and workshop-style discussion on the last day.Complementing the paper tracks, 11 thematic sessions were organized and chaired by industrial, academic, government and international organizations active in the theme of the session, comprising presentations of up to four accepted papers: 1) EGOV for Development chaired by Nag Yeon Lee, ICT Consultant and Instructor for e-Government on behalf of the Asia Pacific Center on ICT for Development, United Nations Economic and Social Commission for Asia Pacific; 2) National Data Policies chaired by Zhanat Zhakhmetova, Head of the Office of State Informatization Policy, Department of State Information Technology Policy, on behalf of the Ministry of Transport and Communications, Republic of Kazakhstan; 3) Governing Ageing Society chaired by Toshio Obi, Professor, Institute of e-Government on behalf of Waseda University, Japan; 4) Governing Smart Cities chaired by Yoon Chang So, Smart Cities Country Leader, IBM Korea on behalf of IBM; 5) Open Government Data Impact chaired by Edwin Lau, Head of Division, Reform of the Public Sector, Public Governance and Territorial Development Directorate, on behalf of the Organization for Economic Co-operation and Development; 6) Interoperability Governance chaired by Jung Sik Hwang, Platform Strategy Lead at Microsoft Korea on behalf of Microsoft; 7) Government on Social Media chaired by Jeanne Holm, Evangelist, Data.Gov and Chief Knowledge Architect at the Jet Propulsion Laboratory, NASA on behalf of the World Wide Web Consortium; 8) Innovative EGOV Applications chaired by Oleg Petrov, Senior Program Officer, ICT, World Bank on behalf of the World Bank; 9) Participatory Government chaired by Bernd Friedrich, Head of the Information and Communications Technologies for Development Project at Deutsche Gesellschaft für Internationale Zusammenarbeit (GIZ) GmbH, Germany on behalf of GTZ; 10) Mobile Governance chaired by Nestor Eduardo Fajardo Infante, Advisor for Research, Development and Innovation, Ministry of Information Technology and Communication, on behalf of the Government of Colombia; and 11) Open Data Ecosystem chaired by Jeanne Holm, Evangelist, Data.Gov and Chief Knowledge Architect, Jet Propulsion Laboratory, NASA on behalf of the U.S. Government and Data.Gov.The program also included poster exhibition, organized in the reception style to allow authors to present their ongoing work, receive feedback and engage in discussions and networking; and an interactive doctoral colloquium, jointly organized by the Center for Electronic Governance at UNU-IIST, Macao, University of Groningen, Netherlands and Chuo University, Japan. The colloquium provided doctoral students from different disciplines an opportunity to discuss a variety of EGOV topics and methods related to their research work, dissertations and career plans. The colloquium was co-chaired by Elsa Estevez, Academic Program Officer, United Nations University International Institute for Software Technology, Macao; Hiroko Kudo, Professor of Public Policy and Public Management, Faculty of Law, Chuo University, Japan; and Henk G. Sol, Professor of Business and ICT and Founding Dean, Faculty of Economics and Business, University of Groningen, Netherlands; and attended by Adegboyega Ojo, Research Fellow and Leader of E-Government Group at the INSIGHT Center for Data Analytics, National University of Ireland, Ireland as invited academic.The conference awarded best paper titles in Best Research Paper and Best Experience Paper categories. The selection was carried out jointly by Elsa Estevez as the ICEGOV2013 Awards Chair, and Tomasz Janowski and Jeanne Holm as the ICEGOV2013 Program Chairs. Three papers were nominated to the Best Experience Paper award: 1) A Reputation Based Electronic Government Procurement Model by Hichem Klabi, Sehl Mellouli and Monia Rekik; 2) Government 3.0 in Korea: Fad or Fashion? byTaewoo Nam; and 3) Secure ID Management for Social Security and Tax Number System by Hisao Sakazaki, Dan Yamamoto, Akihiro Sugimoto and Shinji Hirata. The winner in this category was ""A Reputation Based Electronic Government Procurement Model"" by Hichem Klabi, Sehl Mellouli and Monia Rekik. Three papers were also nominated to the Best Research Paper award: 1) Harnessing the Duality of e-Participation Social Software Infrastructure Design by Lukasz Porwol, Adegboyega Ojo and John Breslin; 2) When Food Quality Control in China Meets Mobile and Wireless Technology: Interactions, Potentials and Pitfalls by Shuhua Liu; and 3) Cross-departmental Collaboration in Government One-Stop Center: Factors and Performance by Xinping Liu. The winner in this category was ""Harnessing the Duality of e-Participation Social Software Infrastructure Design"" by Lukasz Porwol, Adegboyega Ojo and John Breslin.Many people and institutions contributed to the organization of ICEGOV2013. We wish to thank the official patron of ICEGOV2013, the Ministry of Security and Public Administration of the Republic of Korea for endorsing and supporting the conference. Our sincere thanks go to the National Information Society Agency, Republic of Korea (NIA) as the local organizer of the conference, particularly to Jeongwon Yoon for his vision and leadership, and to Dohyoon Kim and the whole team in NIA for their hard work and dedication to making the combined ICEGOV2013 and Global e-Government Forum event successful. We wish to express our most sincere thanks to the key sponsors Macao SAR Government and Macao Foundation and the sponsor Electronic Government of the Republic of Kazakhstan whose generous contributions allowed many academics and practitioners from developing countries to attend the conference. Special gratitude is due to Macao SAR Government, its Public Administration and Civil Service Bureau, and Macao Foundation for continuing support to the ICEGOV conference series and the origin of the series e-Macao Program. We also wish to thank ICEGOV2013 partners for their presence, support and in-kind contributions: Brunel University, London, UK; Center for Technology in Government, University at Albany, USA; Data.Gov, U.S. Government; German Cooperation, Deutsche Zusammenarbeit and Deutsche Gesellschaft fur Internationale Zusammenarbeit, Germany; IBM; Information and Communication Technologies, World Bank; Microsoft; Ministry of Information Technology and Communication, Colombia (MINTIC); Organization for Economic Co-operation and Development; Poznan University of Economics, Poland; The Insight Centre for Data Analytics, National University of Ireland, Ireland; The Science and Technology Development Fund, Government of Macao SAR, Macao; UNESCO Information for All Programme; United Nations Asian and Pacific Training Centre for Information and Communication Technology for Development; Vive Digital Programme, MINTIC, Colombia; Waseda University, Japan; and the World Wide Web Consortium. We also wish to express our thanks to ACM Press for publishing the ICEGOV2013 conference proceedings. We are most grateful to the whole Advisory Committee for supporting the conference and to all members of the Program Committee and additional reviewers for their efforts to carry out quality reviews and to help build a strong conference program. We thank keynote speakers; organizers, chairs and moderators of the plenary sessions, town hall debates, paper tracks, thematic sessions, the doctoral colloquium, and the poster session; and all panelists and speakers for their intellectual contributions. Last but not least, we are most thankful to all authors for their efforts in preparing, submitting and presenting papers at ICEGOV2013.We hope that ICEGOV2013 will further contribute to building, growing and connecting global EGOV research, policy and practice communities, able to cross not only national and regional but also institutional and thematic borders, and that the contacts, discussions and ideas initiated in Seoul in October 2013 will continue well after the conference and towards ICEGOV2014 in Guimaraes, Portugal.",,,,2013,,,,Book
Dynamic Security Analysis of Power Systems by a Sampling-Based Algorithm,"Dynamic security analysis is an important problem of power systems on ensuring safe operation and stable power supply even when certain faults occur. No matter if such faults are caused by vulnerabilities of system components, physical attacks, or cyber-attacks that are more related to cyber-security, they eventually affect the physical stability of a power system. Examples of the loss of physical stability include the Northeast Blackout of 2003 in North America and the 2015 system-wide blackout in Ukraine. The nonlinear hybrid nature, that is, nonlinear continuous dynamics integrated with discrete switching, and the high degree of freedom property of power system dynamics make it challenging to conduct the dynamic security analysis. In this article, we use the hybrid automaton model to describe the dynamics of a power system and mainly deal with the index-1 differential-algebraic equation models regarding the continuous dynamics in different discrete states. The analysis problem is formulated as a reachability problem of the associated hybrid model. A sampling-based algorithm is then proposed by integrating modeling and randomized simulation of the hybrid dynamics to search for a feasible execution connecting an initial state of the post-fault system and a target set in the desired operation mode. The proposed method enables the use of existing power system simulators for the synthesis of discrete switching and control strategies through randomized simulation. The effectiveness and performance of the proposed approach are demonstrated with an application to the dynamic security analysis of the New England 39-bus benchmark power system exhibiting hybrid dynamics. In addition to evaluating the dynamic security, the proposed method searches for a feasible strategy to ensure the dynamic security of the system in the face of disruptions.",,"Wu Q,Koo TJ,Susuki Y",,2018,,10.1145/3208093,https://doi-org.proxy.bnl.lu/10.1145/3208093;http://dx.doi.org/10.1145/3208093,Journal Article
Annual Editions: Computers in Society 08/09,"This Fourteenth Edition of ANNUAL EDITIONS: COMPUTERS IN SOCIETY provides convenient, inexpensive access to current articles selected from the best of the public press. Organizational features include: an annotated listing of selected World Wide Web sites; an annotated table of contents; a topic guide; a general introduction; brief overviews for each section; a topical index; and an instructor’s resource guide with testing materials. USING ANNUAL EDITIONS IN THE CLASSROOM is offered as a practical guide for instructors. ANNUAL EDITIONS titles are supported by our student website, . Table of contents UNIT 1. Introduction 1. 34991 Five Things We Need to Know About Technological Change, Neil Postman, Address to New Tech ’98 conference, March 27, 1998 Neil Postman, a well-known cultural critic, suggests that computer technology is too important to be left entirely to the technologists. “Embedded in every technology,” he says, “is a powerful idea….” 2. 46656 Slouching Toward the Ordinary, Susan C. Herring, New Media & Society, February 2004 Contrary to what we read, changes in the ecology of the computing “will continue to make the internet a simpler, safer and—for better or worse—less fascinating communication environment.” 3. 41735 On the Nature of Computing, Jon Crowcroft, Communications of the ACM, February 2005 The author states, “ Occupying a third place in human intellectual culture, computing is not bound by the need to describe what does exist (as in natural science) or what can be built in the real world (as in engineering).” UNIT 2. The Economy 4. 46657 The Subprime Loan Machine, Lynnley Browning, The New York Times, March 23, 2007 “The rise and fall of the subprime market has been told as a story of a flood of Wall Street money and the desire of Americans desperate to be part of the housing boom,” says Lynnley Browning. Yet, the boom was made possible “by a little-noticed tool of automatic underwriting software.” 5. 46658 Click Fraud, Brian Grow and Ben Elgin, Business Week, October 2, 2006 Internet advertisers think they pay only when an interested customer clicks on their ads. Martin Fleischman, an Atlanta businessman, “noticed a growing number of puzzling clicks coming from such places as Botswana, Mongolia, and Syria.” 6. 41736 The Big Band Era, Christopher Swope, Governing, January 2005 Even as cities like Philadelphia are working to transform the entire city into a wireless hot spot—with government as the internet service provider of last resort—communications companies are fighting to keep local governments out of the broadband business. 7. 45257 The Beauty of Simplicity, Linda Tischler, Fast Company, November 2005 A simple tale about simplicity. One company hired an editor from People Magazine to translate accounting lingo into everyday language, “pared back 125 setup screens to three,” and “sold 100,000 units in its first year on the market.” 8. 41740 The Software Wars, Paul De Palma, American Scholar, Winter 2005 The article argues that software development is like military procurement, and suffers many of the same woes, including excessive complexity and cost overruns. 9. 46659 Scan This Book!, Kevin Kelly, The New York Times Magazine, May 14, 2006 What will happen to libraries, books on paper, and copyright protections if Google’s plans to scan the books of five major research libraries succeeds UNIT 3. Work and the Workplace 10. 46660 National ID, Ryan Singel, Wired, May 15, 2007 Immigration is in the news again. One proposal before Congress is to issue American workers tamper-proof biometric Social Security cards. These would replace the text-only design that’s been issued to Americans almost without change for more than 70 years. 11. 34959 Brain Circulation, AnnaLee Saxenian, Brookings Review, Winter 2002 Do immigrants displace native workers Is the United States siphoning off talent from countries that can ill afford to lose it This Berkeley professor argues that high-skill immigration is more complex than that. 12. 41774 The New Face of the Silicon Age, Daniel H. Pink, Wired, February 12, 2004 This piece on Indian programmers should be enough to keep chairs of American computer science departments awake at night. 13. 46661 Computer Software Engineers, Occupational Outlook Handbook, 200607 Edition Here is one official source that acknowledges the effect of shipping high tech jobs abroad, but still predicts that “ software engineers are projected to be one of the fastest-growing occupations from 2004 to 2014.” 14. 41753 The Computer Evolution, Rob Valletta and Geoffrey MacDonald, FRBSF Economic Letter, July 23, 2004 This article uses data from several surveys “to examine two key aspects of the computer evolution: the spread of PCs at work and the evolving wage differentials between individuals who use them and those who do not.” 15. 41754 Making Yourself Understood, Stuart Crainer and Des Dearlove, Across the Board, MayJune 2004 In a business environment where half of surveyed managers report spending more than two hours each day answering e-mail, “it’s never been so easy to be misunderstood.” 16. 46664 Privacy, Legislation, and Surveillance Software, G. Daryl Nord, Tipton F. McCubbins, and Jeretta Horn Nord, Communications of the ACM, August 2006 The authors tell us that the assumption of employee privacy in the workplace “may be naïve.” Constitutional protections against unreasonable search and seizure “usually apply only to state actions.” UNIT 4. Computers, People, and Social Participation 17. 40654 Romance in the Information Age, Christine Rosen, The New Atlantis, Winter 2004 According to Christine Rosen, “our technologies enable and often promote two detrimental forces in modern relationships: the demand for total transparency and a bias toward the over sharing of personal information.” 18. 46665 How Do I Love Thee , Lori Gottlieb, The Atlantic, March 2006 Some Internet dating sites now use social scientists to “develop a new science of attraction.” Says the author, “My matches included a film editor wearing a kilt—and not in an ironic way. Was this the best science could do ” 19. 46666 The Perfect Mark, Mitchell Zuckoff, The New Yorker, May 15, 2006 A cautionary tale about an African scam and two years in prison for bank fraud and money laundering. 20. 41755 Back-to-School Blogging, Brock Read, The Chronicle of Higher Education, September 3, 2004 It should surprise no one that entering freshmen, who grew up using the Internet, should turn to university-sponsored blogs to ease the transition to college life. 21. 46667 E-Mail Is for Old People, Dan Carnevale, The Chronicle of Higher Education, October 6, 2006 Reaching students through email has become more difficult as students turn to text-messaging and social networking sites. UNIT 5. Societal Institutions: Law, Politics, Education, and the Military 22. 37230 The Copyright Paradox, Jonathan Band, Brookings Review, Winter 2001 According to the author, “the problem with piracy is not the inadequacy of existing laws, but the high cost of enforcing any law against the large universe of infringers.” 23. 46668 Piracy, Computer Crime, and IS Misuse at the University, Timothy Paul Cronan, C. Bryan Foltz, and Thomas W. Jones, Communications of the ACM, June 2006 Who are the students who “openly admit to illegally installing software on home computers or otherwise misusing computer information systems ” This article provides some clues. 24. 41764 Facing Down the E-Maelstrom, Jeffrey Selingo, The Chronicle of Higher Education, April 29, 2005 Never an easy job, leading a college in the age of the Internet requires sifting through e-mail, reading blogs, and fending off criticism, the volume of which would be inconceivable without networked computers. 25. 46669 Can Blogs Revolutionize Progressive Politics , Lakshmi Chaudhry, In These Times, February 2006 Liberals have been envious ever since Richard Viguerie’s computer-generated mailing lists contributed to Ronald Reagan’s victory in 1980. At a time when even Senate Majority Leader Harry Reid has a blog, some Democrats hope that the computer is finally on their side. 26. 46670 Center Stage, Carl Sessions Stepp, American Journalism Review, AprilMay 2006 How do a newspaper’s web and print versions differ Unlike the print version of a newspaper, the Web version receives little editing. 27. 46671 The Coming Robot Army, Steve Featherstone, Harper’s Magazine, February 2007 “Within our lifetime,” says Featherstone, “robots will give us the ability to wage war without committing ourselves to the human cost of actually fighting a war.” Sgt. Jason Mero concurs: “These things are amazing…. They don’t complain…. They don’t cry. They’re not scared. This robot here has no fear.” UNIT 6. Risk and Avoiding Risk 28. 41768 Why Spyware Poses Multiple Threats to Security, Roger Thompson, Communications of the ACM, August 2005 Harm caused by spyware ranges from gobbling up computer speed on your PC to enlisting your machine in attacks that can disrupt major businesses or the government. 29. 41769 Terror’s Server, David Talbot, Technology Review, February 2005 “Most experts agree,” says the author, “that the Internet is not just a tool of terrorist organizations, but is central to their operations.” 30. 37238 The Virus Underground, Clive Thompson, The New York Times Magazine, February 8, 2004 Clive Thompson states, “when Mario is bored…he likes to sit at his laptop and create computer viruses and worms.” 31. 46672 Secrets of the Digital Detectives, The Economist, September 23, 2006 It’s nice to learn that the good guys have some tricks of their own. 32. 46673 Data on the Elderly, Marketed to Thieves, Charles Duhigg, The New York Times, May 20, 2007 Thieves purchase lists of the elderly from consumer databases, then pose as government workers trying to update their files on World War II veterans and retired school teachers. Some seniors find themselves with empty bank accounts. 33. 41770 The Fading Memory of the State, David Talbot, Technology Review, July 2005 Government documents, from the 38 million emails generated by the Clinton administration to electronic records of the 1989 invasion of Panama, are on disintegrating electronic media, stored using now-obsolete formats. 34. 41771 False Reporting on the Internet and the Spread of Rumors, Paul Hitlin, Gnovis, April 26, 2004 Internet news sources can sometimes be unreliable. Paul Hitlin examines Internet coverage of the Vince Foster suicide along with other stories to understand why this is so. UNIT 7. International Perspectives and Issues 35. 46675 China’s Tech Generation Finds a New Chairman to Venerate, Kevin Holden, Wired, May 24, 2007 The new China is not a place that would have made Chairman Mao comfortable. One indication is the popularity of Bill Gates. 36. 46677 Is the Crouching Tiger a Threat , Robert L. Glass, Communications of the ACM, March 2006 All indications suggest that the U.S. domination of computing is about to be eclipsed. Here is one commentator who is not quite convinced. 37. 41776 Restoring the Popularity of Computer Science, David A. Patterson, Communications of the ACM, September 2005 While India turns out more and more programmers willing to work for a fraction of their American counterparts, enrollment in computer science classes across the United States is dropping. The author believes that “inaccurate impressions of opportunities” are behind the decline. 38. 41773 China’s Computer Wasteland, Benjamin Joffe-Walt, The Progressive, January 30, 2005 What to do with the detritus of the digital age is a growing problem. Shipping it to China seems to be one solution. 39. 46678 Cat and Mouse, on the Web, The Economist, December 2, 2006 This article examines censorship on the Internet and the extraordinary steps taken by the anti-censorship community to thwart the efforts of censors. 40. 46681 In Search of a PC for the People, Bruce Einhorn, Business Week, June 12, 2006 What features get included in a $200.00 PC marketed to developing nations and “I think of digital access for kids as a human right,” says Nicholas Negroponte of MIT are two issues explored in this article. UNIT 8. The Frontier of Computing 41. 46682 A Nascent Robotics Culture, Sherry Turkle, AAAI Technical Report, July 2006 “What is a robot kind of love ” and “What will we be like, what kind of people are we becoming as we develop increasingly intimate relationships with machines ” MIT’s pioneering sociologist tries to answer both questions 42. 46683 March of the Robolawyers, The Economist, March 11, 2006 Australian researchers have developed a program that helps divorcing couples divide their property. 43. 46684 Best-Kept Secrets, Gary Stix, Scientific American, January 2005 Public-key cryptography keeps e-commerce secure for now. Quantum cryptography might take its place. 44. 46685 Toward Nature-Inspired Computing, Jiming Liu and K.C. Tsui, Communications of the ACM, October 2006 Computer scientists are turning to biology as a source of inspiration for models of complex systems. These biological models change the rules governing systems behavior. 45. 41778 The Intelligent Internet, William E. Halal, The Futurist, MarchApril 2004 The author claims the Internet will be the “main method used in 30% of courses” by 2014. As with all predictions, enjoy, but read critically. 46. 41780 Mind Control, Richard Martin, Wired, March 2005 What does a quadriplegic young man who plays pong have in common with a monkey mentally moving a joy stick and “soldier-controlled killer robots ” The answer: Brain Computer Interface or BCI.",,De Palma P,,2007,,,,Book
"Technological Innovations in Sensing and Detection of Chemical, Biological, Radiological, Nuclear Threats and Ecological Terrorism","This book arises from the NATO Advanced Study Institute Technological Innovations in Detection and Sensing of CBRN Agents and Ecological Terrorism held in Chisinau, Republic of Moldova in June 2010. It comprises a variety of invited contributions by highly experienced educators, scientists, and industrialists, and is structured to cover important aspects of the field that include developments in chemical-biological, and radiation sensing, synthesis and processing of sensors, and applications of sensors in detecting/monitoring contaminants introduced/dispersed inadvertently or intentionally in air, water, and food supplies. The book emphasizes nanomaterials and nanotechnology based sensing and also includes a section on sensing and detection technologies that can be applied to information security. Finally, it examines regional, national, and international policies and ethics related to nanomaterials and sensing. It will be of considerable interest and value to those already pursuing or considering careers in the field of nanostructured materials and nanotechnology based sensing, In general, it serves as a valuable source of information for those interested in how nanomaterials and nanotechnologies are advancing the field of sensing, detection, and remediation, policy makers, and commanders in the field.",,"Vaseashta A,Braman E,Susmann P",,2012,,,,Book
ICETE 2016: Proceedings of the 13th International Joint Conference on E-Business and Telecommunications,"This book contains the proceedings of the 13th International Joint Conference on e-Business and Telecommunications (ICETE 2016).ICETE 2016 is sponsored by INSTICC (the Institute for Systems and Technologies of Information, Control and Communication), technically co-sponsored by the IEEE System Council and the Photonics21 and held in cooperation with the ACM SIGMIS (Special Interest Group on Management Information Systems), the ACM SIGMM (Special Interest Group on Multimedia), the European Optical Society (EOS), the Information Technology Society (ITG), the European Association for Signal Processing (EURASIP), the Information Systems Security Association (ISSA) and the International Association for Cryptologic Research (IACR). Moreover, ICETE 2016 has WfMC (Workflow Management Coalition), OMG (Object Management Group), FIPA (The Foundation for Intelligent Physical Agents) and FCT (Fundação para a Ciência e Tecnologia e Ensino Superior) as Organizational Co-Sponsor. The purpose of the International Joint Conference on e-Business and Telecommunications is to bring together researchers and practitioners interested in the dissemination of new results in the fields of information and communication technologies, including data communication networking, e-business, optical communication systems, security and cryptography, signal processing and multimedia applications, and wireless networks and mobile systems. These are the main conference areas that define the six component conferences, namely: DCNET, ICE-B, OPTICS, SECRYPT, SIGMAP, and WINSYS, which together form the ICETE joint international conference.We would like to emphasize that ICETE 2016 includes five distinguished keynote lectures, delivered by experts in their fields, including (alphabetically): Anastasios Economides (University of Macedonia, Greece), Erina Ferro (CNR-ISTI, Italy), Giuseppe Bianchi (University of Roma Tor Vergata, Italy), Hamid Aghvami (King's College London, United Kingdom) and Mateo Valero (Universidad Politecnica de Catalunya, Spain).With its six segments, we expect it to appeal to a global audience of the engineers, scientists, business practitioners and policy experts, interested in R&D on Telecommunication Systems and Services. All tracks focus on research related to real world applications and rely on contributions not only from academia, but also from industry, business and government, with different solutions for end-user applications and enabling technologies, in a diversity of communication environments. The accepted papers demonstrate a number of new and innovative solutions and the vitality of these research areas.In response to the call for papers, ICETE has received 241 papers in total, with contributions from 51 different countries in all continents, which demonstrate the success and global dimension of ICETE 2016. To evaluate each submission, a double blind paper evaluation method was used: each paper was reviewed by at least two experts from the International Program Committee in a double-blind review process, and most papers had received 3 reviews or more. The selection process followed strict criteria. So, only 43 papers were accepted and orally presented at ICETE as full papers (18% of submissions) and 47 as short papers (20% of submissions). Additionally, 27 papers were accepted for poster presentations.With this acceptance ratio, ICETE 2016 continues the tradition of previous conferences as a distinguished and high-quality conference. Extended versions of selected best papers of the conference will be invited to appear in a post-conference book that will be published by Springer.A successful conference involves more than paper presentations alone. It is also a meeting place, where ideas about new research projects and other ventures are discussed and debated. Therefore, a social event including a conference dinner/banquet has been planned for evening of July 27th in order to promote this kind of social networking. We would like to express our thanks to all colleagues involved in supporting this conference. We would like to thank in particular: the members of the Program Committee and the external reviewers, who really did a great job, devoting expertise and time in reviewing the papers and participating in the discussion process. We would like to thank all the authors who submitted papers, whether or not the paper was eventually included in the program. We would also like to thank the panelists and invited speakers for their invaluable contribution, in sharing their vision, knowledge and research outcomes. Finally, a word of appreciation for the hard work of the INSTICC team; organizing a conference of this level is a task that can only be achieved by the collaborative effort of a dedicated and highly capable team.We hope that the papers accepted and included in the proceedings may be helpful references in future works for all those who need to address topics in the ICETE knowledge areas. Enjoy the program and your stay in Lisbon.",,,,2016,,,,Book
Technical Aspects of the Online E-Management Control and Evaluation System for Universities,"This paper briefly describes several technical aspects and general structure of the e-Management Control and Evaluation System (e-MCES) at the University of Technology (UTech), Jamaica. Some our original technical and security solutions we have implemented in it. We consider this web application as a base for a full management system for educational institutions that includes strategic, academic, and financial planning and management components. This approach will allow the institution to respond promptly to real-world challenges and opportunities that might affect its short- and long-term strategies.",,"Kulkarni AB,Pougatchev V",,2011,20–25,,,Conference Paper
Digital Osteology: 3D Surface Scanning the Os Coxa,"ABSTRACT This research was designed to isolate a standard 3D scanning methodology using a popular 3D scanner (NextEngine®) used in biological anthropology to reproduce research-quality human skeletal remains. Settings using NextEngine® hardware were determined through three successive studies.The first study used minimum settings to create a topography of nine postcranial bones excavated from a Maya archaeological site, Moho Cay, Belize. The research was conducted in Louisiana State University's (LSU) Digital Imaging and Visualization in Archaeology (DIVA) Lab. The resulting 3D scans were visually compared to the original bone specimens for similarities in five categories. The similarity ratings were used to determine the usefulness of the scans for display, teaching, and research. Display-quality scans were at least 50% similar and represented a general likeness of the bone. Teaching-quality scans were at least 75% similar and showed general bone shape and features. Research-quality scans were required to have a 100% similarity. All of the 3D scans were display-quality, and half were similar enough to be used for teaching. No research-quality scans were created in the first study.The second study expanded that research by using the NextEngine® at variable settings to scan an os coxa (hip bone) from Moho Cay, Belize. The 3D scans were evaluated metrically using arbitrary measurements and evaluated through paired t-tests. The 3D scans were visually assessed using the five categories from the first study. Visual age and sex analyses were also conducted. A scanner setting was isolated that was visually and metrically accurate.To test the applicability of the settings determined from the second study, the third study focused on applying the determined standard to 20 modern hip bones provided by the Forensic Anthropology and Computer Enhancement Services (FACES) Lab at LSU. All hip bones were visually analyzed for age and sex, as well as for similarity using the same five categories. Standard os coxa measurements were taken from the scans and the specimens and compared using paired t-tests. The results from the third study supported the second study's assertions. Using the 3D scanning standards derived through this research produced research-quality hip bone scans.",,Harrington V,,2017,,,,Ph.D. Thesis
Resisting Erasure: The Practice of Learning from Maya Mam Narratives of Survivance in Guatemala / Resistiendo La Borradura: La PráCtica de Aprender de Las Narrativas Mayas de Supervivencia En Guatemala,"Through the implementation of innovative research methods and engagement with Indigenous scholarship, scholars of transitional justice and archival studies can learn important lessons from Indigenous scholars and communities who are working to build brighter futures in the shadow of conflict, violence, and genocide. In my doctoral research, here presented in three articles, I make this argument through exploring the case studies of Rwanda and Guatemala, providing methods of analysis, and identifying opportunities for collaborative research. In part one, ""New Documents Shed Light: Why did Peacekeepers Withdraw During Rwanda's 1994 Genocide?"", an article published in 2018 in Genocide Studies and Prevention: an International Journal, I utilize the ""critical oral history method"" and analysis of recently declassified United States government records that shed light on the failed international response to stop the genocide in Rwanda. While methods used in this study elicit important information, the author finds that the study fails to include voices of people who suffered the consequences of these flawed polices.In part two, ""Collaborative Archival Analysis & Co-Producing Knowledge,"" I discuss the use of innovative archival analysis and ""critical oral history"" to address the flaws in the Rwanda project, drawing on important lessons from Indigenous studies and decolonizing methodologies. I apply these lessons in the case study of a collaborative project with the small town of genocide survivors in Nuevo Amanecer in western Guatemala. I argue that by broadening access to documents from the U.S.' ""colonial archive"" about the Guatemalan genocide, we subvert the colonial archive in important ways. While these methods provide promise, future research needs to further develop these approaches.Finally, in part three, ""Beyond Transitional Justice: Learning from Indigenous Maya Resistance in Guatemala,"" I identify concrete lessons scholars and practitioners can learn from Indigenous scholarship when attempting to determine successes and shortcomings of the international community's approach to transitional justice in the case of Guatemala. Through learning from the Indigenous studies concepts of refusal, survivance, and thrivance, as well as ideas of ""damage-centered research"" and ""desire-centered research,"" non-Native scholars can fundamentally reconfigure ideas of how to do their work by centering the voices and projects of communities who are building brighter futures. Ultimately, I conclude that non-Native scholars and practitioners can learn many lessons from Indigenous scholars and communities who are doing innovative future-building work, especially in the field of archival studies and transitional justice.",,"Willard E,Warren A,Lucero J,Rodríguez-Silva I",,2020,,,,Ph.D. Thesis
Complex Systems Analysis of Hybrid Warfare,,,"Nadolski M,Fairbanks J",,2019,210–217,10.1016/j.procs.2019.05.072,https://doi-org.proxy.bnl.lu/10.1016/j.procs.2019.05.072;http://dx.doi.org/10.1016/j.procs.2019.05.072,Journal Article
Resonance: Dynamic Access Control for Enterprise Networks,"Enterprise network security is typically reactive, and it relies heavily on host security and middleboxes. This approach creates complicated interactions between protocols and systems that can cause incorrect behavior and slow response to attacks. We argue that imbuing the network layer with mechanisms for dynamic access control can remedy these ills. We propose Resonance, a system for securing enterprise networks, where the network elements themselves enforce dynamic access control policies based on both flow-level information and real-time alerts. Resonance uses programmable switches to manipulate traffic at lower layers; these switches take actions (e.g., dropping or redirecting traffic) to enforce high-level security policies based on input from both higherlevel security policies and distributed monitoring and inference systems. We describe the design of Resonance, apply it to Georgia Tech's network access control system, show how it can both overcome the current shortcomings and provide new security functions, describe our proposed deployment, and discuss open research questions.",,"Nayak AK,Reimers A,Feamster N,Clark R",,2009,11–18,10.1145/1592681.1592684,https://doi-org.proxy.bnl.lu/10.1145/1592681.1592684;http://dx.doi.org/10.1145/1592681.1592684,Conference Paper
Implementing Human Rights in Afro-Descendant Communities in Colombia and Nicaragua: An Analysis of Selected Human Rights Claims at the Intersection of Legal Orders,"This dissertation examines the human rights claims of two Afro-descendant communities, namely the community of Tierrabomba in Colombia and the community of Orinoco in Nicaragua. It analyzes how these human rights claims are protected in international human rights and domestic legal systems and in practice. This dissertation demonstrates that there is not one legal framework applicable to all Afro-descendant communities due to the cultural, economic, ethnic, and linguistic diversity of this population group. There is a need to tailor the nature and scope of the relevant international and domestic legal protections to the specific circumstances of the communities and their territorial States in order to adequately protect their legal claims. A locally focused approach is necessary to highlight the gaps in the different legal orders applicable to Afro-descendants and to shed light on the effectiveness of the relevant legal norms on the ground. To demonstrate this argument, this dissertation employed a multi-method approach, combining legal doctrinal research with a field study in the communities concerned. Field research revealed the communities' human rights claims and living conditions. Through the doctrinal legal research, this dissertation analyzed the exact nature and scope of the relevant international human rights provisions and their incorporation into the domestic legislations of Colombia and Nicaragua. It then compared these international and domestic legal frameworks with the realities in the communities concerned to identify any gaps in implementation.This dissertation found that the applicable international human rights law and legal systems of Colombia and Nicaragua only partially protect the human rights claims of the communities concerned, and any protection implemented on the ground is ineffective. The communities concerned demand use of their ancestral land, access to justice, and other fundamental rights to protect their community-oriented way of life. The legal analysis showed that the applicable legal norms primarily focus on the folkloric and material aspects of culture, such as unique religious rituals and traditional economic activities. This does not reflect the communities concerned's understanding of culture as a way of life. Additionally, conflicting legal norms applicable to the human rights claims of the communities concerned weaken their legal protections while domestic laws mostly lack effective remedies to enforce human rights provisions. Fear of threats of violence from non-State actors and State neglect further jeopardize the rights' implementation on the ground.To overcome these legal and practical challenges in the communities concerned, this dissertation suggests a set of recommendations for human rights scholars and practitioners. For human rights scholarship, these recommendations center around the integration of empirical research into legal studies and technical analyses of the law. Human rights practitioners should ensure effective agency of the communities concerned, adopt a more flexible understanding of culture, and strengthen local accountability mechanisms. While these recommendations are tailored to the communities concerned, they also inform human rights scholarship and practice for similarly situated communities in Latin America.",,"Aebersold S,Martin C,McInerney-Lankford S",,2020,,,,Ph.D. Thesis
Malware Analysis through High-Level Behavior,"Malware is becoming more and more stealthy to evade detection and analysis. Stealth techniques often involve code transformation, ranging from equivalent code substitution and junk code injection, to continuously transforming code using a polymorphic or a metamorphic engine. Evasion techniques have a great impact on signature-based malware detection, making it very costly and often unsuccessful.We propose to study a malware's network behavior during its execution. While malware may transform its code to evade analysis, we contend that its behavior must mostly remain the same to achieve the malware's ultimate purpose, such as sending spam, scanning for vulnerable hosts, etc. While live malware analysis is hard, we leverage our Fantasm platform on the Deter-Lab testbed to perform it safely and effectively. Based on observed network traffic we propose a behavior classification approach, which can help us interpret the malware's actions and its ultimate purpose at a high level. We then apply our approach to 999 diverse samples from the Georgia Tech Apiary project to understand current trends in malware behaviors.",,"Deng X,Mirkovic J",,2018,5,,,Conference Paper
A Dynamic Graph-Based Cluster Ensemble Approach to Detect Security Attacks in Surveillance Network,"Wireless sensor networks (WSNs) are underlying network infrastructure for a variety of mission-critical surveillance applications. The network should be tolerant of unexpected failures of sensor nodes to meet the Quality of Service (QoS) requirements of these applications. One major cause of failure is active security attacks such as Denial of Service (DoS) attacks. This paper models the problem of detecting such attacks as an anomaly detection problem in a dynamic graph. The problem is addressed by employing a voting based cluster ensemble approach called the K-Means Spectral and Hierarchical ensemble (KSH) approach. The experimental result shows that KSH detected DoS attacks with better accuracy when compared to baseline approaches. sectionIntroduction and Motivation WSNs play a vital role in a variety of mission-critical surveillance applications, such as military surveillance. These applications demand different QoS, such as energy efficiency, coverage, and connectivity from the underlying network. To meet these QoS requirements, WSNs should be tolerant of sensor node failures. Active security attacks such as DoS attacks are one major cause of such failures. The famous Maroochy water treatment and Ukrainian power grid attacks are good instances of active security attacks over wireless sensor networks. Active security attacks are more dangerous in terms of severity it creates in the network. For instance, such an attack on WSNs deployed for military surveillance applications can lead to physical intrusions to happen without being undetected.",,Thomas D,,2021,194–195,,,Conference Paper
The Making of KECCAK,"The sponge function KECCAK is the versatile successor of SHA-1 and the SHA-2 series of hash functions. Its structure and components are quite different from its predecessors, and at first sight it seems like a complete break with the past. In this article, researchers show that KECCAK is the endpoint of a long learning process involving many intermediate designs, mostly gradual changes, but also some drastic changes of direction. Researchers take off from their attempts at fixing PANAMA [26], resulting in RADIOGATÚN [4], and their insights on trail backtracking applied to generalizations of PANAMA and RADIOGATÚN, known as alternating-input and belt-and-mill structures. They explain how they originally proposed the sponge construction to compactly express security claims for their designs and how they finally decided to use it in an actual design which would become KECCAK. Then, they explain the design choices made in KECCAK and how some of its building blocks can be traced back to its predecessor, RADIOGATÚN, and even earlier.",,"Bertoni G,Daemen J,Peeters M,Van Assche G",,2014,26–60,10.1080/01611194.2013.856818,https://doi-org.proxy.bnl.lu/10.1080/01611194.2013.856818;http://dx.doi.org/10.1080/01611194.2013.856818,Journal Article
Free For All,"—Editorial by Martin K. Starr, Editor-in-Chief, Management Science: Application—Letters to the Editor –“To the Editor” by Eugene P. Saxby, Management Science Department, Security First National Bank, Los Angeles–“To the Editor” by Ray Radosevich, The University of Kansas–“To the Editor” by Geoffrey P. E. Clarkson, Manchester Business School, University of Manchester, England–“To the Editor” by Thomas H. Naylor, Duke University–“To the Editor” by Peter J. Kolesar, Columbia University–“To the Editor” by Al Kahl, College of Business, University of Georgia, Athens, Georgia.",,,,1968,B-545–B-552,10.1287/mnsc.14.10.545,https://doi-org.proxy.bnl.lu/10.1287/mnsc.14.10.545;http://dx.doi.org/10.1287/mnsc.14.10.545,Journal Article
"Soft Computing Applications: Proceedings of the 6th International Workshop Soft Computing Applications, SOFA 2014, Volume 1","These volumes constitute the Proceedings of the 6th International Workshop on Soft Computing Applications, or SOFA 2014, held on 24-26 July 2014 in Timisoara, Romania. This edition was organized by the University of Belgrade, Serbia in conjunction with Romanian Society of Control Engineering and Technical Informatics (SRAIT) - Arad Section, The General Association of Engineers in Romania - Arad Section, Institute of Computer Science, Iasi Branch of the Romanian Academy and IEEE Romanian Section. The Soft Computing concept was introduced by Lotfi Zadeh in 1991 and serves to highlight the emergence of computing methodologies in which the accent is on exploiting the tolerance for imprecision and uncertainty to achieve tractability, robustness and low solution cost. Soft computing facilitates the use of fuzzy logic, neurocomputing, evolutionary computing and probabilistic computing in combination, leading to the concept of hybrid intelligent systems. The combination of such intelligent systems tools and a large number of applications introduce a need for a synergy of scientific and technological disciplines in order to show the great potential of Soft Computing in all domains. The conference papers included in these proceedings, published post conference, were grouped into the following area of research: Image, Text and Signal ProcessingIntelligent TransportationModeling and ApplicationsBiomedical ApplicationsNeural Network and ApplicationsKnowledge-Based Technologies for Web Applications, Cloud Computing, Security, Algorithmsand Computer NetworksKnowledge-Based TechnologiesSoft Computing Techniques for Time Series AnalysisSoft Computing and Fuzzy Logic in BiometricsFuzzy Applications Theory and Fuzzy ControlBussiness Process ManagementMethods and Applications in Electrical EngineeringThe volumes provide useful information to professors, researchers and graduated students in area of soft computing techniques and applications, as they report new research work on challenging issues.",,"Balas VE,Jain LC,Kovaevi B",,2015,,,,Book
IWCMC '09: Proceedings of the 2009 International Conference on Wireless Communications and Mobile Computing: Connecting the World Wirelessly,"Welcome to the ACM International Wireless Communications and Mobile Computing Conference (IWCMC 2009) with the theme this year of ""Connecting the world wirelessly."" This year's Conference continues its tradition of being the premier forum for presentation of research results on leading research issues in wireless networking, wireless communications and mobile computing. The mission of the conference is to share novel basic research ideas as well as experimental applications in the wireless area in addition to identifying new directions for future research and development. ACM IWCMC 2009 gives researchers a unique opportunity to share their perspectives with others interested in the various aspects of wireless networking, wireless communication, information theory and mobile computing as well as other related topics. The conference consists of 11 symposia and 5 workshops that cover a broad range of research aspects. In addition, the program includes two outstanding keynote speakers: Prof. Gordon Stuber from Georgia Tech and Dr. Stefan Schmid of NEC Labs-Europe. There will also be two Tutorials given free of charge to all IWCMC attendees. We hope that the conference proceedings will serve as a valuable reference to researchers and developers in the area.This year, we have received more than 600 paper submissions from all over the world. All papers received rigorous peer reviews from our Technical Program Committee (TPC). After carefully examining all the received review reports, the TPC finally selected 285 high-quality papers for presentation at the conference and publication in the ACM IWCMC 2009 proceedings with a total of 43 technical sessions, organized in five parallel tracks. This year, the technical sessions reflect the continued and growing interests in a wide range of spectrum, including wireless communications and networks, crosslayer design and optimization, mobile computing, wireless sensor networks, network security, information theory and applications, etc.",,,,2009,,,,Book
"Soft Computing Applications: Proceedings of the 6th International Workshop Soft Computing Applications (SOFA 2014), Volume 2","These volumes constitute the Proceedings of the 6th International Workshop on Soft Computing Applications, or SOFA 2014, held on 24-26 July 2014 in Timisoara, Romania. This edition was organized by the University of Belgrade, Serbia in conjunction with Romanian Society of Control Engineering and Technical Informatics (SRAIT) - Arad Section, The General Association of Engineers in Romania - Arad Section, Institute of Computer Science, Iasi Branch of the Romanian Academy and IEEE Romanian Section.The Soft Computing concept was introduced by Lotfi Zadeh in 1991 and serves to highlight the emergence of computing methodologies in which the accent is on exploiting the tolerance for imprecision and uncertainty to achieve tractability, robustness and low solution cost. Soft computing facilitates the use of fuzzy logic, neurocomputing, evolutionary computing and probabilistic computing in combination, leading to the concept of hybrid intelligent systems. The combination of such intelligent systems tools and a large number of applications introduce a need for a synergy of scientific and technological disciplines in order to show the great potential of Soft Computing in all domains.The conference papers included in these proceedings, published post conference, were grouped into the following area of research: Image, Text and Signal ProcessingIntelligent TransportationModeling and ApplicationsBiomedical ApplicationsNeural Network and ApplicationsKnowledge-Based Technologies for Web Applications, Cloud Computing, Security, Algorithmsand Computer NetworksKnowledge-Based TechnologiesSoft Computing Techniques for Time Series AnalysisSoft Computing and Fuzzy Logic in BiometricsFuzzy Applications Theory and Fuzzy ControlBusiness Process ManagementMethods and Applications in ElectricalEngineeringThe volumes provide useful information to professors, researchers and graduated students in area of soft computing techniques and applications, as they report new research work on challenging issues.",,"Balas VE,Jain LC,Kovacevic B",,2015,,,,Book
Now I Know Who My Comrades Are: Voices from the Internet Underground,"In China, university students use the Internet to save the life of an attempted murder victim. In Cuba, authorities unsuccessfully try to silence an online critic by sowing seeds of distrust in her marriage. And in Russia, a lone blogger rises to become one of the most prominent opposition figures since the fall of the Soviet Union. Authoritarian governments try to isolate individuals from one another, but in the age of social media freedom of speech is impossible to contain. Online, people discover that they are not alone. As one blogger put it, ""Now I know who my comrades are."" In her groundbreaking book, Now I Know Who My Comrades Are: Voices from the Internet Underground, Emily Parker, formerly a State Department policy advisor, writer at The Wall Street Journal and editor at The New York Times, provides on-the-ground accounts of how the Internet is transforming lives in China, Cuba, and Russia. It's a new phenomenon, but one that's already brought about significant political change. In 2011 ordinary Egyptians, many armed with little more than mobile phones, helped topple a thirty-year-old dictatorship. It was an extraordinary moment in modern history--and Now I Know Who My Comrades Are takes us beyond the Middle East to the next major civil rights battles between the Internet and state control. Star dissidents such as Cuba's Yoani Snchez and China's Ai Weiwei are profiled. Here you'll also find lesser-known bloggers, as well as the back-stories of Internet activism celebrities. Parker charts the rise of Russia's Alexey Navalny from ordinary blogger to one of the greatest threats to Vladimir Putin's regime. This book introduces us to an army of bloggers and tweeters--generals and foot soldiers alike. These activists write in code to outsmart censors and launch online campaigns to get their friends out of jail. They refuse to be intimidated by surveillance cameras or citizen informers. Even as they navigate the risks of authoritarian life, they feel free. Now I Know Who My Comrades Are is their story.",,Parker E,,2015,,,,Book
"Soft Computing Applications: Proceedings of the 7th International Workshop Soft Computing Applications (SOFA 2016), Volume 2","These two volumes constitute the Proceedings of the 7th International Workshop on Soft Computing Applications (SOFA 2016), held on 2426 August 2016 in Arad, Romania. This edition was organized by Aurel Vlaicu University of Arad, Romania, University of Belgrade, Serbia, in conjunction with the Institute of Computer Science, Iasi Branch of the Romanian Academy, IEEE Romanian Section, Romanian Society of Control Engineering and Technical Informatics (SRAIT) - Arad Section, General Association of Engineers in Romania - Arad Section, and BTM Resources Arad. The soft computing concept was introduced by Lotfi Zadeh in 1991 and serves to highlight the emergence of computing methodologies in which the accent is on exploiting the tolerance for imprecision and uncertainty to achieve tractability, robustness and lower costs. Soft computing facilitates the combined use of fuzzy logic, neurocomputing, evolutionary computing and probabilistic computing, leading to the concept of hybrid intelligent systems. The rapid emergence of new tools and applications calls for a synergy of scientific and technological disciplines in order to reveal the great potential of soft computing in all domains. The conference papers included in these proceedings, published post-conference, were grouped into the following areas of research: Methods and Applications in Electrical Engineering Knowledge-Based Technologies for Web Applications, Cloud Computing, Security Algorithms and Computer Networks Biomedical Applications Image, Text and Signal Processing Machine Learning and Applications Business Process Management Fuzzy Applications, Theory and Fuzzy Control Computational Intelligence in Education Soft Computing & Fuzzy Logic in Biometrics (SCFLB) Soft Computing Algorithms Applied in Economy, Industry and Communication Technology Modelling and Applications in Textiles The book helps to disseminate advances in selected active research directions in the field of soft computing, along with current issues and applications of related topics. As such, it provides valuable information for professors, researchers and graduate students in the area of soft computing techniques and applications.",,"Balas VE,Jain LC,Balas MM",,2017,,,,Book
"Never Retreat, Never Retract: Argumentation Analysis for Political Speeches","In this work, we apply argumentation mining techniques, in particular relation prediction, to study political speeches in monological form, where there is no direct interaction between opponents. We argue that this kind of technique can effectively support researchers in history, social and political sciences, which must deal with an increasing amount of data in digital form and need ways to automatically extract and analyse argumentation patterns. We test and discuss our approach based on the analysis of documents issued by R. Nixon and J. F. Kennedy during 1960 presidential campaign. We rely on a supervised classifier to predict argument relations (i.e., support and attack), obtaining an accuracy of 0.72 on a dataset of 1,462 argument pairs. The application of argument mining to such data allows not only to highlight the main points of agreement and disagreement between the candidates' arguments over the campaign issues such as Cuba, disarmament and health-care, but also an in-depth argumentative analysis of the respective viewpoints on these topics.",,"Menini S,Cabrio E,Tonelli S,Villata S",,2018,,,,Conference Paper
A Voice in the Crowd: Broader Implications for Crowdsourcing Translation during Crisis,"Both international non-governmental organizations and government actors have embraced the technological union of humans and software, known as crowdsourcing, to manage the flood of information produced during recent crises. However, unlike a business solution, the task of translation is unique during a crisis situation; the costs are human, and the impact is social and political. This paper follows four crises in which different crowdsourcing applications were developed by a range of actors. In each instance, the design approach failed to incorporate the unique circumstances of the conflict context, resulting in a translation application that removed authorship, dissolved intentionality, and shed contextual markers from original sources. This flawed application prevented the original contributors from interacting with the information directly related to their own life-threatening situation, and the information it amassed formed an unsound basis for decision-making by international actors. The associated consequences during: post-earthquake Haiti 2010, Libya and Egypt 2011 and Somalia 2011/12 are intended to provoke process improvement among all stakeholders.",,Sutherlin G,,2013,397–409,10.1177/0165551512471593,https://doi-org.proxy.bnl.lu/10.1177/0165551512471593;http://dx.doi.org/10.1177/0165551512471593,Journal Article
ACSC '08: Proceedings of the Thirty-First Australasian Conference on Computer Science - Volume 74,"The Australasian Computer Science Conference (ACSC) series is an annual forum, bringing together research sub-disciplines in Computer Science. The meeting allows academics and researchers to discuss research topics as well as progress in the field, and policies to stimulate its growth. This volume contains papers presented at the Thirty First ACSC in Wollongong, NSW, Australia. ACSC 2008 is part of the Australasian Computer Science Week which ran from Jan 22nd to 25th, 2008.The ACSC 2008 call for papers solicited contributions in all areas of computer science research. This years conference received submissions from Australia, New Zealand, China, France, India, Iran, Jamaica, Jordon, Malaysia, Pakistan, South Africa, Turkey, UK, and Taiwan. The topics addressed by the submitted papers illustrate the broadness of the discipline. The authors categorised their submissions into one or more of the following topics:- Algorithms (9 papers)- Artificial Intelligence (7 papers)- Communications and Networks (4 papers)- Computer Architecture (2 paper)- Computer Vision (4 papers)- Databases (5 papers)- Distributed Systems (6 papers)- E-Commerce (4 papers)- Formal Methods (6 papers)- Graphics (6 papers)- High Performance Computing (7 papers)- Human-Computer Interaction (8 papers)- Mobile Computing (6 papers)- Multimedia (1 paper)- Object Oriented Systems (3 papers)- Ontologies (1 paper)- Operating Systems (5 papers)- Programming Languages (4 papers)- Robotics (1 paper)- Scientific Computing (5 papers)- Security and Trusted Systems (5 papers)- Simulation (6 papers)- Software Engineering (5 papers)- Speech (1 paper)- Theory (3 papers)- Visualization (6 papers)- Web Services (3 papers)The programme committee consisted of 28 highly regarded academics from around the globe, including Australia, Brazil, Canada, Japan, New Zealand, Singapore and USA. All papers were sent to at least three programme committee members for review and every effort was made to obtain at least three reviews. Of the 47 papers submitted, 16 were selected for presentation at the conference.The programme committee invited Professor Joxan Jaffar, to give a keynote on Constraint Logic Programming for Program Analysis. Professor Jaffar has recently completed a stint as Dean of the School of Computing from 2001-2007 at the National University of Singapore. His interests are in programming languages and applications, with emphasis on the logic and constraint programming paradigms. Amongst his main contributions are the principles of constraint logic programming, and the widely-used CLP(R) system. The committee also invited Dr Benjamin Burton and Associate Professor Ewan Tempero to give invited talks. Dr Burtons talk was entitled Informatics Olympiads:Challenges in Programming and Algorithm Design. Associate Professor Temperos talk is entitled On Measuring Java Software.",,,,2008,,,,Book
Uncertainty Management with Fuzzy and Rough Sets: Recent Advances and Applications,"This book offers a timely overview of fuzzy and rough set theories and methods. Based on selected contributions presented at the International Symposium on Fuzzy and Rough Sets, ISFUROS 2017, held in Varadero, Cuba, on October 24-26, 2017, the book also covers related approaches, such as hybrid rough-fuzzy sets and hybrid fuzzy-rough sets and granular computing, as well as a number of applications, from big data analytics, to business intelligence, security, robotics, logistics, wireless sensor networks and many more. It is intended as a source of inspiration for PhD students and researchers in the field, fostering not only new ideas but also collaboration between young researchers and institutions and established ones.",,"Bello R,Falcon R,Verdegay JL",,2019,,,,Book
Dynamic Data Driven Transportation Systems,"Congestion-induced delays and pollution in modern transportation systems remain formidable impediments to the sustainable growth of our cities. Next generation Intelligent Transportation Systems (ITS) will attack these problems by relying on extensive in-vehicle sensing, crowd-sourced data, ubiquitous computing, and communications to augment existing infrastructure-based deployments. Advances in wireless networking and mobile computing have made it possible to create dynamic, data driven application systems (DDDAS) that address many challenges in modern transportation systems. We outline a vision for future dynamic data-driven transportation systems, and focus on the effectiveness of an approach to real-time management based on online simulations. The online simulations are embedded in the traffic network where distributed simulators perform the modeling task individually but project the future states collectively. A real-time data driven arterial simulation methodology is proposed to assist such computations that are performed over a testbed in the midtown area of Atlanta, Georgia. Field results are presented that provide evidence to validate the proposed approach.",,"Suh W,Henclewood D,Guin A,Guensler R,Hunter M,Fujimoto R",,2017,25253–25269,10.1007/s11042-016-4318-x,https://doi-org.proxy.bnl.lu/10.1007/s11042-016-4318-x;http://dx.doi.org/10.1007/s11042-016-4318-x,Journal Article
Final Jeopardy: Man vs. Machine and the Quest to Know Everything,"What if there were a computer that could answer virtually any question? IBM engineers are developing such a machine, teaching it to compete on the quiz show Jeopardy. In February 2011, it will face off in a nationally televised game against two of the games greatest all-time winners, Ken Jennings and Brad Rutter. Final Jeopardy tells the riveting story behind the match. Final Jeopardy carries readers on a captivating journey from the IBM lab to the podium. The story features brilliant Ph.D.s, Hollywood moguls, knowledge-obsessed Jeopardy masters and a very special collection of silicon and circuitry named Watson. It is a classic match of Man vs. Machine, not seen since Deep Blue bested chess grandmaster Garry Kasparov. But Watson will need to do more than churn through chess moves or find a relevant web page. It will have to understand language, including puns and irony, and master everything from history and literature to science, arts, and entertainment. At its heart, Final Jeopardy is about the future of knowledge. What can we teach machines? What will Watsons heirs be capable of in ten or twenty years? And where does that leave humans? As fast and fun as the game itself, Final Jeopardy shows how smart machines will fit into our world and how theyll disrupt it. www.finaljeopardy.net A Q & A With Author Stephen Baker Q: What did you come to most admire about the researchers working to develop Watson? A: I found myself admiring their meticulous engineering. Ive always enjoyed stories of great engineering, from the building of the Panama Canal to the rescue of Apollo 13. The work on Watson fits into that genre. It involves continual problem-solving, innovations, incremental improvements, and above all, endless patience. To do this work, the Jeopardy team had to break down the way we think, the way we understand sentences and concepts and facts, into tiny components, and then teach them to Watson. I have to say, I came out of this process with an ever greater appreciation for the magic that takes place between our ears. We dont give ourselves enough credit. Seriously. They had to build a roaring complex of computers and supply it with enough electricity to light up an entire town, all this just to approximate the question-answering part of the organ we carry in our heads. Unlike Watson, our thinking machines can run for hours on just a cup of coffee and a donut. Q: What was most surprising to you about Watsons behavior? A: Two things: First, its speed. When researchers describe all of the work it takes for a machine to make sense of a question and hunt down possible answers, it makes perfect sense that the process would take a computer two hours. And in the beginning, it did. The fact that they engineered that two-hour process into a mere three seconds is astounding. The second surprise was that Watson could be so amazingly smart on one question, then laughably clueless on the next. How could it ever conclude that the Russian word for good-bye would be ""cholesterol""? How could it confuse Oliver Twist with the Pet Shop Boys? But you know what? I found that when I watched Watson screw up, I had even greater appreciation for the work involved when it got things right. If it got everything right, Watson wouldnt be the fallible (and entertaining) machine that it is. It would just be magic--which really is not nearly as impressive. Q: So who is in charge of picking the clues for the final match? Do you think the arrangements for the match are fair? A: In the end, Jeopardy chose 30 games that the writers had prepared for the Jeopardy season that began in the summer of 2010before they knew for sure that a man-machine match would take place. Each of the games was given a number. Then they had an outside compliance company select two of the games by number. I think theyve made the game as fair as possible. I should add that Watsons scientific test comes from a bigger set of matches. The machine took on human champions in 56 matches in the fall of 2010. It won a majority of those matches. And for the field of question-answering, those games mean more than the televised showdown, simply because there are more of them. Q: If you had to place bets, who do you think will win the match? A: I would bet on the computer, because theres only one computer facing two humans. Ken Jennings and Brad Rutter will compete against each other in areas where humans are presumably strongestclues associated with puns, humor, and difficult context. The computer, I would think, might dominate in the factual realms it ""understands."" That said, Id never wager much on one game. One lucky Daily Double or Final Jeopardy can decide a single match. Q: Should people who dont watch Jeopardy care about this story? If so, why? A: Jeopardy is just a showcase for a new type of machine. Look, were going to be living with these things, working with them, and using them as external lobes of our brains. Final Jeopardy follows the education of one such machine. Readers, Im hoping, will get a feel for its potential as well as its limitations. And that will help them understand what skills and knowledge theyll need to carry around in their own heads. Of course, Im also hoping theyll enjoy the story. Q: Doesnt Google already answer all our questions? What makes Watson so special? A: Google is so useful that we sometimes forget how much more it could tell us. First, it doesnt answer questions. It usually just points in the direction of the information were looking for and leaves the rest of the brain work to us. Watson, by contrast, puts together pieces of information from different sources and comes up with answers or hypotheses. A simple example. Lets say you want to buy a dog for your aging parents. It has to be small, quiet, well-behaved, able to tolerate long periods indoors, and friendly enough to sit on a lap. A hunt for such a dog on a search engine would require lots of different searches (unless someone had happened to write an article about precisely that challenge). But a machine like Watson would understand the sentence in English, read through several thousand documents, and put together a list of candidate dogs. It acts much more like a knowledgeable person. Q: Did Watson become more or less ""human-like"" to you over the course of the project? A: Its funny. When its playing Jeopardy, I find myself referring to it as him . But when you hang around with the researchers, the human evaporates and Watson returns to its truer form, that of an enormously sophisticated computer program. Sometimes its tempting to think of Watson as a brain. But as you deal with it, you see that its not even close to a full brain. It just handles information retrieval and question-answering. In short, its a tool. Q: How does the rest of the computing world see Watson? Is this the true path to Artificial Intelligence? A: Many folks in AI resent Watson, some of them to the point of loathing. You see, theres this dream of building machines that blend the intelligence of humans with the data-processing wizardry of computers. For people who hold that visionand many still doWatson is almost a false prophet. In the realm of Jeopardy, it acts like a human. But instead of processing information the way we do, it just crunches numbers. It doesnt really know or understand anything, or come up with ideas. Yet it worksand it gets to parade its smarts on national TV! If we as a society settle for machines like Watson, will we continue to fund the ambitious research that seeks to replicate the magic of the human brain? For many, thats the true path to AI. But as far as Im concerned, there doesnt have to be just one path. Q:How did IBM decide what to name Watson and who created its public image? A:There was lots of debate within IBM about Watsons name and image. How human should it be? Many worried that the public would view Watson as scary: a machine that learns our secrets and steals our jobs. So they decided to limit Watsons human qualities. They would give its friendly, masculine voice a machine-like overtone. And its face, if you could call it that, would simply be a circular avatarno eyes, nose or mouth, just streaming patterns representing flowing data. Despite these choices, Ive noticed that fellow Jeopardy players immediately start to respond to Watson as another humanand not necessarily a friendly one. Its playing the game, after all. And it usually beats them. As far as the name, IBM entertained loads of possibilities. They considered THINQ, Ace, even EureQA, a blend of Eureka with QA, for question answering. In the end, they picked Watson, for IBMs founder, Thomas J. Watson. In the literary world, it also fit into the stories of Sherlock Holmes, a master question-answerer. Of course, in those stories, Watson was only the assistant to the true genius. But considering the widespread fears surrounding smart computers, maybe it made sense to name the question-answering machine after Holmes plodding number two. Q:If Watson-like systems do become ubiquitous, what will that mean for humans? A:Theres no question about it. Machines like Watson are going to become part of our lives. Theyll be manning call centers, answering questions in offices, factories and emergency rooms. And theyll be available to all of us through our smart phones, often answering spoken questions. This intelligence will become, de facto, part of the human brain. Each one of us will have to figure out how to leverage these smart systems for our own goodand not be replaced by them. Our brains are still the most intricate, complex and brilliant thinking machines on earth. But we have to figure out how to use them in concert with the machinery were building.",,Baker S,,2011,,,,Book
"Soft Computing Applications: Proceedings of the 7th International Workshop Soft Computing Applications (SOFA 2016) , Volume 1","These two volumes constitute the Proceedings of the 7th International Workshop on Soft Computing Applications (SOFA 2016), held on 2426 August 2016 in Arad, Romania. This edition was organized by Aurel Vlaicu University of Arad, Romania, University of Belgrade, Serbia, in conjunction with the Institute of Computer Science, Iasi Branch of the Romanian Academy, IEEE Romanian Section, Romanian Society of Control Engineering and Technical Informatics (SRAIT) - Arad Section, General Association of Engineers in Romania - Arad Section, and BTM Resources Arad. The soft computing concept was introduced by Lotfi Zadeh in 1991 and serves to highlight the emergence of computing methodologies in which the accent is on exploiting the tolerance for imprecision and uncertainty to achieve tractability, robustness and lower costs. Soft computing facilitates the combined use of fuzzy logic, neurocomputing, evolutionary computing and probabilistic computing, leading to the concept of hybrid intelligent systems. The rapid emergence of new tools and applications calls for a synergy of scientific and technological disciplines in order to reveal the great potential of soft computing in all domains. The conference papers included in these proceedings, published post-conference, were grouped into the following areas of research: Methods and Applications in Electrical Engineering Knowledge-Based Technologies for Web Applications, Cloud Computing, Security Algorithms and Computer Networks Biomedical Applications Image, Text and Signal Processing Machine Learning and Applications Business Process Management Fuzzy Applications, Theory and Fuzzy Control Computational Intelligence in Education Soft Computing & Fuzzy Logic in Biometrics (SCFLB) Soft Computing Algorithms Applied in Economy, Industry and Communication Technology Modelling and Applications in Textiles The book helps to disseminate advances in selected active research directions in the field of soft computing, along with current issues and applications of related topics. As such, it provides valuable information for professors, researchers and graduate students in the area of soft computing techniques and applications.",,"Balas VE,Jain LC,Balas MM",,2017,,,,Book
Investigating Current-Based And Gating Approaches For Accurate And Energy-Efficient Spiking Recurrent Neural Networks,"Spiking Neural Networks (SNNs) with spike-based computations and communications may be more energy-efficient than Artificial Neural Networks (ANNs) for embedded applications. However, SNNs have mostly been applied to image processing, although audio applications may better fit their temporal dynamics. We evaluate the accuracy and energy-efficiency of Leaky Integrate-and-Fire (LIF) models on spiking audio datasets compared to ANNs. We demonstrate that, for processing temporal sequences, the Current-based LIF (Cuba-LIF) outperforms the LIF. Moreover, gated recurrent networks have demonstrated superior accuracy than simple recurrent networks for such tasks. Therefore, we introduce SpikGRU, a gated version of the Cuba-LIF. SpikGRU achieves higher accuracy than other recurrent SNNs on the most difficult task studied in this work. The Cuba-LIF and SpikGRU reach state-of-the-art accuracy, only <1.1% below the accuracy of the best ANNs, while showing up to a 49x reduction in the number of operations compared to ANNs, due to the high spike sparsity.",,"Dampfhoffer M,Mesquida T,Valentian A,Anghel L",,2022,359–370,10.1007/978-3-031-15934-3_30,https://doi-org.proxy.bnl.lu/10.1007/978-3-031-15934-3_30;http://dx.doi.org/10.1007/978-3-031-15934-3_30,Conference Paper
Reverse Engineering Human Mobility in Large-Scale Natural Disasters,"Delay/Disruption-Tolerant Networks (DTNs) have been around for more than a decade and have especially been proposed to be used in scenarios where communication infrastructure is unavailable. In such scenarios, DTNs can offer a best-effort communication service by exploiting user mobility. Natural disasters are an important application scenario for DTNs when the cellular network is destroyed by natural forces. To assess the performance of such networks before deployment, we require appropriate knowledge of human mobility. In this paper, we address this problem by designing, implementing, and evaluating a novel mobility model for large-scale natural disasters. Due to the lack of GPS traces, we reverse-engineer human mobility of past natural disasters (focusing on 2010 Haiti earthquake and 2013 Typhoon Haiyan) by leveraging knowledge of 126 experts from 71 Disaster Response Organizations (DROs). By means of simulation-based experiments, we compare and contrast our mobility model to other well-known models, and evaluate their impact on DTN performance. Finally, we make our source code available to the public.",,"Stute M,Maass M,Schons T,Hollick M",,2017,219–226,10.1145/3127540.3127542,https://doi-org.proxy.bnl.lu/10.1145/3127540.3127542;http://dx.doi.org/10.1145/3127540.3127542,Conference Paper
ICT Innovations 2013: ICT Innovations and Education,"Information communication technologies have become the necessity in everyday life enabling increased level of communication, processing and information exchange to extent that one could not imagine only a decade ago. Innovations in these technologies open new fields in areas such as: language processing, biology, medicine, robotics, security, urban planning, networking, governance and many others. The applications of these innovations are used to define services that not only ease, but also increase the quality of life.Good education is essential for establishing solid basis of individual development and performance. ICT is integrated part of education at every level and type. Therefore, the special focus should be given to possible deployment of the novel technologies in order to achieve educational paradigms adapted to possible educational consumer specific and individual needs. This book offers a collection of papers presented at the Fifth International Conference on ICT Innovations held in September 2013, in Ohrid, Macedonia. The conference gathered academics, professionals and practitioners in developing solutions and systems in the industrial and business arena especially innovative commercial implementations, novel applications of technology, and experience in applying recent ICT research advances to practical solutions.",,"Trajkovik V,Anastas M",,2013,,,,Book
Now I Know Who My Comrades Are: Voices from the Internet Underground,"In China, university students use the Internet to save the life of an attempted murder victim. In Cuba, authorities unsuccessfully try to silence an online critic by sowing seeds of distrust in her marriage. And in Russia, a lone blogger rises to become one of the most prominent opposition figures since the fall of the Soviet Union. Authoritarian governments try to isolate individuals from one another, but in the age of social media this is impossible to do. Online, people discover that they are not alone. As one blogger put it, ""Now I know who my comrades are.""In her groundbreaking book, Now I Know Who My Comrades Are: Voices from the Internet Underground, Emily Parker, formerly a State Department policy advisor, writer at The Wall Street Journal and editor at The New York Times, provides on-the-ground accounts of how the Internet is transforming lives in China, Cuba, and Russia.Its a new phenomenon, but one thats already brought about significant political change. In 2011 ordinary Egyptians, many armed with little more than mobile phones, helped topple a thirty-year-old dictatorship. It was an extraordinary moment in modern historyand Now I Know Who My Comrades Are takes us beyond the Middle East to the next major battles between the Internet and state control.Star dissidents such as Cubas Yoani Snchez and China's Ai Weiwei are profiled. Here youll also find lesser-known bloggers, as well as the back-stories of Internet celebrities. Parker charts the rise of Russias Alexey Navalny from ordinary blogger to one of the greatest threats to Vladimir Putins regime.This book introduces us to an army of bloggers and tweetersgenerals and foot soldiers alike. They write in code to outsmart censors and launch online campaigns to get their friends out of jail. They refuse to be intimidated by surveillance cameras or citizen informers. Even as they navigate the risks of authoritarian life, they feel free. Now I Know Who My Comrades Are is their story.",,Parker E,,2014,,,,Book
Design and Capabilities of an Enhanced Naval Mine Warfare Simulation Framework,"The Naval Surface Warfare Center, Panama City Division (NSWC PCD) designed and implemented a new tool, The Rapid Mine Simulation System Enterprise Architecture (RMSSEA), to support existing naval mine warfare simulations and to provide enhanced future mine warfare capabilities. RMSSEA supports existing physics-based models of Navy assets and threats in order to provide ship susceptibility and sweep effectiveness measures. The tool expands support for modeling of future systems, including maneuverable surface and underwater unmanned systems. Additionally, RMSSEA allows for simulations of distributed sensor and mobile warhead devices. The tool incorporates improved automation and visualization, which reduces simulation setup time and supports increased focus on results analysis.",,"Floore TE,Gilman GH",,2011,2612–2618,,,Conference Paper
Social Network Theory: A Comparative Analysis of the Jewish Revolt in Antiquity and the Cyber Terrorism Incident over Kosovo,This paper uses social network theory to compare the social network of the Jewish Revolt in 66-73 AD and the cyber terrorist attacks during the Kosovo war in 1999. The goal is to demonstrate that terrorist networks in Antiquity and cyber terrorist networks today not only mirror each other in their patterns of “terror” stratagems but also differ in key ways. The use of social network theory is appropriate because the theory embodies a particular theoretical orientation towards the structure of terrorist networks. This is why social network theory applies well to the Jewish Revolt and the Kosovo war. The ultimate goal of this analysis is to bridge the gap between theory and practice.,,Matusitz J,,2011,34–44,10.1080/19393555.2010.544702,https://doi-org.proxy.bnl.lu/10.1080/19393555.2010.544702;http://dx.doi.org/10.1080/19393555.2010.544702,Journal Article
The Theory of Plafales: The Proof of P versus NP Problem,"""The theory of plafales: the proof of P versus NP problem, the new proof of continuum hypothesis, the identication system of military objects"" postulated the theory of new mathematical objects-plafales. The theory of plafales is the eclecticism of different mathematical sciences: mathematical analysis, the theory of sets, the theory of categories, differential geometry, harmonic analysis, vector analysis and the theory of manifolds. In the work exist the drawings which are the illustrations of mathematical representations. The drawings were made in engineering design software. Work structure is as follows: thesis mathematical basis of the thesis the drawings Work has the ""smooth transition"":each section is a logical consequence of another. The proof of P versus NP problem -is one of the consequence of the theory of plafales. The proof of continuum hypothesis is one of the consequence of the theory of plafales. The work was done in the style of the ""orchestra"". This was achieved by introducing systems: the system of camoufleur the system of observer the system of kapellmeister the system of master the system of principal the system of president The practical side: on the basis of above theory exist open cryptographic algorithms that can be applied in different spheres: the identifcation system of military objects the system of bank security the alarm system In the work was demonstrated one of the cryptographic algorithm for the identification system of military objects. Everyone can built own cryptographic algorithm with private key. About the Author Dmytro Topchyi was born 03 february 1987 in the Ukraine, he is highly educated at Admiral Makarov National University of Shipbuilding and graduated in 2008 with an applied Applied Mathematics Major. He lead The group of programmers and developers management, Creation of shared and mathematical algorithms for web projects, was Teaching of higher mathematics for programmers at University ""Ukraine"", and worked as a mathematician at Prima Sp.z.o.o.,created of the mathematical tools for ReduxCO catalyst and hazardous chemicals destruction reactor engineering. He is currently Specialist in Applied Mathematics in Engineering and Creating of inorganic slabs with special properties of thermal power.",,Topchyi D,,2011,,,,Book
Serverless Science Gateway Development for Ca2+ Binding Site Prediction on Amazon Web Services: Case Study,"In this paper we discuss the development of a science gateway; identifying Ca2+ binding sites in proteins using a java application developed by Dr. Jenny Yang at the Chemistry department, Georgia State University. Starting with a Protein Data Bank (PDB) X-ray or NMR structure file, MUGC application predicts calcium binding sites using a graph theory-based algorithm [1]. The project creates a science gateway to provide access to the MUGC algorithm using tools provided by Amazon Web Services. The full-stack solution uses S3 storage, AWS Lambda functions, and API gateway to relay the PDB files to the back-end computing in EC2. Architecture for a full stack serverless processing pipeline is implemented which allows users to access the application. The design is optimized for scalability, reliability, security, performance, and cost.",,"Mashiku M,Edirisinghe N",,2019,,10.1145/3332186.3333050,https://doi-org.proxy.bnl.lu/10.1145/3332186.3333050;http://dx.doi.org/10.1145/3332186.3333050,Conference Paper
PPDP '12: Proceedings of the 14th Symposium on Principles and Practice of Declarative Programming,"It is our great pleasure to welcome you to the 14th Symposium on Principles and Practice of Declarative Programming -- PPDP'12. This year's symposium continues its long tradition of bringing together researchers from the various declarative programming communities, including those working in the logic, constraint and functional programming paradigms. Our mission is to stimulate research in the use of logical formalisms and methods for specifying, performing, and share novel solutions for analysing computations, including mechanisms for mobility, modularity, concurrency, object-orientation, security, verification and static analysis.The call for papers attracted 42 submissions from Australia, Austria, Belgium, Brazil, Canada, Colombia, Denmark, France, Germany, Italy, Netherlands, Portugal, Qatar, Reunion, Serbia, Singapore, Slovenia, Spain, Sweden, Turkey, the United Kingdom and the United States, although some authors withdrew their papers for various reasons. The Program Committee (PC) too had a very international feel, drawn from 16 different countries. The PC accepted 18 papers in all that cover a variety of topics, including the applications of declarative languages. All papers received at least 3 reviews; external reviewers contributed 38 of these reviews. In addition, the proceedings includes the invited paper, ""Symbolic Evaluation Graphs and Term Rewriting -- A General Methodology for Analyzing Logic Programs"", kindly contributed by Jürgen Giesl and his colleagues, that supports the PPDP'12/LOPSTR'12 joint invited talk. We are delighted that Torben Schaub agreed to give the PPDP'12 invited talk on ""Answer Set Solving in Practice"". In addition, the program includes the invited talk, ""Indexing Structures in Programming Language Semantics and Design"" by Marcelo Fiore. This talk celebrates the paper ""Semantic analysis of normalisation by evaluation for typed lambda calculus"" by Marcelo that (at the time of writing) is the most highly cited paper from the PDPP'02 proceedings.",,,,2012,,,,Book
Review Article: RePIDS: A Multi Tier Real-Time Payload-Based Intrusion Detection System,"Intrusion Detection System (IDS) deals with huge amount of network traffic and uses large feature set to discriminate normal pattern and intrusive pattern. However, most of existing systems lack the ability to process data for real-time anomaly detection. In this paper, we propose a 3-Tier Iterative Feature Selection Engine (IFSEng) for feature subspace selection. Principal Component Analysis (PCA) technique is used for the pre-processing of data. Mahalanobis Distance Map (MDM) is used to discover hidden correlations between the features and between the packets. We also propose a novel Real-time Payload-based Intrusion Detection System (RePIDS) that integrates a 3-Tier IFSEng and the MDM approach. Mahalanobis Distance (MD) dissimilarity criterion is used to classify each packet as either a normal or an attack packet. The effectiveness of the proposed RePIDS is evaluated using DARPA 99 dataset and Georgia Institute of Technology attack dataset. The traffic for Web-based application is considered for validating our model. F-value, a criterion, is used to evaluate the detection performance of RePIDS. Experimental results show that RePIDS achieves better performance (high F-values, 0.9958 for DARPA 99 dataset and 0.976 for Georgia Institute of Technology attack dataset respectively, with only 0.85% false alarm rate) and lower computational complexity when compared against two state-of-the-art payload-based intrusion detection systems. Additionally, it has 1.3 time higher throughput in comparison with real scenario of medium sized enterprise network.",,"Jamdagni A,Tan Z,He X,Nanda P,Liu RP",,2013,811–824,10.1016/j.comnet.2012.10.002,https://doi-org.proxy.bnl.lu/10.1016/j.comnet.2012.10.002;http://dx.doi.org/10.1016/j.comnet.2012.10.002,Journal Article
System Performance and Layered Analysis Tool,"Naval Surface Warfare Center Panama City Division (NSWC-PCD) has developed a System Performance and Layered Analysis Tool (SPLAT) using MATLAB. The overall goal is to detect terrorist threats, particularly in an open crowded area, in a timely manner. Given a sensor configuration and a scenario specification, it combines a layered set of threat detection sensors to determine overall system performance in terms of probability of detection, probability of false alarm, and cost. SPLAT avoids overly optimistic performance estimates inherent when a series of closely spaced detection events are modeled as discrete, independent Bernoulli trials. SPLAT describes all sensors using multi-dimensional lookup tables, thereby circumventing the need to mathematically model complex sensor performance functions. This methodology is sufficiently general that it can be applied to a broad class of problems where multiple stationary sensors attempt to detect a moving target.",,"Hyland JC,Smith CM",,2011,2600–2611,,,Conference Paper
SPIMACS '09: Proceedings of the First ACM Workshop on Security and Privacy in Medical and Home-Care Systems,"A critical role of computing and information technologies is and will increasingly be the provision of personal health and safety. As computing becomes ever more deeply integrated into daily life computer security will necessarily takes on increasingly personal domains beyond that of traditional corporate and military security. Examples of emerging domains include home monitoring for elders, monitoring homes for break-ins, active medical devices embedded in the body, health monitoring, sharing of protected genetic data, and ubiquitous health monitoring. Research on computer security as personal safety requires blending of social context, qualitative understandings, and human behavior with technical security aspects. As illustrated so dramatically in the main conference, such fundamental physical issues as the transmission of wireless signals through flesh can be a source of defense for in-body computing. This intersection of technology, data that are detailed to the point of intimacy, and a user population that is not interested in managing the technical minutia of securing systems requires a holistic approach to computer security.The first Security and Privacy in Medical and Home-Care Systems (SPIMACS) answers the challenges of this emerging reality with a call for more diverse, interdisciplinary and nuanced design and evaluation of security and privacy technologies. SPIMACS includes a wide range of methods, topics and even more diverse presentations. Submissions included multimedia presentations on the use of video in communicating privacy risks, architectural proposals, and prototype demonstrations. The keynote speaker, Latanya Sweeney, is both the Director of the Data Privacy Laboratory at Carnegie Mellon University and a public servant in her role as a member of the Federal Health IT Policy Committee. The importance and interactions of her two roles, as policy analyst and laboratory scientist, illustrate some of the challenges in researching these emerging arenas.The presentations in this inaugural workshop range from the traditional technological to the inherently qualitatively personal. In a more traditional approach to securing health information Carl Gunter offers an approach to software engineering that embeds the safety, usability and life-critical issues associated with those people in assisted living environments. He then joins researchers to examine an new approach to access control optimized for medical records. Kevin Fu, with Andres Molina and Mastooreh Salajegheh provide a distinctive approach to health information by building upon the observation that anonymity loves company. Avi Rubin focuses on the mobile domain and the need to secure records on devices with limited processing capacity, and power in work with Ryan W. Gardner, Sujata Garera, Matthew W. Pagano and Matthew Green.Sasikanth Avancha, Amit Baxi and David Kotz offer a new framework for the evaluation of privacy when the data may be sensitive in terms of both location and activity, with a particular focus on mobile health and home-care systems. Unfortunately for the policy framework which current exists, Ajit Appari, Eric Johnson and Denise Anthony examine the quality and breadth of HIPAA diffusion. Their findings address the limitations of policy to solve the problems of security and privacy created by technological innovation.Short works on innovative methodologies include organization theory, in-situ examinations of the efficacy of computer security in medical environments, and a view from the video lens focused on the home. Fabio Massacci, Viet Hung Nguyen and Ayda Saidane bring organizational perspectives to issues of assisted living, where privacy is sometimes an afterthought. Scout Sinclair illustrates that even the best-laid abstract security technologies may be subverted in practice when they are experienced as technologies of inconvenience. Finally, Kelly Caine takes the organizational concerns of Massacci to the personal by asking the subjects of video and privacy loss about their own risk calculus, when faced with digital privacy and physical safety risks.The workshop closes with those on the front lines of security technologies: vendors and consultants trying to bring workable technology to the market now. The panel presents representatives from Google, Microsoft and Mitre, with Kevin Fu moderating and Patrick Traynor providing a perspective from the years of Smart Home work at Georgia Institute of Technology.",,,,2009,,,,Book
"MSWiM '05: Proceedings of the 8th ACM International Symposium on Modeling, Analysis and Simulation of Wireless and Mobile Systems","Welcome to Montreal and MSWiM 2005, the Eighth ACM International Symposium on Modeling, Analysis and Simulation of Wireless and Mobile Systems.We have an exciting program consisting of 48 high quality of papers on several aspects of wireless communication and mobile networking, such as wireless multimedia, ad hoc routing, sensor networks, and performance modeling and analysis of wireless and mobile systems.We wish to thank Carla-Fabiana Chiasserini and Vikram Srinivasan, TPC Co-Chairs and their team who have worked really hard for putting together an excellent program. The contributions of the external referees are very much appreciated.The three-day event consists of technical sessions, Poster and Demos Sessions, and two keynote addresses from well-known experts in the field. We would like to thank Prof. Ian Akyildiz (Georgia Institute of Technology, USA) and Prof. Catherine Rosenberg (University of Waterloo, Canada) for agreeing to deliver these addresses.In addition to the Technical Program, MSWiM 2005 also has several workshops and two tutorials on IP QoS over wireless and intermittently connected mobile ad hoc networks. We are happy to see that the workshop on Performance Evaluation of Wireless Ad hoc, Sensor and Ubiquitpus Networks (PE-WASUN) is again held jointly with MSWiM. Two other workshops on wireless multimedia networking, and QoS and Security for heterogeneous wireless and mobile networks (Q2SWinet) are also being organized. We believe that these workshops will continue to grow and attract more attention in the years to come. They promise to be very interesting, so please don't miss them.",,,,2005,,,,Book
Plenary Talk,"Michael Merritt is Executive Director of the Cross-Layer Analytics and Design Research Department, responsible for applied research directed at application, network, and infrastructure design and performance with particular emphasis on interactions that cross layers of abstraction and technology. Michael has published over thirty-five research articles, co-authored a book on database concurrency control, holds five patents, and served for many years as an area editor of Distributed Computing and the Journal of the ACM. He is a recognized expert in distributed computing, computer security, and network traffic analysis. He has taught at Georgia Tech, MIT, Stevens Institute of Technology, and Columbia University.",,Merritt M,,2013,1,10.1145/2484239.2493965,https://doi-org.proxy.bnl.lu/10.1145/2484239.2493965;http://dx.doi.org/10.1145/2484239.2493965,Conference Paper
Multi-Modal Biometric Fusion Based Continuous User Authentication for E-Proctoring Using Hybrid LCNN-Salp Swarm Optimization,"In Covid 19, pandemic remote proctoring of the employee or human being is evolved as a big challenge for the information retrieval process. On the other side, memory-based system access authentication is becoming outdated and less preferred for live applications, especially where data security and customer privacy are crucial. Multi-modal authentication has outperformed the unimodal process with high accuracy and improved security in the user authentication field. Multi-modal biometric verification includes user attributes such as keystrokes, iris, speech, face, etc. For real-time execution of multi-modal biometric fusion-based live tracking for compatible applications. The study proposes an efficient continuous biometric user authentication system for a new challenge of pandemic time, a live online authentication of the evaluation process (CBUA-OE). The proposed CBUA-OE system can address the challenges associated with live proctoring and is also compatible with real-time implementation, deployment of authentication systems. The modified wolf optimization algorithm and CUBA-OE's optimal feature fusion algorithm give an edge over the other contemporary methods and make it more robust. In modern forms of authentication, the classification stage affects the overall outcome of the system, and the model's performance is also a factor of varying quality of datasets. In contrast, a hybrid LCNN-Salp swarm optimization-based classifier is more efficient and consistent in continuous user authentication. Here the performance of the proposed hybrid LCNN-Salp swarm optimization classifier is analyzed with different standard datasets. The results are compared with the existing state-of-art classifiers regarding the accuracy, precision, recall, and F-measure. This projected work is novel in terms of usability factors and scalability to live tracking systems.",,"Purohit H,Ajmera PK",,2022,827–846,10.1007/s10586-021-03450-w,https://doi-org.proxy.bnl.lu/10.1007/s10586-021-03450-w;http://dx.doi.org/10.1007/s10586-021-03450-w,Journal Article
Modern Stochastics and Applications,"This volume presents an extensive overview of all major modern trends in applications of probability and stochastic analysis. It will be a great source of inspiration for designing new algorithms, modeling procedures and experiments. Accessible to researchers, practitioners, as well as graduate and postgraduate students, this volume presents a variety of new tools, ideas and methodologies in the fields of optimization, physics, finance, probability, hydrodynamics, reliability, decision making, mathematical finance, mathematical physics and economics. Contributions to this Work include those of selected speakers from the international conference entitled Modern Stochastics: Theory and Applications III, held on September 10 14, 2012 at Taras Shevchenko National University of Kyiv, Ukraine. The conference covered the following areas of research in probability theory and its applications: stochastic analysis, stochastic processes and fields, random matrices, optimization methods in probability, stochastic models of evolution systems, financial mathematics, risk processes and actuarial mathematics and information security.",,"Korolyuk V,Limnios N,Mishura Y,Sakhno L,Shevchenko G",,2014,,,,Book
Towards Improving Data Validity of Cyber-Physical Systems through Path Redundancy,"Cyber-physical systems have shown to be susceptible to cyber-attacks. Incidents such as Stuxnet Attack and Ukraine power outage have shown that attackers are capable of penetrating into industrial control systems, compromising PLCs, and sending false commands to physical devices while reporting normal sensing values. Therefore, one of the critical needs of CPS is to ensure the validity of the sensor values. In this paper, we explore path diversity in SCADA networks and develop Path Redundancy to improve data validity. The proposed solution is shown to be able to effectively prevent data integrity attacks and detect false command attacks from a single compromised path or PLC. We provide detailed analysis on solution design and implement an application of the technique in building automation networks. Our cost-efficient and easy-to-deploy solution improves the resilience of SCADA networks.",,"Zheng Z,Reddy AL",,2017,91–102,10.1145/3055186.3055189,https://doi-org.proxy.bnl.lu/10.1145/3055186.3055189;http://dx.doi.org/10.1145/3055186.3055189,Conference Paper
ICETE 2014: Proceedings of the 11th International Joint Conference on E-Business and Telecommunications - Volume 4,"The SECRYPT 2014 is integrated in the ICETE (International Conference on e-Business and Telecommunications) joint conference, which puts together six complementary conferences, namely DCNET, ICE-B, OPTICS, SECRYPT, SIGMAP and WINSYS, covering a broad range of related fields, including data communication networking, e-business, optical communication systems, security and privacy, signal processing and multimedia applications, and wireless networks and information systems. Sponsored by INSTICC (the Institute for Systems and Technologies of Information, Control and Communication) and co-organized by the Austrian Computer Society and the Vienna University of Technology, the SECRYPT is technically sponsored by SBA Research and held in cooperation with ACM SIGDOC (Special Interest Group on Design of Communication), ACM SIGSAC (Special Interest Group on Security, Audit and Control) and IACR (International Association for Cryptologic Research). Moreover, it has WfMC (Workflow Management Coalition), OMG (Object Management Group), and FIPA (The Foundation for Intelligent Physical Agents) as an Organizational Sponsors.We would like to emphasize that ICETE 2014 includes five distinguished keynote lectures, delivered by experts in their fields, including (alphabetically): Ivona Brandic (Vienna UT, Austria), Matteo Golfarelli (University of Bologna, Italy), Seymour Goodman (Georgia Institute of Technology, United States), Dimitris Karagiannis (University of Vienna, Austria), and Edgar Weippl (Secure Business Austria - Vienna University of Technology, Austria).With its six segments, we expect it to appeal to a global audience of the engineers, scientists, business practitioners and policy experts, interested in R&D on Telecommunication Systems and Services. All tracks focus on research related to real world applications and rely on contributions not only from academia, but also from industry, business and government, with different solutions for end-user applications and enabling technologies, in a diversity of communication environments. The accepted papers demonstrate a number of new and innovative solutions and the vitality of these research areas.In response to the call for papers, 139 manuscripts were submitted to SECRYPT 2014. The reviewing process was double blind (meaning the identity of the authors was not known to reviewers, and vice-versa). Each paper was reviewed by at least 3 members of the Program Committee. The review process was rigorous and selective, resulting in the acceptance of 13 full papers (9% acceptance rate). With this acceptance ratio, SECRYPT 2014 continues the tradition of previous conferences as a distinguished and high-quality conference. Extended versions of selected best papers of the conference will be invited to appear in a post-conference book that will be published by Springer.A successful conference involves more than paper presentations alone. It is also a meeting place, where ideas about new research projects and other ventures are discussed and debated. Therefore, a social event including a conference dinner/banquet has been planned for evening of August 29th in order to promote this kind of social networking. We would like to express our thanks to all colleagues involved in supporting this conference. . We would like to thank in particular: the members of the Program Committee and the external reviewers, who really did a great job, devoting expertise and time in reviewing the papers and participating in the discussion process. We would like to thank all the authors who submitted papers, whether or not the paper was eventually included in the program. We would like to thank Giovanni Livraga for serving as publicity chair. We would also like to thank the panelists and invited speakers for their invaluable contribution, in sharing their vision, knowledge and research outcomes. Special thanks go to the leadership, faculty and staff of Vienna University of Technology for hosting the conference.Finally, a word of appreciation for the hard work of the INSTICC team; organizing a conference of this level is a task that can only be achieved by the collaborative effort of a dedicated and highly capable team.We hope that the papers accepted and included in the proceedings may be helpful references in future works for all those who need to address topics in security and cryptography, as well as other ICETE knowledge areas. Enjoy the program and your stay in Vienna.",,,,2014,,,,Book
Detecting Voter Fraud in an Electronic Voting Context: An Analysis of the Unlimited Reelection Vote in Venezuela,"Between December 2007 and February 2009, Venezuelans participated twice in constitutional referenda where the elimination of presidential term limits was one of the most salient proposals. Assuming voter preferences did not change significantly during that period, the 'repeated' character of these elections provide us with an excellent opportunity to apply forensic tools designed to detect anomalies and outliers in election returns in elections where electronic voting technologies were used. Similar tools were first applied by Myagkov et al. ([20], [21], [22], [23]) to the study of electoral fraud in Russia and Ukraine, and were effective in the isolation of potential cases of manipulation of electoral returns. The case of Venezuela is different because there exists no widespread agreement about the integrity or otherwise fraudulent nature of national elections, and because it is a nation where electronic voting technologies are used. Unless electoral fraud takes place in exactly the same manner in each election, an analysis of the 'flow of votes' between elections can be used to detect suspicious patterns in electoral returns. Although we do not find evidence of pervasive electoral fraud compared, for instance, to the Russian case, our analysis is useful to detect polling places or regions deviating considerably from the more general pattern.",,"Levin I,Cohn GA,Ordeshook PC,Alvarez RM",,2009,4,,,Conference Paper
Modern Stochastics and Applications,"This volume presents an extensive overview of all major modern trends in applications of probability and stochastic analysis. It will be a great source of inspiration for designing new algorithms, modeling procedures and experiments. Accessible to researchers, practitioners, as well as graduate and postgraduate students, this volume presents a variety of new tools, ideas and methodologies in the fields of optimization, physics, finance, probability, hydrodynamics, reliability, decision making, mathematical finance, mathematical physics and economics. Contributions to this Work include those of selected speakers from the international conference entitled Modern Stochastics: Theory and Applications III, held on September 10 14, 2012 at Taras Shevchenko National University of Kyiv, Ukraine. The conference covered the following areas of research in probability theory and its applications: stochastic analysis, stochastic processes and fields, random matrices, optimization methods in probability, stochastic models of evolution systems, financial mathematics, risk processes and actuarial mathematics and information security.",,"Korolyuk V,Limnios N,Mishura Y,Sakhno L,Shevchenko G",,2016,,,,Book
The Georgia Tech Aware Home,"The Aware Home Research Initiative (AHRI) at Georgia Tech is devoted to the multidisciplinary exploration of emerging technologies and services based in the home. Starting in 1998, our collection of faculty and students has created a unique research facility that allows us to simulate and evaluate user experiences with off-the-shelf and state-of-the-art technologies. With specific expertise in health, education, entertainment and usable security, we are able to apply our research to problems of significant social and economic impact.",,"Kientz JA,Patel SN,Jones B,Price E,Mynatt ED,Abowd GD",,2008,3675–3680,10.1145/1358628.1358911,https://doi-org.proxy.bnl.lu/10.1145/1358628.1358911;http://dx.doi.org/10.1145/1358628.1358911,Conference Paper
Low-Infrastructure Methods to Improve Internet Access for Mobile Users in Emerging Regions,"As information technology supports more aspects of modern life, digital access has become an important tool for developing regions to lift themselves from poverty. Though broadband internet connectivity will not be universally available in the short-term, widely-employed mobile devices coupled with novel delay-tolerant networking do allow limited forms of connectivity. This paper explores the design space for internet access systems operating with constrained connectivity. Our starting point is C-LINK, a collaborative caching system that enhances the performance of interactive web access over DTN and cellular connectivity. We discuss our experiences and results from deploying C-LINK in Nicaragua, before moving on to a broader design study of other issues that further influence operation. We consider the impact of (i) storing web content collaboratively cached across all user nodes, (ii) hybrid transport layers exploiting the best attributes of limited cellular and DTN-style connectivity. We also explore the behavior of future systems under a range of usage and mobility scenarios. Even under adverse conditions, our techniques can improve average service latency for page requests by a factor of 2X. Our results point to the considerable power of leveraging user mobility and collaboration in providing very-low-infrastructure internet access to developing regions.",,"Isaacman S,Martonosi M",,2011,473–482,10.1145/1963192.1963361,https://doi-org.proxy.bnl.lu/10.1145/1963192.1963361;http://dx.doi.org/10.1145/1963192.1963361,Conference Paper
CPS '17: Proceedings of the 2017 Workshop on Cyber-Physical Systems Security and PrivaCy,"It is our great pleasure to welcome you to the Third ACM Workshop on Cyber-Physical Systems Security and PrivaCy (CPS-SPC) in conjunction with the 24th ACM Conference on Computer and Communications Security (CCS) in Dallas, Texas USA.Our increased dependency on cyber-physical systems (CPS) has amplified concerns of cyber attacks on these systems. Recent attacks such as the one on the Ukrainian electrical grid have only made these concerns more urgent. However, most of the published literature addressing the security and privacy of CPS reflects a field still in its infancy; as such, the overall principles, models, and theories for securing CPS have not yet emerged. It is hoped that this workshop will provide a focal point for the research community to begin addressing the security and privacy of CPS in a comprehensive and multidisciplinary manner and, in tandem with other efforts, build a comprehensive research roadmap.This year's workshop builds on the foundations laid over the last two years to become one of the premier forums for interdisciplinary research results and experience reports at the interface of control theory, information security, embedded and real-time systems and human factors applied to CPS. The mission of the workshop is to create a community of researchers focusing on diverse aspects of CPS Security and Privacy, and to provide researchers and practitioners a premier forum to share their perspectives with others interested in interdisciplinary approaches to solve the challenging security and privacy problems in CPS.The workshop attracted 26 submissions from across 11 countries, and all papers were reviewed by at least two program committee members. 9 full papers and 4 short papers were accepted leading to a 35% acceptance rate.",,,,2017,,,,Book
Security Control Computations for Large Power Systems,"This thesis addresses the problem of determining the optimal operator initiated corrective control action for the secure operation of power systems. The problem of the optimal security control computation can be stated as follows: Given a power system in an emergency or insecure state, determine the corrective control action to be taken, so that all operating constraints of the system are satisfied, and in addition, a predetermined objective is optimized.The problem is a large scale nonlinear optimization problem, with both continuous and discrete control variables. The objective to be optimized, differs according to the operating practices of the particular utility. It may be minimum cost of operation, minimum departure from the current operating point, etc. Operating constraints are imposed, because of equipment ratings and voltage requirements. The controls available to the operator are the real and reactive output of generators, transformer tap setting, capacitor and reactor bank switching and if necessary transmission line switching and load shedding. It is therefore a characteristic of the problem that both continuous and discrete controls are available to the operator.Efficient model reduction and linearization techniques are used to reduce the dimensionality and complexity of the original large scale nonlinear optimization problem. A series of small linearized problems is solved using a specifically designed dual simplex based optimization procedure. An efficient suboptimal method is used for the handling of the discrete control variables (capacitor bank and transmission line switching).The method has been implemented and results from the application of the method to two test systems are presented. The performance of the method is evaluated with tests on an actual system, namely the Georgia Power 500 kV/230 kV/115 kV system.In summary this thesis presents a fast, reliable and robust methodology for corrective control computations in large power systems.",,Bakirtzis AG,,1984,,,,Ph.D. Thesis
Boost.Asio C++ Network Programming Cookbook,"Key FeaturesBuild highly efficient distributed applications with easeEnhance your cross-platform network programming skills with one of the most reputable C++ librariesFind solutions to real-world problems related to network programming with ready-to-use recipes using this detailed and practical handbookBook DescriptionStarting with recipes demonstrating the execution of basic Boost.Asio operations, the book goes on to provide ready-to-use implementations of client and server applications from simple synchronous ones to powerful multithreaded scalable solutions. Finally, you are presented with advanced topics such as implementing a chat application, implementing an HTTP client, and adding SSL support. All the samples presented in the book are ready to be used in real projects just out of the box.As well as excellent practical examples, the book also includes extended supportive theoretical material on distributed application design and construction.What you will learnBoost your working knowledge of one of the most reputable C++ networking librariesBoost.AsioFamiliarize yourself with the basics of TCP and UDP protocolsCreate scalable and highly-efficient client and server applicationsUnderstand the theory behind development of distributed applicationsIncrease the security of your distributed applications by adding SSL supportImplement a HTTP client easilyUse iostreams, scatter-gather buffers, and timersAbout the AuthorDmytro Radchuk is a software engineer from Kyiv, Ukraine. His passion for science started at a young age and led him to receive a degree in computer science from Kyiv Polytechnic University. As of today, he has deep technical knowledge and more than 8 years of experience in the field of software development for several industries and businesses. He strongly believes that the whole is greater than the sum of its parts and this is one of the reasons why the development of distributed applications has become his main specialization.Dmytro has always supported the idea of sharing knowledge and this has resulted in Boost.Asio C++ Network Programming Cookbook, which has become his first officially published book. He believes that science will help us deal with the monotony of everyday life. When he is not exploring another facet of computer engineering, he is probably learning a new aspect of psychology, history, or the arts, which are also of great interest to him.Table of ContentsThe BasicsI/O OperationsImplementing Client ApplicationsImplementing Server ApplicationsHTTP and SSL/TLSOther Topics",,Radchuk D,,2016,,,,Book
Boosting Archimedes Optimization Algorithm Using Trigonometric Operators Based on Feature Selection for Facial Analysis,"Due to technical advancements and the proliferation of mobile applications, facial analysis (FA) of humans has recently become an important area for computer vision research. FA investigates a variety of difficulties, including gender recognition, facial expression recognition, age and race recognition, with the goal of automatically comprehending social interactions. Due to the dimensional challenge posed by pre-trained CNN networks, the scientific community has developed numerous techniques inspired by biology, swarm intelligence theory, physics, and mathematical rules. This article presents a gender recognition system based on scAOA, that is a modified version of the Archimedes optimization algorithm (AOA). The latest variant (scAOA) enhances the exploitation stage by using trigonometric operators inspired by the sine cosine algorithm (SCA) in order to prevent local optima and to accelerate the convergence. The main purpose of this paper is to apply scAOA to select the relevant deep features provided by two pretrained models of CNN (AlexNet & ResNet) to recognize the gender of a human person categorized into two classes (men and women). Two datasets are used to evaluate the proposed approach (scAOA): the Brazilian FEI dataset and the Georgia Tech Face dataset (GT). In terms of accuracy, Fscore and statistical test, the comparison analysis demonstrates that scAOA outperforms other modern and competitive optimizers such as AOA, SCA, Ant lion optimizer (ALO), Salp swarm algorithm (SSA), Grey wolf optimizer (GWO), Simple genetic algorithm (SGA), Grasshopper optimization algorithm (GOA) and Particle swarm optimizer (PSO).",,"Neggaz I,Neggaz N,Fizazi H",,2022,3903–3923,10.1007/s00521-022-07925-8,https://doi-org.proxy.bnl.lu/10.1007/s00521-022-07925-8;http://dx.doi.org/10.1007/s00521-022-07925-8,Journal Article
ICETE 2014: Proceedings of the 11th International Joint Conference on E-Business and Telecommunications - Volume 2,"The ICE-B 2014 conference is integrated in the ICETE (International Conference on e-Business and Telecommunications) joint conference, which puts together six complementary conferences, namely DCNET, ICE-B, OPTICS, SECRYPT, SIGMAP and WINSYS, covering a broad range of related fields, including data communication networking, e-business, optical communication systems, security and cryptography, signal processing and multimedia applications, and wireless networks and information systems. Sponsored by INSTICC (the Institute for Systems and Technologies of Information, Control and Communication) and co-organized by the Austrian Computer Society and the Vienna University of Technology, the ICE-B is in cooperation with ACM SIGDOC (Special Interest Group on Design of Communication) and ACM SIGMIS (Special Interest Group on Management Information Systems) and has WfMC (Workflow Management Coalition), OMG (Object Management Group), and FIPA (The Foundation for Intelligent Physical Agents) as Organizational Sponsors.We would like to emphasize that ICETE 2014 includes five distinguished keynote lectures, delivered by experts in their fields, including (alphabetically): Ivona Brandic (Vienna UT, Austria), Matteo Golfarelli (University of Bologna, Italy), Seymour Goodman (Georgia Institute of Technology, United States), Dimitris Karagiannis (University of Vienna, Austria), Edgar Weippl (Secure Business Austria - Vienna University of Technology, Austria).With its six tracks, we expect it to appeal to a global audience of the engineers, scientists, business practitioners and policy experts, interested in R&D on Telecommunication Systems and Services. All tracks focus on research related to real world applications and rely on contributions not only from academia, but also from industry, business and government with different solutions for end-user applications and enabling technologies, in a diversity of communication environments. The accepted papers demonstrate a number of new and innovative solutions and the vitality of these research areas.ICETE 2014 received 335 papers in total, with contributions from 56 different countries, in all continents, of which 11% were accepted and orally presented as full papers. To evaluate each submission, a double-blind paper evaluation method was used: each paper was blindly reviewed by at least two experts from the International Program Committee. In fact, most papers had 3 reviews or more. With this acceptance ratio, ICETE 2014 continues the tradition of previous conferences as a distinguished and high-quality conference. Extended versions of selected best papers of the conference will be invited to appear in a post-conference book that will be published by Springer.A successful conference involves more than paper presentations alone. It is also a meeting place, where ideas about new research projects and other ventures are discussed and debated. Therefore, a social event including a conference diner/banquet has been planned for evening of August 29th in order to promote this kind of social networking. We would like to express our thanks to all colleagues involved in supporting this conference. First of all, we thank all authors including those whose papers were not included in the program. We also would like to thank all members of the international program committee and reviewers, who provided an invaluable help with their expertise, dedication and time. We would also like to thank the panelists and invited speakers for their invaluable contribution, in sharing their vision and knowledge. Special thanks go to the leadership, faculty and staff of Vienna University of Technology for hosting the conference.Finally, a word of appreciation for the hard work of the INSTICC team; organizing a conference of this level is a task that can only be achieved by the collaborative effort of a dedicated and highly capable team.We hope that the papers accepted and included in the proceedings may be a helpful reference in future works for all those who need to address topics in e-business, especially the identified knowledge areas from the ICE-B call for papers. Enjoy the program and your stay in Vienna.",,,,2014,,,,Book
ICETE 2014: Proceedings of the 11th International Joint Conference on E-Business and Telecommunications - Volume 6,"WINSYS 2014, is integrated in the ICETE (International Conference on e-Business and Telecommunications) joint conference, which puts together six complementary conferences, namely, DCNET, ICE-B, OPTICS, SECRYPT, SIGMAP and WINSYS, covering a broad range of related fields, including data communication networking, e-business, optical communication systems and networks, security and cryptography, signal processing and multimedia applications, and wireless networks and information systems. Sponsored by INSTICC (the Institute for Systems and Technologies of Information, Control and Communication) and co-organized by the Austrian Computer Society and the Vienna University of Technology, WINSYS is organized in cooperation with ACM SIGDOC (Special Interest Group on Design of Communication). Moreover, it has WfMC (Workflow Management Coalition), OMG (Object Management Group) and FIPA (The Foundation for Intelligent Physical Agents) as Organizational Sponsors.We would like to emphasize that ICETE 2014 includes five distinguished keynote lectures, delivered by experts in their fields, including (alphabetically): Ivona Brandic (Vienna UT, Austria), Matteo Golfarelli (University of Bologna, Italy), Seymour Goodman (Georgia Institute of Technology, United States), Dimitris Karagiannis (University of Vienna, Austria), and Edgar Weippl (Secure Business Austria - Vienna University of Technology, Austria).With its six segments, we expect it to appeal to a global audience of the engineers, scientists, business practitioners and policy experts, interested in R&D on Telecommunication Systems and Services. All tracks focus on research related to real world applications and rely on contributions not only from academia, but also from industry, business and government, with different solutions for end-user applications and enabling technologies, in a diversity of communication environments. The accepted papers demonstrate a number of new and innovative solutions and the vitality of these research areas.ICETE 2014 received 335 papers in total, with contributions from 56 different countries, in all continents, of which 11% were accepted and orally presented as full papers. To evaluate each submission, a double-blind paper evaluation method was used: each paper was blindly reviewed by at least two experts from the International Program Committee. In fact, most papers had 3 reviews or more. With this acceptance ratio, ICETE 2014 continues the tradition of previous conferences as a distinguished and high-quality conference. Extended versions of selected best papers of the conference will be invited to appear in a post-conference book that will be published by Springer.A successful conference involves more than paper presentations alone. It is also a meeting place, where ideas about new research projects and other ventures are discussed and debated. Therefore, a social event including a conference diner/banquet has been planned for evening of August 29th in order to promote this kind of social networking. We would like to express our thanks to all colleagues involved in supporting this conference. First of all, we thank all authors including those whose papers were not included in the program. We also would like to thank all members of the International Program Committee and reviewers, who provided invaluable help with their expertise, dedication and time. We would also like to thank the panelists and invited speakers for their invaluable contribution, in sharing their vision, knowledge and research outcomes. Special thanks go to the leadership, faculty and staff of Vienna University of Technology for hosting the conference.Finally, a word of appreciation for the hard work of the INSTICC team; organizing a conference of this level is a task that can only be achieved by the collaborative effort of a dedicated and highly capable team.We hope that the papers accepted and included in the proceedings may be helpful references in future works for all those who need to address topics in wireless information networks and systems, as well as other ICETE knowledge areas. Enjoy the program and your stay in Vienna.",,,,2014,,,,Book
ICETE 2014: Proceedings of the 11th International Joint Conference on E-Business and Telecommunications - Volume 3,"The OPTICS 2014, is integrated in the ICETE (International Conference on e-Business and Telecommunications) joint conference, which puts together six complementary conferences, namely DCNET, ICE-B, OPTICS, SECRYPT, SIGMAP and WINSYS, covering a broad range of related fields, including data communication networking, e-business, optical communication systems, security and cryptography, signal processing and multimedia applications, and wireless networks and information systems. Sponsored by INSTICC (the Institute for Systems and Technologies of Information, Control and Communication) and co-organized by the Austrian Computer Society and the Vienna University of Technology, the OPTICS is organized in cooperation with ACM SIGDOC (Special Interest Group on Design of Communication) and technically sponsored by Photonics21 (European Technology Platform for Photonics). Moreover, it has WfMC (Workflow Management Coalition), OMG (Object Management Group), and FIPA (The Foundation for Intelligent Physical Agents) as an Organizational Sponsors.We would like to emphasize that ICETE 2014 includes five distinguished keynote lectures, delivered by experts in their fields, including (alphabetically): Ivona Brandic (Vienna UT, Austria), Matteo Golfarelli (University of Bologna, Italy), Seymour Goodman (Georgia Institute of Technology, United States), Dimitris Karagiannis (University of Vienna, Austria), Edgar Weippl (Secure Business Austria - Vienna University of Technology, Austria).With its six segments, we expect it to appeal to a global audience of the engineers, scientists, business practitioners and policy experts, interested in R&D on Telecommunication Systems and Services. All tracks focus on research related to real world applications and rely on contributions not only from academia, but also from industry, business and government with different solutions for end-user applications and enabling technologies, in a diversity of communication environments. The accepted papers demonstrate a number of new and innovative solutions and the vitality of these research areas.ICETE 2014 received 335 papers in total, with contributions from 56 different countries, in all continents, of which 11% were accepted and orally presented as full papers. To evaluate each submission, a double-blind paper evaluation method was used: each paper was blindly reviewed by at least two experts from the International Program Committee. In fact, most papers had 3 reviews or more. With this acceptance ratio, ICETE 2014 continues the tradition of previous conferences as a distinguished and high-quality conference. Extended versions of selected best papers of the conference will be invited to appear in a post-conference book that will be published by Springer.A successful conference involves more than paper presentations alone. It is also a meeting place, where ideas about new research projects and other ventures are discussed and debated. Therefore, a social event including a conference diner/banquet has been planned for evening of August 29th in order to promote this kind of social networking. We would like to express our thanks to all colleagues involved in supporting this conference. First of all, we thank all authors including those whose papers were not included in the program. We also would like to thank all members of the international program committee and reviewers, who provided an invaluable help with their expertise, dedication and time. We would also like to thank the panelists and invited speakers for their invaluable contribution, in sharing their vision and knowledge. Special thanks go to the leadership, faculty and staff of Vienna University of Technology for hosting the conference.Finally, a word of appreciation for the hard work of the INSTICC team; organizing a conference of this level is a task that can only be achieved by the collaborative effort of a dedicated and highly capable team.We hope that the papers accepted and included in the proceedings may be a helpful reference in future works for all those who need to address topics in optical communication systems, as well as other ICETE knowledge areas. Enjoy the program and your stay in Vienna.",,,,2014,,,,Book
ICETE 2014: Proceedings of the 11th International Joint Conference on E-Business and Telecommunications - Volume 5,"The SIGMAP 2014, is integrated in the ICETE (International Conference on e-Business and Telecommunications) joint conference, which puts together six complementary conferences, namely DCNET, ICE-B, OPTICS, SECRYPT, SIGMAP and WINSYS, covering a broad range of related fields, including data communication networking, e-business, optical communication systems, security and cryptography, signal processing and multimedia applications, and wireless networks and information systems. Sponsored by INSTICC (the Institute for Systems and Technologies of Information, Control and Communication) and co-organized by the Austrian Computer Society and the Vienna University of Technology, the SIGMAP is organized in cooperation with ACM SIGDOC (Special Interest Group on Design of Communication) and ACM SIGMM (Special Interest Group on Multimedia). Moreover, it has WfMC (Workflow Management Coalition), OMG (Object Management Group), and FIPA (The Foundation for Intelligent Physical Agents) as an Organizational Sponsor.We would like to emphasize that ICETE 2014 includes five distinguished keynote lectures, delivered by experts in their fields, including (alphabetically): Ivona Brandic (Vienna UT, Austria), Matteo Golfarelli (University of Bologna, Italy), Seymour Goodman (Georgia Institute of Technology, United States), Dimitris Karagiannis (University of Vienna, Austria), and Edgar Weippl (Secure Business Austria - Vienna University of Technology, Austria).With its six segments, we expect it to appeal to a global audience of the engineers, scientists, business practitioners and policy experts, interested in R&D on Telecommunication Systems and Services. All tracks focus on research related to real world applications and rely on contributions not only from academia, but also from industry, business and government, with different solutions for end-user applications and enabling technologies, in a diversity of communication environments. The accepted papers demonstrate a number of new and innovative solutions and the vitality of these research areas.ICETE 2014 received 335 papers in total, with contributions from 56 different countries, in all continents, of which 11% were accepted and orally presented as full papers. To evaluate each submission, a double-blind paper evaluation method was used: each paper was blindly reviewed by at least two experts from the International Program Committee. In fact, most papers had 3 reviews or more. With this acceptance ratio, ICETE 2014 continues the tradition of previous conferences as a distinguished and high-quality conference. Extended versions of selected best papers of the conference will be invited to appear in a post-conference book that will be published by Springer.A successful conference involves more than paper presentations alone. It is also a meeting place, where ideas about new research projects and other ventures are discussed and debated. Therefore, a social event including a conference diner/banquet has been planned for evening of August 29th in order to promote this kind of social networking. We would like to express our thanks to all colleagues involved in supporting this conference. First of all, we thank all authors including those whose papers were not included in the program. We also would like to thank all members of the International Program Committee and reviewers, who provided invaluable help with their expertise, dedication and time. We would also like to thank the panelists and invited speakers for their invaluable contribution, in sharing their vision, knowledge and research outcomes. Special thanks go to the leadership, faculty and staff of Vienna University of Technology for hosting the conference.Finally, a word of appreciation for the hard work of the INSTICC team; organizing a conference of this level is a task that can only be achieved by the collaborative effort of a dedicated and highly capable team.We hope that the papers accepted and included in the proceedings may be helpful references in future works for all those who need to address topics in signal processing and multimedia applications, as well as other ICETE knowledge areas. Enjoy the program and your stay in Vienna.",,,,2014,,,,Book
ICETE 2014: Proceedings of the 11th International Joint Conference on E-Business and Telecommunications - Volume 1,"The DCNET 2014 is integrated in the ICETE (International Conference on e-Business and Telecommunications) joint conference, which puts together six complementary conferences, namely DCNET, ICE-B, OPTICS, SECRYPT, SIGMAP and WINSYS, covering a broad range of related fields, including data communication networking, e-business, optical communication systems, security and cryptography, signal processing and multimedia applications, and wireless networks and information systems. Sponsored by INSTICC (the Institute for Systems and Technologies of Information, Control and Communication) and co-organized by the Austrian Computer Society and the Vienna University of Technology, the DCNET is organized in cooperation with ACM SIGDOC (Special Interest Group on Design of Communication). Moreover, it has WfMC (Workflow Management Coalition), OMG (Object Management Group) and FIPA (The Foundation for Intelligent Physical Agents) as an Organizational Sponsors.We would like to emphasize that ICETE 2014 includes five distinguished keynote lectures, delivered by experts in their fields, including (alphabetically): Ivona Brandic (Vienna UT, Austria), Matteo Golfarelli (University of Bologna, Italy), Seymour Goodman (Georgia Institute of Technology, United States), Dimitris Karagiannis (University of Vienna, Austria), and Edgar Weippl (Secure Business Austria - Vienna University of Technology, Austria).With its six segments, we expect it to appeal to a global audience of the engineers, scientists, business practitioners and policy experts, interested in R&D on Telecommunication Systems and Services. All tracks focus on research related to real world applications and rely on contributions not only from academia, but also from industry, business and government, with different solutions for end-user applications and enabling technologies, in a diversity of communication environments. The accepted papers demonstrate a number of new and innovative solutions and the vitality of these research areas.ICETE 2014 received 335 papers in total, with contributions from 56 different countries, in all continents, of which 11% were accepted and orally presented as full papers. To evaluate each submission, a double-blind paper evaluation method was used: each paper was blindly reviewed by at least two experts from the International Program Committee. In fact, most papers had 3 reviews or more. With this acceptance ratio, ICETE 2014 continues the tradition of previous conferences as a distinguished and high-quality conference. Extended versions of selected best papers of the conference will be invited to appear in a post-conference book that will be published by Springer.A successful conference involves more than paper presentations alone. It is also a meeting place, where ideas about new research projects and other ventures are discussed and debated. Therefore, a social event including a conference diner/banquet has been planned for evening of August 29th in order to promote this kind of social networking. We would like to express our thanks to all colleagues involved in supporting this conference. First of all, we thank all authors including those whose papers were not included in the program. We also would like to thank all members of the International Program Committee and reviewers, who provided invaluable help with their expertise, dedication and time. We would also like to thank the panelists and invited speakers for their invaluable contribution, in sharing their vision, knowledge and research outcomes. Special thanks go to the leadership, faculty and staff of Vienna University of Technology for hosting the conference.Finally, a word of appreciation for the hard work of the INSTICC team; organizing a conference of this level is a task that can only be achieved by the collaborative effort of a dedicated and highly capable team.We hope that the papers accepted and included in the proceedings may be helpful references in future works for all those who need to address topics in data communication networking, as well as other ICETE knowledge areas. Enjoy the program and your stay in Vienna.",,,,2014,,,,Book
Experiences in MHealth for Chronic Disease Management in 4 Countries,"This paper describes mHealth applications to deal with Non Communicable Diseases in North and Latin America: In Chile, a project focused on Diabetes Mellitus type 2; In the United States, Honduras, and Mexico, projects focused in diabetes, heart failure, depression, hypertension, and cancer. Information Technologies used include voice and sms on cell phones and electronic health records systems.",,"Piette JD,Blaya JA,Lange I,Sanchis JB",,2011,,10.1145/2093698.2093868,https://doi-org.proxy.bnl.lu/10.1145/2093698.2093868;http://dx.doi.org/10.1145/2093698.2093868,Conference Paper
Computational Challenges and Opportunities of Simulating Cosmic Ray Showers at Global Scale,"Galactic cosmic rays are the high-energy particles that stream into our solar system from distant corners of our Galaxy and some low energy particles are from the Sun which are associated with solar flares. The Earth atmosphere serves as an ideal detector for the high energy cosmic rays which interact with the air molecule nuclei causing propagation of extensive air showers. In recent years, there are growing interests in the applications of the cosmic ray measurements which range from the space/earth weather monitoring, homeland security based on the cosmic ray muon tomography, radiation effects on health via air travel, etc. A simulation program (based on the GEANT4 software package developed at CERN) has been developed at Georgia State University for studying the cosmic ray showers in atmosphere. The results of this simulation study will provide unprecedented knowledge of the geo-position-dependent cosmic ray shower profiles and significantly enhance the applicability of the cosmic ray applications. In the paper, we present the computational challenges and the opportunities for carrying out the cosmic ray shower simulations at the global scale using various computing resources including XSEDE.",,"Sarajlic O,He X,Sarajlic S,Wei TC",,2018,,10.1145/3219104.3229281,https://doi-org.proxy.bnl.lu/10.1145/3219104.3229281;http://dx.doi.org/10.1145/3219104.3229281,Conference Paper
PacketCable Implementation (Networking Technology),"PacketCable ImplementationDesign, provision, configure, manage, and secure tomorrow's high-value PacketCable networksJeff Riddel, CCIE® No. 12798PacketCable networks use IP technology to enable a wide range of multimedia services, from IP telephony to interactive gaming and beyond. Because PacketCable-based business and residential services are central to the cable industry's strategy for growth, the industry's need for PacketCable expertise is expected to increase dramatically.PacketCable Implementation is the first complete primer on PacketCable network design, provisioning, configuration, management, and security. Drawing on consulting experience with every leading cable operator, Jeff Riddel presents real-world case studies, sample network designs, configurations, and practical tips for all facets of PacketCable planning and deployment.This book's end-to-end coverage has been designed for cable engineers and networking professionals with widely diverse backgrounds and experience. Topics covered include PacketCable specifications and functional components, multimedia terminal adapters (MTA) provisioning, call signaling, media streaming, quality of service (QoS), event messaging, security, and much more. Every chapter contains tables and charts that serve as quick, easy references to key points. Each chapter closes with a summary and chapter review questions designed to help you assess and deepen your understanding.PacketCable Implementation brings together everything you need to know about cable networking to service delivery.Jeff Riddel, CCIE® No. 12798, is a network consulting engineer on the Cisco® Broadband Advanced Services team. There, he has consulted with all major U.S. MSOs, including Charter Communications, Comcast, AT&T, and Time Warner Cable. Riddel has represented Cisco in PacketCable interoperability testing at CableLabs in Louisville, Colorado. He holds a master's in electrical engineering from the Georgia Institute of Technology. Discover the PacketCable ""big picture,"" including key application opportunities Learn about the latest generation of PacketCable standards and specifications, including PacketCable 2.0 and DOCSIS 3.0 Understand the functional components of a PacketCable network and how they fit together Walk step-by-step through provisioning, including protocols, flows, and MTA configuration Gain an in-depth understanding of call signaling: message formats, Network-based Call Signaling (NCS), PSTN interconnects, Call Management Server Signaling (CMSS), and more Implement efficient, high-performance media streaming Deploy, analyze, manage, and troubleshoot a state-of-the-art QoS framework Manage crucial network considerations, including lawful interceptIntroductionPart I Introduction and Overview of PacketCableChapter 1PacketCable OverviewChapter 2 PacketCable Functional ComponentsChapter 3 Provisioning OverviewChapter 4 Provisioning FlowsChapter 5 The MTA Configuration FileChapter 6 Signaling Interfaces and MGCP OverviewChapter 7 NCS (Network-based Call Signaling)Chapter 8 TGCP and the PSTN InterconnectChapter 9 Call Management Server Signaling Protocol (CMSS)Chapter 10 Audio CODECsChapter 11 RTP and RTCPChapter 12 DQoS Architecture and FrameworkChapter 13 Analyzing, Implementing, and Troubleshooting DQoSChapter 14 Multimedia ApplicationsChapter 15 Event Messaging and Lawful InterceptChapter 16 PacketCable Network Design ConsiderationsAppendix A Standards and SpecificationsAppendix B Cable Monitor and EtherealAppendix C Complete Call FlowsIndexThis book is part of the Networking Technology Series from Cisco Press®, which offers networking professionals valuable information for constructing efficient networks, understanding new technologies, and building successful careers.Category: Cisco Pressï NetworkingCovers: Broadband Multimedia$75.00 USA / $94.00 CAN",,Riddel J,,2007,,,,Book
A Data Mining Approach for Risk Assessment in Car Insurance: Evidence from Montenegro,"This paper has proposed a data mining approach for risk assessment in car insurance. Standard methods imply classification of policies to great number of tariff classes and assessment of risk on basis of them. With application of data mining techniques, it is possible to get functional dependencies between the level of risk and risk factors as well as better results in predictions. On the case study data it has been proved that data mining techniques can, with better accuracy than the standard methods, predict claim sizes and occurrence of claims, and this represents the basis for calculation of net risk premium and risk classification. This paper, also, discusses advantages of data mining methods compared to standard methods for risk assessment in car insurance, as well as the specificities of the obtained results due to small insurance market, such is the one in Montenegro.",,"Kašćelan L,Kašćelan V,Novović-Burić M",,2014,11–28,10.4018/ijbir.2014070102,https://doi-org.proxy.bnl.lu/10.4018/ijbir.2014070102;http://dx.doi.org/10.4018/ijbir.2014070102,Journal Article
MobiCom '02: Proceedings of the 8th Annual International Conference on Mobile Computing and Networking,"It gives us great pleasure to welcome you to ACM MobiCom 2002, the Eighth ACM International Conference on Mobile Computing and Networking in Atlanta, Georgia. MobiCom has grown in prestige since its inception in 1995 to become the leading technical meeting to hear about the latest and most exiting developments in mobile networking and computing. As the global interest in mobile and wireless research and development surges, the interest in MobiCom has grown each year. This year's response to the call for papers reflects that growing interest. We received 364 paper submissions from 28 countries, representing a growth of 30% over last year's record-breaking number of submissions.The job of the program committee was made more difficult by the large volume of submissions. However, our program committee responded to this challenge with their willingness to go the extra mile. All submitted papers were judged based on their quality through double-blind reviewing, where the identities of the authors were withheld from the reviewers. The review process included a quick reject phase (for papers that were clearly out of scope or substandard), and, first and second round reviews. The program committee members reviewed all papers with each paper receiving three independent first round reviews. This was followed by a second round review, which promoted consensus between different reviewers. During the second round review phase there was considerable amount of online discussion for each paper in order to reach consensus, with the result that papers were either recommended to be rejected or to be further discussed at the program committee meeting. Additional reviews were sort prior to the program committee meeting particularly in the case of papers that had reviews with diverging opinions or where no consensus could be reached during the second round review phase. Prior to the meeting each paper had on average five reviews (including the second round review) from program committee members.The program committee meeting was held at Columbia University on Monday June 24, 2002. Sixty-six papers were discussed at the meeting which started at 8.30 AM and completed with the selection of a program of 26 papers at 7.30 PM. Forty-five out of fifty program committee members were physically present at the meeting with each committee member reviewing on average 26 papers. We are indebted to the program committee for their time, effort and enthusiasm. The quality of the conference is testament to their expertise and dedication. We thoroughly enjoyed working with them.We are extremely pleased with the final technical paper program and would like to thank all the authors that submitted papers to MobiCom 2002. The selected papers cover a broad range of topics that attendees will find of great interest including sessions on security, media access control, transport layer issues, WLANs, sensor networks, energy efficient systems, wireless QOS, systems issues, and future research challenges. The paper program is complemented by informative tutorials, thought provoking panels, an exceptional student poster session (new to MobiCom), and workshops that reflect important new research directions.",,,,2002,,,,Book
"ICT Innovations 2018. Engineering and Life Sciences: 10th International Conference, ICT Innovations 2018, Ohrid, Macedonia, September 1719, 2018","This book constitutes the refereed proceedings of the 10th International ICT Innovations Conference, ICT Innovations 2018, held in Ohrid, Macedonia, in September 2018. The 21 full papers presented were carefully reviewed and selected from 81 submissions. They cover the following topics:sensor applications and deployments, embedded and cyber-physical systems, robotics, network architectures, cloud computing, software infrastructure, software creation and management, models of computation, computational complexity and cryptography, design and analysis of algorithms, mathematical optimization, probability and statistics, data management systems, data mining, human computer interaction (HCI), artificial intelligence, machine learning, life and medical sciences, health care information systems, bioinformatics.",,"Kalajdziski S,Ackovska N",,2018,,,,Book
IPM-Model: AI and Metaheuristic-Enabled Face Recognition Using Image Partial Matching for Multimedia Forensics Investigation with Genetic Algorithm,"The rapid enhancement in the development of information technology has driven the development of human facial image recognition. Recently, facial recognition has been successfully applied in several distinct domains with the help of computing and information technology. This kind of application plays a significant role in the process of digital forensics investigation, recognizing the patterns of a human face based on the partial matching of images that would be in 24-bit color image format, including the spacing of the eyes, the bridging of the nose, the contour of the lips, ears, and chin. In this paper, we have proposed and implemented an image recognition model based on principal component analysis, genetic algorithms, and neural networks, in which PCA reduces the dimension of the benchmark dataset, while genetic algorithms and neural nets optimize the searching patterns of image matching and provide highly efficient output with a minimal amount of time. Through the experiment results on the human facial images dataset of the Georgia Institute of Technology, the overall match showed that the proposed model can achieve the recognition of human face images with an accuracy rate of 93.7%. Moreover, this model helps to examine, analyze, and detect individuals by partial matching with reidentification in the procedure of forensics investigation. The experimental result shows the robustness of the proposed model in terms of efficiency compared to other state-of-the-art methods.",,"Khan AA,Shaikh AA,Shaikh ZA,Laghari AA,Karim S",,2022,23533–23549,10.1007/s11042-022-12398-x,https://doi-org.proxy.bnl.lu/10.1007/s11042-022-12398-x;http://dx.doi.org/10.1007/s11042-022-12398-x,Journal Article
MobiHoc '06: Proceedings of the 7th ACM International Symposium on Mobile Ad Hoc Networking and Computing,"It is a great pleasure to present the technical program of the 7th ACM International Symposium on Mobile Ad Hoc Networking and Computing, ACM MobiHoc 2006.This year, the conference received 321 submissions, representing about a 15% increase over last year's submissions. These numbers show that the worldwide community of researchers working on ad hoc networks is growing. The large number of submissions also confirms MobiHoc as the leading technical meeting for researchers who work in ad hoc networking and computing.With so many papers to choose from, the Technical Program Committee' job to select a technical program with the highest excellence was challenging and time consuming. All papers were judged based on their quality through a three-phase, double-blind, reviewing process. Each paper was analyzed by at-least three program committee members. During the first quick reject phase the program committee members identified and recommended for reject the papers that were clearly below the quality threshold, or out of scope for the conference. The remaining papers underwent an in-depth review process. At the end of this second phase, an on-line discussion phase was conducted to reach consensus between different reviewers. In the case of papers with diverging evaluations additional reviews were requested.As the result of this three-phase process, each paper was either recommended to be rejected, or to be further discussed at the program committee meeting. Specifically, fifty-nine papers were recommended for discussion at the meeting. The program committee meeting was held in Atlanta, and was hosted by the Georgia Institute of Technology, on 4-5 March 2006. Twenty-two program committee members were physically present at the meeting in Atlanta, while others joined the meeting via teleconference. After two days of intensive discussions, the Technical Program Committee finally selected 31 papers for presentation in eight single-track technical sessions. These selected contributions cover a broad range of topics including Theory, Medium Access Control, Routing & Forwarding, Sensor Networks, Location Services, Security, Topology Control, Mobility Models; thus providing a very good representation of the excellent research being undertaken worldwide in the area.",,,,2006,,,,Book
Experiences in Implementing an Experimental Wide-Area GMPLS Network,"In this article, we describe our experiences in implementing an experimental wide-area GMPLS network called CHEETAH (circuit-switched end-to-end transport architecture). The key concept is to add a complementary end-to-end circuit based service with dynamic call-by-call bandwidth sharing to the connectionless service already available to end hosts via the Internet. The current CHEETAH experimental network consists of off-the-shelf GMPLS-capable SONET switches (with Ethernet interfaces) deployed at three locations, Research Triangle Park, North Carolina, Atlanta, Georgia, and Oak Ridge, Tennessee. We describe our solutions to various problems relating to control-plane design, IP addressing and control-plane security. We designed and implemented a CHEETAH software package to run on Linux end hosts connected to the CHEETAH network. Among other functions, this software package includes an RSVP-TE module to enable end users and applications to dynamically initiate requests for dedicated end-to-end circuits and receive/respond to requests for circuits. We present measurements for typical end-to-end circuit setup delays across this network. For example, end-to-end circuit setup delay from a Linux end host in NC to a host in Atlanta is 166 ms",,"Zhu X,Zheng X,Veeraraghavan M",,2007,82–92,10.1109/TWC.2007.026906,https://doi-org.proxy.bnl.lu/10.1109/TWC.2007.026906;http://dx.doi.org/10.1109/TWC.2007.026906,Journal Article
"MobiSys '13: Proceeding of the 11th Annual International Conference on Mobile Systems, Applications, and Services","Welcome to MobiSys 2013: The Eleventh International Conference on Mobile Systems, Applications, and Services. As the premier forum for presenting the latest research in mobile and wireless systems, this year's conference attracted a record number of 211 submissions, a sign of continuing vibrancy and increasing maturity of our community. In addition to the strong technical program covering virtually every major topic of mobile systems research, we've incorporated a new video program showcasing innovative ideas selected from submissions, and have continued the tradition of having an interactive demo and poster fest.The Technical Program Committee and Industrial Reviewer Panel are to be lauded for their extraordinary effort in vetting each paper with care during the rigorous review process. Each paper was assigned to three Program Committee members for a first round of reviews. Each of the 211 papers with at least one positive review (weak accept) was assigned two additional reviewers from the Program Committee in the second round. A paper with potential industrial applications was further assigned to an Industrial Reviewer Committee member. In all, the Program Committee members and industrial reviewers generated 866 reviews and the top 60 papers received at least 5 reviews.The Program Committee met for a full day at Jekyll Island, Georgia, USA. The Committee discussed the top 60 papers and accepted 33 papers for the program. Each of these 33 papers was shepherded by a member of the Program Committee, to ensure that recommendations made by the reviewers and during the Program Committee meeting were satisfactorily addressed. The resulting technical program is the fruit of labor by many. We want to thank all the authors of submitted papers. We are indebted to the Technical Program Committee and Industry Reviewer Panel for their extraordinary work in reviewing papers and providing feedback for the authors, and the Video Program Committee for reviewing and selecting the short videos.We hope you will find the program interesting and thought-provoking, and you will take this opportunity to share your ideas while also networking with other researchers and practitioners from around the world.",,,,2013,,,,Book
The Carpathian-Balkans During the Holocene: Reconstructing Human Influences and Climatic Changes,"The Carpathian-Balkan region in south-eastern Europe is one of the longest inhabited regions in Europe, with evidence of some of the earliest examples of European agriculture, farming and metallurgy. Despite its importance for understanding past human activity and climate change, high-resolution reconstructions of Holocene hydroclimate variability and human impact are rare. This thesis provides a series of new high-resolution Holocene (the past 11,700 years) palaeoenvironmental records derived from peat bogs in the Carpathian Mountains of Romania, and the Dinaric Alps of Serbia, to investigate climate variation and human impact. Two peat-derived archives of environmental change in Romania are presented. First, a 7500-year record of minerogenic deposition from the Southern Carpathians linked to heavy rainfall events provides the first record of extreme precipitation for the Carpathians. Such minerogenic depositional events began 4000 calibrated years before present (yr BP, where present is 1950 CE), with increased depositional rates during the Medieval Warm Period (1150 – 850yr BP), the Little Ice Age (350 – 100 yr BP) and during periods of societal upheaval (e.g. the Roman conquest of Dacia). The timing of minerogenic events appears to indicate a teleconnection between the North Atlantic Oscillation (NAO) and hydroclimate variability in south-eastern Europe, which persists throughout the mid-to-late Holocene. Secondly, a 10,800-year record of geochemically-derived dust deposition and testate amoeba-derived local wetness from the Eastern Romanian Carpathians highlights several discrepancies between eastern and western European dust depositional records and the impact of highly complex hydrological regimes in the Carpathian region. Specifically, the record outlines the increased impact of Saharan dust after 6100 yr BP which is associated with the end of the African Humid Period. A lead (Pb) record from a peat bog in Western Serbia provides an unprecedented view on past pollution related to metal exploitation in the Balkans. Environmental Pb pollution is first observed in the very earliest Bronze Age, the oldest environmental Pb pollution in Europe. After 600 CE an almost linearly increasing Pb trend until the Medieval period is observed. Comparison with western European records suggests an alternative history of European metallurgy, one in which metal-related pollution does not cease with the fall of the Roman Empire, and which displays major Medieval pollution. Pb isotopes provide a valuable insight into the sources of Pb observed within a sample, allowing for the fingerprinting of their metal's geological source, or production site. Presented here is the application of a state of the art Bayesian mixing model to such a purpose, outlining a 'best practice' and testing of the approach via a number of real-world examples.",,Longman J,,2018,,,,Ph.D. Thesis
How Methods Make Designers,"Through their combination of lifestyle and method, Silicon Valley models for tech production such as design thinking, startup incubators, lean management, etc. are spreading across the globe. These paradigms are positioned by product designers, politicians, investors and corporations alike as replicable routes to individual and national empowerment. They are portrayed as universal templates, portable across national borders and applicable to local needs. We draw from our ethnographic engagements with tech entrepreneurial efforts in Ghana, China, and Jamaica to unpack the stakes involved in their uptake, showing that while local actors produce situated alternatives, their work nevertheless often results in a continued valorization of these seemingly universal methods. We argue that design methods shape not only use practices, but have consequences for the life worlds of professional designers. This includes how they impact personal and national identities, confer legitimacy in transnational innovation circles, and secure access to social and economic resources. Ultimately, we call for an inclusion of these factors in ongoing conversations about design and design methods.",,"Avle S,Lindtner S,Williams K",,2017,472–483,10.1145/3025453.3025864,https://doi-org.proxy.bnl.lu/10.1145/3025453.3025864;http://dx.doi.org/10.1145/3025453.3025864,Conference Paper
"Annual Editions: Computers in Education, 12/e",This Twelfth Edition of ANNUAL EDITIONS COMPUTERS IN EDUCATION provides convenient inexpensive access to current articles selected from the best of the public press Organizational features include an annotated listing of selected World Wide Web sites an annotated table of contents a topic guide a general introduction brief overviews for each section a topical index and an instructors resource guide with testing materials USING ANNUAL EDITIONS IN THE CLASSROOM is offered as a practical guide for instructors ANNUAL EDITIONS titles are supported by our student website wwwmhclscomonline Table of contents UNIT 1 Introduction 1 42913 Digital Natives Digital Immigrants Marc Prensky On the Horizon October 2001 Marc Prensky states that our students have changed radically Todays students are no longer the people for which our educational system was designed The traditional age of college students Generation NeXt is the product of a very different social reality than the members of the Baby Boom that predominate college faculty and staff Postmodern influences and sensibilities permeate the expectations of students and may be at odds with what the schools intend to offer 2 42914 The Myth about Online Course Development Diana G Oblinger and Brian L Hawkins EDUCAUSE Review JanuaryFebruary 2006 Oblinger and Hawkins claim that online instruction is more than a series of readings posted to a Web site it requires deliberate instructional design that hinges on linking learning objectives to specific learning activities and measurable outcomes Although the Lone Ranger approach to online learning has worked in the past it does not scale well Institutions are finding that teamsnot individualsdevelop and deliver the most effective online courses 3 42915 Creating Flexible ELearning Through the Use of Learning Objects Marie Lasseter and Michael Rogers EDUCAUSE Quarterly 2004 The university system of Georgia deconstructed their existing online courses to create separate files of reusable content They rearranged the individual pieces of content and placed them into a hierarchy consisting of learning objects that fall under objectives named by topic Instead of navigating to a course faculty would navigate a new course section What they created was a course that is SCORM compliant Everything is tied to learning objectives 4 42916 Meeting Generation NeXt Todays Postmodern College Student Mark L Taylor 2005 Collection of Papers on Self Study and Institutional Improvement April 2005 This paper provides an overview of some of the characteristics of Generation NeXt their social genesis and these Postmodern times with suggestions for assisting Generation NeXt to be successful in higher education 5 42917 General Education Issues Distance Education Practices Jeri L Childers and R Thomas Berner Journal of General Education vol 49 no 1 2000 Childers and Berners approach to designing a distance education course is to learn by doing They present a discussion of what worked and what didnt work within the context of general principles outlined by researchers in the area of general education and distance learning They found that distance learning practices actually enhanced delivery of the content and increased interaction between the students to maximize the goals of general education UNIT 2 Curriculum and Instructional Design 6 35333 Designing for Learning The Pursuit of WellStructured Content Judith V Boettcher Syllabus January 2003 This essay describes how to make course content really accessible to students Judith Boettcher takes a look at the characteristics of wellstructured content as it relates to the design of instructional technology resources Boettcher describes the meaning of wellstructured content and focuses on the principles of designing for learning In addition she describes each of the three levels that formulate the characteristics of digital learning resources 7 42918 Integrating Technology into the Instructional Process Good Practice Guides the Way Marianne Handler Learning Point Winter 2005 Marianne Handler states that the computer is a tool for students and teachers She describes in detail the ways computers can be integrated into the curriculum by thinking of this resource as another tool available to students rather than thinking of teaching computers as a subject itself One way to think about this is to aim for the use of curriculumdriven software not softwaredriven curriculum 8 42919 On the Right Track Technology for Organizing and Presenting Digital Information Sean J Smith and Steven B Smith Intervention in School and Clinic May 2002 This article describes an online resource TrackStar that helps teachers and students organize and annotate Web sties into lessons presentations assignments or instructional resources Structuring Webbased resources is important if teachers are to use these resources in an effective instructional manner 9 42920 A Brief History of Instructional Design Douglas Leigh Performance Improvement Global Network July 10 1998 This article describes the discipline of Instructional Systems Design from Aristotle through Seymour Papert Throughout the article the events of cognition are described from the cognitive basis of learning and memory to the current trends of constructivism This decade by decade description to the development of Instructional Systems Designs taxonomy is very informative 10 42921 Designing Statistics Instruction for Middle School Students In Brief Winter 2004 This article capitalizes on the notion of distribution as a key concept in statistics and makes generalization and justification an explicit focus of instruction Designing instructional sequences that fostered student ability to analyze data and understand statistical inference as well as developing the ability to design procedures for generating sound data were items of focus 11 43301 Changes in Brain Function in Children with Dyslexia after Training Elise Temple The Phonics Bulletin May 2003 This article shows that it is possible to study the brain effects of training in human children opening up the possibility for further research that explores different interventions and educational strategies This study also shows that a specific remediation program Fast ForWard Language resulted in changes in the brain function of children with dyslexia while improving their reading ability UNIT 3 Classroom Application and Software Evaluations 12 42922 Implementing PDAs in a College Course One Professors Perspective Doug Peterson Syllabus November 2002 Personal digital assistants PDAs have been a mainstay in the business world for several years but their adoption in higher education is relatively recent This article discusses how PDAs are likely to become an integral part of the educational landscape 13 42923 Digital GameBased Learning Richard Van Eck EDUCAUSE Review MarchApril 2006 This article asks the questions Will we continue to learn from the past Will we realize the potential that DGBL has to revolutionize how students learn The author believes that this has much less to do with attitude and learner preferences than it does with a technology that supports some of the most effective learning principles identified during the last hundred years 14 42924 Podcasting and VODcasting A White Paper Peter Meng University of Missouri IAT Services March 2005 This paper describes why the rapid evolution of audiophotovideo recording capabilities through phones and inexpensive handheld devices will create a flood of multimedia content Ultimately from this flood of content there will be a growing need for a centralized content management and monetization infrastructure as well as an education support architecture to assist faculty in the integration 15 43335 Type II Technology Applications in Teacher Education Using Instant Messenger to Implement Structured Online Class Discussions LihChing Chen Wang and William Beasley Computers in the Schools vol 22 no 1 2005 In this article the use of the Instant Messenger IM environment to carry out structured online class discussions in graduate teacher education courses is described Properties of IM are delineated and specific procedures in using IM as a vehicle for class discussions are discussed In closing the authors draw a clear distinction between casual IM use in a class setting and planned structured implementation of IM as an example of a Type II technology application UNIT 4 Teacher Training 16 35346 Student Teachers Perceptions of Instructional Technology Developing Materials Based on a Constructivist Approach Tugba Yanpar Sahin British Journal of Educational Technology January 2003 The author presents a new course for elementary student teachers that has been developed at the Zonguldak Karaelmas University in Turkey This article reports on a study of the course during the academic year 20002001 and concludes that a constructivist approach should be adopted 17 35347 Assessing and Monitoring Student Progress in an ELearning Personnel Preparation Environment Edward L Meyen et al Teacher Education and Special Education vol 25 no 3 2002 The authors draw upon their personal online teaching experiences in addressing strategies for assessing student performance and using electronic portfolios in elearning environments both presented as integral aspects of the elearning instructional process 18 42925 Assessing the Technology Training Needs of Elementary School Teachers Melinda McCannon and Tena B Crews Journal of Technology and Teacher Education vol 8 no 2 2000 This study found that computers are prevalent in the elementary schools and that teachers are using them However they are using them for administrative tasks instead of part of the student learning process The article recommends that teacher educators offer staff development courses in curriculum integrationpresentation software and researchusing the World Wide Web and CDROMs 19 42926 An Investment in Tomorrows University Students Enhancing the Multimedia Skills of Todays K12 Teachers John Minor Ross Journal of Computing in Small Colleges March 1999 Teaching multimedia to computing majors is no longer new What this article focuses on is the need to understand problems in K12 before offering technology solutions UNIT 5 Multimedia and Technology 20 42927 The Value of Teaching and Learning Technology Beyond ROI Jonathan D Mott and Garin Granata EDUCAUSE Quarterly vol 29 no 2 2006 Just as it is difficult to demonstrate ROI for broad IT initiatives it is difficult to show ROI for money and time spent building implementing and supporting a teaching and learning infrastructure This article provides what the authors believe is a more realistic and helpful approach in terms of prioritizing and assigning scarce resources to maximize institutional effectiveness 21 42928 Boomers and GenXers Millenials Understanding the New Students Diana G Oblinger EDUCAUSE Review JulyAugust 2003 Diana Oblinger states that an essential component of facilitating learning is understanding learners The learning styles attitudes and approaches of high school students differ from those of eighteen to twentytwo yearold college students How well do faculty administrators and staff understand these differences How often do they take the differences into account when designing programs or courses 22 42929 Science 38 Technology Its A Perfect Match Lucille Renwick Instructor March 2003 Many teachers are using technologies such as computers scanners and digital cameras to motivate and excite their students Whether studying ecosystems or the solar system teachers have found super and simple ways to embed technology in their science lessons This article provides examples of these exciting uses of multimedia 23 42930 Technologies for Teaching Science and Mathematics in K12 Schools Richard B Speaker Jr Proceedings of ComputerBased Learning in Sciences 2003 This article discusses the issues and practices in using technologies for teaching and learning science and mathematics in the K12 schools in the Southern United States where the digital divide between technology rich schools and technology poor schools is growing wider despite attempts to provide funds and standards 24 42931 SmartTutor Combining SmartBooks and Peer Tutors for MultiMedia OnLine Instruction Danny Kopec Paul Whitlock and Myra Kogen Proceedings of the International Conference on Engineering Education August 1821 2002 This article discusses SmartTutor a comprehensive webbased peertutoring service geared to the needs of urban commuter college students This technology has provided a userfriendly selfpaced easy to modify software environment intended to serve the users learning needs UNIT 6 Learning Management Systems and Learning Objects 25 42932 Changing Course Management Systems Lessons Learned Kathy A Smart and Katrina A Meyer EDUCAUSE Quarterly vol 28 no 2 2005 The authors describe the process of converting a Blackboard course to Desire2Learn The task of moving all of the learning management systems to one CMS was difficult and costly The article describes how they did it and the benefits of their efforts 26 42933 Classroom Assessment in WebBased Instructional Environment Instructors Experience Xin Liang and Kim Creasy Practical Assessment Research 38 Evaluation March 2004 This article uses the perceptions and experiences of instructors to investigate the dynamics of WebCT The findings from this study indicate that performancebased assessment writing skills interactiveassessment and learner autonomy are major assessment aspects to inform teaching and enhance learning 27 42934 MISESS WebBased Examination Evaluation and Guidance Zuhal Tanrikulu EDUCAUSE Quarterly vol 29 no 1 2006 Tanrikulu describes the development of an electronic support system that was developed specifically to provide Webbased support for students and instructors in the Management Information Systems MIS Department at Bogazici University in Turkey The resulting system called MISESS is so flexible that it could easily support other departments and other universities wanting to offer course materials exams and tutorial services online UNIT 7 The Internet and Computer Networks 28 42935 The Myth about Student Competency Diana G Oblinger and Brian L Hawkins EDUCAUSE Review MarchApril 2006 The authors analyze the real IT competence of todays college and university students who seem to be technologically competent Whereas colleges and universities often focus on technology skills it is actually information literacy that should be the concern This article puts the focus on what information literate people need to know 29 42936 Promoting Academic Literacy with Technology Successful Laptop Programs in K12 Schools Mark Warschauer et al System vol 32 2004 This paper presents case studies of two K12 schools that successfully employ hightechnology environments In both schools technology is used to engage students in cognitively demanding activity motivate independent reading and provide scaffolding for language development 30 35359 Probing for Plagiarism in the Virtual Classroom Lindsey S Hamlin and William T Ryan Syllabus July 2003 The authors believe that educators are skeptical about preservation of academic integrity in the virtual classroom They believe that Web sites and software now available to educators have the ability to detect and battle plagiarism and cheating They also believe that the various types of online assessment tools assignments and activities available with a virtual course are a deterrent for cheating 31 35361 The Webs Impact On Student Learning Katrina A Meyer THE Journal May 2003 Katrina Meyer provides a good start on the research that is needed to ensure that the Web is used effectively for student learning UNIT 8 Distributed Learning 32 42937 Software Agents to Assist in Distance Learning Environments SheungOn Choy SinChun Ng and YiuChung Tsang EDUCAUSE Quarterly vol 28 no 2 2005 The authors investigated employing a software agent to act as a teaching assistant to the course coordinator by monitoring and managing course activities The study showed that such a method may provide timely and meaningful feedback to students as well as early detection of problems in both the teaching and learning processes 33 42938 The Virtual Revolution Randall Greenway and Gregg Vanourek Education Next Spring 2006 The authors work with virtual schools has led them to a number of observations about their current practice that they believe can guide policymakers Without good curriculum instruction tracking resources support and leadership virtual schools will flounder Read this article to get the history and details of how to create a good virtual school 34 35362 Learner Support Services for Online Students Scaffolding for Success Stacey LudwigHardman and Joanna C Dunlap International Review of Research in Open and Distance Learning IRRODL April 2003 The authors describe a critical component of an effective retention program for online students learner support services for online learning The authors describe the strategies that can address the retention challenges Examples from Western Governors University describe these strategies in action,,"Hirschbuhl J,Kelley J",,2006,,,,Book
An Expert System to Evaluate the Rural Community Development,"In this research, the potential application of expert systems technology to rural economic development is illustrated by developing a prototype diagnostic expert system. This system, Expert System for Rural Economic Development (ESRED), is developed based on the knowledge of experts in the area of economic development. ESRED interprets given data for certain variables in a community, and makes qualitative decisions about the community's economic well-being. These variables and their relationships were identified by the selected experts. The system is also capable of suggesting courses of economic actions to improve the weaknesses of a community after the diagnosis.ESRED consists of several knowledge bases developed using the XI Plus expert system shell and an external graphics program. The system's knowledge bases contain heuristic rules and facts that are constructed based on the knowledge extracted from a recognized expert in the problem domain. The external program is used to present the graphical view of the variables' behavior. The performance of ESRED is tested for a small town, Cuba, Missouri.",,Alasya D,,1989,,,,Ph.D. Thesis
An Intelligent Handcrafted Feature Selection Using Archimedes Optimization Algorithm for Facial Analysis,"Human facial analysis (HFA) has recently become an attractive topic for computer vision research due to technological progress and mobile applications. HFA explores several issues as gender recognition (GR), facial expression, age, and race recognition for automatically understanding social life. This study explores HFA from the angle of recognizing a person’s gender from their face. Several hard challenges are provoked, such as illumination, occlusion, facial emotions, quality, and angle of capture by cameras, making gender recognition more difficult for machines. The Archimedes optimization algorithm (AOA) was recently designed as a metaheuristic-based population optimization method, inspired by the Archimedes theory’s physical notion. Compared to other swarm algorithms in the realm of optimization, this method promotes a good balance between exploration and exploitation. The convergence area is increased By incorporating extra data into the solution, such as volume and density. Because of the preceding benefits of AOA and the fact that it has not been used to choose the best area of the face, we propose utilizing a wrapper feature selection technique, which is a real motivation in the field of computer vision and machine learning. The paper’s primary purpose is to automatically determine the optimal face area using AOA to recognize the gender of a human person categorized by two classes (Men and women). In this paper, the facial image is divided into several subregions (blocks), where each area provides a vector of characteristics using one method from handcrafted techniques as the local binary pattern (LBP), histogram-oriented gradient (HOG), or gray-level co-occurrence matrix (GLCM). Two experiments assess the proposed method (AOA): The first employs two benchmarking datasets: the Georgia Tech Face dataset (GT) and the Brazilian FEI dataset. The second experiment represents a more challenging large dataset that uses Gallagher’s uncontrolled dataset. The experimental results show the good performance of AOA compared to other recent and competitive optimizers for all datasets. In terms of accuracy, the AOA-based LBP outperforms the state-of-the-art deep convolutional neural network (CNN) with 96.08% for the Gallagher’s dataset.",,"Neggaz I,Fizazi H",,2022,10435–10464,10.1007/s00500-022-06886-3,https://doi-org.proxy.bnl.lu/10.1007/s00500-022-06886-3;http://dx.doi.org/10.1007/s00500-022-06886-3,Journal Article
Understanding Gender Bias: Differences in Tech Stereotypes According to the Socio-Economic Background of Girls,"Of all the countries that belong to OCDE, the Latin American countries have the highest levels of inequality. Chile is among them, with scores similar to Bolivia and Guatemala [10]. Also, the number of women living in poverty is higher than that of men [4]. Women’s economic context is essential for their families, as 90% of single-parent families are supported by women [12]. One way of achieving economic development may be choosing a career in technology, as tech jobs are among the highest-paid in the country [9]. Also, they are flexible, allowing women to balance work and family, and have been proven to promote social mobility and country economic growth [2, 7]. However, there is a well-known gender gap in technology; for example, only 24% of computer science students are women in Chile [9]. To inspire women to have a computer science career, interventions should be undertaken while they are girls, by addressing stereotypes that influence their attitude towards technology [6]. These stereotypes are influenced by the context in which girls grow [5]; in particular their socio-economic context [3]. Therefore, it may be essential to understand the context of girls, and their thoughts towards tech stereotypes, to create better computer science education interventions. We conducted a preliminary interview study with sixth grade girls, since at this age, stereotypes can still be challenged [1, 11], while the opportunities to challenge stereotypes decrease from the eighth grade on [8]. The research question of this study is whether there are different stereotypes regarding technology among girls with different socio-economical levels. It has been hypothesized that there will be different stereotypes among these girls. Twelve interviews were done: 6 with girls from low vulnerability contexts, and 6 with girls from high vulnerability contexts. The interview data were transcribed and analyzed using grounded theory under Charmaz’s perspective. The results from this preliminary study were that girls from a high vulnerability context have a negative attitude towards technology mainly because of misconceptions regarding technology, e.g. what it does and how to work with it. On the other hand, girls from low vulnerability contexts have stereotypes in which tech careers were considered to be manly, and they also had concerns about family-work balance. With this information, a semi-structured interview has been developed to apply to girls from low and high vulnerability contexts and analyzed with ground theory. Further, with this qualitative information, a quantitative tool will be developed. A national survey will be created to determine if these different stereotypes are also present in the larger population of girls. With this information, better computer science education interventions may be created, especially focused on high vulnerability contexts, considering the particular stereotypes that these girls have regarding tech stereotypes, that keep them away from computer science careers.",,"Vergara K,Herskovic V,Guerrero P",,2022,55–56,10.1145/3501709.3544289,https://doi-org.proxy.bnl.lu/10.1145/3501709.3544289;http://dx.doi.org/10.1145/3501709.3544289,Conference Paper
Comparative Risk Analysis of Development of the Lignite Basins in Serbian Part of the Danube Region,"The paper gives an overview of the global business risks and risks in the mining development in the Kolubara and Kostolac lignite basins in the area of the Danube river in Serbia. An identification of main risks is undertaken by application of a comprehensive development framework approach, comparative analysis and Spider method. Risks in the development of mining are emphasized by global economic and financial crisis, as well as by the adoption of Kyoto Protocol regulations and mechanisms. The paper shows that the consideration and elimination of risk factors is important for the increase of competitiveness and energy efficiency in the lignite basins as an integral part of the efforts for achieving the sustainable development in the Serbian part of the Danube region.",,"Zeković S,Maričić T",,2011,171–176,,,Conference Paper
European Union's Green Smart Directive or How Resource-Conscious Smart Systems Saved the World,"In the 2020s, the devastating effects of anthropogenic climate change became unmistakable. Floods, storms, the mass extinction of flora and fauna, the threats of further pandemics, as well as Russia's war with Ukraine made it necessary for Europeans to act immediately. While the public was still wrangling about the best way to become more resource-conscious, in 2026 the European Union passed a directive that bound “smart”, artificial-intelligence-infused technologies to the goal of reducing resource consumption by law – the Green Smart directive. The present paper traces the impact of Green Smart in three everyday domains: laundry, mobility, and gardening. It shows that Green Smart led to a “decentering” of the human with beneficial effects on the planet as well as individual wellbeing. Upon release of the directive, smart systems immediately installed policies of CO2 avoidance and resource-saving, which had been discussed for centuries already, but had never actually been implemented. While this was accompanied by fierce debates about “freedom” and the “enslavement” of humanity by technology, in everyday life, the policies led to an almost instant reduction in resource consumption. New and adapted everyday practices appeared quickly, and many of them changed people's lives clearly for the better. In general, humanity's attitude toward technology changed profoundly. Instead of expecting technology to be a mere tool, always under the immediate control of people and to be used for whatever people saw fit, technology became understood as a powerful “other”, with its own needs and goals – even if those were designed by humans themselves. Instead of using technology, people started to cooperate and negotiate with technology about better ways of living.",,"Hassenzahl M,Dörrenbächer J,Laschke M,Sadeghian S",,2022,,10.1145/3546155.3547277,https://doi-org.proxy.bnl.lu/10.1145/3546155.3547277;http://dx.doi.org/10.1145/3546155.3547277,Conference Paper
Detection of Stored-Grain Insects Using Deep Learning,,,"Shen Y,Zhou H,Li J,Jian F,Jayas DS",,2018,319–325,10.1016/j.compag.2017.11.039,https://doi-org.proxy.bnl.lu/10.1016/j.compag.2017.11.039;http://dx.doi.org/10.1016/j.compag.2017.11.039,Journal Article
Metaphors and Narratives in Exile : Understanding the Experiences of Forced Migrants in Britain,"The research aimed to reach an understanding of the experience of exile by investigating aspects of life in exile and how refugees portray their own experiences of asylum and define what asylum has meant to them in the process of living in and integrating their host society. The study, focusing on 30 refugees from Congo, Kosovo and Somalia, has sought to examine some of the metaphors that refugees associated with the experience of exile and the narration of these experiences in their own words. The search for meaning has dominated the research and throughout the analysis metaphors formulated by the refugees have been used to elucidate the argument. Some key findings have revealed the following: - There are contradictions in what asylum means to different refugees. The perception of exile ranges from 'hell' to 'heaven' and from 'safe haven' to further 'trauma'. - The process to re-socialisation is not straightforward and many different factors, in isolation or in combination, deeply affect the process. These range from the degree of coercion leading to exile, racialisation in the host country to the multiplicity and types of networks available to the exile in the host country and the degree to which the exile exploits them successfully. - The perception of home is difficult to agree. For people torn apart such as the Somali, Congolese and Kosovan refugees studied, home is neither here or there (i. e. neither in the host country or the country of origin); home is nowhere because events in the home country have disconnected them from feeling a sense of belonging and often isolation and racialisation in the host country comes short of enabling them to connect psychologically, socially and culturally with the new place. The metaphors used by the refugees carry a sense of nostalgia for the past and the lost land for many refugees. The nostalgia and loss encompasses the deficit in social status and mobility, the diminishing cultural identity including language as well as mere familiarity with the environment. But the narratives, with all their emotional contents, carry both a sense of hope and despair. The hope resided in the forecasting of better days in the native country which might trigger the refugees' return to their ""natural waters"" as a respondent put it metaphorically to refer to the original socio-cultural milieu the refugees originated from. For the stayers, hope was expressed in their seeing themselves finding 'a place under the sun' in the land of their exile. However, at the same time, the stagnation or worsening of the situation in the native land and often combined with the feeling of or categorisation as outsiders and others in the host country brought a sense of despair. The study has enabled the drawing of the conclusion that life in exile can often be ambiguity, uncertainty, loss but could also be new light, salvation and opportunity. This dialectics seems to be at the heart and the essence of exile, so far as the humans involved are both psychologically and social beings.",,Hack-Polay DD,,2006,,,,Ph.D. Thesis
The Perception of Hypertension among Haitian Adults: A Focused Ethnography,"The terms ""health disparities"" and ""social justice"" are popular buzz words in health care. These terms have been applied locally and internationally when examining the current health conditions and health resources. Since 2010, Haiti has gained much attention from the world with relief efforts and increased attention on the apparent health needs in the country. Despite these efforts, the overall health statistics of the country have not improved (Brown, 2010; Garfield & Berryman, 2012; WHO, 2014). In 2010, in response to the global epidemiological transition, the World Health Organization (WHO) shifted its attention to worldwide non-communicable diseases (NCDs) such as cardiovascular disease, cancer, diabetes, and respiratory diseases. With this shift in attention, hypertension has been identified as a worldwide health concern. The purpose of this focused ethnography is to describe Haitians' perceptions of hypertension which contribute to the meaning of and beliefs about this chronic illness, in order to more fully understand the needs of Haitian adults with hypertension. Kleinman's Explanatory Model of Illness (Kleinman, A., Eisenberg, L., & Good, B., (1978) serves as the theoretical background for the study. The overarching theme identified is that Haitians perceive hypertension to be a feeling that one gets which should be treated at that moment to prevent falling down. This feeling presents differently and can vary with occurrences and individuals. The feelings identified as being associated with hypertension can actually be a variety of symptoms to include: headache, blurry vision, dizziness, burning, weakness, and shortness of breath. These feelings, known as symptoms in allopathic medicine, are consistent with presenting clinical manifestations of hypertension as well as consistent with the complication of stroke often associated with uncontrolled hypertension. The findings in this study can be expanded upon to inform management and treatment options for this population as well as provide recommendations for healthcare providers serving in developing countries.",,"Feurer AE,Lindgren T,Beneson I,Chase S",,2020,,,,Ph.D. Thesis
Fraud Detection in the Distributed Graph Database,"Over the last few decades, graphs have become increasingly important in many applications and domains for managing Big data. Big data analysis in a graph database is described as an analysis of exponentially increasing massive interconnected data concerning time. However, analyzing big connected data in social networks and synthetic identity detection is challenging. In previous approaches, fraud detection has been done on the complete graph data, which is a time-consuming process and will create bottlenecks while query execution. To overcome the issue, this paper proposes a new fraud detection technique to unveil synthetic identities involved in the Panama Paper leak dataset (unprecedented leak of 11.5 m data from the database of the world’s fourth-biggest offshore law arm, Mossack Fonseca) using a Node rank-based fraud detection algorithm by integrating distributed data profiling techniques on a minimized graph by minimizing the least influential nodes. The proposed model is verified on the three nodes cluster to improve data scalability, reduce the query execution time by an average of 30–36% and finally reduce the fraud detection time by 18.2%.",,"Srivastava S,Singh AK",,2022,515–537,10.1007/s10586-022-03540-3,https://doi-org.proxy.bnl.lu/10.1007/s10586-022-03540-3;http://dx.doi.org/10.1007/s10586-022-03540-3,Journal Article
Analysis of the Kupyna-256 Hash Function,"The hash function Kupyna was recently published as the Ukrainian standard DSTU 7564:2014. It is structurally very similar to the SHA-3 finalist GrØstl, but differs in details of the round transformations. Most notably, some of the round constants are added with a modular addition, rather than bitwise xor. This change prevents a straightforward application of some recent attacks, in particular of the rebound attacks on the compression function of similar AES-like hash constructions. However, we show that it is actually possible to mount rebound attacks, despite the presence of modular constant additions. More specifically, we describe collision attacks on the compression function for 6 out of 10 rounds of Kupyna-256 with an attack complexity of $$2^70$$, and for 7 rounds with complexity $$2^125.8$$. In addition, we can use the rebound attack for creating collisions for the round-reduced hash function itself. This is possible for 4 rounds of Kupyna-256 with complexity $$2^67$$ and for 5 rounds with complexity $$2^120$$.",,"Dobraunig C,Eichlseder M,Mendel F",,2016,575–590,10.1007/978-3-662-52993-5_29,https://doi-org.proxy.bnl.lu/10.1007/978-3-662-52993-5_29;http://dx.doi.org/10.1007/978-3-662-52993-5_29,Conference Paper
RIIT '15: Proceedings of the 4th Annual ACM Conference on Research in Information Technology,"Welcome to the 16th Annual Conference on Information Technology Education (SIGITE 2015) and the 4th Annual Conference on Research in Information Technology (RIIT 2015)! The theme this year is ""Creating the Future"", with a focus on the innovative approaches in information technology that continue to define and strengthen our field. What better place to convene than at DePaul University, where this occurs every day! In addition, Chicago offers waterfront parks, amazing architecture, a vibrant cultural scene and incredible cuisine, all within steps of the DePaul campus.SIGITE/RIIT 2015 features a very insightful and inspiring program. As in past years, the synergies between research and education in information technology are prevalent, and several themes emerged from the accepted submissions. The technical areas of networking, mobile devices, security, and development continue to be popular with researchers; as does innovation in content, pedagogy and assessment.This strong program would not have been possible without a pool of quality papers, panels, posters and lightning talks. The call for participation attracted 98 submissions, 72 of which were submitted to SIGITE and 26 to RIIT. Eighty of the submissions were papers, with 58 papers submitted to SIGITE and 22 papers submitted to RIIT. SIGITE has 24 papers in its program for an acceptance rate of 41% and RIIT has 10 papers for an acceptance rate of 45%. All of the authors presenting should be congratulated on their excellent work!A conference relies on the assistance and expertise from a multitude of peer reviewers, and this year was no exception. Forty-five reviewers and two meta-reviewers worked diligently to ensure that every paper had at least three independent reviews and that, where divergences occurred, consensus was reached. Over 300 reviews were ultimately conducted which hopefully resulted in constructive feedback that has benefitted the authors regardless of the decision.SIGITE/RIIT 2015 runs from Thursday to Saturday and is preceded by a Chicago Highlights Tour on Wednesday night and a Chairs Meeting and Oracle workshop on Thursday morning. The formal program begins on Thursday at noon with a keynote by Mike Shannon, a 2012 graduate of Illinois State University and co-founder of the Chicago-based Packback. This startup provides college students a variety of affordable eTextbook offerings and a platform for student-faculty engagement, and its founders appeared on ABC's ""Shark Tank"" in March 2014 where they reached an agreement with Mark Cuban. The program continues with a combination of papers and lightning talks on research in progress and concludes with a reception, which we know will be useful for networking with colleagues old and new. Friday features additional paper sessions and lightning talks for SIGITE and RIIT, a poster session in the afternoon, concluding with a reception for Community College educators. The conference closes on Saturday with more presentations and a final session where we will share plans for SIGITE/RIIT 2016 in picturesque New England!I hope you find the conference compelling and motivating with opportunities to reconnect with colleagues you know, identify new collaborators, and leave with a commitment to research and innovation that will result in new submissions to SIGITE or RIIT next year. The excellence you see at SIGITE/RIIT 2015 depends on your participation and engagement and, on behalf of all that collaborated to make this happen, we thank you!",,,,2015,,,,Book
SIGITE '15: Proceedings of the 16th Annual Conference on Information Technology Education,"Welcome to the 16th Annual Conference on Information Technology Education (SIGITE 2015) and the 4th Annual Conference on Research in Information Technology (RIIT 2015)! The theme this year is ""Creating the Future"", with a focus on the innovative approaches in information technology that continue to define and strengthen our field. What better place to convene than at DePaul University, where this occurs every day! In addition, Chicago offers waterfront parks, amazing architecture, a vibrant cultural scene and incredible cuisine, all within steps of the DePaul campus.SIGITE/RIIT 2015 features a very insightful and inspiring program. As in past years, the synergies between research and education in information technology are prevalent, and several themes emerged from the accepted submissions. The technical areas of networking, mobile devices, security, and development continue to be popular with researchers; as does innovation in content, pedagogy and assessment.This strong program would not have been possible without a pool of quality papers, panels, posters and lightning talks. The call for participation attracted 98 submissions, 72 of which were submitted to SIGITE and 26 to RIIT. Eighty of the submissions were papers, with 58 papers submitted to SIGITE and 22 papers submitted to RIIT. SIGITE has 24 papers in its program for an acceptance rate of 41% and RIIT has 10 papers for an acceptance rate of 45%. All of the authors presenting should be congratulated on their excellent work!A conference relies on the assistance and expertise from a multitude of peer reviewers, and this year was no exception. Forty-five reviewers and two meta-reviewers worked diligently to ensure that every paper had at least three independent reviews and that, where divergences occurred, consensus was reached. Over 300 reviews were ultimately conducted which hopefully resulted in constructive feedback that has benefitted the authors regardless of the decision.SIGITE/RIIT 2015 runs from Thursday to Saturday and is preceded by a Chicago Highlights Tour on Wednesday night and a Chairs Meeting and Oracle workshop on Thursday morning. The formal program begins on Thursday at noon with a keynote by Mike Shannon, a 2012 graduate of Illinois State University and co-founder of the Chicago-based Packback. This startup provides college students a variety of affordable eTextbook offerings and a platform for student-faculty engagement, and its founders appeared on ABC's ""Shark Tank"" in March 2014 where they reached an agreement with Mark Cuban. The program continues with a combination of papers and lightning talks on research in progress and concludes with a reception, which we know will be useful for networking with colleagues old and new. Friday features additional paper sessions and lightning talks for SIGITE and RIIT, a poster session in the afternoon, concluding with a reception for Community College educators. The conference closes on Saturday with more presentations and a final session where we will share plans for SIGITE/RIIT 2016 in picturesque New England!I hope you find the conference compelling and motivating with opportunities to reconnect with colleagues you know, identify new collaborators, and leave with a commitment to research and innovation that will result in new submissions to SIGITE or RIIT next year. The excellence you see at SIGITE/RIIT 2015 depends on your participation and engagement and, on behalf of all that collaborated to make this happen, we thank you!",,,,2015,,,,Book
The Junior High Years: A Time for Beginning Engineering Orientation,"The thesis of this paper is that the junior high school years are a key time in the development of orientation towards careers. It is suggested that since women are so grossly underrepresented in the engineering profession, efforts to attack the problem should include work with students at this period in their lives. A team-taught discussion of engineering as an interesting and vital career for women and men, accompanied by a dynamic demonstration of applied scientific principles, has been offered by Georgia Institute of Technology to two school systems in the Metropolitan Atlanta area. Three programs-Lasers and Holograms (Electrical Engineering), Polymer Chemistry (Textile Engineering), and High Temperature and High Temperature Materials (Ceramic Engineering) have been developed by faculty in the respective schools and are described in this paper. Reception has been enthusiastic and the suggestion is made that such programs can act as catalysts to produce longer range programs which can help move us toward parity for women in the engineering profession.",,Burks EL,,1975,15–20,10.1109/TE.1975.4320938,https://doi-org.proxy.bnl.lu/10.1109/TE.1975.4320938;http://dx.doi.org/10.1109/TE.1975.4320938,Journal Article
Contributors,"Ivo Adan (“Exact FCFS Matching Rates for Two Infinite Multitype Sequences”) is a full professor of manufacturing networks in the Department of Mechanical Engineering at the Eindhoven University of Technology. His current research focuses on the modeling, analysis, and design of manufacturing, warehousing, and healthcare systems, and more specifically, the analysis of multidimensional Markov processes and queueing models.Alper Atamtürk (“A Conic Integer Programming Approach to Stochastic Joint Location-Inventory Problems”) is a Chancellor's Professor in the Industrial Engineering and Operations Research Department at the University of California, Berkeley. His current research interests are in optimization, integer programming, optimization under uncertainty with applications to energy, finance, operations, cancer therapy, and defense. He was appointed a National Security Fellow by the United States Department of Defense in 2010.Rami Atar (“A Diffusion Regime with Nondegenerate Slowdown”) is a professor in the Department of Electrical Engineering, Technion, Israel. His research interests are in stochastic processes. These include asymptotic analysis of queueing and stochastic network models in diffusion and large deviation regimes, PDE techniques in stochastic control and differential games, filtering, and estimation.Derek Atkins (“A Simulation Optimization Approach to Long-Term Care Capacity Planning”) is a professor in the Sauder School of Business at the University of British Columbia, Canada. His research interests are in supply chains and healthcare operations. He was formerly director of the Centre for Operations Excellence at Sauder, which undertook a project for a local health authority that triggered the need for the paper presented in this issue. Gemma Berenguer (“A Conic Integer Programming Approach to Stochastic Joint Location-Inventory Problems”) is a Ph.D. candidate in the Industrial Engineering and Operations Research Department at the University of California, Berkeley. She is doing research on integrated supply chain design problems, nonprofit supply chain management problems, and the design of regulatory mechanisms for environmental policies.Ya Ping Fang (“Piecewise Linear Multicriteria Programs: The Continuous Case and Its Discontinuous Generalization”) is an associate professor in the Department of Mathematics at Sichuan University. His research interests are in the area of optimization problems, equilibrium problems, and variational inequalities.Michael C. Fu (“A New Stochastic Derivative Estimator for Discontinuous Payoff Functions with Application to Financial Derivatives”) is the Ralph J. Tyser Professor of Management Science in the Robert H. Smith School of Business at the University of Maryland. His research interests include simulation and applied probability modeling, particularly with applications toward manufacturing systems, supply chain management, and financial engineering. He is a Fellow of INFORMS and IEEE.David Gamarnik (“Belief Propagation for Min-Cost Network Flow: Convergence and Correctness”) is an associate professor of operations research at the Sloan School of Management at the Massachusetts Institute of Technology. His research interests include applied probability and stochastic processes, theory of random graphs and algorithms, combinatorial optimization, statistical learning theory, and various applications. He is a recipient of the Erlang Prize from the INFORMS Applied Probability Society, IBM Faculty Partnership Award, and several NSF-sponsored grants.Nir Halman (“Approximating the Nonlinear Newsvendor and Single-Item Stochastic Lot-Sizing Problems When Data Is Given by an Oracle”) is a lecturer of operations research in the school of business administration at the Hebrew University of Jerusalem. His research focuses on optimization methods that yield efficient algorithms in combinatorial optimization.Jonathan Kluberg (“Generalized Quantity Competition for Multiple Products and Loss of Efficiency”) is an investment analyst at High Vista Strategies.Yuri Levin (“Cargo Capacity Management with Allotments and Spot Market Demand”) is a Distinguished Professor of Operations Management at Queen's School of Business in Kingston, Ontario, Canada. His research interests include revenue management, dynamic pricing, numerical optimization, and machine learning applications.Qing Li (“On the Quasiconcavity of Lost-Sales Inventory Models with Fixed Costs”) is an associate professor at the School of Business and Management, Hong Kong University of Science and Technology. His research interests include supply chain management, marketing/operations interfaces, stochastic dynamic inventory models, and economics of waste. Steven I. Marcus (“A New Stochastic Derivative Estimator for Discontinuous Payoff Functions with Application to Financial Derivatives”) is a professor in the Department of Electrical and Computer Engineering and the Institute for Systems Research at the University of Maryland. His research focuses on stochastic control and estimation, with applications in manufacturing and telecommunication networks.Kaiwen Meng (“Piecewise Linear Multicriteria Programs: The Continuous Case and Its Discontinuous Generalization”) holds a Ph.D. degree (2011) in optimization and operations research from the Hong Kong Polytechnic University. His research interests are in the areas of variational analysis, optimization theory, and operations research. S. Michel (“A Column-Generation Based Tactical Planning Method for Inventory Routing”) is an assistant professor of operations research at Le Havre University. She is a member of the Laboratory of Applied Mathematics and the Logistics Engineering Institute and an associate member of the INRIA research team REALOPT. Her research projects concern sea ship and vehicle routing, as well as generic primal heuristics.Anton Molyboha (“Stochastic Optimization of Sensor Placement for Diver Detection”) is a quantitative analyst at Teza Technologies. He holds a Ph.D. degree in mathematics with concentration in stochastic systems (2009) from the Department of Mathematical Sciences at Stevens Institute of Technology.Mikhail Nediak (“Cargo Capacity Management with Allotments and Spot Market Demand”) is an assistant professor in the School of Business at Queen's University in Kingston, Ontario, Canada. His research focuses on new models in revenue management and dynamic pricing.Matthew Nelson (“A Simulation Optimization Approach to Long-Term Care Capacity Planning”) is a project lead in the Centre for Research in Healthcare Engineering at the University of Toronto. He received his master's degree from the Centre for Operations Excellence in the Sauder School of Business at the University of British Columbia. James B. Orlin (“Approximating the Nonlinear Newsvendor and Single-Item Stochastic Lot-Sizing Problems When Data Is Given by an Oracle”) is the Edward Pennell Brooks Professor of Operations Research in the Sloan School of Management at the Massachusetts Institute of Technology. His research focuses on optimization methods, especially in combinatorial and network optimization. He is a coauthor of Network Flows: Theory, Algorithms, and Applications (Prentice-Hall, 1993), for which he was awarded the Lanchester Prize in 1993. He is an INFORMS Fellow.Georgia Perakis (“Generalized Quantity Competition for Multiple Products and Loss of Efficiency”) is the William F. Pounds Professor at the Sloan School of Management at Massachusetts Institute of Technology.Dzung T. Phan (“Lagrangian Duality and Branch-and-Bound Algorithms for Optimal Power Flow”) is a research staff member in the Mathematical Sciences Department at IBM T. J. Watson Research Center, Yorktown Heights, New York, where he spent one year as a postdoctoral researcher. His research interests lie in the field of optimization theory and algorithms. Recently at IBM, he developed several numerical algorithms for optimization problems arising from power system analysis. Martin L. Puterman (“A Simulation Optimization Approach to Long-Term Care Capacity Planning”) is Advisory Board Professor of Operations in the Sauder School of Business at the University of British Columbia, Canada. He was founder and director of the Centre for Operations Excellence (in Sauder), the UBC Centre for Health Care Management, and the Biostatistical Consulting Service at BC Children's Hospital. He received the INFORMS Lanchester Prize for his book Markov Decision Processes: Discrete Stochastic Dynamic Programming (Wiley-Interscience, 2005). He is an INFORMS Fellow and recipient of the Canadian Operations Research Society (CORS) Award of Merit, the CORS Practice Prize, and the INFORMS case prize. Richard Ratliff (“Estimating Primary Demand for Substitutable Products from Sales Transaction Data”) is the Senior Research Scientist at Sabre Research. His primary focus is on applied research and development in travel revenue management. His work has included prototyping new technologies applicable to travel distribution, as well as major travel suppliers.Devavrat Shah (“Belief Propagation for Min-Cost Network Flow: Convergence and Correctness”) is a Jamieson career development associate professor in the Department of Electrical Engineering and Computer Science at Massachusetts Institute of Technology. He is a member of the Laboratory for Information and Decision Systems and Operations Research Center. His research focus is on theory of large complex networks and includes network algorithms, stochastic networks, network information theory, and large-scale statistical inference. He received the George B. Dantzig Dissertation Award from INFORMS in 2005, the ACM SIGMETRICS Rising Star Award in 2008, and the Erlang Prize from INFORMS in 2010.Zuo-Jun (Max) Shen (“A Conic Integer Programming Approach to Stochastic Joint Location-Inventory Problems”) is a Chancellors Professor of Industrial Engineering and Operations Research at the University of California, Berkeley. He has been active in the following research areas: integrated supply chain design and management, market mechanism design, marketing-operations management interface issues, and decision making with limited information. David Simchi-Levi (“Approximating the Nonlinear Newsvendor and Single-Item Stochastic Lot-Sizing Problems When Data Is Given by an Oracle”) is a professor of engineering systems at Massachusetts Institute of Technology. The work described in this paper is part of a larger research project that deals with effective supply chain and procurement strategies that improve supply chain performance.James E. Smith (“Technology Adoption with Uncertain Future Costs and Quality”) is J. B. Fuqua Professor of Business Administration at the Fuqua School of Business, Duke University. His research interests are primarily in decision analysis and focus on developing methods for formulating and solving dynamic decision problems and valuing risky investments. He is an INFORMS Fellow and past president of the Decision Analysis Society.Huseyin Topaloglu (“Cargo Capacity Management with Allotments and Spot Market Demand”) is an associate professor in the School of Operations Research and Information Engineering at Cornell University. His research interests include stochastic programming and optimal control with applications in revenue management, pricing, and inventory control.Canan Ulu (“Technology Adoption with Uncertain Future Costs and Quality”) is an assistant professor in the Department of Information, Risk, and Operations Management (IROM) at the McCombs School of Business, University of Texas at Austin. Her research interests include Bayesian learning in sequential decision problems and the impact of behavioral decision theory on decision analysis methods. F. Vanderbeck (“A Column-Generation Based Tactical Planning Method for Inventory Routing”) is a professor in the Department of Mathematics at the University of Bordeaux. He is a affiliated with the Institute of Mathematics of Bordeaux and the INRIA research center where he leads the research team REALOPT specializing in reformulation and algorithms for combinatorial optimization. His main activity is in decomposition approaches in integer programming with applications in routing and operation planning. He develops a branch-and-price platform named BAPCOD.Johan S. H. van Leeuwaarden (“Staffing Call Centers with Impatient Customers: Refinements to Many-Server Asymptotics”) is an associate professor of probability theory and stochastic networks at the Eindhoven University of Technology and a research fellow at the research institute EURANDOM. He received the INFORMS Telecommunication Dissertation Award (2008), a Veni Grant (2006--2009) from the Netherlands Organisation for Scientific Research, and a Starting Grant (2010--2015) from the European Research Council.Garrett van Ryzin (“Estimating Primary Demand for Substitutable Products from Sales Transaction Data”) is Paul M. Montrone Professor of Business and Chair of the Decision, Risk, and Operations Division at Columbia Business School. His research interests include revenue management, consumer behavior modeling, operations management, and stochastic optimization.Gustavo Vulcano (“Estimating Primary Demand for Substitutable Products from Sales Transaction Data”) is an associate professor at the Leonard N. Stern School of Business at New York University. His research interests are primarily in revenue management, including pricing mechanisms and capacity control. This paper is part of his current research on customer choice and strategic consumer behavior.Yongqiang Wang (“A New Stochastic Derivative Estimator for Discontinuous Payoff Functions with Application to Financial Derivatives”) is a research associate in the Department of Electrical and Computer Engineering and the Institute for Systems Research at the University of Maryland. He received the 2010 INFORMS Computing Society Student Paper Award and the 2010 Winter Simulation Best Student Paper Award. His research interests lie in the areas of simulation optimization, Markov decision process, and stochastic control, with applications toward supply chain management and financial engineering.Yehua Wei (“Belief Propagation for Min-Cost Network Flow: Convergence and Correctness”) is a Ph.D student in the Operations Research Center at the Massachusetts Institute of Technology. His research interests include the design of process flexibility and optimization of supply chains. He has also worked in the area of distributed algorithms, including belief propagation and divide and conquer algorithms.Gideon Weiss (“Exact FCFS Matching Rates for Two Infinite Multitype Sequences”) is a professor of statistics and operations research in the Department of Statistics at the University of Haifa, Israel. His current research focuses on scheduling and control of processing networks, with applications to manufacturing, communications, and service systems. In particular, he is studying fluid approximations to queueing networks, and simplex algorithms for continuous, infinite dimensional linear programs.Xiao Qi Yang (“Piecewise Linear Multicriteria Programs: The Continuous Case and Its Discontinuous Generalization”) is a professor in the Department of Applied Mathematics at the Hong Kong Polytechnic University. His research interests are in the areas of variational analysis, multicriteria optimization, and financial optimization.Peiwen Yu (“On the Quasiconcavity of Lost-Sales Inventory Models with Fixed Costs”) is a Ph.D. candidate in the School of Business and Management at the Hong Kong University of Science and Technology. His main research interests include inventory management and optimization. He received a B.S. degree in mathematics from the University of Science and Technology of China.Michael Zabarankin (“Stochastic Optimization of Sensor Placement for Diver Detection”) is an associate professor in the Department of Mathematical Sciences at Stevens Institute of Technology.Bo Zhang (“Staffing Call Centers with Impatient Customers: Refinements to Many-Server Asymptotics”) received his Ph.D. degree from Georgia Institute of Technology in 2011 and is a research staff member in the Business Analytics and Mathematical Sciences Department at the IBM T. J. Watson Research Center. He is broadly interested in decision making under uncertainty in various application domains, with an emphasis on stochastic modeling, analysis, and optimization. His research has won the first place in the 2010 INFORMS Nicholson Student Paper Competition and the Best Student Paper award at the 28th International Symposium on Computer Performance, Modeling, Measurements, and Evaluation.Yue Zhang (“A Simulation Optimization Approach to Long-Term Care Capacity Planning”) is an assistant professor of operations management in the College of Business and Innovation at the University of Toledo. His research interests include service and healthcare operations, location analysis and network design, logistics and transportation, and simulation optimization. The paper in this issue is part of his postdoctoral research at the Sauder School of Business, University of British Columbia. Bert Zwart (“Staffing Call Centers with Impatient Customers: Refinements to Many-Server Asymptotics”) is with the Center of Mathematics and Computer Science in Amsterdam, where he leads the Probability and Stochastic Networks Group. He is a professor at VU University Amsterdam. His honors include an IBM Faculty Award, the Erlang Prize, and VENI and VIDI awards from the Dutch Science Foundation.",,,,2012,501–504,10.1287/opre.1120.1076,https://doi-org.proxy.bnl.lu/10.1287/opre.1120.1076;http://dx.doi.org/10.1287/opre.1120.1076,Journal Article
A Review of Geospatial Information Technology for Natural Disaster Management in Developing Countries,"Disasters are deadly and destructive events, particularly in developing countries where economic, social, political and cultural factors increase natural hazard vulnerability. The recent devastation of the Haiti earthquake on January 12th, 2010 was a prime example of the human toll a natural disaster can take in developing regions of the world. There is an imminent need to improve natural disaster management capacity in developing countries to reduce disaster impacts. Given that disasters are spatial phenomenon, the application of geospatial information technology GIT is essential to the natural disaster management process. However, in developing countries there are numerous barriers to the effective use of GIT, especially at the local level, including limited financial and human resources and a lack of critical spatial data required to support GIT use to improve disaster management related decision making processes. The results of a thorough literature review suggests that currently available free and open source GIT FOS GIT offers great potential to overcome some of these barriers. Thus, disaster management practitioners in developing countries could harness this potential in an attempt to reduce hazard vulnerability and improve disaster management capacity. The use of FOS GIT significantly reduces software costs and can help build local level GIT knowledge/technical skills that are required for successful GIT implementation.",,"Herold S,Sawada MC",,2012,24–62,10.4018/jagr.2012040103,https://doi-org.proxy.bnl.lu/10.4018/jagr.2012040103;http://dx.doi.org/10.4018/jagr.2012040103,Journal Article
Europeanisation Through Crossloading : Polish-German Relations on Eastern Policy,"This PhD examines the Europeanisation of Polish-German relations regarding Eastern policy. It contributes to existing studies of Polish and German Eastern policies by explaining whether, how and why the EU has shaped their bilateral impacts. The thesis re-examines the well-known Europeanisation models of up- and downloading and develops the under-researched model of crossloading by putting forward a rational institutionalist explanation. Applying a process tracing method, the thesis argues that Polish and German foreign policy makers sought to maximise the strategic and legitimising usage of the EU to attain their own strategic Eastern policy interests. The thesis identifies strategic socialisation and experiential learning within the cooperative CFSP framework as two key causal mechanisms explaining bilateral compromises between these two countries, resulting in bilateral policy transfers on their Eastern policies. It also shows that Germany and Poland sought to achieve a politics of scale through recognising each other's influence within the EU. The thesis explains that their interests in common EU Eastern policies enabled Germany to influence Poland towards a more cooperative Russia policy. Meanwhile, Poland convinced Germany of the Eastern Partnership in return for Polish concessions relating to an EU enlargement policy towards Eastern Europe. The thesis also discusses the mutual Polish-German influences in the contexts of the Association Agreement with Ukraine, as well as Poland's role in Germany's recent criticism of Russian aggressive foreign policy. Finally, the thesis explains their inability to overcome bilateral Eastern policy divergences in the pre- and post-enlargement periods, as well as Poland's constrained influence over Germany in the recent negotiations on the Ukraine-Russia crisis. The thesis concludes that while the Europeanisation processes have led to Polish-German bilateral impacts, their lacking uploading interests and their weaker orientation towards compromise on EU policies have limited their potential for such impacts.",,Czulno P,,2019,,,,Ph.D. Thesis
Picture Fuzzy Extension of the CODAS Method for Multi-Criteria Vehicle Shredding Facility Location,,,"Simic V,Karagoz S,Deveci M,Aydin N",,2021,,10.1016/j.eswa.2021.114644,https://doi-org.proxy.bnl.lu/10.1016/j.eswa.2021.114644;http://dx.doi.org/10.1016/j.eswa.2021.114644,Journal Article
Important Role of the Hall Effect Measurement System in a Modified Course of Materials in Electrical Engineering,"The course ldquoMaterials in Electrical Engineeringrdquo is a core course in the Mechatronics curriculum at the Faculty of Technical Sciences, University of Novi Sad, Serbia. In the past, this course was comprehensive and mainly theory-based. Teaching methods used in this course had not been changed for many years, and were mainly based on a traditional approach. They were therefore often outdated, and boring for students. In addition, the lack of modern materials characterization equipment was a significant weakness of the course in its early stages. This paper presents the main aspects of the modified course, which features a greater inclusion of modern equipment in its teaching methodology; in particular it introduces the Hall effect measurement system as an indispensable characterization technique in education, research and in the semiconductor industry. This paper also describes how students can be taught to use the Hall effect measurement system to determine the structural and electrical characteristics of different materials. Finally, students' feedback and observations on the modified course and the applied teaching methodology are discussed.",,"Stojanovic G,Savic S,Zivanov L",,2009,297–304,10.1109/TE.2008.928206,https://doi-org.proxy.bnl.lu/10.1109/TE.2008.928206;http://dx.doi.org/10.1109/TE.2008.928206,Journal Article
Noisy SMS Machine Translation in Low-Density Languages,"This paper presents the system we developed for the 2011 WMT Haitian Creole--English SMS featured translation task. Applying standard statistical machine translation methods to noisy real-world SMS data in a low-density language setting such as Haitian Creole poses a unique set of challenges, which we attempt to address in this work. Along with techniques to better exploit the limited available training data, we explore the benefits of several methods for alleviating the additional noise inherent in the SMS and transforming it to better suite the assumptions of our hierarchical phrase-based model system. We show that these methods lead to significant improvements in BLEU score over the baseline.",,"Eidelman V,Hollingshead K,Resnik P",,2011,344–350,,,Conference Paper
Control of Multi-Agent Networks: From Network Design to Decentralized Coordination,"This dissertation presents a suite of design tools for multi-agent systems that address three main areas: network design, decentralized controller generation, and the synthesis of decentralized control strategies by combining individual decentralized controllers. First, a new metric for quantifying heterogeneity in multi-agent systems is presented based on combining different notions of entropy, and is shown to overcome the drawbacks associated with existing diversity metrics in various scientific fields. Moreover, a new method of controlling multi-agent networks through the single-leader network paradigm is presented where by directly exploiting the homogeneity of agent capabilities, a network which is not completely controllable can be driven closer to a desired target configuration than by using traditional control techniques. An algorithm is presented for generating decentralized control laws that allow for agents to best satisfy a desired global objective, while taking into account network topological constraints and limitations on how agents can compute their control signals. Then, a scripting tool is developed to aid in specifying sequences of decentralized controllers to be executed consecutively, while helping ensure that the required network topological requirements needed for each controller to execute properly are maintained throughout mode switches. Finally, the underlying concepts behind the developed tools are showcased in three example applications: distributed merging and spacing for heterogeneous aircraft during terminal approaches, collaborative multi-UAV convoy protection in dynamic environments, and an educational tool used to teach a graduate-level networked controls course at the Georgia Institute of Technology.",,Twu PY,,2012,,,,Ph.D. Thesis
A Framework to Study the Emergence of Non-Communicable Diseases,"Objective: To design a framework for creating computational models that support the understanding of emergent mechanisms for non-communicable diseases.Scope: The national and global burden of Non-Communicable Diseases (NCDs) represent a major public health challenge that undermines the social and economic development in countries with limited resources. For instance, Cardiovascular Diseases (CVDs) accounted for four out of five deaths in Jamaica and it was the most significant contributor to non-communicable diseases. CVDs are caused by a cluster of comorbidities and lifestyle behaviors that interact to promote a vascular risk. However, their combined effect on CVDs have not been established because of interdependency, nonlinearity and feedback loops. In this research, we developed a computational model based on the Overview, Design concepts, and Details (OOD) protocol to understand the emergence of CVDs in the population. The pattern of this behavior is determined by the interaction of causal relationships among the risk factors. This work has inspired a framework that can be used to model the emergence of non-communicable diseases as complex systems.Results: Understanding the behavior of the Jamaican system provided insights to the creation of a novel framework for NCD emergent patterns. The framework could potentially reduce development cost and time of NCD management systems, since it supports an iterative modeling paradigm. The framework is dynamic enough to be applied in different populations, health states, and various NCDs.",,"Simpson O,Camorlinga SG",,2017,116–125,10.1016/j.procs.2017.09.026,https://doi-org.proxy.bnl.lu/10.1016/j.procs.2017.09.026;http://dx.doi.org/10.1016/j.procs.2017.09.026,Journal Article
ITCS '13: Proceedings of the 4th Conference on Innovations in Theoretical Computer Science,"The papers in this volume were presented at the 4th Innovations in Theoretical Computer Science (ITCS 2013) conference, sponsored by the ACM Special Interest Group on Algorithms and Computation Theory (SIGACT). The conference was held in Berkeley, California, USA, January 9--12, 2013.The program committee consisted of Mark Braverman (Princeton University), Shuchi Chawla (University of Wisconsin), Julia Chuzhoy (TTI-Chicago), Yevgeniy Dodis (New York University), Andrew Drucker (MIT and IAS), Uriel Feige (Weizmann Institute), Ravi Kannan (Microsoft Research India), Robert Kleinberg (Chair, Cornell University), Eyal Kushilevitz (Technion), S. Muthukrishnan (Rutgers University), Ronitt Rubinfeld (MIT and Tel Aviv University), Atri Rudra (University at Buffalo), Adam Smith (Pennsylvania State University), Santosh Vempala (Georgia Tech), and Andrew Yao (Tsinghua University).The call for papers attracted 123 submissions worldwide. The program committee accepted 49 papers that cover a wide range of topics in theoretical computer science, including algorithms, complexity, cryptography, learning, data privacy, quantum computing, and relations between computing and the biological and social sciences.The program committee selected two papers for the Best Student Paper award: ""Properties and Applications of Boolean Function Composition"" by Avishay Tal, and ""Time Hierarchies for Sampling Distributions"" by Thomas Watson.",,,,2013,,,,Book
ASE '07: Proceedings of the 22nd IEEE/ACM International Conference on Automated Software Engineering,"Welcome to Atlanta and the 22nd IEEE/ACM International Conference on Automated Software Engineering, the major annual forum on the theory and practice of automating all aspects of the software-development process. Software engineering is concerned with the analysis, design, implementation, testing, and maintenance of large software systems. Automated software engineering focuses on how to automate or partially automate these tasks to achieve significant improvements in quality and productivity. Formerly known as Knowledge-Based Software Engineering (KBSE), this meeting marks the 10th Anniversary of the conference under its current nameThis year's program comprises three days of technical paper presentations, a poster session, a series of mini-tutorials presented in parallel with the technical sessions, two full-day and two half-day tutorials, tool demonstrations, a doctoral symposium, and keynote addresses by three prominent members of the community. In addition, five workshops were selected to run prior to the opening of the conference. The evaluation of technical papers was rigorous and highly selective. Each paper was reviewed by at least three reviewers drawn from our international program committee and a pool of expert reviewers. During a two-day meeting, the committee then selected 37 papers from a pool of 312 submissions. In addition, the committee selected 38 submissions for inclusion as short papers, which will be presented in two poster sessions at the conference. We thank all of the members of the program committee and expert review panel for their hard work in putting together such a strong programThe meeting, especially the social activities, would not have been possible without generous support from three industrial sponsors - LogicBlox (platinum), Google (silver), and Aflac Insurance (bronze). LogicBlox is an Atlanta-based company that develops high-performance database infrastructure used to power decision-support and financial-planning applications for large retailers worldwide. The company exploits ASE technologies and invests heavily in ASE research. Google, based in Mountain View California, also invests heavily in ASE research and technology. Aflac is a provider of supplemental insurance and is based in Columbus, Georgia. The company invests in IT research and development and is sponsoring the Doctoral Symposium",,,,2007,,,,Book
An Evolved VIKOR Method for Multiple-Criteria Compromise Ranking Modeling under T-Spherical Fuzzy Uncertainty,,,Chen TY,,2022,,10.1016/j.aei.2022.101802,https://doi-org.proxy.bnl.lu/10.1016/j.aei.2022.101802;http://dx.doi.org/10.1016/j.aei.2022.101802,Journal Article
"Urben Research in Ethnic, Demographic and Household-Economic Structures with Small Area, Micro-Databases (Abstract Only)","Computerized U.S. Census data has been most widely used for (1) employment, fertility, demographic and stratification research involving Public Use Sample (PUS) microdata on the national level, and (2) applied research (for planning, administration, marketing, and other applications) with summary (aggregated) data for localized (i.e., block, tract, community, etc.) geographic units. A third, highly productive avenue of research, involving Census PUS micro-data for localized urban units (i.e., SMSAs, counties and especially selected large-city neighborhoods), has not received the attention it merits, either among sophisticated public data users or among novice users.Three forms of current or future small area, Census microdata constitute resources for urban research. First, conventional 1970 Census PUS data sets are available for counties and/or SMSAs (with minimum populations of 250,000). Second, special tabulations for the two largest U.S. cities permit analysis of 1970 household and person records group by (sub-county) urban neighborhoods (27 in New York City; 12 in Chicago). Third, 1980 Census microdata, by allowing identification of geographic areas of smaller population size (100,000 population), will vastly expand the applications of localized research with the conventional PUS or special tabulations. In addition, the 1980 PUS microdata will, for the first time, allow comparative time-series analyses of county (or SMSA) area populations, over the 1970-1980 decade.In contrast to national PUS microdata research, local level analyses have the advantages of (1) smaller data set size and processing costs, (2) more immediate integration of computerized research hypotheses with additional sources of (qualitative) information and questions (stemming from direct knowledge of the communities studied), and (3) increased ability to zero in on specialized ethnic, occupational-industrial, migrant, age, etc. urban population groups which are disproportionately represented in particular local environments. Our own research projects (at various stages of development) which attempt to exploit these advantages include computerized analysis of:1. Patterns of household composition, and source and structure of family income, among Upper East Side and Upper West Side Manhattan residents with family incomes of $ 50,000. or more (as reported in the 1970 Census)2. Employment patterns of married women of Cuban immigrant background, in relation to family class position and period of immigration, for Hudson County, New Jersey3. Contrasts in the occupational positions and household patterns of first-generation and second-generation husbands and wives of Italian background in a New York City working class community (Astoria-Long Island City, Queens)4. Wives' employment patterns in relation to ethnic background and husbands' occupations and income levels in a working class community located in a manufacturing center (South Side, Chicago)5. Change in the social and demographic characteristics of succeeding groups of migrants to an expanding ""sunbelt"" metropolitan area (Albuquerque, New Mexico)6. Contrasts in local housing markets and housing availability, involving analysis of the number and characteristics of vacant housing units for New Jersey counties.These, as well as other projects we have assisted, have been undertaken with varied software resources, including packages (such as CENTS-AID) with unique hierarchical file processing capabilities, as well as more versatile (non-hierarchical), general purpose packages (such as SPSS). The advantages and research applications of small area, micro-databases can be realized with a range of software techniques and user-formulated research strategies.",,"Benenson H,Just S",,1981,62,10.1145/800275.810943,https://doi-org.proxy.bnl.lu/10.1145/800275.810943;http://dx.doi.org/10.1145/800275.810943,Conference Paper
"Urben Research in Ethnic, Demographic and Household-Economic Structures with Small Area, Micro-Databases (Abstract Only)","Computerized U.S. Census data has been most widely used for (1) employment, fertility, demographic and stratification research involving Public Use Sample (PUS) microdata on the national level, and (2) applied research (for planning, administration, marketing, and other applications) with summary (aggregated) data for localized (i.e., block, tract, community, etc.) geographic units. A third, highly productive avenue of research, involving Census PUS micro-data for localized urban units (i.e., SMSAs, counties and especially selected large-city neighborhoods), has not received the attention it merits, either among sophisticated public data users or among novice users.Three forms of current or future small area, Census microdata constitute resources for urban research. First, conventional 1970 Census PUS data sets are available for counties and/or SMSAs (with minimum populations of 250,000). Second, special tabulations for the two largest U.S. cities permit analysis of 1970 household and person records group by (sub-county) urban neighborhoods (27 in New York City; 12 in Chicago). Third, 1980 Census microdata, by allowing identification of geographic areas of smaller population size (100,000 population), will vastly expand the applications of localized research with the conventional PUS or special tabulations. In addition, the 1980 PUS microdata will, for the first time, allow comparative time-series analyses of county (or SMSA) area populations, over the 1970-1980 decade.In contrast to national PUS microdata research, local level analyses have the advantages of (1) smaller data set size and processing costs, (2) more immediate integration of computerized research hypotheses with additional sources of (qualitative) information and questions (stemming from direct knowledge of the communities studied), and (3) increased ability to zero in on specialized ethnic, occupational-industrial, migrant, age, etc. urban population groups which are disproportionately represented in particular local environments. Our own research projects (at various stages of development) which attempt to exploit these advantages include computerized analysis of:1. Patterns of household composition, and source and structure of family income, among Upper East Side and Upper West Side Manhattan residents with family incomes of $ 50,000. or more (as reported in the 1970 Census)2. Employment patterns of married women of Cuban immigrant background, in relation to family class position and period of immigration, for Hudson County, New Jersey3. Contrasts in the occupational positions and household patterns of first-generation and second-generation husbands and wives of Italian background in a New York City working class community (Astoria-Long Island City, Queens)4. Wives' employment patterns in relation to ethnic background and husbands' occupations and income levels in a working class community located in a manufacturing center (South Side, Chicago)5. Change in the social and demographic characteristics of succeeding groups of migrants to an expanding ""sunbelt"" metropolitan area (Albuquerque, New Mexico)6. Contrasts in local housing markets and housing availability, involving analysis of the number and characteristics of vacant housing units for New Jersey counties.These, as well as other projects we have assisted, have been undertaken with varied software resources, including packages (such as CENTS-AID) with unique hierarchical file processing capabilities, as well as more versatile (non-hierarchical), general purpose packages (such as SPSS). The advantages and research applications of small area, micro-databases can be realized with a range of software techniques and user-formulated research strategies.",,"Benenson H,Just S",,1981,62,10.1145/1015528.810943,https://doi-org.proxy.bnl.lu/10.1145/1015528.810943;http://dx.doi.org/10.1145/1015528.810943,Journal Article
Equivalences Among Polarity Algorithms,"The concept of polarity is pervasive in natural language. It relates syntax, semantics and pragmatics narrowly (Giannakidou, in: Maienborn, von Heusinger, Portner (eds.), Semantics: an international handbook of natural language meaning, De Gruyter Mouton, Berlin, 2011; Israel in The grammar of polarity: pragmatics, sensitivity, and the logic of scales, Cambridge studies in linguistics, Cambridge University Press, Cambridge, 2014), it refers to items of many syntactic categories such as nouns, verbs and adverbs. Neutral polarity items appear in affirmative and negative sentences, negative polarity items cannot appear in affirmative sentences, and positive polarity items cannot appear in negative sentences. A way of reasoning in Natural Language is through Natural Logic (van Benthem in Essays in logical semantics, vol. 29 of Studies in linguistics and philosophy, Reidel, Dordrecht, 1986; Language in action: categories, lambdas, and dynamic logic, vol. 130 of Studies in logic, Elsevier, Amsterdam, 1991). This logic is based on the concept of polarity in order to make the meaning of a sentence weaker o stronger without changing its truth value. There exist many proposals to compute polarity in the Natural Logic context, the most widely known are the ones by: van Benthem (1986, 1991), Sánchez-Valencia (Studies on natural logic and categorial grammar, Ph.D. thesis, Universiteit van Amsterdam, 1991), Dowty (Proceedings of the 4th conference on semantics and theoretical linguistics, Cornel University, CLC Publications, Rochester, 1994), and van Eijck (in: ten Cate, Zeevat (eds.), 6th international Tbilisi symposium on logic, language, and computation, Batumi, Georgia, Springer, 2007). If Natural Logic is going to be used, as an inferential mechanism between text fragments, in Natural Language Processing applications such as text summarization, question answering, and information extraction, it is a priority to know what the existing relationship among the aforementioned algorithms is; for example, to implement the most general. We show in this paper the equivalence among the analyzed algorithms, filling a gap in Natural Logic research, particularly in computing polarity, and the soundness of their algorithms.",,"Lavalle-Martínez JJ,Montes-Y-Gómez M,Villaseñor-Pineda L,Jiménez-Salazar H,Bárcenas-Patiño IE",,2018,371–395,10.1007/s11225-017-9743-y,https://doi-org.proxy.bnl.lu/10.1007/s11225-017-9743-y;http://dx.doi.org/10.1007/s11225-017-9743-y,Journal Article
Effects of Adaptive Discretization on Numerical Computation Using Meshless Method with Live-Object Handling Applications,"Numerical methods that are more reliable, general and stable have become increasingly popular in industry. As the most widely applied engineering computational method, the Finite Element Method (FEM) has difficulty solving certain problems where its mesh has to be modified during the computation. In this research, we focus on a new computational method called the Meshless Method (MLM). This method is built upon the same theoretical framework as FEM but needs no mesh. Consequently, the computation becomes more stable and the adaptive computational scheme becomes easier to develop. The major issue associated with MLM is its lower computational efficiency compared with FEM. Adaptive computations can help reduce the number of nodes used in computation and improve the efficiency. For this reason, this research investigates practical issues related to the MLM and develops an adaptive algorithm to automatically insert additional nodes and improve computational accuracy. The study has been in the context of the two engineering problems: magnetic field computation and large deformation contact. First, we investigate the effect of two discretization methods (strong-form and weak-form) in MLM for solving linear magnetic field problems. Special techniques for handling the discontinuity boundary condition at material interfaces are proposed in both discretization methods to improve the computational accuracy. Next, we develop an adaptive computational scheme in MLM that is comprised of an error estimation algorithm, a nodal insertion scheme and a numerical integration scheme. As a more general approach, this method can automatically locate the large error region around the material interface and insert nodes accordingly to reduce the error. We further extend the adaptive method to solve nonlinear large deformation contact problems. Contact problems are time-consuming to solve since they are highly nonlinear problems and often need a lot of iterations to converge. With the ability to adaptively insert nodes during the computation, the developed method is capable of using fewer nodes for initial computation and thus, effectively improves the computational efficiency. Engineering applications of the developed methods have been demonstrated by two practical engineering problems encountered in the development of the live object transfer project at Georgia Tech. In the first problem, the MLM has been utilized to simulate the dynamic response of a non-contact mechanical-magnetic actuator for optimizing the design of the actuator. In the second problem, the contact between the flexible finger and the live poultry product has been analyzed by using MLM. These applications show the developed method can be applied to a broad spectrum of engineering applications where an adaptive mesh is needed.",,Li Q,,2007,,,,Ph.D. Thesis
Matching Ukrainian Wikipedia Red Links with English Wikipedia’s Articles,"This work tackles the problem of matching Wikipedia red links with existing articles. Links in Wikipedia pages are considered red when lead to nonexistent articles. In other Wikipedia editions could exist articles that correspond to such red links. In our work, we propose a way to match red links in one Wikipedia edition to existent pages in another edition. We define the task as a Named Entity Linking problem because red link titles are mostly named entities. We solve it in a context of Ukrainian red links and English existing pages. We created a dataset of 3171 most frequent Ukrainian red links and a dataset of almost 3 million pairs of red links and the most probable candidates for the correspondent pages in English Wikipedia. This dataset is publicly released1. In this work we define conceptual characteristics of the data — word and graph properties — based on its analysis and exploit these properties in entity resolution. BabelNet knowledge base was applied to this task and was regarded as a baseline for our approach (F1 score = 32 %). To improve the result we introduced several similarity metrics based on mentioned red links characteristics. Combined in a linear model they resulted in F1 score = 85 %. To the best of our knowledge, we are the first to state the problem and propose a solution for red links in Ukrainian Wikipedia edition.",,"Liubonko K,Sáez-Trumper D",,2020,819–826,10.1145/3366424.3383571,https://doi-org.proxy.bnl.lu/10.1145/3366424.3383571;http://dx.doi.org/10.1145/3366424.3383571,Conference Paper
"La Jungla: Globalization, Transnational Migrant Labor and the Meatpacking Industry","This dissertation analyzes the transformations that the meat processing industry in the U.S., underwent as it adapted to the era of globalization, such as restructuring, relocation and de-unionization, and ethnic labor force displacement. I show how jobs in this industry went from being among the most exploitative and underpaid in manufacturing during the days of The Jungle (1906) to some of the highest paying and more desirable during the heyday of Fordist-Keynesian compromise. These transformations included, significantly, demographic transitions in the make-up of the industry's labor force, going from predominantly migrant during at the beginning of the 20th century to mostly U.S. born, towards the middle of the century and back to immigrant once again by the end it. My research centers on meatpacking plants in North Carolina.To theoretically situate this research, I analyze the category of migrant labor and explore what it is about this category that renders migrants, of various legal statuses, vulnerable to super-exploitation. As a way to explore this constructed vulnerability, I apply a Marxist analysis to explore the historical creation of borders within capitalism through a discussion of the enclosure of the commons and the uprooting of peasants from the land. I call for the need to interrogate the concept of illegality as it pertains to people who transgress political borders, suggesting instead an analysis of the process of illegalization that renders those people's presence ""illegal."" Rather than understanding illegality and citizenship as two discrete categories, I offer the concept of the ""illegality continuum"" which sees them as opposite poles between which lay various immigration statuses with varying degrees of built-in precarity. I analyze the meatpacking industry's need to procure alternative sources of labor in the aftermath of the mid-2000s ICE raids which substantially compromised access to undocumented workers to the industry. These efforts led to the further incorporation of migrants with legal-but-temporary statuses, such as refugees and Temporary Protected Status recipients. Concretely, I provide a case study of the incorporation of Haitian TPS recipients into the meat processing plants of eastern North Carolina, showing how their situation still proves to be quite susceptible to super-exploitation despite their ""protected"" status. I then analyze how workers with these types of legal-but-temporary statuses are a potential replacement for unauthorized Latino workers. I discuss how this trend of incorporating migrants with these statuses is leading to the formation of a new regime of labor in the meat processing industry.",,"Rangel SL,Lipsitz G,De Genova N,Telles E,Bhavnani KK",,2020,,,,Ph.D. Thesis
Deterministic Multikernel Extreme Learning Machine with Fuzzy Feature Extraction for Pattern Classification,"In this paper a novel multikernel deterministic extreme learning machine (ELM) and its variants are developed for classification of non-linear problems. Over a decade ELM is proved to be efficacious learning algorithms, but due to the non-deterministic and single kernel dependent feature mapping proprietary, it cannot be efficiently applied to real time classification problems that require invariant output solution. We address this problem by analytically calculation of input and hidden layer parameters for achieving the deterministic solution and exploiting the data fusion proficiency of multiple kernel learning. This investigation originates a novel deterministic ELM with single layer architecture in which kernel function is aggregation of linear combination of disparate base kernels. The weight of kernels depends upon perspicacity of problem and is empirically calculated. To further enhance the performance we utilize the capabilities of fuzzy set to find the pixel-wise coalition of face images with different classes. This handles the uncertainty involved in face recognition under varying environment condition. The pixel-wise membership value extracts the unseen information from images up to significant extent. The validity of the proposed approach is tested extensively on diverse set of face databases: databases with and without illumination variations and discrete types of kernels. The proposed algorithms achieve 100% recognition rate for Yale database, when seven and eight images per identity are considered for training. Also, the superior recognition rate is achieved for AT & T, Georgia Tech and AR databases, when compared with contemporary methods that prove the efficacy of proposed approaches in uncontrolled conditions significantly.",,"Ahuja B,Vishwakarma VP",,2021,32423–32447,10.1007/s11042-021-11097-3,https://doi-org.proxy.bnl.lu/10.1007/s11042-021-11097-3;http://dx.doi.org/10.1007/s11042-021-11097-3,Journal Article
Risk Quantification of Metabolic Syndrome with Quantum Particle Swarm Optimisation,"Metabolic syndrome (MetS) is a combination of interrelated risk factors associated with an increased risk of developing type II diabetes Mellitus (T2DM), stroke and cardiovascular diseases (CVD). The economic, social and medical burden coupled with increased morbidity of the aforementioned diseases makes their prevention an active research area. Currently, the traditional method of MetS diagnosis is based on dichotomised definitions provided by various expert health organisations. However, this method is laced with the indetermination of MetS in individuals with borderline risk factor values due to a binary diagnosis and the assumption of equal weighting for all risk factors during diagnosis. The purpose of this paper is to examine the use of the MetS areal similarity degree risk analysis based on weighted radar charts comprising of diagnostic thresholds and risk factor results of an individual. We further enhance this risk quantification method by applying quantum particle swarm optimization to derive the weights. The proposed risk quantification was carried out using a sample of 528 individuals from an examination survey conducted between 2007 and 2014 in Serbia. The results are evaluated with the traditional dichotomised method of MetS diagnosis, in this case the joint interim statement (JIS). The results obtained showed that the proposed risk quantification method outperformed the dichotomised method at diagnosing MetS even in individuals who present risk factor examination values at the threshold borderlines.",,"Kakudi HA,Loo CK,Pasupa K",,2017,1141–1147,10.1145/3041021.3054935,https://doi-org.proxy.bnl.lu/10.1145/3041021.3054935;http://dx.doi.org/10.1145/3041021.3054935,Conference Paper
Eliciting Local Spatial Knowledge for Community-Based Disaster Risk Management: Working with Cybertracker in Georgian Caucasus,"CyberTracker CT participatory field data collection software is used as an element of Participatory GIS for acquiring, geo-referencing, storing and transferring local spatial knowledge. It has been developed initially for animal tracking, ecological surveys and conservation management activities, but has extended into the social environment for health and welfare surveys, and it is being applied to social data collection about hazards, vulnerability and coping mechanisms in disaster risk management. This article provides a critical guide of CyberTracker under field conditions with representative participation. The practical experiences informing this critical review of field operations come from employing CyberTracker with staff of NGOs and local government agencies in a workshop in two hazard-prone communities in the Caucasus Mountains of Georgia.",,"Spanu V,McCall MK",,2013,45–59,,,Journal Article
The Geek Atlas: 128 Places Where Science and Technology Come Alive,"The Geek Atlas is a list of sites to visit where science, mathematics, or technology happened or is happening. The book can be used as a true travel guide or as inspiration for the armchair traveler. Each place has its own chapter that includes a general introduction to the place's significance, a related technical subject covered in more detail, and practical visiting information. From Kiev to Jaipur with The Geek Atlas in hand This is the Captain speaking. Welcome aboard flight NB1729, the Nerd Bird, stopping in Kiev, Munich, Paris, London, Dublin, New York, San Francisco and Jaipur. Seat belts fastened please: were about to apply Newtons laws of motion and take off. Pripyat First stop is Kiev, Ukraine and its straight from the airport to the National Museum of Chernobyl that explains the events of April 26, 1986 when reactor number 4 of the Chernobyl nuclear power station blew open and released a cloud of radioactivity that covered Europe. The following morning your tour bus leaves Kiev and makes the drive out to the Chernobyl Exclusion Zone. Inside the zone you see the entombed reactor and the abandoned town of Pripyat, which is forever stuck in the mid-1980s. During the trip youve got plenty of time to read The Geek Atlas explanation of the dangers of radioactive iodine and its effect on the thyroid gland. Next, its back aboard the plane for the ride down to the gleaming airport in Munich, Germany. From there its a short train ride to the Deutsches Museum--probably the greatest science museum in the world. Youll be staying all day in the museum because of its sheer size (there are 28,000 objects on display) and the highlight will be the Electric Power demonstration where 300 kV of AC are generated and then an 800 kV lightning strike is set off. On the train ride into Munich theres time to read The Geek Atlas explanation of the operation of the Diesel engine and find out what a planimeter is. Paris is up next. Your walking tour of the City of Lights starts at the Paris Observatory at the feet of Franois Arago, director of the observatory in the 19th century. You are looking for a small brass disk set into the sidewalk. Written on the disk is the word ARAGO and the letters N and S. You follow the northerly direction towards the observatory staying on the old Paris meridian (the French 0 degrees of longitude). Along the way youll search for more of these Arago medallions marking the meridian and end up seeing the sights of Paris. The meridian passes through the city center and without straying far youll see The Pantheon (with Foucaults Pendulum inside), the Jardin de Luxembourg, the Eiffel Tower and le Muse du Louvre. The Brunel Museum Stop for a coffee near the river Seine halfway through the trip and read The Geek Atlas description of how to find your local meridian at home using a stick and some string. The next day, you leave the airplane behind and hurtle under the English Channel on a train to arrive in London in just over two hours. In London your tour avoids the major tourist attractions and takes you by underground train to The Brunel Museum. You arrive by passing through the first tunnel built under a body of water. If you are lucky you can take the museum tour back through the floodlit tunnel in an underground train that creeps through at walking pace. While in London the tour stops for lunch at Bunhill Fields Cemetery, a quiet spot in the City of London, where you can hunt down the grave of Reverend, and pioneer of probability theory, Thomas Bayes. The Geek Atlas contains a probability brainteaser to ponder while thinking about the famous Bayes Theorem (which is explained). Before leaving Europe the airplane makes a stop in Dublin for a bit more mathematics. Crossing Broom Bridge across the Royal Canal you come to a plaque on the bridge itself. This is the spot where Sir William Rowan Hamilton, out on a walk with his wife in 1843, scratched the fundamental equation of the theory of quaternions into the stonework using a knife. The equation had just come to him and he needed to write it down. Opening The Geek Atlas to page 91, youll find a description of the quaternions and the complex numbers. Deep Space Communications Complex After the long flight to New Yorks JFK and a bumpy cab ride into the city you avoid the crowds around Times Square and head straight for the General Society of Mechanics and Tradesmen of New York City. Inside is the small and wonderful John M. Mossman collection of locks. Since New York is an important banking center locks are very important and the collection is filled with beautiful examples of complex, mechanical time locks used to secure vaults. Many of the locks were built by the Yale Company, and The Geek Atlas explains how the familiar home tumbler (or Yale) lock works. Flying over the US towards California theres plenty of time to read up on the The Geek Atlas highlights of Silicon Valley, but after leaving San Francisco airport your tour heads south and out towards Fort Irwin, CA where NASA has the headquarters of the Deep Space Communications Complex with its multiple parabolic dishes that point skyward and chat with man-made probes that are exploring the solar system. Some of the probes have been phoning home to Fort Irwin for over 30 years. Since its a long ride to Fort Irwin youll have time to get your head around The Geek Atlas section on error-detecting and correcting codes used to transmit information across the reaches of space (and ensure your credit card number is accurate). To complete the tour its a change of scene and continent: you leave high-tech California and dial back time to visit one of the oldest stone observatories in the world at the Jantar Mantar in Jaipur, India. In Jaipur youll be seeing the largest sundial in the world and a host of beautiful and massive instruments used for astronomical observations since the 18th century. This is the Captain speaking once again. Thank you for taking The Geek Atlas world tour. Your trip is free if you can tell the chief flight attendant the significance of our flight number while deplaning.",,Graham-Cumming J,,2009,,,,Book
A Comparative Study of Population-Based Optimization Algorithms for Downstream River Flow Forecasting by a Hybrid Neural Network Model,"Population-based optimization algorithms have been successfully applied to hydrological forecasting recently owing to their powerful ability of global optimization. This paper investigates three algorithms, i.e. differential evolution (DE), artificial bee colony (ABC) and ant colony optimization (ACO), to determine the optimal one for forecasting downstream river flow. A hybrid neural network (HNN) model, which incorporates fuzzy pattern-recognition and a continuity equation into the artificial neural network, is proposed to forecast downstream river flow based on upstream river flows and areal precipitation. The optimization algorithm is employed to determine the premise parameters of the HNN model. Daily data from the Altamaha River basin of Georgia is applied in the forecasting analysis. Discussions on the forecasting performances, convergence speed and stability of various algorithms are presented. For completeness' sake, particle swarm optimization (PSO) is included as a benchmark case for the comparison of forecasting performances. Results show that the DE algorithm attains the best performance in generalization and forecasting. The forecasting accuracy of the DE algorithm is comparable to that of the PSO, and yet presents weak superiority over the ABC and ACO. The Diebold-Mariano (DM) test indicates that each pair of algorithms has no difference under the null hypothesis of equal forecasting accuracy. The DE and ACO algorithms are both favorable for searching parameters of the HNN model, including the recession coefficient and initial storage. Further analysis reveals the drawback of slow convergence and time-consumption of the ABC algorithm. The three algorithms present stability and reliability with respect to their control parameters on the whole. It can be concluded that the DE and ACO algorithms are considerably more adaptive in optimizing the forecasting problem for the HNN model. Comparison of performances of population-based optimization algorithms in forecasting downstream river flow.Differential evolution (DE), artificial bee colony and ant colony optimization (ACO).Particle swarm optimization is included as a benchmark comparison for forecasting performances.Hybrid neural network (HNN) model incorporating fuzzy pattern-recognition and continuity equation.DE and ACO algorithms are considerably more adaptive in optimizing the forecasting problem for the HNN model.",,"Chen XY,Chau KW,Busari AO",,2015,258–268,10.1016/j.engappai.2015.09.010,https://doi-org.proxy.bnl.lu/10.1016/j.engappai.2015.09.010;http://dx.doi.org/10.1016/j.engappai.2015.09.010,Journal Article
Focus on Authors,"Sreekumar R. Bhaskaran (“Consumer Mental Accounts and Implications to Selling Base Products and Add-ons”) is an assistant professor of operations management at the Cox School of Business, Southern Methodist University. He has a B.E. in mechanical engineering from the Indian Institute of Technology Madras, an MBA in operations and marketing from the Indian Institute of Management Calcutta, and a Ph.D. in supply chain and operations management from the McCombs School of Business, University of Texas at Austin. His primary research interests include new product development, supply chain management, and marketing and operation interfaces. His work has previously appeared in Management Science, Marketing Science, and Production and Operations Management.Dondeena Bradley (“Further Examining the Impact of the NLEA on Nutrition”) is the Vice President, Global R&D and Nutrition Ventures, at PepsiCo, where she is responsible for designing new solutions that target the special needs of consumers with diverse health and nutrition challenges. Prior to joining PepsiCo in 2007, she held numerous roles in the areas of strategy, nutrition, and health with Johnson & Johnson, Mars Inc., the Stepan Company, and the Campbell Soup Company. She received her Ph.D. in food science from The Ohio State University, her M.S. in nutrition from Purdue University, and her B.S. from Anderson University.Dipankar Chakravarti (“Bidding Behavior in Descending and Ascending Auctions”) is a professor of marketing at the Johns Hopkins Carey Business School, where he served as Vice Dean, Programs, and is also a professor emeritus at the University of Colorado, Boulder, where he was the Ortloff Professor of Business. He holds a Ph.D. in industrial administration from Carnegie Mellon University and has taught previously at the University of Arizona, Duke, and University of Florida. His current research examines marketing and consumer behavior issues in emerging economies, with a focus on the psychology of consumption in poverty and development. His research on consumer and managerial decision making in marketing contexts has been published in the field's leading scholarly journals and received several significant academic recognitions. Among his other contributions to the marketing field are two sons---one a practitioner and the other an academic; he also has three grandsons who he hopes will also publish in Marketing Science one day.Amar Cheema (“Bidding Behavior in Descending and Ascending Auctions”) is an associate professor of marketing at the McIntire School of Commerce, University of Virginia. He received his Ph.D. from the University of Colorado, Boulder. His research interests include auctions and online purchase behavior, pricing and promotion effects, behavioral decision theory, and word-of-mouth influences.Lesley Chiou (“How Does the Use of Trademarks by Third-Party Sellers Affect Online Search?”) is an associate professor of economics at Occidental College. She received her Ph.D. in economics from the Massachusetts Institute of Technology. She is interested in industrial organization and applied econometrics, and her research focuses on online advertising and competition in the retail sector.Martijn G. de Jong (“State-Dependence Effects in Surveys”) holds a Chair in Marketing Research at the Erasmus School of Economics, Erasmus University, and is a Tinbergen Research Fellow. He has a B.Sc. and M.Sc. in econometrics from Erasmus University and a Ph.D. in marketing from Tilburg University. He is mainly interested in consumer preference measurement; often his research is cross-cultural in nature, relying on large-scale data sets. He received several major research grants, including an NWO (Netherlands Organization for Scientific Research) innovation grant. His awards include the J. C. Ruigrok Prize (awarded once every four years to the most productive young scholar in the Economic Sciences in the Netherlands) and the Christiaan Huygens Science Award (presented by HRH Princess Máxima of the Netherlands; awarded once every five years to a young economist in the Netherlands).Sanjiv Erat (“Consumer Mental Accounts and Implications to Selling Base Products and Add-ons”) is an assistant professor of innovation, technology, and operations management at the Rady School of Management, University of California, San Diego. He has a B.E. in computer science from the Indian Institute of Technology Madras and a Ph.D. in operations management from the College of Management, Georgia Institute of Technology. His primary research interests include new product development, marketing and operation interfaces, and behavioral economics. His work has previously appeared in Management Science.Rosellina Ferraro (“Unintended Nutrition Consequences: Firm Responses to the Nutrition Labeling and Education Act”; “From Consumer Information Regulation to Nutrition Competition: A Response”) is an associate professor of marketing at the Robert H. Smith School of Business, University of Maryland. Her research focuses on consumer behavior---specifically, on the effects of social influence on choice and preference and the effects of external threats on consumption behavior. Her work has been published in the Journal of Consumer Research, Journal of Marketing, and Journal of Consumer Psychology. She serves on the editorial review board for the Journal of Consumer Research and was named a 2011 MSI Young Scholar.Joel Huber (“Unintended Nutrition Consequences: Firm Responses to the Nutrition Labeling and Education Act”; “From Consumer Information Regulation to Nutrition Competition: A Response”) is the Alan D. Schwartz Professor of Business Administration at the Fuqua School of Business at Duke University. He has a B.A. from Princeton University and an MBA and Ph.D. from the Wharton School of the University of Pennsylvania. His research centers on ways relatively minor changes in the competitive context can have a large impact on market choice and the impact of this context dependency on appropriate ways to measure value. Recent work has focused on valuation of environmental changes, insurance programs, and health systems. He has been an associate editor for the Journal of Consumer Research for 12 years and the editor of Journal of Marketing Research for 3 years.Sanjay Jain (“Self-Control and Incentives: An Analysis of Multiperiod Quota Plans”) is a professor and the JCPenney Chair of Marketing and Retailing Studies at the Mays Business School, Texas A&M University. His research interests are in the areas of competitive strategy, behavioral economics, and experimental game theory. His research has been published in the Journal of Marketing Research, Management Science, and Marketing Science. He is an associate editor for Management Science and serves on the editorial boards of the Journal of Marketing Research and Marketing Science.Kevin Lane Keller (“Economic and Behavioral Perspectives on Brand Extension”) is the E. B. Osborn Professor of Marketing at the Tuck School of Business at Dartmouth College. His academic resume includes degrees from Cornell, Duke, and Carnegie Mellon universities, award-winning research, and faculty positions at the University of California at Berkeley, Stanford, and the University of North Carolina. His textbook, Strategic Brand Management, has been adopted at the top business schools and leading firms around the world. He is also the coauthor (with Philip Kotler) of the all-time best-selling introductory marketing textbook, Marketing Management.Donald R. Lehmann (“State-Dependence Effects in Surveys”) is the George E. Warren Professor of Business at the Columbia Business School. He has a B.S. in mathematics from Union College, Schenectady, NY, and an MSIA and Ph.D. from the Krannert School of Purdue University. His research interests include individual and group choice and decision making, empirical generalizations and meta-analysis, the introduction and adoption of new products and innovations, and measuring the value of marketing assets such as brands and customers. He has published numerous journal articles and six books. He was the founding editor of Marketing Letters; has served on the editorial boards of the Journal of Consumer Research, the Journal of Marketing, the Journal of Marketing Research, Management Science, and Marketing Science; and has served as executive director of the Marketing Science Institute and as president of the Association for Consumer Research.Christine Moorman (“Unintended Nutrition Consequences: Firm Responses to the Nutrition Labeling and Education Act”; “From Consumer Information Regulation to Nutrition Competition: A Response”) is the T. Austin Finch, Sr. Professor of Business Administration at the Fuqua School of Business, Duke University. She has published research on consumers, managers, and organization learning and the use of information in a range of marketing strategy and public policy contexts. Founder of the CMO Survey, author of the book Strategy from the Outside In: Profiting from Customer Value (recipient of the 2011 Berry Book Prize), and winner of the Paul D. Converse award, she has also served as a trustee for the Marketing Science Institute and on the Board of Directors of the American Marketing Association.Sridhar Moorthy (“Can Brand Extension Signal Product Quality?”; “On Brand Extension as a Signal of Product Quality: A Reply to Keller and Wernerfelt”) is the Manny Rotman Professor of Marketing at the Rotman School of Management, University of Toronto. He received his Ph.D. from Stanford University, and he has taught previously at the University of Rochester, Yale School of Management, INSEAD, the University of California at Los Angeles, the Wharton School, and the Indian School of Business. His current research focuses on branding, advertising, and retailing issues; previous work published here and in other journals has examined the relationship between advertising and product quality, product differentiation in a competitive environment, and price-matching guarantees in retailing. He is coeditor of Quantitative Marketing and Economics, associate editor of Management Science, and a member of the editorial board of Journal of Marketing Research. He is a coauthor (with Philip Kotler and Gary Lilien) of Marketing Models (Prentice-Hall 1992).Oded Netzer (“State-Dependence Effects in Surveys”) is the Phillip H. Geier Jr. Associate Professor of Business at Columbia University. He received an M.Sc. in statistics and a Ph.D. in business, both from Stanford University, and he also holds a B.Sc. in industrial engineering and management from the Technion (Israel Institute of Technology). His research interests focus on modeling customer relationships, preference measurement methods, and modeling various aspects of choice behavior, including how choices change over time, contexts, and customers. His research has appeared in the top academic journals. He is the recipient of the John D. C. Little, Frank M. Bass, and Society of Consumer Psychology Best Competitive Paper awards.Janis K. Pappalardo (“Are Unintended Effects of the Marketing Regulations Unexpected?”) is the Assistant Director for Consumer Protection in the Bureau of Economics at the Federal Trade Commission. She majored in economics at Catholic University and received her Ph.D. from Cornell University in 1986, with a primary focus in consumer economics and secondary fields in statistics and industrial organization. Research that she coauthored on health claims regulation earned her two outstanding article awards from the Journal of Public Policy and Marketing. Her research on mortgage disclosures, coauthored with James Lacko, has been published in the American Economic Review (Papers and Proceedings) and has been cited in congressional testimony and newspapers such as the Washington Post, USA Today, and the Wall Street Journal. She currently serves on the editorial boards of the Journal of Public Policy and Marketing and the Journal of Consumer Affairs.Brian T. Ratchford (“Suggestions for Further Research on Firm Responses to NLEA and Other Disclosure Laws”) is the Charles and Nancy Davidson Professor of Marketing, University of Texas at Dallas. He has MBA and Ph.D. degrees from the University of Rochester. His research interests are in economics applied to the study of consumer behavior, information economics, marketing productivity, marketing research, and electronic commerce. He has published over 80 articles in marketing and related fields. He was the Editor of Marketing Science (from 1998 to 2002); is currently an associate editor of the Journal of Consumer Research; serves on the editorial review boards of the Journal of Marketing Research, Journal of Marketing, Journal of Retailing, Journal of Interactive Marketing, Journal of Public Policy and Marketing, and Journal of Service Research; and serves on the advisory editorial board of Marketing Science.Atanu R. Sinha (“Bidding Behavior in Descending and Ascending Auctions”) is an associate professor of marketing at the Leeds School of Business, University of Colorado, Boulder. His research interests include, among others, pricing, theoretical and empirical models of auctions, negotiations, social media, online two-sided markets, and loyalty programs.Catherine Tucker (“How Does the Use of Trademarks by Third-Party Sellers Affect Online Search?”) is currently the Douglas Drane Career Development Professor in IT and Management and an associate professor of marketing at the MIT Sloan School of Management, and she is a faculty research fellow at the National Bureau of Economic Research. She received her Ph.D. in economics from Stanford University. She specializes in understanding how the huge amounts of data generated by the information and communication technology revolution can better guide marketing and advertising decisions. She has also done substantial research into how healthcare information technology is transforming the healthcare sector. She also focuses on the privacy concerns that such data raise and how firms and policy makers can best address these; she received a National Science Foundation CAREER award for her work on digital privacy.Birger Wernerfelt (“On Brand Extension as a Signal of Product Quality”) is the JC Penney Professor of Management at the MIT Sloan School of Management. He has taught marketing, strategy, and economics, and he has published in all three areas.",,,,2012,870–872,10.1287/mksc.1120.0741,https://doi-org.proxy.bnl.lu/10.1287/mksc.1120.0741;http://dx.doi.org/10.1287/mksc.1120.0741,Journal Article
On the Convergence of Newton-Type Methods Using Recurrent Functions,"We introduce the new idea of recurrent functions to provide a new semilocal convergence analysis for Newton-type methods. It turns out that our sufficient convergence conditions are weaker, and the error bounds are tighter than in earlier studies in many interesting cases [X. Chen, On the convergence of Broyden-like methods for nonlinear equations with nondifferentiable terms, Ann. Inst. Statist. Math. 42 (1990), pp. 387-401; X. Chen and T. Yamamoto, Convergence domains of certain iterative methods for solving nonlinear equations, Numer. Funct. Anal. Optim. 10 (1989), pp. 37-48; Y. Chen and D. Cai, Inexact overlapped block Broyden methods for solving nonlinear equations, Appl. Math. Comput. 136 (2003), pp. 215-228; J.E. Dennis, Toward a unified convergence theory for Newton-like methods, in Nonlinear Functional Analysis and Applications, L.B. Rall, ed., Academic Press, New York, 1971, pp. 425-472; P. Deuflhard, Newton Methods for Nonlinear Problems. Affine Invariance and Adaptive Algorithms, Springer Series in Computational Mathematics, Vol. 35, Springer-Verlag, Berlin, 2004; P. Deuflhard and G. Heindl, Affine invariant convergence theorems for Newton's method and extensions to related methods, SIAM J. Numer. Anal. 16 (1979), pp. 1-10; Z. Huang, A note of Kantorovich theorem for Newton iteration, J. Comput. Appl. Math. 47 (1993), pp. 211-217; L.V. Kantorovich and G.P. Akilov, Functional Analysis, Pergamon Press, Oxford, 1982; D. Li and M. Fukushima, Globally Convergent Broyden-like Methods for Semismooth Equations and Applications to VIP, NCP and MCP, Optimization and Numerical Algebra (Nanjing, 1999), Ann. Oper. Res. 103 (2001), pp. 71-97; C. Ma, A smoothing Broyden-like method for the mixed complementarity problems, Math. Comput. Modelling 41 (2005), pp. 523-538; G.J. Miel, Unified error analysis for Newton-type methods, Numer. Math. 33 (1979), pp. 391-396; G.J. Miel, Majorizing sequences and error bounds for iterative methods, Math. Comp. 34 (1980), pp. 185-202; I. Moret, A note on Newton type iterative methods, Computing 33 (1984), pp. 65-73; F.A. Potra, Sharp error bounds for a class of Newton-like methods, Libertas Math. 5 (1985), pp. 71-84; W.C. Rheinboldt, A unified convergence theory for a class of iterative processes, SIAM J. Numer. Anal. 5 (1968), pp. 42-63; T. Yamamoto, A convergence theorem for Newton-like methods in Banach spaces, Numer. Math. 51 (1987), pp. 545-557; P.P. Zabrejko and D.F. Nguen, The majorant method in the theory of Newton-Kantorovich approximations and the Ptak error estimates, Numer. Funct. Anal. Optim. 9 (1987), pp. 671-684; A.I. Zinc˘enko, Some approximate methods of solving equations with non-differentiable operators, (Ukrainian), Dopovidi Akad. Nauk Ukrain. RSR (1963), pp. 156-161]. Applications and numerical examples, involving a nonlinear integral equation of Chandrasekhar-type, and a differential equation are also provided in this study.",,"Argyros IK,Hilout S",,2010,3273–3296,10.1080/00207160903023557,https://doi-org.proxy.bnl.lu/10.1080/00207160903023557;http://dx.doi.org/10.1080/00207160903023557,Journal Article
Reducing Over-Dispersion by Generalized Degree of Freedom and Propensity Score,"Assume y is a response variable, x is a risk factor of interest, and z's are covariates, or sometime called ""confounders of x"" if they are correlated with both x and y. If the covariates are numerous, then model selection procedures are applied on z's while x is usually forced into the model before or after the selection. In this situation, over-dispersion will occur to bias the inference on the relation between x and y. In a linear model, the over-dispersion comes from two sources: an underestimation of the mean-squared error, and a dependency between the estimator of the x-effect and its standard error. The author proposed a method that incorporates the ideas of Ye's generalized degree of freedom and Rosenbaum and Rubin's propensity score. The method reduces the bias and over-dispersion effect to acceptable levels. Data from the Georgia capital charging and sentencing study, which included 1077 observations and 295 covariates, were analyzed as an illustration.",,Lian IB,,2003,197–214,10.1016/S0167-9473(02)00223-2,https://doi-org.proxy.bnl.lu/10.1016/S0167-9473(02)00223-2;http://dx.doi.org/10.1016/S0167-9473(02)00223-2,Journal Article
Assessment of Third-Party Logistics Providers by Introducing a New Stochastic Two-Phase Compromise Solution Model with Last Aggregation,,,"Mohammadkhani A,Mousavi SM",,2022,,10.1016/j.cie.2022.108324,https://doi-org.proxy.bnl.lu/10.1016/j.cie.2022.108324;http://dx.doi.org/10.1016/j.cie.2022.108324,Journal Article
"Concurrent, Performance-Based Methodology for Increasing the Accuracy and Certainty of Short-Term Neural Prediction Systems","Accurate prediction of the short time series with highly irregular behavior is a challenging task found in many areas of modern science. Such data fluctuations are not systematic and hardly predictable. In recent years, artificial neural networks have widely been exploited for those purposes. Although it is possible to model nonlinear behavior of short time series by using ANNs, very often they are not able to handle all events equally well. Therefore, alternative approaches have to be applied. In this study, a new, concurrent, performance-based methodology that combines best ANN topologies in order to decrease the forecasting errors and increase the forecasting certainty is proposed. The proposed approach is verified on three different data sets: the Serbian Gross National Income time series, the municipal traffic flow for a particular observation point, and the daily electric load consumption time series. It is shown that the method can significantly increase the forecasting accuracy of the individual networks, regardless of their topologies, which makes the methodology more applicable. For quantitative comparison of the accuracy of the proposed methodology with that of similar methodologies, a series of additional forecasting experiments that include a state-of-the-art ARIMA modelling and a combination of ANN and linear regression forecasting have been conducted.",,"Milić M,Milojković J,Marković I,Nikolić P,Delic V",,2019,,10.1155/2019/9323482,https://doi-org.proxy.bnl.lu/10.1155/2019/9323482;http://dx.doi.org/10.1155/2019/9323482,Journal Article
A Role-Playing Virtual World for Web-Based Application Courses,"With the rapid development of the information communication and technology (ICT) infrastructure in the Caribbean, there is an increasing demand for skilled software developers to meet the ICT needs of the region. Consequently, the web-based applications course offered at the University of the West Indies, has been redeveloped. One major part of its upgrading is the use of virtual worlds, such as the negotiate and deal environment (NADE) system, to bridge the disconnect that can occur between the technical/academic skills and the issues associated with developing software in a competitive business environment. NADE is a role-playing virtual environment for teaching the art of creating secure, Java, web-based systems for information processing and backend applications. The system provides an environment where group interactivity and strategic planning are key success factors.",,Depradine C,,2007,1081–1096,10.1016/j.compedu.2006.01.002,https://doi-org.proxy.bnl.lu/10.1016/j.compedu.2006.01.002;http://dx.doi.org/10.1016/j.compedu.2006.01.002,Journal Article
Unsecured Economies Panel: Symposium Summary,"A panel summary by Kripa Shankar.Panel Members:•Karthik Kannan, Krannert School of Management, Purdue University•Jackie Rees, Krannert School of Management, Purdue University•Dmitri Alperovitch, McAfee•Paul Doyle, ProofSpace•Kevin Morgan, Arxan TechnologiesAdding a new dimension to the CERIAS 10th Annual Security Symposium, five of the panelists with varied background came together on the final day to share their work and experiences on ""Unsecured Economies: Protecting Vital IP"".Setting the platform for this discussion was this report (http://resources.mcafee.com/content/NAUnsecuredEconomiesReport). ""Together with McAfee, an international team of data protection and intellectual property experts undertook extensive research and surveyed more than 1,000 senior IT decision makers in the US, UK, Japan, China, India, Brazil and the Middle East regarding how they currently protect their companies digital data assets and intellectual property. A distributed network of unsecured economies has emerged with the globalization of many organizations, leaving informational assets even more at risk to theft and misuse. This report investigates the cybercrime risks in various global economies, and the need for organizations to take a more holistic approach to vulnerability management and risk mitigation in this ever-evolving global business climate.""Karthik Kannan, Assistant Professor of Management Information Systems, CERIAS, Krannert School of Management, Purdue University was the first to start the proceedings. He gave a brief overview of the above report, which was the product of the collaborative research done by him, Dr. Jackie Rees and Prof. Eugene Spafford as well. The motivation behind this work, was that more and more information was becoming digital and traditional geographic boundaries were blurring. Information was being outsourced to faraway lands and as a result protecting leaks was becoming harder and harder. Kannan, put forth questions like: ""How do perceptions and practices vary across economies and cultures?"", and sighted an example from India where salary was not personal information, and was shared and discussed informally. To get answers for more such questions, a survey was devised. This survey was targeted at senior IT decision makers, Chief Information Officers and directors of various firms across the globe. US, UK, Germany, Brazil, China and India were among the countries chosen, giving the survey the cultural diversity element that it needed. Adding more value to the survey was the variety of sectors: Defense, Retail, Product Development, Manufacturing and Financial Services. According to results of the survey, a majority of the intellectual property (47%) originates from North America and Western Europe, and on an average firms lost $4.6 million worth of IP last year. Kannan went on to explain how security was being perceived in developing countries, and also discussed how respondents reacted to security investment during the downturn. Statistics like: 42% of the respondents saying laid-off employees are the biggest threat caused by the economic downturn, showed that insider threats were on the rise. The study put forth many case studies to show that data thefts from insiders tend to have greater financial impact given the high level of data access, and an even greater financial risk to corporations.Jackie Rees, also an Assistant Professor of Management Information Systems, CERIAS, Krannert School of Management, Purdue University took it up from where Kannan had left and brought to light some of the stories that did not go into the report. Rees explained the reasons behind the various sectors storing information outside the home country. While Finance sector viewed it as being safer to store data elsewhere; the IT , Product Development and Manufacturing sectors found it to be more efficient for the supply chain; and the Retail and Defense sector felt better expertise was available elsewhere. Looking at the perspective on the amount that these sectors were spending on security, 67% of the Finance industry said it was ""just right"", while ""30%"" of Retail felt it was ""too little"". The other results seemed varied but consistent with our intuitions, however all sectors seemed to agree that the major threat to deal with was ""its own employees"". The worst impact of a breach was on the reputation of the organization. Moving on to the global scene where geopolitical perceptions have become a reality in information security policies, Rees shared that certain countries are emerging as clear sources of threats to sensitive data. She added that Pakistan is seen as big threat by most industries according to respondents while China and Russia are in the mix. Poor law enforcement, corruption and lack of cooperation in these economies were sighted as a few reasons for them to emerge as threats.Dmitri Alperovitch, Vice President of Threat Research, McAfee Corporation began by expressing his concern over the fact that Cybercrime is one of the headwinds hitting our economy. He pointed out that the economic downturn has resulted in less spending on security, and as a result increased vulnerabilities and laid of employees were now the serious threats. Elucidating, he added that most of the vulnerabilities are used by insiders who not only know what is valuable, but also know how to get it. Looking back at the days when a worm such as Melissa that was named after the attacker's favorite stripper seems to be having a much lesser malicious intent that those of today, where virtually all threats now are financially motivated and more to do with money laundering. Sighting examples, Alperovitch told us stories of an organization in Turkey that was recently caught for credit and identity theft, of members of law enforcement being kidnapped, and of how Al-Qaeda and other terrorist groups were using such tools to finance terrorist groups and activities. Alperovitch vehemently stressed on the problem that this threat model was not understood by the industry and hence the industry is not well protected.Paul Doyle, Founder Chairman & CEO, Proofspace began by thanking CERIAS and congratulating the researchers at McAfee for their contributions. Adding a new perspective of thinking to the discussion, Doyle proposed that there has not been enough control over the data. Data moves over supply chain, but ""Control"" does not move. Referring to yesterday's discussion on cloud computing, where it was pointed out that availability is a freebie, Doyle said the big challenge here was that of handling integrity of data. Stressing on the point he added that integrity of data is the least common divisor, and that it was the least understood area in security as well. How do we understand when a change has occurred? In the legal industry, we have a threat factor in the form of a cross-examining attorney. What gives us certainty in other industries? We have not architected our systems to handle the legal threat vector. Systems lack the controls and audit ability needed for provenance and ensured integrity. Trust Anchor of Time has to be explored. ""How do we establish the trust anchor of time and how confidentiality tools can help in increasing reliabilities?"" are important areas to work on.Kevin Morgan, Vice President of Engineering, Arxan Technologies began with an insight on how crime evolves in perfect synchrony with the socio-economic system. Every single business record is accessible in the world of global networking, and access enables crime. Sealing enterprise perimeters has failed, as there is no perimeter any more. Thousands and thousands of nodes execute business activity, and most of the nodes (like laptops and smart phones) are mobile, which in turn means that data is mobile and perimeter-less. Boundary protection is not the answer. We have to assume that criminals have access to enterprise data and applications. Assets, data and applications must be intrinsically secure and the keys protecting them must be secure too. Technology can help a great deal in increasing the bar for criminals and the recent trends are really encouraging.After the highly informative presentations, the panel opened up for questions for the next hour. A glimpse of the session can be found in the transcript of the Q&A session below.Q&A Session: A transcript snapshotQ: We are in the Mid-West, no one is going to come after us. What should I as a security manager consider doing? How do you change the perception that organizations in ""remote"" locations are also subject to attack?•Alperovitch: You are cyber and if you have valuable information you will be targeted. Data manipulation is what one has to worry about the most.•Morgan: Form Red teams, perform penetration tests and share the results with the company.•Doyle: Employ allies and make sure you are litigation ready. Build a ROI model and lower total cost of litigation.Q: CEOs consider cutting costs. They cut bodies. One of the biggest threats to security is letting the people go. It's a paradox. How do we handle this?•Kannan: We have not been able to put a dollar value to loss of information. Lawrence Livermore National Lab has a paper on this issue which might be of interest to you.•Rees: Try to turn it into a way where you can manage information better by adding more controls.Q: How do we stress our stand on why compliance is important?•Doyle: One of our flaws as a professional committee is that we are bad in formulating business cases. We have to take a leaf out of Kevin's (of Cisco) books who formulates security challenges into business proposals. Quoting an analogy, at the end of the day it is the brakes and suspensions are the ones that decide the maximum speed of the automobile, not the engine or the aerodynamics. The question is: How fast we can go safely? Hence compliance becomes important.Q: Where do we go from here to find out how data is actually being protected?•Kannan: Economics and behavioral issues are more important for information security. We need to define these into information security models.•Rees: Governance structure of information must also be studied.•Alperovitch: The study has put forth those who may be impacted by the economy. We need to expose them to the problem. Besides we also need to help law enforcement get information from the private sector as the laws are not in place. We also need to figure out a way to motivate companies to share security information and threats with the community.•Doyle: Stop thinking about security and start thinking about risk and risk management. Model return-reward proposition in terms of risk.•Morgan: We need to step up as both developers and consumers.Q: The $4.6 million estimate. How was it estimated?•Rees: We did a rolling average across the respondents, keeping in mind the assumption that people underestimate problems.Q: Was IP integral to the business model of a company that there was a total loss causing the company to go bust?•Rees: We did not come across any direct examples of firms that tanked and fell because of IP loss.Q: Could you suggest new processes to enforce security of data?•Doyle: We need to find ways from the other side. If we cannot stop them, how do we restrict and penalize them using the law?Q: Infrastructure in Purdue and US has been there for long and we have adapted and evolved to newer technologies. However other old organization and developing countries are still backward, and it actually seems to be helping them, as they need to be less bothered with the new-age threats. What's your take on that?•Kannan: True. We spoke to the CISO of a company in India. His issues were much less as it was a company with legacy systems.•Alperovitch: There is a paradigm shift in the industry. Security is now becoming a business enabler.",,Shankar K,,2009,,,,Conference Paper
Mainstreaming Disaster Risk Reduction into Community Development in the Windward Islands,"The Windward Islands are vulnerable to a number of natural hazards. This thesis examines the possibilities for Disaster Risk Reduction (DRR) in the Windward Islands. The Windward Islands offer a special case of ""Island Vulnerability"". Island vulnerability is essentially defined as an increased probability in disaster events against what would be expected if vulnerability were to be measured against international levels of poverty, defined as Gross National Product per capita. There are three reasons for this namely the topography of islands, the site characteristics and the socio-economic setting. The topography is one where islands, largely of volcanic or coral origins, face multi-hazard experience particularly from flooding and storm surge. The site issue is that islands usually have a high ratio of coastline to land mass implying a relatively higher exposure to extreme events. The socio-economic conditions are peculiar to island including isolation, mono-agriculture and mono-industry essentially laid down by colonial experience, an absence of formal employment opportunities and weak capacity in local governance including the absence of NGOs. Though DRR has evolved over the last 20 years, some islands and communities remain more vulnerable than others. This research investigates the mainstreaming of DRR in the Windward Islands of Dominica, Grenada, Saint Lucia and St Vincent and the Grenadines. The key issue researched was whether DRR could be effectively implemented at the community level. To address this issue, the research investigates the vulnerability and capacity of communities to hazards in the Windward Islands and suggests ways to reduce risk and build community resilience. The factors affecting vulnerability and capacity to hazards in the Anglophone Windward Islands were identified as a means of determining how to reduce risks and build resilience to hazards in the Windward Islands. Efforts to enhance community development and build resilience are not effective as they fail to address fully community needs. This research concluded that some communities are more vulnerable than others and a major contributor to their vulnerability is poverty. None of the methods used in this research are unique to island vulnerability analysis as they have been applied elsewhere in DRR. What is unique is the scoping of the application of these methods to gain an overview of DRR possibilities. What emerges as a conclusion is the limited impact of top down interventions, especially those interventions that try to address poverty alleviation to lower risk. This is essentially because the poor themselves barricade their own coping mechanisms against external interventions, thus building a wall against external help. Building on local organisational capacity, including religious groups, can help address this problem. Research in this area is limited for the Anglophone Windward Islands and this thesis on vulnerability of household and communities will contribute to knowledge in this field.",,Ferdinand I,,2013,,,,Ph.D. Thesis
Analysis of Appropriate Standards to Solve Cybersecurity Problems in Public Organizations,"The development of ICTs in the globalized world has forced countries to develop problems or implement affected problems because new vulnerabilities have always been presented every day, so not having implemented a standard security standard brings problems such as data theft, leaks, malicious software among others. The objective of this investigative work is to analyze the cybersecurity analyzes that are used in public organizations in the Republic of Ecuador, for this purpose, a search was made of the different national and international security controls in order to compare them and by their similarities determine the security guidelines that are used in public organizations and any that apply. The deductive and exploratory method was used for the development of this research, it resulted in the analysis of EGSI data and after analyzing the percentage of initial and final compliance, it was concluded that the cybersecurity sensors in public organizations were low and are currently can be considered as means, since even the assessment does not incorporate all the international security parameters.",,"Toapanta SM,E. GS,Gallegos LE",,2020,14–19,10.1145/3404663.3404678,https://doi-org.proxy.bnl.lu/10.1145/3404663.3404678;http://dx.doi.org/10.1145/3404663.3404678,Conference Paper
Implementing Cisco IOS Network Security (IINS): CCNA Security Exam 640-553,"Authorized Self-Study Guide Implementing Cisco IOS Network Security (IINS) Foundation learning for CCNA Security IINS 640-553 exam Catherine Paquet Implementing Cisco IOS Network Security (IINS) is a Cisco-authorized, self-paced learning tool for CCNA Security foundation learning. This book provides you with the knowledge needed to secure Cisco routers and switches and their associated networks. By reading this book, you will gain a thorough understanding of how to troubleshoot and monitor network devices to maintain integrity, confidentiality, and availability of data and devices, as well as the technologies that Cisco uses in its security infrastructure. This book focuses on the necessity of a comprehensive security policy and how it affects the posture of the network. You will learn how to perform basic tasks to secure a small branch type office network using Cisco IOS security features available through the Cisco Router and Security Device Manager (SDM) web-based graphical user interface (GUI) and through the command-line interface (CLI) on Cisco routers and switches. The author also provides, when appropriate, parallels with Cisco ASA appliances. Whether you are preparing for CCNA Security certification or simply want to gain a better understanding of Cisco IOS security fundamentals, you will benefit from the information provided in this book. Implementing Cisco IOS Network Security (IINS) is part of a recommended learning path from Cisco that includes simulation and hands-on training from authorized Cisco Learning Partners and self-study products from Cisco Press. To find out more about instructor-led training, e-learning, and hands-on instruction offered by authorized Cisco Learning Partners worldwide, please visit www.cisco.com/go/authorizedtraining. Catherine Paquet is a practitioner in the field of internetworking, network security and security financials. Catherine has in-depth knowledge of security systems, remote access, and routing technology. She is CCSP, CCNP, and CompTIA Security+ certified. She is also a certified Cisco instructor with the largest training partner of Cisco. Catherine, who has her M.B.A. from York University, is the director of her consulting company, Netrisec, Inc., specializing in making the business case for network security. In 2002 and 2003, Catherine volunteered with the UN mission in Kabul, Afghanistan, to train Afghan public servants in the area of networking. Develop a comprehensive network security policy to counter threats against information security Configure routers on the network perimeter with Cisco IOS Software security features Configure firewall features including ACLs and Cisco IOS zone-based policy firewalls to perform basic security operations on a network Configure site-to-site VPNs using Cisco IOS features Configure IPS on Cisco network routers Configure LAN devices to control access, resist attacks, shield other network devices and systems, and protect the integrity and confidentiality of network traffic This volume is in the Certification Self-Study Series offered by Cisco Press. Books in this series provide officially developed self-study solutions to help networking professionals understand technology implementations and prepare for the Cisco Career Certifications examinations. Category: Cisco Certification Covers: IINS exam 640-553",,Paquet C,,2009,,,,Book
LANC '05: Proceedings of the 3rd International IFIP/ACM Latin American Conference on Networking,"It is our distinct pleasure to welcome you to this year's IFIP/ACM Latin America Networking Conference.LANC takes place every odd year in conjunction with CLEI's annual flagship event, which comprises several informatics related conferences. In addition to LANC 2005, CLEI (Centro Latino Americano de Estudios en Informática) organizes this year the Thirty-First Latin American Computing Conference, the Thirteenth Latin American Congress of Higher Education on Computing, the Twelfth Latin American Master's Thesis Contest, and the First Colombian Computation Congress. LANC 2005 takes place in Cali, Colombia. This year, the organization of LANC is more closely integrated into that of the CLEI conferences than in 2003 (in La Paz, Bolivia). We trust that the organizers of CLEI 2005 provide a smoothly functioning and interesting venue and hope that the papers that are presented during LANC 2005 prove to be enlightening and insightful.The Call for Papers for LANC 2005 stated a number of areas of interest in current network research, including: Low cost access methods; Sensor and actuator networks; Low rate communications; National regional communication infrastructures; Wireless and mobile networking; PC cluster computation; Applications for productivity; Internet based applications; Network management; Network security; Optical networks; and Protocols and routing. In response to its Call for Papers, LANC 2005 received 33 submissions, of which the conference's Program Committee selected twelve for presentation and publication in its proceedings. Every effort was made to subject each paper to three independent evaluations. We are pleased to note that in addition to Spain, five Latin American Countries (Brazil, Colombia, Paraguay, Uruguay, and Venezuela) are represented by the authors of the accepted papers. The acceptance criteria were the standard ones for quality international conferences, with the additional desideratum of relevance to Latin America.",,,,2005,,,,Book
Exploiting Multi-Vendor Vulnerabilities as Back-Doors to Counter the Threat of Rogue Small Unmanned Aerial Systems,"A recent trend for many malicious actors, such as: (1) terrorists in Iraq and Syria, (2) lone wolf domestic terrorists, (3) drug cartels, or (4) espionage-minded corporations, has been to use commercial-off-the-shelf (COTS) small unmanned aerial systems (sUAS) (i.e., drones) that can circumvent ground-based defenses to attack or spy on targets, to transport contraband, or to steal information. Because of the low cost of COTS sUAS and the prior success of these uses, this trend is increasing at an alarming rate, leading to the need to counter the malicious usage of sUAS (i.e., rogue sUAS). Researchers, the armed forces, and technologists have all proposed disparate solutions to this problem. There are no comprehensive and compact solutions capable of effectively tracking, identifying, and actively neutralizing the threats associated with rogue sUAS. Thus, we have developed a mobile cyber solution, using rigorous penetration testing across the top sUAS COTS vendors. Based on the market share of these top vendors, our approach is applicable to approximately 90% of all COTS sUAS. We demonstrate that hard-to-patch vulnerabilities (i.e., vulnerabilities that exist across all the top vendors of sUAS) can be used as back-doors to counter the threat of rouge sUAS. Our solution can be launched from a standard laptop or Android mobile device with an external antenna, and is capable of tracking, identifying, and disrupting all Parrot and 3DR sUAS, as well as almost all DJI sUAS (i.e., renders them incapable of video flight) within a 300-meter radius.",,"Watkins L,Ramos J,Snow G,Vallejo J,Robinson WH,Rubin AD,Ciocco J,Jedrzejewski F,Liu J,Li C",,2018,,10.1145/3215466.3215467,https://doi-org.proxy.bnl.lu/10.1145/3215466.3215467;http://dx.doi.org/10.1145/3215466.3215467,Conference Paper
Terminal Ballistics of Intercept Ammunition against Mortar Targets,"The threat imposed by rockets, artillery projectiles, and mortar grenades (RAM) is of major concern for military installations and objects, e.g., in Iraq or Afghanistan. A good portion of these attacks are undertaken with unguided rockets and mortar projectiles with calibers up to 120 mm. This paper concentrates on mortars. A counter RAM system based on 155 mm HE projectiles is applied to intercept and destroy incoming targets at a safe distance of the installation. Therefore, the impact of fragments and a blast wave as warhead mechanisms against a typical Russian mortar projectile with a caliber of 82 mm is investigated by adopting different empirical equations. A response plot overlayed on the fragment map is derived from the detonation and penetration threshold. These results are input parameters for a statistical approach of estimating the ammunition consumption as a function of the multi-shot kill probability.",,"Graswald M,Rothe H",,2008,720–728,,,Conference Paper
HIPAA and QMS Based Architectural Requirements to Cope with the OCR Audit Program,"The United States legislation known as the Health Insurance Portability and Accountability Act of 1996 (HIPAA) is aimed at strengthening patient rights, increasing efficiency and decreasing administrative costs in the healthcare industry. Under HIPAA all Covered Entities are required to ensure compliance with certain privacy and security rules concerned with protecting private patient health information. Building upon the objectives of HIPAA, the American Recovery and Reinvestment Act (ARRA) of 2009, in Section 13411 of the Health Information Technology for Economic and Clinical Health (HITECH) Act, required the Department of Health and Human Services (HHS) to conduct periodic audits of Covered Entities against HIPAA Security Rule. This paper presents and evaluates a new approach which might be used by Covered Entities to achieve compliance with HIPAA by adopting the ISO 9001 guidelines. A United States based Healthcare IT Company (UHITC) with a backup office in Pakistan was taken as a case study for this approach. UHITC develops software for mobile devices along with providing third party medical billing services. In connection with its achieving ISO 9001 certification since 2004, UHITC had already developed a company-wide quality audit protocol based on the ISO 9001 standard. For purposes of conforming the ISO standards to the HIPAA audit protocol in a streamlined fashion, UHITC examined the HIPAA requirements to determine whether the existing protocol could be tailored to achieve HIPAA compliance. In order to accomplish this evaluation, the two standards were compared by cross-mapping their components. The comparison revealed that the controls mentioned in the ISO 9001 guideline meet or exceed the HIPAA Security Rule for 36% of the implementation requirements. UHITC was also able to increase customer satisfaction by achieving compliance with HIPAA Security Rule using a quality management system (QMS) model. At the next level, Compliance Attributes (CA) were derived from these requirements and classified as architectural and non-architectural in nature. A new approach to define compliance oriented software architecture using compliance tactic was also proposed.",,"Gardazi SU,Shahid AA,Salimbene C",,2012,246–253,10.1109/MUSIC.2012.50,https://doi-org.proxy.bnl.lu/10.1109/MUSIC.2012.50;http://dx.doi.org/10.1109/MUSIC.2012.50,Conference Paper
Weather Forecast Information Dissemination Design For Low-Literate Farmers: An Exploratory Study,"Pakistan's agricultural sector has been making gigantic contributions towards the nation's economy, with agriculture accounting for 22% of the gross domestic product (GDP) while engaging approximately half of the country's labor force. A significant developmental challenge in this sector is inadequacy and inaccessibility of information regarding weather forecast. In this paper, we propose an Android-based solution for farmers that can facilitate the timely, localized, and customized dissemination of granular weather forecast that shields the whole agricultural ecosystem and supply chain from weather variability by appropriate decision-making. We describe our Android mobile application that sends a customized weather forecast that is configured according to the user preferences. Information is disseminated by the cloud server through encrypted SMS to the subscribing farmers containing weather information. This information is encoded through visuals and icons in a simple to understand user-interface that is accompanied by Urdu language text in a design tailored for low-literate farmers of Pakistan. The testing, feedback, and evaluation include design understanding, the effectiveness of icons and images, usability, adaptation to touch screen is in progress which will help us to reiterate the mobile app user interface (UI) to improve the preliminary design.",,"Idrees F,Batool A,Qadir J",,2017,,10.1145/3136560.3136596,https://doi-org.proxy.bnl.lu/10.1145/3136560.3136596;http://dx.doi.org/10.1145/3136560.3136596,Conference Paper
An Assessment of SMS Fraud in Pakistan,"SMS fraud has become a growing concern for those working toward financial inclusion, however, it is often unclear how widespread such threats are in practice. This multi-method study investigates SMS fraud in Pakistan through identification and categorization of fraudulent messages as well as the impact on those who receive such messages. We collect fraudulent SMS messages by various means, including byway of a custom-built Android smartphone application. To complement this, we interview people exposed to SMS fraud and representatives of mobile network operators. Based on our analysis, lottery type fraud schemes dominate SMS fraud in Pakistan, and these schemes have the greatest impact on vulnerable low-income, rural populations. We offer a simple heuristic for fraud detection that has a high accuracy rate and is adaptable to evolving fraud schemes, and conclude with a recommendation for a fraud mitigation strategy to target fraudster call back numbers.",,"Pervaiz F,Nawaz RS,Ramzan MU,Usmani MZ,Mare S,Heimerl K,Kamiran F,Anderson R,Razaq L",,2019,195–205,10.1145/3314344.3332500,https://doi-org.proxy.bnl.lu/10.1145/3314344.3332500;http://dx.doi.org/10.1145/3314344.3332500,Conference Paper
MehfoozAurat: Transforming Smart Phones into Women Safety Devices Against Harassment,"In Pakistan, workingwomen often become victims of harassment during traveling which exacerbates uncertainty and hesitance among other workingwomen regarding their security. This paper discusses the android application, MehfoozAurat (safe woman), which we developed to support the lower socio-economic income bracket of working women who use public transport. The key features include safe routes, emergency alerts and audio recording with a unique self-defense section. The app has text based output in the national language, Urdu, which makes it accessible to the majority who are unfamiliar with the English language. Our usability tests reveal that the system is perceived to be useful and easy to learn; with brief learning, even the uneducated workingwomen were able to benefit from the application.",,"Sarosh MY,Yousaf MA,Javed MM,Shahid S",,2016,,10.1145/2909609.2909645,https://doi-org.proxy.bnl.lu/10.1145/2909609.2909645;http://dx.doi.org/10.1145/2909609.2909645,Conference Paper
Tracking and Disrupting Dark Networks: Challenges of Data Collection and Analysis,"The attack on September 11, 2001 set off numerous efforts to counter terrorism and insurgencies. Central to these efforts has been the drive to improve data collection and analysis. Section 1 summarizes some of the more notable improvements among U.S. government agencies as they strive to develop their capabilities. Although progress has been made, daunting challenges remain. Section 2 reviews the basic challenges to data collection and analysis focusing in some depth on the difficulties of data integration. Three general approaches to data integration are identified--discipline-centric, placed-centric and virtual. A summary of the major challenges in data integration confronting field operators in Iraq and Afghanistan illustrates the work that lies ahead. Section 3 shifts gears to focus on the future and introduces the discipline of Visual Analytics--an emerging field dedicated to improving data collection and analysis through the use of computer-mediated visualization techniques and tools. The purpose of Visual Analytics is to maximize human capability to perceive, understand, reason, make judgments and work collaboratively with multidimensional, conflicting, and dynamic data. The paper concludes with two excellent examples of analytic software platforms that have been developed for the intelligence community--Palantir and ORA. They signal the progress made in the field of Visual Analytics to date and illustrate the opportunities that await other IS researchers interested in applying their knowledge and skills to the tracking and disrupting of dark networks.",,Roberts NC,,2011,5–19,10.1007/s10796-010-9271-z,https://doi-org.proxy.bnl.lu/10.1007/s10796-010-9271-z;http://dx.doi.org/10.1007/s10796-010-9271-z,Journal Article
"Improving the Privacy, Security, and Performance of Biometric Systems","Biometric systems now represent a critical piece of the world's security infrastructure. Such systems are deployed at our borders, our workplaces, and even our amusement parks. Biometrics have found their way into military security checkpoints in Iraq and naval and coast guard operations around the globe. From the simplest time and attendance application, to the most sophisticated access control security installation, biometrics present a common set of benefits and problems for those that use and maintain such systems. This dissertation examines issues in three distinct, yet related components of biometric recognition systems: Privacy, Security, and Performance.Beginning with Privacy, a series of new attacks against existing template protection schemes is introduced, leading to a formulation of sound security requirements for revocable template technologies. Two novel secure revocable biotoken schemes are then introduced that conform to the aforementioned security requirements, including a base biotoken encoding method, and a bipartite biotoken method supporting secure data release. Using the biotoken technology as a basis, the Security of biometrics is considered through the development of a series of bio-cryptographic protocols, culminating in the development of a full biometric key infrastructure that enhances existing public key infrastructure. To address the threat of physical spoofing attacks, a low-cost spectrometer design is introduced that is capable of distinguishing between real live fingers and common artificial material. Regarding Performance, issues in long-range unconstrained face acquisition are discussed, followed by the introduction of a new facial feature detector and secure face recognition approach designed for this specific problem. Finally, a machine learning approach and a pure statistical approach are introduced for predicting the failure of biometric recognition systems. This research, taken as a whole, represents various embodiments of good and functional biometric systems that have yet to be realized in deployment.",,Scheirer WJ,,2009,,,,Ph.D. Thesis
My Morning Routine: An Interactive UDL Compliant E-Book on Health and Hygiene for Learners with Visual Difficulties,"Health instruction assembles students’ information, abilities, and positive perspectives about wellbeing. However, 40% of the children in Pakistan are visually impaired and 38% have some cognitive disability due to malnutrition and poor hygiene practices. Despite COVID-19 putting the focus on the significance of hand cleanliness to forestall the spread of illness, reports from different countries have shown that the hand hygiene compliance rate has been estimated at only 40%. Using a Universal Design of Learning (UDL) approach, this research developed and produced a UDL compliant interactive android e-book that teaches young minds about the importance of staying clean and sanitization, especially to those learners who respond better when materials are paired with lights, sounds and/or movement. The scripting, story layout, design, and development were implemented using the seven stages of action and Nielson’s design heuristics. Drawing upon the cyclic process of developing prototypes and then learners providing feedback upon each version, this e-book was designed to optimize learning by minimizing the cognitive load on the learners, providing ease for learners with visual or cognitive learning disabilities. Results showed that the visually challenged learners responded well to the minimal design interface and graphics. Major impact of the research was students developing the habits of handwashing and following healthy morning routines, as indicated in the interviews and surveys conducted from the learners.",,"Rizwan A,Abid M,Quidwai NU,Kiyani MN",,2021,470–482,10.1007/978-3-030-91540-7_48,https://doi-org.proxy.bnl.lu/10.1007/978-3-030-91540-7_48;http://dx.doi.org/10.1007/978-3-030-91540-7_48,Conference Paper
Network Security Principles and Practices,"From the Publisher: Design and Implementation recommendations for secure network infrastructures and CCIE preparation Prepare for the CCIE Security exam with a thorough explanation of key network security topics Written by a CCIE who helped create the new CCIE Security Recertification Exam Expert level, practical advice for securing a variety of network environments Includes implementation advice that illustrates the concepts in real-world settings Network Security Principles and Practices is a comprehensive guide to network security threats and the policies and tools developed specifically to combat those threats. Starting with a general discussion of network security concepts and design philosophy, the book shows readers how they can build secure network architectures from the ground up. Taking a practical, applied approach to building security into networks, the book focuses on showing readers how to implement and verify security features and products in a variety of environments. Security aspects of routing protocols are discussed and various options for choosing and using them analyzed. The book goes into a detailed discussion of the security threats posed by increasingly prevalent LAN to LAN Virtual Private Networks and remote access VPN installations and how to minimize large vulnerabilities caused by these non-traditional network portals. Firewalls, including the PIX and IOS firewalls, and underlying protocols are presented in depth. Intrusion Detection, is fully examined. The book shows the reader how to control dial-in access by setting up access servers with AAA, PPP, Tacacs+, and Radius. Finally, protections at the service provider arediscussed by showing the reader how to provision security at the service provider level. All topics covered in the book are cross-referenced to Cisco implementations and case studies utilizing Cisco solutions are presented throughout. Saadat Malik, CCIE #4955, is the manager of technical support for the VPN solutions team at Cisco Systems, and was a key participant in the recent development of the new CCIE Security Recertification Exam. Saadat holds a Masters degree in electrical engineering from Purdue University, as well as a Bachelors degree from GIK Institute in Pakistan.",,Malik S,,2002,,,,Book
Commercial-Off-the-Shelf-Technology in UK Military Training,"Aim. This article gives an overview of how commercial computer game technology was introduced for training, education and decision support within the British Army. Value of the article. It records the narrative of the introduction and development of first person shooter computer games into the British Army; an area where developments are not routinely reported outside the closed world of defence training. Methodology. The research was based on interviews of key staff who worked in procurement at the Defence Academy of the UK and for the MoD during 2002 to 2012. The interviewees included two officers, an experienced defence contractor and a senior civil servant. These interviews were given on the understanding that the views expressed would not be individually attributable as they might not represent those of their current employers. The authors were also given access to a unique collection of documents, some of which were not publically available, but are held in the archives of the UK Defence Academy. These are cited in the bibliography. Limitations of the article. This article cites the evidence from the time that supported the continued use of what was a radical and contentious new way of training. Since the introduction of Virtual Battle Space 2 into the British Army, further research into the effectiveness of games based training in the military has been published. Analysis. Games based training has become a significant part of the training cycle for many parts of the British Army. These games have limitations, but are the only alternative to real operations for some types of training. However, the difficult topic of what is the correct proportion of games based training to other types__ __ is a contested area within defence training in the UK. Conclusions. Initial evaluations on the effectiveness of the use of computer games in preparing UK forces for operations in Iraq and Afghanistan showed they had a significant positive impact. The first experience of the British Army with these games has secured the long-term application of this technology and it is unrealistic to imagine future military training without some degree of games technology.",,"Curry J,Price T,Sabin P",,2016,7–30,10.1177/1046878115600578,https://doi-org.proxy.bnl.lu/10.1177/1046878115600578;http://dx.doi.org/10.1177/1046878115600578,Journal Article
Appropriate Security Algorithms to Mitigate the Risks in a Database of the National Customs Service of Ecuador,"Currently, the use of technological resources, within public and private institutions, has become essential, due to the need to modernize processes, high commercial needs and mandatory continuous improvement. A problem has been observed at the national level, in relation to the ease with which the databases of natural or legal persons are accessed, which has motivated them to provide the respective security measures. For this reason, a study has been carried out on the security measures of the database of the National Customs Service of Ecuador, with a quantitative methodology which allowed to know, objectively, what are the different security measures that must be implemented to improve the operations with the utmost care. In addition, a qualitative analysis of the revised texts was carried out to minimize the threats and vulnerabilities present in security systems. The objective of this study was to establish adequate security algorithms to mitigate risks in a database of the National Customs Service of Ecuador, which has been working, since 2007, on risk management, training personnel, following strategies international and implementing technological aspects to systematize the processes that until then were mainly carried out physically. The need for the institution to take care of its database was evident, applying HASH computer security measures that reduce or eliminate computer thefts, with encrypted algorithms, training of personnel throughout the process and articles of the law that allow the use of adequate mechanisms for the care and protection of information, as appropriate.",,"Toapanta SM,Astudillo KL,Gallegos LE",,2020,89–95,10.1145/3404663.3404669,https://doi-org.proxy.bnl.lu/10.1145/3404663.3404669;http://dx.doi.org/10.1145/3404663.3404669,Conference Paper
Poster - Mapping Opium Poppy Cultivation in Afghanistan Using Satellite Imagery,"Afghanistan is the world’s largest supplier of illicit opium, accounting for an estimated 70-80% of supply. In 2019, this generated an estimated income of $1.2-$2.1 billion domestically, or around 10% of Afghanistan’s gross domestic product. The illicit drug economy has provided livelihoods to millions of Afghans, but has also had numerous negative effects, including funding insurgent groups, exacerbating corruption and insecurity, and contributing to high domestic levels of drug addiction. From 2002 to 2017, the U.S. government spent over $8 billion on counter-narcotics efforts in Afghanistan, achieving little long-term success. The lack of reliable data has contributed to this failure; the robustness and interpretation of top-line estimates of area under cultivation have been questioned and criticized. Counter-narcotics efforts have focused on reducing total cultivation area, rather than trying to understand local socioeconomic or political conditions. The lack of granularity in official cultivation statistics has also impeded efforts by aid agencies to evaluate the impact of various interventions aimed at transitioning farmers away from poppy. Currently, official statistics on poppy cultivation are released annually by the United Nations Office on Drugs and Crime (UNODC) at a district level.1 These are produced using commercial high-resolution (0.5m × 0.5m) imagery, manually annotated by analysts and verified with ground imagery. In districts with substantial cultivation, a limited number of sites are sampled for labeling, while in other districts, all known cultivation areas are annotated. Only aggregate district-level cultivation figures are published and no detailed maps are available. The UNODC also conducts in-person surveys to characterize socioeconomic conditions. These methods, while undoubtedly valuable, are costly and difficult to undertake under poor security conditions. Furthermore, reports are released after long delays, with the government being suspected of blocking publication in some years. This paper investigates the possibility of using publicly available satellite imagery to generate poppy cultivation maps at high resolution. Some advantages of this source of data include its timeliness and cost-effectiveness, easy availability of data, and high level of granularity. These maps can then be combined with other data sources, such as grid-level data on climate, population, and healthcare, to further our quantitative understanding of the socioeconomic circumstances associated with poppy cultivation, a complex and persistent development challenge. This work complements official estimates, as well as related work that relies on commercial high-resolution satellite imagery, manual labelers, expert knowledge or qualitative methods. In developing these methods, we build on work using automated methods and spectral imagery to classify opium poppy, wheat, as well as other agricultural crops. In initial work, we limit our analysis to Helmand, a province accounting for more than half of all cultivation, where crop cycles are well-known and there are few major alternative crops. We carefully choose image acquisition dates based on crop cycles, and measure levels of vegetation growth in the pre- and post-harvest stages. We then apply a rule-based classification approach to infer areas under poppy cultivation, finding that our aggregate area estimates track official statistics closely at a district level (Pearson’s correlation ρ ≥ .8). Future work will involve refining the methodology and extending it to the rest of Afghanistan over multiple years. Early analysis suggests that this approach could generalize to other provinces in Southern and Western Afghanistan, but we expect to face more difficulty especially in the Northern parts of Afghanistan, due to smaller plot sizes, mixed cultivation patterns, complex terrain and the close proximity of agriculture to natural vegetation. Some potential solutions include automated strategies to infer best acquisition windows, and classifying agricultural land and opium poppy using more flexible approaches, such as unsupervised clustering methods. We hope that an extension of our current approach can provide additional quantitative insight to the local circumstances surrounding poppy cultivation, and ultimately contribute to the design of effective policies to protect the welfare of farmers while governments work towards their counter-narcotics goals.",,"Tai XH,Nair SR,Mehra S",,2021,416,10.1145/3460112.3472308,https://doi-org.proxy.bnl.lu/10.1145/3460112.3472308;http://dx.doi.org/10.1145/3460112.3472308,Conference Paper
Cybersecurity Analysis to Determine the Impact on the Social Area in Latin America and the Caribbean,"This document presents an introduction to the events that occurred in Latin America and the Caribbean, where banks are compromised by cyber-attacks, the same case is also seen in Ecuador, where losses from these attacks would amount to $ 6 billion by 2021. Our goal is to provide an analysis that demonstrates the social impact that cybersecurity has in Latin America and the Caribbean and a way to measure cybersecurity to combat threats due to constant technological progress and the great privacy risks presented to the day with cyber systems. The deductive method was used, adopting and generating a set of activities to evaluate the Total Cybersecurity (TC) of the cyber systems and / or companies to have a control of the security that it has and the improvements that can be implemented to prevent or combat Possible cyber threats or vulnerabilities. It resulted in cyber-resilience activities and activities based on the main functions before, during and after a cyber-attack. These activities are evaluated by the frequency of use, allowing each activity to assign a value and in this way apply the formulas proposed and obtain the CT, achieving the percentage of cybersecurity that has been implemented and discovering the weaknesses of the systems that They must be strengthened. It was concluded that in certain countries of Latin America and the Caribbean they have very little interest in the development and expansion of cybersecurity in their lands, unlike the progress of cybersecurity in European countries.",,"Toapanta SM,Jaramillo JM,Gallegos LE",,2020,73–78,10.1145/3375900.3375911,https://doi-org.proxy.bnl.lu/10.1145/3375900.3375911;http://dx.doi.org/10.1145/3375900.3375911,Conference Paper
IT Enabled Counter Terrorism Infrastructure: Issues and Challenges,"In the post 11 September 2001, terrorism has been an immediate and most serious threat to the free world because of its real and potential damage to the infrastructure, economy and people. In response to the 11 September 2001 terrorist attacks, developed and developing countries, such as USA and Pakistan, have emerged as front line states in the fight against terrorism with the following objectives: (1) prevent future terrorist attacks, (2) reduce the nations vulnerability and (3) minimise the damage and recovery from attacks that occur. In order to achieve these objectives, we require new approaches to intelligence and information gathering and its analysis through the use of information technology. In this paper, we attempt to identify (1) the areas where IT can contribute in accomplishing these three strategic security objectives, (2) the unique IT problems and challenges in counter terrorism applications where such applications are being used and developed such as in USA and (3) lessons learned for developing countries such as Pakistan, so that an IT counter terrorism infrastructure can be established with minimum cost in terms of time and money.",,Ahsan S,,2007,117–124,10.1504/IJESDF.2007.013597,https://doi-org.proxy.bnl.lu/10.1504/IJESDF.2007.013597;http://dx.doi.org/10.1504/IJESDF.2007.013597,Journal Article
Front Matter,"In the summer of 1956, John McCarthy organized the famous Dartmouth Conference which is now commonly viewed as the founding event for the field of Artificial Intelligence. During the last 50 years, AI has seen a tremendous development and is now a well-established scientific discipline all over the world. Also in Europe AI is in excellent shape, as witnessed by the large number of high quality submissions we received. About 600 papers and posters were registered for ECAI-06, out of which 550 were actually reviewed (501 paper submissions and 49 poster submissions). The program committee decided to accept 131 full papers, which amounts to an acceptance rate of 26.1%, and 75 posters (authors of full papers had the possibility to opt for acceptance as poster in case of rejectance as full paper).We received submissions from 43 different countries, and accepted papers from 25 countries. The following table shows the number of submissions and accepted papers per country, based on the contact author affiliation:Algeria 5 0Australia 19 9Austria 9 5Belgium 4 2Brazil 12 0Bulgaria 1 0Canada 13 4China 4 0Cyprus 1 0Czechia 4 0Egypt 1 0Estonia 2 0France 111 46Finland 3 2Germany 47 19Greece 15 4Hungary 1 0India 1 0Iran 5 1Ireland 14 9Israel 8 2Italy 83 36Japan 9 1Luxemburg 5 3Mexico 2 0Netherlands 26 12New Zealand 3 0Pakistan 1 0Poland 3 0Portugal 1 0Romania 4 1Russia 3 0Singapore 4 4Spain 32 5Slovenia 2 2Slovakia 3 3Sweden 5 5Switzerland 5 3Thailand 2 0Turkey 3 0UK 49 22USA 22 5Venezuela 1 1It is also interesting to look at the areas of the submitted and accepted papers/posters. We show both absolute numbers and percentage. The area information is based on the first two key words chosen by the authors:# submitted % # accepted %Case-Based Reasoning 10.5 1.9 0.5 0.2Cognitive Modelling 33.5 6.1 13 6.3Constraints & Search 54.5 10.0 26 12.6Distributed AI/Agents 107 19.6 36.5 17.7KR & Reasoning 141.5 25.9 55.5 26.9Machine Learning 83 15.2 29 14.1Model-Based Reasoning 32 5.9 8 3.9Natural Language 21.5 3.9 7.5 3.6Perception/Vision 6 1.1 2 1.0Planning and Scheduling 27 4.9 12 5.8Robotics 12.5 2.3 5 2.4PAIS 18 3.3 11 5.3In comparison with ECAI 2004, we see a strong increase in the relative number of submissions from Distributed AI/Agents and Cognitive Modelling. Knowledge Representation & Reasoning is traditionally strong in Europe and remains the biggest area of ECAI-06. One reason the figures for Case-Based Reasoning are rather low is that much of the high quality work in this area has found its way into prestigious applications and is thus represented under the heading of PAIS.The ECAI-06 best paper award, sponsored by Elsevier, goes to a machine learning paper:A Real Generalization of Discrete AdaBoost, by Richard Nock and Frank NielsenCongratulations! The best poster award, sponsored by IOS Press, will be decided after the poster sessions in Riva. The 10 best papers are invited to a fast track of the Artificial Intelligence Journal.A conference of the size of ECAI needs a lot of support from many people. At this point we would like to thank all those who helped to make ECAI-06 a tremendous success: the poster, workshop, PAIS and area chairs faced a heavy workload and they all did an excellent job. The PC members and additional reviewers came up with timely, high quality reviews and made the life of the PC chair as easy as it can be. Thanks also to Alex Nittka who ran the conference management software, to the PC chair's great relief. We also want to thank the many people involved in the local organization of the conference: there would be no conference without you.Our very special thanks go to a person who is no longer with us. Rob Milne, chair of ECAI-06 until he died of a heart attack close to the summit of Mount Everest on June 5th, 2005. He shaped this conference from the beginning, and we did our best to organize ECAI-06 in his spirit.June 2006, Gerhard Brewka, Silvia Coradeschi, Anna Perini, Paolo Traverso",,,,2006,i–xxvi,,,Conference Paper
Annual Editions: Computers in Society 09/10,"Annual Editions is a series of over 65 volumes, each designed to provide convenient, inexpensive access to a wide range of current articles from some of the most respected magazines, newspapers, and journals published today. Annual Editions are updated on a regular basis through a continuous monitoring of over 300 periodical sources. The articles selected are authored by prominent scholars, researchers, and commentators writing for a general audience. The Annual Editions volumes have a number of common organizational features designed to make them particularly useful in the classroom: a general introduction; an annotated table of contents; a topic guide; an annotated listing of selected World Wide Web sites; and a brief overview for each section. Each volume also offers an online Instructor's Resource Guide with testing materials. Using Annual Editions in the Classroom is the general instructor's guide for our popular Annual Editions series and is available in print (0073301906) or online. Visit www.mhcls.com for more details. Table of contents AE_Computers in Society 09/10 Preface Correlation Guide Topic Guide Internet References Unit 1: Introduction Unit Overview 1. Five Things We Need to Know about Technological Change, Neil Postman, Address to New Tech '98 Conference, March 27, 1998 Neil Postman, a well-known cultural critic, suggests that computer technology is too important to be left entirely to the technologists. Embedded in every technology, he says, is a powerful idea. . . . 2. On the Nature of Computing, Jon Crowcroft, Communications of the ACM, February 2005 The author states, Occupying a third place in human intellectual culture, computing is not bound by the need to describe what does exist (as in natural science) or what can be built in the real world (as in engineering). 3. A Place for Hype, Edward Tenner, London Review of Books, May 10, 2007 A new golden age of technological hype seems to be dawning, Tenner writes. Despite all predictions, computers still can't tell whether an object is a hat, a chair, or a shoe. Unit 2: The Economy Unit Overview 4. Click Fraud: The Dark Side of Online Advertising, Brian Grow and Ben Elgin, BusinessWeek, October 2, 2006 Internet advertisers think they pay only when an interested customer clicks on their ads. Martin Fleischman, an Atlanta businessman, noticed a growing number of puzzling clicks coming from such places as Botswana, Mongolia, and Syria. 5. Online Salvation , Paul Farhi, American Journalism Review, December 2007/January 2008 In 2003, newspapers earned $1.2 billion through online services. By 2006, the figure had grown to $2.7 billion. Will the Internet save the beleaguered newspaper business 6. The Big Band Era, Christopher Swope, Governing, January 2005 Even as cities like Philadelphiaare working to transform the, entire city into a wireless hot spotwith government as the Internet service provider of last resortcommunications companies are fighting to keep local governments out of the broadband business. 7. The Beauty of Simplicity, Linda Tischler, Fast Company, November 2005 A simple tale about simplicity. One company hired an editor from People Magazine to translate accounting lingo into everyday language, pared back 125 setup screens to three, and sold 100,000 units in its first year on the market. 8. The Software Wars: Why You Can't Understand Your Computer, Paul De Palma, American Scholar, Winter 2005 The article argues that software development is like military procurement, and suffers many of the same woes, including excessive complexity and cost overruns. Unit 3: Work and the Workplace Unit Overview 9. National ID: Biometrics Pinned to Social Security Cards, Ryan Singel, Wired, May 15, 2007 Immigration is in the news again. One proposal before Congress is to issue American workers tamper-proof biometric Social Security cards. These would replace the text-only design that's been issued to Americans almost without change for more than 70 years. 10. Dilberts of the World, Unite!, David Sirota, The Nation, June 23, 2008 Faced with industry giants who outsource work to India on one hand and import lower cost engineers on the other, software developers have begun to organize. 11. Computer Software Engineers, Occupational Outlook Handbook, 2006/07 Edition Here is one official source that acknowledges the effect of shipping high tech jobs abroad, but still predicts that software engineers are projected to be one of the fastest-growing occupations from 2004 to 2014. 12. How Deep Can You Probe , Rita Zeidner, HR Magazine, October 2007 Tales of employers searching MySpace pages notwithstanding, Many states limit the extent to which employers can consider off duty conduct in making a hiring decision. . . . 13. Privacy, Legislation, and Surveillance Software, G. Daryl Nord, Tipton F. McCubbins, and Jeretta Horn Nord, Communications of the ACM, August 2006 The authors tell us that the assumption of employee privacy in the workplace may be naïve. Constitutional protections against unreasonable search and seizure usually apply only to state actions. 14. The Computer Evolution, Rob Valletta and Geoffrey MacDonald, FRBSF Economic Letter, July 23, 2004 This article uses data from several surveys to examine two key aspects of the computer evolution: the spread of PCs at work and the evolving wage differentials between individuals who use them and those who do not. Unit 4: Computers, People, and Social Participation Unit Overview 15. Back-to-School Blogging, Brock Read, The Chronicle of Higher Education, September 3, 2004 It should surprise no one that entering freshmen, who grew up using the Internet, should turn to university-sponsored blogs to ease the transition to college life. 16. Romance in the Information Age, Christine Rosen, The New Atlantis, Winter 2004 According to Rosen, our technologies enable and often promote two detrimental forces in modern relationships: the demand for total transparency and a bias toward the over sharing of personal information. 17. E-Mail Is for Old People, Dan Carnevale, The Chronicle of Higher Education, October 6, 2006 Reaching students through e-mail has become more difficult as students turn to text messaging and social networking sites. 18. Girl Power, Chuck Salter, Fast Company, September 2007 How does a seventeen year old run a million dollar web site 19. Bloggers against Torture, Negar Azimi, The Nation, February 19, 2007 Authoritarian regimes can't always operate in secret, now that bloggers are writing. Unit 5: Societal Institutions: Law, Politics, Education, and the Military Unit Overview 20. Piracy, Computer Crime, and IS Misuse at the University, Timothy Paul Cronan, C. Bryan Foltz, and Thomas W. Jones, Communications of the ACM, June 2006 Who are the students who openly admit to illegally installing software on home computers or otherwise misusing computer information systems This article provides some clues. 21. Can Blogs Revolutionize Progressive Politics , Lakshmi Chaudhry, In These Times, February 2006 Liberals have been envious ever since Richard Viguerie's computer-generated mailing lists contributed to Ronald Reagan's victory in 1980. At a time when even Senate Majority Leader Harry Reid has a blog, some Democrats hope that the computer is finally on their side. 22. Center Stage, Carl Sessions Stepp, American Journalism Review, April/May 2006 How does a newspaper's web version differ from the print version Unlike the print version of a newspaper, the Web version receives little editing. 23. The Coming Robot Army, Steve Featherstone, Harper's Magazine, February 2007 Within our lifetime, says Featherstone, robots will give us the ability to wage war without committing ourselves to the human cost of actually fighting a war. Sgt. Jason Mero concurs: These things are amazing. . . . They don't complain. . . . They don't cry. They're not scared. This robot here has no fear. 24. A Technology Surges, David Talbot, Technology Review, March/April 2008 Real live soldiers still fighting real live wars. A new on-ground reporting system, Google Maps for the Iraq counterinsurgency might help keep these soldiers safe. 25. Wikipedia in the Newsroom, Donna Shaw, American Journalism Review, February/March 2008 Whether professionals can cite a source that is collective and anonymous remains problematic. 26. E-Mail in Academia: Expectations, Use, and Instructional Impact, Meredith Weiss and Dana Hanson-Baldauf, EDUCAUSE Quarterly, January-March 2008 Studies have shown that there is a relationship between a student's success and the quality of one-on-one communication between teacher and student. What happens when you add e-mail to the mix Unit 6: Risk and Avoiding Risk Unit Overview 27. Why Spyware Poses Multiple Threats to Security, Roger Thompson, Communications of the ACM, August 2005 Harm caused by spyware ranges from gobbling up computer speed on your PC to enlisting your machine in attacks that can disrupt major businesses or the government. 28. The Virus Underground, Clive Thompson, The New York Times Magazine, February 8, 2004 Clive Thompson states, when Mario is bored . . . he likes to sit at his laptop and create computer viruses and worms. 29. False Reporting on the Internet and the Spread of Rumors: Three Case Studies, Paul Hitlin, gnovis, April 26, 2004 Internet news sources can sometimes be unreliable. Paul Hitlin examines Internet coverage of the Vince Foster suicide along with other stories to understand why this is so. 30. The New Right-Wing Smear Machine, Christopher Hayes, The Nation, November 12, 2007 Some e-mails that have gone viral in recent political campaigns. 31. A Growing Watch List, Karen DeYoung, The Washington Post National Weekly Edition, April 2-8, 2007 The Terrorist Identities Datamart Environment database contains information on over 450,000 persons, many of them U.S. citizens. What happens if there is an error Unit 7: International Perspectives and Issues Unit Overview 32. China's Tech Generation Finds a New Chairman to Venerate, Kevin Holden, Wired, May 24, 2007 The new China is not a place that would have made Chairman Mao comfortable. One indication is the popularity of Bill Gates. 33. Restoring the Popularity of Computer Science, David A. Patterson, Communications of the ACM, September 2005 While India turns out more and more programmers who are willing to work for a fraction of their American counterparts, enrollment in computer science classes across the United States is dropping. The author believes that inaccurate impressions of opportunities are behind the decline. 34. China's Computer Wasteland, Benjamin Joffe-Walt, The Progressive, January 30, 2005 What to do with the detritus of the digital age is a growing problem. Shipping it to China seems to be one solution. 35. In Search of a PC for the People, Bruce Einhorn, BusinessWeek, June 12, 2006 What features get included in a $200.00 PC marketed to developing nations and Nicholas Negroponte's remark, I think of digital access for kids as a human right, are two issues explored in this article. 36. In Korea, a Boot Camp Cure for Web Obsession, Martin Fackler, The New York Times, November 18, 2007 In a country where online gaming is a professional sport, up to 30% of South Koreans under 18 . . . are at risk of Internet addiction. 37. New Tech, Old Habits, Moon Ihlwan and Kenji Hall, BusinessWeek, March 26, 2007 Japan and South Korea are behind the United States when it comes to the productivity of information technology workers. Why The answer may be as simple as telecommuting. Unit 8: The Frontier of Computing Unit Overview 38. A Nascent Robotics Culture: New Complicities for Companionship, Sherry Turkle, AAAI Technical Report, July 2006 What is a robot kind of love and What will we be like, what kind of people are we becoming as we develop increasingly intimate relationships with machines MIT's pioneering sociologist tries to answer both questions. 39. Toward Nature-Inspired Computing, Jiming Liu and K. C. Tsui, Communications of the ACM, October 2006 Computer scientists are turning to biology as a source of inspiration for models of complex systems. These biological models change the rules governing systems behavior. 40. Google and the Wisdom of Clouds, Stephen Baker, BusinessWeek, December 24, 2007 Google is teaching researchers around the world how to extract patterns using clusters of computers that it calls, the cloud. Test-Your-Knowledge Form Article Rating Form",,De Palma P,,2009,,,,Book
Private Security and the Law,"Private Security and the Law, 4th Edition, is a unique resource that provides analysis of practices in the security industry as they relate to law, regulation, licensure, and constitutional questions of case and statutory authority. This book describes the legal requirements faced in the area of private security. It emphasizes the liability problems common to security operations, including negligence and tortious liability, civil actions frequently litigated, and strategies to avoid legal actions that affect business efficiency. The text also examines the constitutional and due-process dimensions of private security both domestically and internationally, including recent cases and trends that will set pace for future private security laws and regulations. As private security becomes more closely involved in national and international security, cases like Blackwater are examined. Charles Nemeth takes you step by step through the analysis of case law as it applies to situations commonly faced in the private security practice, providing a solid introduction to the legal and ethical standards that shape the industry. *Authoritative, scholarly treatise sheds light on this increasingly important area of the law *Historical background helps readers understand the present by seeing the full context of recent developments *National scope provides crucial parameters to security practitioners throughout the US *NEW TO THIS EDITION! A chapter on the legal implications of private contractors operating in war zones like Afghanistan, updated coverage of statutory authority, updated coverage of state and federal processes of oversight and licensure, special analysis of public-private cooperative relationships in law enforcement Table of Contents Chapter 1: Historical Foundations of Private Security Chapter 2: Regulation, Licensing, Education, and Training: The Path to Professionalism in the Security Industry Chapter 3: The Law of Arrest, Search, and Seizure: Applications in the Private Sector Chapter 4: Civil Liability of Security Personnel Chapter 5: Criminal Liability of Security Personnel Chapter 6: The Enforcement of Laws and the Collection, Preservation and Interpretation of Evidence Chapter 7: Public and Private Law Enforcement: A Blueprint for Cooperation Chapter 8: Selected Case Readings Appendix 1 - Florida Statutes Appendix 2 - List of Associations and Groups Appendix 3 - Sample Forms Appendix 4 - The Law Enforcement-Private Security Consortium, Operation Partnership: Trends and Practices in Law Enforcement and Private Security Collaborations 119-122 (2005).",,Nemeth C,,2011,,,,Book
"Social Media Time Series Forecasting and User-Level Activity Prediction with Gradient Boosting, Deep Learning, and Data Augmentation","In the overall history of technological innovations, social media has only existed for a brief time, however its influence is undeniable. Researchers have found that it can be used to influence elections, spread health misinformation, and aid with financial pump-and-dump schemes. Keeping all this in mind, it is clear that more research is needed to predict the spread of information on social media in order to combat its malicious use.To that end, in this dissertation, we explore the use of Machine Learning algorithms to perform time series forecasting and user-level activity prediction in social media. We address the different types of challenges that come with predicting social media activity such as (1) accounting for the differences in user engagement among different social media platforms, (2) identifying the data required for accurate predictions, (3) selecting the appropriate prediction framework, and (4) metric selection.We address the aforementioned challenges in multiple ways. Firstly, we introduce an end-to-end simulator called the Volume Audience Match simulator, or VAM. VAM is comprised of two modules called the (1) Volume Prediction Module and (2) the User-Assignment Module. VAM performs both time series prediction and user-assignment. It predicts the overall volume time series of (1) new users, (2) old users, and (3) activities. It then assigns the predicted actions to both old and new users over time.We evaluate VAM's predictive prowess on 2 geopolitical datasets: the Venezuela 2019 Twitter dataset (Vz19), and the China-Pakistan Economic Corridor Twitter dataset (CPEC). We show that VAM outperforms various traditional time series baselines for the Volume-Prediction task, specifically the Persistence Baseline, ARIMA, ARMA, AR, and MA models. We show that it outperforms the Persistence Baseline and several state-of-the-art embedding methods for the user-assignment task, specifcally, tNE-node2vec-S, tNE-node2vec-H, and tNE-DeepWalk.We also find that exogenous features from Reddit and YouTube improve VAM's time series prediction accuracy. Furthermore, we perform an in-depth analysis of VAM's performance using a wide-range of metrics that analyze many dimensions of the resulting predictions, such as magnitude, burstiness, temporal pattern matching, user-level prediction accuracy, and overall network structure. Lastly, we compare the XGBoost-based VAM models to the Recurrent Neural Network-based (RNN) VAM models, and find that the XGBoost models are much faster to train and more accurate. This is notable because RNNs are one of the most frequently used machine learning algorithms for social media prediction. Perhaps this insight will prompt other researchers to consider using XGBoost for their own modeling purposes instead of RNNs.We also introduce a variant of VAM that performs data-augmentation called SMOTER- VAM. This version of VAM utilitzes data-augmentation as a prepreprocessing step via 2 different algorithms: SMOTER-Binning (SMOTER-B) and SMOTER-NB (No-Binning). These two algorithms are variants of the SMOTER algorithm (Synthetic Minority Oversampling Technique for Regression).Two different VAM models are trained on the 2 augmented datasets. We found that using the SMOTER-B and SMOTER-NB algorithms improve VAM's performance on time series prediction, especially on low-volume topics. These SMOTER variations are also generalizable to any machine learning algorithm and any dataset that has multiple-continuous outputs. Therefore, these variations can have many potential applications beyond VAM or social media time series prediction usage.Lastly, we analyze the differences between 2 commonly used baselines within the realm of social media prediction- the Persistence Baseline and ARIMA models. We evaluate their performances on different datasets and in different contexts, and through our analysis, we better understand which situations the baselines are useful and why.",,"Mubang F,Adriana Iamnitchi,Yu Sun,John Skvoretz,Mingyang Li",,2022,,,,Ph.D. Thesis
Generation of High-Resolution Mosaic for Photo-Realistic Texture-Mapping of Cultural Heritage 3D Models,"The work investigates the problem of how information contained in different overlapping images of a scene can be combined to produce larger images of higher quality. The resulted images can be used for different applications like forensic image analysis, computer animation, special effects, 3D model texture mapping or panorama mosaic. In our case, high-resolution image mosaics of mural frescos are required for the texturing of a 3D model that will be used in a movie production. We developed a novel method for the derivation of a high quality mosaic using multi-resolution and multi-temporal images acquired from arbitrary positions and cameras. This method named 'constrained mesh-wise affine transformation' allows for seamless enhancement of the scene in the areas where higher resolution images are available. In this paper, we also discuss alternative procedures for the texture mapping of a 3D model using existing multi-resolution and multi-temporal imagery. The work has been done within a project aimed at a virtual and physical reconstruction of the destroyed Buddha statues of Bamiyan, Afghanistan.",,"Remondino F,Niederoest J",,2004,85–92,,,Conference Paper
"Development and Design of a Unified Remote Video Surveillance System for Homes, Using Free Software Tools","In this article we present the design and implementation of a prototype for video surveillance that allows to manage IP cameras from different manufacturers through a single application implemented with free software tools and free hardware. Several currently existing applications work with proprietary applications and IP cameras from the same manufacturer, however, in this article we demonstrate that management is more efficient through our unique system that allows generating remote alerts through SMS messages and notifications by electronic mail after the activation of a sensor. Our article describes the existing problems in residential security systems, applied to the case of the City of Cuenca-Ecuador, as well as the technical development of the system in relation to server configuration, client equipment and an Android application developed in IONIC Framework. Finally, we describe the results of the different connectivity tests of the system generating events through the internet cloud, to determine performance and connectivity times in a real operating environment.",,"Abril B,Jara JD,Cuzco P,Gallegos P",,2020,,10.1145/3387168.3387194,https://doi-org.proxy.bnl.lu/10.1145/3387168.3387194;http://dx.doi.org/10.1145/3387168.3387194,Conference Paper
A Terrain Risk Assessment Method for Military Surveillance Applications for Mobile Assets,"We developed a terrain risk assessment method for military surveillance.We developed the method for UAVs used for military combat operating posts.We modeled the terrain by calculating 5 risk factors using geographical parameters.We modeled high risk spots for methodical UAV surveillance plans.We tested the method using real-life scenario data provided by the sponsor. This study proposes an analytical and flexible terrain risk assessment method for military surveillance applications for mobile assets. Considering the risk as the degree of possibility of insurgent presence, the assessment method offers an efficient evaluation of risk in the surrounding terrain for military combat operating posts or observation posts. The method is designed for unmanned aerial vehicles as the surveillance assets of choice to improve the effectiveness of their use. Starting with the area map and geographical data, the target terrain is first digitized for space representation. Then the data of nine geographical parameters are used to formulate five contributing risk factors. These factors are incorporated in an analytical framework to generate a composite map with risk scores that reveal the potential high-risk spots in the terrain. The proposed method is also applied to a real-life case study of COP Kahler in Afghanistan, which was a target for insurgent attacks in 2008. The results confirm that when evaluated with the developed method, the region that the insurgents used to approach COP Kahler has high concentration of high-risk cells.",,"Buyurgan N,Lehlou N",,2015,88–99,10.1016/j.cie.2015.06.025,https://doi-org.proxy.bnl.lu/10.1016/j.cie.2015.06.025;http://dx.doi.org/10.1016/j.cie.2015.06.025,Journal Article
Design and Implementation of a Non-Ionizing Radiation Measuring System Evaluated with an Unmanned Aerial Vehicle,"Nowadays the growing number of mobile phones has increased the number of stations needed in order to improve the quality of service for users. This growth of cellular antennas has caused a degree of worry and fear among citizens due to locations where they are installed, ever closer to our homes. The International Commission on Non-Ionizing Radiation Protection (ICNIRP) has established maximum limits that must be met to ensure that no negative effects on health will be generated to people. In Perú, informality regarding delivery of authorizations, control and monitoring of cellular antennas, is one of the causes of insecurity. The uncertainty about the harmful effects that might result in a society surrounded by stations grows. This paper focuses on the design and implementation of a system for measuring non-ionizing radiation of cell phone antennas in order to verify compliance with the maximum permissible limits. The design is made in the downlink ranges bands of 850 MHz and 1900 MHz, and measurement tests were conducted in two stages: at ground level and mounted on an UAV flying around a cell phone antenna.",,"Prado GV,Medina MA",,2015,52–57,10.1109/APCASE.2015.17,https://doi-org.proxy.bnl.lu/10.1109/APCASE.2015.17;http://dx.doi.org/10.1109/APCASE.2015.17,Conference Paper
"Proposed Framework of Smart Transportation in Pakistan: Issues, Challenges, Vulnerabilities,and Solutions","This paper proposes a framework for a smart transportation system of Pakistan and discusses modern approaches as solutions to emerging threats and vulnerabilities of STS. In addition, STS changes the life of people and decreases the number of accidents, deaths, and traffic incidents. STS is saving the time of users and makes the urban city even smarter. The aim of STS is to accomplish more efficiency of traffic by decreasing traffic issues. It also provides the information about route traffic toward destination, local expediency, passing, vehicle dynamic or requested information, availability of seats to users, which decreases journey time of commuters and improves their security and ease. This paper discusses STS, its application and working mechanism, and the main parameters of the proposed framework of STS of Pakistan such as road condition monitoring, traffic management, municipal involvement, link data for society, accidental measures, and security, which are necessary while designing or implementing such systems.",,"Awan JH,Memon S,Shah AA,Pathan KT",,2020,48–63,10.4018/IJCWT.2020100104,https://doi-org.proxy.bnl.lu/10.4018/IJCWT.2020100104;http://dx.doi.org/10.4018/IJCWT.2020100104,Journal Article
A Flexible Service-Oriented Approach to Address Hydroinformatic Challenges in Large-Scale Hydrologic Predictions,"Water security is defined as a combination of water for achieving our goals as a society, and an acceptable level of water-related risks. Hydrologic modeling can be used to predict streamflow and aid in the decision-making process with the goal of attaining water security.Developed countries usually have their own hydrologic models; however, developing countries often lack hydrologic models due to factors such as the maintenance, computational costs, and technical capacity needed to run models. A global streamflow prediction system (GSPS) would help decrease vulnerabilities in developing countries and fill gaps in areas where no local models exist by providing extensive results that can be filtered for specific locations.The development of a GSPS has been deemed a grand challenge of the hydrologic community. To this end, many scientists and engineers have started to develop large-scale systems to an acceptable degree of success. Renowned models like the Global Flood Awareness System (GloFAS), the US National Water Model (NWM), and NASA's Land Assimilation System (LDAS) are proof that our ability to model large areas has improved remarkably. Even so, during this evolution the hydrologic community has started to realize that having a large-scale forecasting system does not make it immediately useful. New hydroinformatic challenges have surfaced that prevent these models from reaching their full potential. I have divided these challenges in four main categories: big data, data communication, adoption, and validation.I present a description of the background leading to the development of a GSPS including existing models, and the components needed to create an operational system. A case study with the NWM is also presented where I address the big data and data communication challenges by developing cyberinfrastructure and accessibility tools such as web applications and services.Finally, I used the GloFAS-RAPID model to create a forecasting system covering Africa, North America, South America, and South Asia using a service-oriented approach that includes the development of web applications, and services for providing improved data accessibility, and helping address adoption and validation challenges. I have developed customized services in collaboration with countries that include Argentina, Bangladesh, Colombia, Peru, Nepal, and the Dominican Republic. I also conducted validation tests to ensure that results are acceptable. Overall, a model-agnostic approach to operationalize a GSPS and provide meaningful results at the local level is provided with the potential to allow decision makers to focus on solving some of the most pressing water-related issues we face as a society.",,Souffront Alcantara MA,,2018,,,,Ph.D. Thesis
Improving Security Awareness in the Government Sector,"The increasing use of internet and smart mobile devices for accessing, storing and generating sensitive e-government data makes them an attractive attack vector for cyber-criminals. New technology adoption typically requires specialized training and awareness campaigns where people acquire new skills, learn about best practices and potential pitfalls of adoption. In this paper, we present results from a survey in Pakistan to help understand the level of cyber security awareness and understanding in the Government sector. The goal of the survey is to help identify at-risk demographics, problems, risks and key areas of concern that need to be addressed through customized education material and trainings. We then discuss design strategies to reduce the security and privacy risk faced by mobile device users of Government departments.",,"Amjad HA,Naeem U,Zaffar MA,Zaffar MF,Choo KK",,2016,1–7,10.1145/2912160.2912186,https://doi-org.proxy.bnl.lu/10.1145/2912160.2912186;http://dx.doi.org/10.1145/2912160.2912186,Conference Paper
Analysis of Cybersecurity Models Suitable to Apply in an Electoral Process in Ecuador,"Were analyzed different cybersecurity proposals to protect information, such as models, prototypes, approaches, frameworks, algorithms and evaluations. The problem is the lack of application of cybersecurity to electoral processes in Ecuador. The objective is to make an appropriate cybersecurity model to apply in an electoral process in Ecuador. The quantitative, descriptive, deductive reasoning was used to analyze the reference documents. It turned out a Conceptual Cybersecurity Model, a Cybersecurity Algorithm and a General Risk Formula. It was concluded that the model strengthens information security in the electoral process and asset risk assessment prioritizes the attention of vulnerabilities.",,"Toapanta SM,Armijos MA,Gallegos LE",,2020,84–90,10.1145/3375900.3375912,https://doi-org.proxy.bnl.lu/10.1145/3375900.3375912;http://dx.doi.org/10.1145/3375900.3375912,Conference Paper
Counter-Terrorism Financing: The Case of the Islamic State (ISIS),"Since the Islamic State (ISIS), an offshoot of Al-Qaida in Iraq, transformed itself into a lethal independent terrorist organization in 2012, the threat of terrorism has increased significantly. ISIS seized territory and took populations hostage across Iraq and Syria, unleashing untold suffering to victims. Within 4 years, the organization expanded its terror networks worldwide, including to the United States. The increased terrorism threat posed by ISIS prompted individual countries such as the U.S. to implement counter-terrorism financing measures to safeguard U.S. interests abroad and in the homeland and the United Nations to impose sanctions pursuant to United Nations Security Council resolution 2253 (2015).This research study examined perceptions of UN-imposed sanctions on ISIS held by Member States of the United Nations. A quantitative research method in the form of a survey was used to collect data from 90 participants representing 90 member countries of the United Nations at the UN headquarters in New York. The survey data were subsequently analyzed quantitatively. Descriptive and inferential statistics were applied. Various tests, including correlations, chi square, t test, and ANOVA, were performed to examine relationshipsResearch findings suggest that the majority of Member States (95%) perceived ISIS sanctions as essential tools for fighting terrorist finances. However, Member States' perceptions of the adequacy, effectiveness, and management of sanctions varied across Financial Action Task Force-Style Regional Bodies. Also, the findings revealed that UN Member State representatives were reluctant to express their views on how the UN Security Council managed sanctions. Moreover, the results confirmed that while the UN requires Member States to implement all three tools prescribed in the sanctions (i.e., asset freeze, arms embargo, and travel ban), perceptions varied on the most effective tool to deal with terrorist finances.To strengthen the impact of UN sanctions on terrorist finances, this study recommends that the UN invest in increasing Member States' buy-in. The UN Security Council could benefit from this study by drawing on Member States' perceptions of the workings of the UNSC to improve trust and legitimacy.Future research is recommended to investigate UN Member States' terrorism risk perceptions as a yardstick for determining global support for UN counter-terrorism sanctions, the relationship between UN sanctions and unilateral sanctions, and the management of the UN sanctions list.",,"Tanui DJ,Fisher S,Molnar L",,2020,,,,Ph.D. Thesis
Assessing Security and Ipa in Afghanistan: A Comparative Case Study on the Assessment of Security and Internal Protection Alternative in Sweden and Norway,"It is common by states to deny asylum for asylum-seekers with the argument that the applicant could find protection within their own country of residence instead of receiving international protection. This is called internal protection alternative (IPA). This research is a comparative case study and aims to explore and compare two neighbouring countries, Sweden and Norway, on how their immigration authorities differ in their assessment on both the security situation and IPA in Afghanistan. Furthermore, this thesis aims to compare the Swedish and Norwegian immigration authorities with international laws, agreements and guidelines which, therefore, is the conceptional framework for this research.This study concludes that IPA is not mentioned in the 1951 Refugee Convention and that there are no clear directives on how to apply it. States tend to interpret the already existing laws and guidelines in their own way. The result of this is that there are differences between states practice and the consequence could be that asylum-seekers could receive different assessments and decisions from different countries.This research is, therefore, highly relevant from a humanitarian- and academia perspective as it highlights differences in national practice which is crucial since these differences will affect the refugee situation of individuals and the possibility of obtaining asylum.",,Kjellberg Stjernström I,,2020,,,,Ph.D. Thesis
"Flexible Options for Cyber Deterrence - Terrorism, Problem of Attribution, Cyber Attack, Espionage, Defense, Nation State Peer Competitors, China Conflict, SCADA, Network Equipment","This excellent report has been professionally converted for accurate flowing-text e-book format reproduction. This paper describes options for cyber deterrence to address both asymmetric threats from terrorists and the intimidation associated with nation-state peer competitors in the cyber domain. It presents recent National Security Strategy interests and demonstrates a lack of focus upon cyber infrastructure. The paper will examine challenges associated with legal aspects in the cyber domain as well as the issue of attribution. It will analyze terrorist and nation-state usage of cyberspace and potential threats aimed at the United States related to each. Finally, the paper concludes with several recommendations for tailored cyber deterrence focused on terrorists and peer nation-states. The idea of deterrence has existed since the beginning of humanity. Lawrence Freedman in his book Deterrence uses the biblical tale of Adam, Eve, and the forbidden fruit as an example of deterrence. Webster defines deterrence as ""maintenance of military power for the purpose of discouraging attack."" The threat of war has always been a tool used by leaders to influence foreign powers to avoid acts of aggression. Ultimately, deterrence became synonymous with American Cold War strategic thinking and foreign policy. Mutually assured destruction was a classic adoption of deterrence through punishment. However, deterrence through punishment requires the demonstration of offensive capabilities. The highly classified nature of the United States cyber-based offensive tools makes this approach unlikely. In addition, deterrence by punishment does not work without identification and attribution. Lastly, any assumption of rationality demonstrates the fallacy of Cold War deterrence applied to the cyber domain. Today's multi-polar world provides multiple threats aimed at the United States in the cyber domain. From cyber terrorists to sophisticated nation-states, adversaries are increasing their cyber capabilities on a daily basis. Some argue for an offensive cyber doctrine of preemption, but as demonstrated in Iraq, preemption can be destabilizing. Acts of war may justify an offensive response, but conventional or nuclear deterrence is more appropriate when attempting to deter aggression defined by war. Complicating cyberspace deterrence is the lack of attribution, no traditional constraints associated with rational behavior of extremists, and a deficient United States cyber national strategy. The next chapter of this paper reviews recent United States strategies and critical cyber infrastructure, attribution in the cyber domain, and cyber espionage. Chapter three provides analysis of cyber terrorism and nation-state operations in the cyber domain. Chapter four describes cyber deterrence recommendations aimed at countering terrorists as well as United States peer competitors. The final chapter presents conclusions.",,"Government US,Military US,of Defense (DoD) D,(usn) US",,2017,,,,Book
A Methodological Tool for Asset Identification in Web Applications: Security Risk Assessment,"Security risk assessment in Web Engineering is an emerging discipline, where security is given a special attention, allowing software engineers to develop high quality and secure Web-based applications. A preliminary study revealed that asset identification (and evaluation) is an essential phase in risk assessment practices. This phase represents a degree of complexity and is the primary activity in the assessment process. This work focuses on asset identification and contributes to security risk assessment, which is essential part of software security. Specifically, the research goal is to design a methodological tool (instrument) for asset identification in web applications for the purpose of risk assessment. The proposed tool helps identify assets with security risks in web applications. The tool involves direct observations and survey questionnaires as data collection techniques used for this work. The research methodology is based on qualitative and quantitative analysis of a case study that focused on web-based application for Student Opinion Survey Coordination (EOE) developed in Simón Bolívar University, Venezuela. The data analysis required the use of cross-case analysis supported by the software application MAXQDA2007, which helps identify assets according to categories, such as Environment, Software, Hardware, Information and Networks. Under this work, students, faculty, staff, and software developers at Simón Bolívar University have participated in this study.",,"M. BD,Haddad HM,A. JE",,2009,413–418,10.1109/ICSEA.2009.66,https://doi-org.proxy.bnl.lu/10.1109/ICSEA.2009.66;http://dx.doi.org/10.1109/ICSEA.2009.66,Conference Paper
Data Mining Applications Using Artificial Adaptive Systems,"This volume directly addresses the complexities involved in data mining and the development of new algorithms, built on an underlying theory consisting of linear and non-linear dynamics, data selection, filtering, and analysis, while including analytical projection and prediction. The results derived from the analysis are then further manipulated such that a visual representation is derived with an accompanying analysis. The book brings very current methods of analysis to the forefront of the discipline, provides researchers and practitioners the mathematical underpinning of the algorithms, and the non-specialist with a visual representation such that a valid understanding of the meaning of the adaptive system can be attained with careful attention to the visual representation. The book presents, as a collection of documents, sophisticated and meaningful methods that can be immediately understood and applied to various other disciplines of research. The content is composed of chapters addressing: An application of adaptive systems methodology in the field of post-radiation treatment involving brain volume differences in children;A new adaptive system for computer-aided diagnosis of the characterization of lung nodules;A new method of multi-dimensional scaling with minimal loss of information;A description of the semantics of point spaces with an application on the analysis of terrorist attacks in Afghanistan;The description of a new family of meta-classifiers;A new method of optimal informational sorting;A general method for the unsupervised adaptive classification for learning; and the presentation of two new theories, one in target diffusion and the other in twisting theory.""",,Tastle WJ,,2016,,,,Book
Data Mining Applications Using Artificial Adaptive Systems,"This volume directly addresses the complexities involved in data mining and the development of new algorithms, built on an underlying theory consisting of linear and non-linear dynamics, data selection, filtering, and analysis, while including analytical projection and prediction. The results derived from the analysis are then further manipulated such that a visual representation is derived with an accompanying analysis. The book brings very current methods of analysis to the forefront of the discipline, provides researchers and practitioners the mathematical underpinning of the algorithms, and the non-specialist with a visual representation such that a valid understanding of the meaning of the adaptive system can be attained with careful attention to the visual representation. The book presents, as a collection of documents, sophisticated and meaningful methods that can be immediately understood and applied to various other disciplines of research. The content is composed of chapters addressing: An application of adaptive systems methodology in the field of post-radiation treatment involving brain volume differences in children;A new adaptive system for computer-aided diagnosis of the characterization of lung nodules;A new method of multi-dimensional scaling with minimal loss of information;A description of the semantics of point spaces with an application on the analysis of terrorist attacks in Afghanistan;The description of a new family of meta-classifiers;A new method of optimal informational sorting;A general method for the unsupervised adaptive classification for learning; and the presentation of two new theories, one in target diffusion and the other in twisting theory.",,Tastle WJ,,2012,,,,Book
Food System Resilience and Sustainability in Cambodia,"Cambodia is witnessing a “Goldilocks moment” in demographic change concurrent with shifts in land use, hydrology, and climate. These trends interact and affect food production, food costs, and food security. Drivers of these trends are typically examined separately with interacting factors considered along disciplinary margins. While science models to explore these interacting effects have been proposed, there remains an applied research gap in integrating these pieces and assessing interdisciplinary opportunities for developing food security solutions. Developed following a request from USAID to elucidate food security conditions in Cambodia, here the authors present their geospatial synthesis of the biophysical and socioeconomic drivers of current food security risk, as well as explore future trends for those conditions. The overall structure shows several interlocking or mutually reinforcing trends in systems that point towards a significant intensification of food insecurity in the near future. They offer an assessment of future targets for food systems innovation.",,"Messina J,Suepa T,Snapp S,Olson J,Nejadhashemi AP,Murray S,Moore N,Frake A,Fan P,Adhikari U",,2022,1–23,10.4018/ijagr.2017070104,https://doi-org.proxy.bnl.lu/10.4018/ijagr.2017070104;http://dx.doi.org/10.4018/ijagr.2017070104,Journal Article
Wireless Communication as a Reshaping Tool for Internet of Things (IoT) and Internet of Underwater Things (IoUT) Business in Pakistan: A Technical and Financial Review,"Pakistan is one of the growing nations, specifically in the field of Information and Communication Technology (ICT). During the last decade, an intense rise in the adaptation of ICT has been observed in all the major cities of Pakistan. This includes, but not limited to, e-commerce, mobile technology, computer communication networks, embedded systems, software engineering, etc. Due to the resource constraints, Pakistan is not the producer of any technology; however, it is a potential consumer of numerous technologies and their products. It therefore, attracts most of the producers around the globe to invest in the technology business in Pakistan. According to the Board of Investment (BOI) Pakistan, the country has received more than US$5.7 billion during the last decade as the foreign investment in IT and Telecommunication sectors only. Moreover, it has more than 140 million cellular subscribers, around 45 million 3G/4G subscribers, more than 3 million fixed local line subscribers and approximately 48 million broadband subscribers [1]. Likewise, Pakistan is also one of the biggest buyers of Consumer Electronics (CE). Very few of the local companies are producing CE products, however, a major share of the CE market has been captured by the international brands of China, Japan, Korea, USA, Germany, etc. In the light of the facts, it can be inferred that the application of ICT such as the Internet of Things (IoT) and Internet of Underwater Things (IoUT) in consumer electronics has the strong potential in shaping a new dimension of CE business in Pakistan. Moreover, the recent literature has strongly advocated for the scope of 5G IoT/IoUT. This is due to the fact that existing communication infrastructure will not be sufficient to handle modern day IoT/IoUT need. In this article, a comprehensive study on the scope of IoT/IoUT enabled consumer electronics business is presented. In addition, the rationale of 5G IoT/IoUT integration in the developing countries like Pakistan is discussed. Moreover, the threats and opportunities in the business of IoT/IoUT enabled CE devices are also been presented. Finally, this study submits the recommendations to establish IoT/IoUT enabled CE business in Pakistan.",,"Rizvi SS,Zubair M,Ahmad J,Hashmani M,Khan MW",,2021,1087–1105,10.1007/s11277-019-06937-3,https://doi-org.proxy.bnl.lu/10.1007/s11277-019-06937-3;http://dx.doi.org/10.1007/s11277-019-06937-3,Journal Article
Annual Editions: Computers in Society 08/09,"This Fourteenth Edition of ANNUAL EDITIONS: COMPUTERS IN SOCIETY provides convenient, inexpensive access to current articles selected from the best of the public press. Organizational features include: an annotated listing of selected World Wide Web sites; an annotated table of contents; a topic guide; a general introduction; brief overviews for each section; a topical index; and an instructor’s resource guide with testing materials. USING ANNUAL EDITIONS IN THE CLASSROOM is offered as a practical guide for instructors. ANNUAL EDITIONS titles are supported by our student website, . Table of contents UNIT 1. Introduction 1. 34991 Five Things We Need to Know About Technological Change, Neil Postman, Address to New Tech ’98 conference, March 27, 1998 Neil Postman, a well-known cultural critic, suggests that computer technology is too important to be left entirely to the technologists. “Embedded in every technology,” he says, “is a powerful idea….” 2. 46656 Slouching Toward the Ordinary, Susan C. Herring, New Media & Society, February 2004 Contrary to what we read, changes in the ecology of the computing “will continue to make the internet a simpler, safer and—for better or worse—less fascinating communication environment.” 3. 41735 On the Nature of Computing, Jon Crowcroft, Communications of the ACM, February 2005 The author states, “ Occupying a third place in human intellectual culture, computing is not bound by the need to describe what does exist (as in natural science) or what can be built in the real world (as in engineering).” UNIT 2. The Economy 4. 46657 The Subprime Loan Machine, Lynnley Browning, The New York Times, March 23, 2007 “The rise and fall of the subprime market has been told as a story of a flood of Wall Street money and the desire of Americans desperate to be part of the housing boom,” says Lynnley Browning. Yet, the boom was made possible “by a little-noticed tool of automatic underwriting software.” 5. 46658 Click Fraud, Brian Grow and Ben Elgin, Business Week, October 2, 2006 Internet advertisers think they pay only when an interested customer clicks on their ads. Martin Fleischman, an Atlanta businessman, “noticed a growing number of puzzling clicks coming from such places as Botswana, Mongolia, and Syria.” 6. 41736 The Big Band Era, Christopher Swope, Governing, January 2005 Even as cities like Philadelphia are working to transform the entire city into a wireless hot spot—with government as the internet service provider of last resort—communications companies are fighting to keep local governments out of the broadband business. 7. 45257 The Beauty of Simplicity, Linda Tischler, Fast Company, November 2005 A simple tale about simplicity. One company hired an editor from People Magazine to translate accounting lingo into everyday language, “pared back 125 setup screens to three,” and “sold 100,000 units in its first year on the market.” 8. 41740 The Software Wars, Paul De Palma, American Scholar, Winter 2005 The article argues that software development is like military procurement, and suffers many of the same woes, including excessive complexity and cost overruns. 9. 46659 Scan This Book!, Kevin Kelly, The New York Times Magazine, May 14, 2006 What will happen to libraries, books on paper, and copyright protections if Google’s plans to scan the books of five major research libraries succeeds UNIT 3. Work and the Workplace 10. 46660 National ID, Ryan Singel, Wired, May 15, 2007 Immigration is in the news again. One proposal before Congress is to issue American workers tamper-proof biometric Social Security cards. These would replace the text-only design that’s been issued to Americans almost without change for more than 70 years. 11. 34959 Brain Circulation, AnnaLee Saxenian, Brookings Review, Winter 2002 Do immigrants displace native workers Is the United States siphoning off talent from countries that can ill afford to lose it This Berkeley professor argues that high-skill immigration is more complex than that. 12. 41774 The New Face of the Silicon Age, Daniel H. Pink, Wired, February 12, 2004 This piece on Indian programmers should be enough to keep chairs of American computer science departments awake at night. 13. 46661 Computer Software Engineers, Occupational Outlook Handbook, 200607 Edition Here is one official source that acknowledges the effect of shipping high tech jobs abroad, but still predicts that “ software engineers are projected to be one of the fastest-growing occupations from 2004 to 2014.” 14. 41753 The Computer Evolution, Rob Valletta and Geoffrey MacDonald, FRBSF Economic Letter, July 23, 2004 This article uses data from several surveys “to examine two key aspects of the computer evolution: the spread of PCs at work and the evolving wage differentials between individuals who use them and those who do not.” 15. 41754 Making Yourself Understood, Stuart Crainer and Des Dearlove, Across the Board, MayJune 2004 In a business environment where half of surveyed managers report spending more than two hours each day answering e-mail, “it’s never been so easy to be misunderstood.” 16. 46664 Privacy, Legislation, and Surveillance Software, G. Daryl Nord, Tipton F. McCubbins, and Jeretta Horn Nord, Communications of the ACM, August 2006 The authors tell us that the assumption of employee privacy in the workplace “may be naïve.” Constitutional protections against unreasonable search and seizure “usually apply only to state actions.” UNIT 4. Computers, People, and Social Participation 17. 40654 Romance in the Information Age, Christine Rosen, The New Atlantis, Winter 2004 According to Christine Rosen, “our technologies enable and often promote two detrimental forces in modern relationships: the demand for total transparency and a bias toward the over sharing of personal information.” 18. 46665 How Do I Love Thee , Lori Gottlieb, The Atlantic, March 2006 Some Internet dating sites now use social scientists to “develop a new science of attraction.” Says the author, “My matches included a film editor wearing a kilt—and not in an ironic way. Was this the best science could do ” 19. 46666 The Perfect Mark, Mitchell Zuckoff, The New Yorker, May 15, 2006 A cautionary tale about an African scam and two years in prison for bank fraud and money laundering. 20. 41755 Back-to-School Blogging, Brock Read, The Chronicle of Higher Education, September 3, 2004 It should surprise no one that entering freshmen, who grew up using the Internet, should turn to university-sponsored blogs to ease the transition to college life. 21. 46667 E-Mail Is for Old People, Dan Carnevale, The Chronicle of Higher Education, October 6, 2006 Reaching students through email has become more difficult as students turn to text-messaging and social networking sites. UNIT 5. Societal Institutions: Law, Politics, Education, and the Military 22. 37230 The Copyright Paradox, Jonathan Band, Brookings Review, Winter 2001 According to the author, “the problem with piracy is not the inadequacy of existing laws, but the high cost of enforcing any law against the large universe of infringers.” 23. 46668 Piracy, Computer Crime, and IS Misuse at the University, Timothy Paul Cronan, C. Bryan Foltz, and Thomas W. Jones, Communications of the ACM, June 2006 Who are the students who “openly admit to illegally installing software on home computers or otherwise misusing computer information systems ” This article provides some clues. 24. 41764 Facing Down the E-Maelstrom, Jeffrey Selingo, The Chronicle of Higher Education, April 29, 2005 Never an easy job, leading a college in the age of the Internet requires sifting through e-mail, reading blogs, and fending off criticism, the volume of which would be inconceivable without networked computers. 25. 46669 Can Blogs Revolutionize Progressive Politics , Lakshmi Chaudhry, In These Times, February 2006 Liberals have been envious ever since Richard Viguerie’s computer-generated mailing lists contributed to Ronald Reagan’s victory in 1980. At a time when even Senate Majority Leader Harry Reid has a blog, some Democrats hope that the computer is finally on their side. 26. 46670 Center Stage, Carl Sessions Stepp, American Journalism Review, AprilMay 2006 How do a newspaper’s web and print versions differ Unlike the print version of a newspaper, the Web version receives little editing. 27. 46671 The Coming Robot Army, Steve Featherstone, Harper’s Magazine, February 2007 “Within our lifetime,” says Featherstone, “robots will give us the ability to wage war without committing ourselves to the human cost of actually fighting a war.” Sgt. Jason Mero concurs: “These things are amazing…. They don’t complain…. They don’t cry. They’re not scared. This robot here has no fear.” UNIT 6. Risk and Avoiding Risk 28. 41768 Why Spyware Poses Multiple Threats to Security, Roger Thompson, Communications of the ACM, August 2005 Harm caused by spyware ranges from gobbling up computer speed on your PC to enlisting your machine in attacks that can disrupt major businesses or the government. 29. 41769 Terror’s Server, David Talbot, Technology Review, February 2005 “Most experts agree,” says the author, “that the Internet is not just a tool of terrorist organizations, but is central to their operations.” 30. 37238 The Virus Underground, Clive Thompson, The New York Times Magazine, February 8, 2004 Clive Thompson states, “when Mario is bored…he likes to sit at his laptop and create computer viruses and worms.” 31. 46672 Secrets of the Digital Detectives, The Economist, September 23, 2006 It’s nice to learn that the good guys have some tricks of their own. 32. 46673 Data on the Elderly, Marketed to Thieves, Charles Duhigg, The New York Times, May 20, 2007 Thieves purchase lists of the elderly from consumer databases, then pose as government workers trying to update their files on World War II veterans and retired school teachers. Some seniors find themselves with empty bank accounts. 33. 41770 The Fading Memory of the State, David Talbot, Technology Review, July 2005 Government documents, from the 38 million emails generated by the Clinton administration to electronic records of the 1989 invasion of Panama, are on disintegrating electronic media, stored using now-obsolete formats. 34. 41771 False Reporting on the Internet and the Spread of Rumors, Paul Hitlin, Gnovis, April 26, 2004 Internet news sources can sometimes be unreliable. Paul Hitlin examines Internet coverage of the Vince Foster suicide along with other stories to understand why this is so. UNIT 7. International Perspectives and Issues 35. 46675 China’s Tech Generation Finds a New Chairman to Venerate, Kevin Holden, Wired, May 24, 2007 The new China is not a place that would have made Chairman Mao comfortable. One indication is the popularity of Bill Gates. 36. 46677 Is the Crouching Tiger a Threat , Robert L. Glass, Communications of the ACM, March 2006 All indications suggest that the U.S. domination of computing is about to be eclipsed. Here is one commentator who is not quite convinced. 37. 41776 Restoring the Popularity of Computer Science, David A. Patterson, Communications of the ACM, September 2005 While India turns out more and more programmers willing to work for a fraction of their American counterparts, enrollment in computer science classes across the United States is dropping. The author believes that “inaccurate impressions of opportunities” are behind the decline. 38. 41773 China’s Computer Wasteland, Benjamin Joffe-Walt, The Progressive, January 30, 2005 What to do with the detritus of the digital age is a growing problem. Shipping it to China seems to be one solution. 39. 46678 Cat and Mouse, on the Web, The Economist, December 2, 2006 This article examines censorship on the Internet and the extraordinary steps taken by the anti-censorship community to thwart the efforts of censors. 40. 46681 In Search of a PC for the People, Bruce Einhorn, Business Week, June 12, 2006 What features get included in a $200.00 PC marketed to developing nations and “I think of digital access for kids as a human right,” says Nicholas Negroponte of MIT are two issues explored in this article. UNIT 8. The Frontier of Computing 41. 46682 A Nascent Robotics Culture, Sherry Turkle, AAAI Technical Report, July 2006 “What is a robot kind of love ” and “What will we be like, what kind of people are we becoming as we develop increasingly intimate relationships with machines ” MIT’s pioneering sociologist tries to answer both questions 42. 46683 March of the Robolawyers, The Economist, March 11, 2006 Australian researchers have developed a program that helps divorcing couples divide their property. 43. 46684 Best-Kept Secrets, Gary Stix, Scientific American, January 2005 Public-key cryptography keeps e-commerce secure for now. Quantum cryptography might take its place. 44. 46685 Toward Nature-Inspired Computing, Jiming Liu and K.C. Tsui, Communications of the ACM, October 2006 Computer scientists are turning to biology as a source of inspiration for models of complex systems. These biological models change the rules governing systems behavior. 45. 41778 The Intelligent Internet, William E. Halal, The Futurist, MarchApril 2004 The author claims the Internet will be the “main method used in 30% of courses” by 2014. As with all predictions, enjoy, but read critically. 46. 41780 Mind Control, Richard Martin, Wired, March 2005 What does a quadriplegic young man who plays pong have in common with a monkey mentally moving a joy stick and “soldier-controlled killer robots ” The answer: Brain Computer Interface or BCI.",,De Palma P,,2007,,,,Book
Group-Based Password Characteristics Analysis,"In this article, we analyze password characteristics from the perspective of user groups in different countries and web-based services. We collect a dataset from the Chinese railway website www.12306.cn. which contains data from four provinces, Hubei, Zhejiang, Inner Mongolia and Xinjiang. Additionally, we select datasets from two English based Internet applications, Faithwrit-er and Facebook. We analyze these six datasets based on several common indicators, including popular passwords, password structure and letter distribution. The analysis results show that there are remarkable differences in different user groups. The experiments show that geographical factors (embodied in the native language) and types of website services play a significant role in password creation. We further evaluate the security of these passwords by employing two state-of-the-art password cracking techniques. The attack results show that datasets of different provinces and different types of website services have different password strength. To the best of our knowledge, this is the first time passwords are analyzed based on different user groups.",,"He D,Zhou B,Yu H,Cheng Y,Chan S,Zhang M,Guizani N",,2021,311–317,10.1109/MNET.011.2000354,https://doi-org.proxy.bnl.lu/10.1109/MNET.011.2000354;http://dx.doi.org/10.1109/MNET.011.2000354,Journal Article
Timely Detection and Mitigation of Stealthy DDoS Attacks Via IoT Networks,"Internet of Things (IoT) networks consist of sensors, actuators, mobile and wearable devices that can connect to the Internet. With billions of such devices already in the market which have significant vulnerabilities, there is a dangerous threat to the Internet services and also some cyber-physical systems that are also connected to the Internet. Specifically, due to their existing vulnerabilities IoT devices are susceptible to being compromised and being part of a new type of stealthy Distributed Denial of Service (DDoS) attack, called Mongolian DDoS, which is characterized by its widely distributed nature and small attack size from each source. This article proposes a novel anomaly-based Intrusion Detection System (IDS) that is capable of timely detecting and mitigating this emerging type of DDoS attacks. The proposed IDS’s capability of detecting and mitigating stealthy DDoS attacks with even very low attack size per source is demonstrated through numerical and testbed experiments.",,"Doshi K,Yilmaz Y,Uludag S",,2021,2164–2176,10.1109/TDSC.2021.3049942,https://doi-org.proxy.bnl.lu/10.1109/TDSC.2021.3049942;http://dx.doi.org/10.1109/TDSC.2021.3049942,Journal Article
Understanding Consumer Intention Towards Blockchain-Based Mobile Payment Adoption Services in Pakistan,"The paper reflects the application of a positivist study and quantitative methods for evaluation of variable relationships. The researcher developed a survey-based methodology and disseminated the questionnaire to consumers of Pakistan through online. Structural equation modeling, confirmatory factor analysis and all supporting statistical tests have been applied on the data in order to evaluate the accuracy;the findings suggest that the usability and factors like the security of online services influence the consumers. Moreover, the performance and effectiveness of these measures influence the decision to use and recommend the services to others as well. The study contributed to the literature by validating and supporting the applicability, usage and development of mobile payment services and produces important implications for the managerial and policymakers of these industries.",,"Ali Z,Bano H",,2022,1–21,10.4018/IJeC.307136,https://doi-org.proxy.bnl.lu/10.4018/IJeC.307136;http://dx.doi.org/10.4018/IJeC.307136,Journal Article
"Kinetic Action and Radicalization: Theory, Data, and Model","Drone strikes appear to drive terrorist events with a lag that can be determined analytically. There is an ongoing debate as to the net value of drone strikes when all unintended consequences have been evaluated. Those in current and previous U.S. administrations have argued that the benefits of conducting drone strikes outweigh their costs. Others, however, have cited unintended consequences, such as collateral damage, which might muddy the waters when senior decisionmakers within the defense and intelligence communities decide to develop and execute various strategies to combat terrorism. Even previous U.S. administrations have questioned whether the Global War on Terrorism was successful in reducing the threat. A data-driven approach explores the relationship between drone strikes and subsequent responses—often in the form of terrorist attacks—carried out by those in the communities targeted by these counterterrorism measures. This research uses natural language processing, social network analysis, and other text analytic techniques to characterize the air and drone strike narrative reported by the news media. Leveraging evidence gleaned from that exploratory analysis, along with data collected from a variety of other sources, this research builds a computational agent-based model of opinion dynamics and applies it to the case study of Pakistan. Analysis of the drone strike campaign in Pakistan and terrorist attacks carried out in that country shows that the two series are highly correlated with a lag structure. A simple model—which accounts for drone strikes in Pakistan and previously-conducted terrorist attacks in that country—explains nearly 100 percent of the variability of the terrorist attacks occurring in Pakistan during the entirety of the U.S. drone strike campaign. By using data from Pakistan to inform and build an agent-based model to simulate the dissemination of opinions through a notional terrorist network to generate terrorist attacks, which approximates the rate and magnitude observed in Pakistan from 2007 through 2018, this dissertation advances the field while at the same time laying the foundation for further work in the area of data-driven modeling and kinetic actions.",,"Shapiro BA,Robert Axtell,Kenneth Comer,Trevor Thrall A",,2021,,,,Ph.D. Thesis
Carrier Class High Density VoIP Media Gateway Using Hardware Software Distributed Architecture,"This paper presents a system on chip (SOC) based high-density carrier class Voice over Internet Protocol (VoIP) media gateway. The design exploits three levels of parallelism namely HW/SWpartitioning, multiprocessing and instruction level parallelism. The system facilities the carrier class service providers to transform channels on PSTN network to IP network, enabling the consumers to enjoy several value added services, such as voice, data, fax and video all through a uniform network at lower cost. The SoC implements a high-density media gateway switch for carrier class VoIP applications. The solution is synthesized to operate at 266 MHz handling up to OC-3 data rate port density equivalent to 2016 simultaneous calls on a single chip and it can be cascaded to support OC-12 and OC-48 rate port density on single board in different configurations. On each channel the SoC supports up to 128 ms tail length G.168 compliant Line Echo Cancellation (LEC), voice activity detection (VAD), comfort noise generation (CNG) and discontinuous transmission (DTX), dual tone multi frequency (DTMF) detection and generation, variety of G.xxx voice codecs, V. 17 Fax and V.34 data modem standards and MPEG4 video compression/decompression algorithms. The solution is compared with the existing architectures and commercial products; it offers 3x to 10x cost, size, power and port density performance improvements with IP and PSTN internetworking support. The media gateway system based on the presented architecture has been used to develop IP-based automatic call distribution (ACD) by etel Pvt. Ltd and installed in Telephone Inquiry Exchange of Pakistan Telecom and its variants are being tested by Buraq Telecom for installation in their service network.",,"Rahmatullah MM,Khan SA,Jamal H",,2007,1513–1520,10.1109/TCE.2007.4429246,https://doi-org.proxy.bnl.lu/10.1109/TCE.2007.4429246;http://dx.doi.org/10.1109/TCE.2007.4429246,Journal Article
"Wireless Sensor Networks for Developing Countries: First International Conference, WSN4DC 2013, Jamshoro, Pakistan, April 24-26, 2013, Revised","This book constitutes the refereed proceedings of the First International Conference on Wireless Sensor Networks for Developing Countries, WSN4DC 2013, held in Jamshoro, Pakistan, in April 2013. The 10 revised full papers presented were carefully reviewed and selected from 30 submissions. The papers are organized in topical sections on WSN applications/services for developing countries; mobile WSN; underwater WSN; VANETS; body area networks; energy harvesting in WSN; WSN and cloud integration; WSN and IoT; QoS and Qot; WSN MAC, network and transport protocols; cross layer approaches; security aspects in WSN; WSN applications in smart grid and energy management; WSN in structural health monitoring.",,"Shaikh FK,Chowdhry BS,Ammari HM,Uqaili MA,Shah A",,2013,,,,Book
"How Relevant Are Risk Perceptions, Effort, and Performance Expectancy in Mobile Banking Adoption?","This article provides a comprehensive overview of the adoption process using evidence from m-banking adoption in Pakistan. A survey design was used and 189 responses were received from across Pakistan and analyzed using Smart PLS application. Findings suggest that research on the effect of risk in the adoption process remains inconclusive. Contrarily, consumers have overcome many fears due to the usefulness, indispensability, high security features, and effort expended in the use of financial services delivered through m-banking. Perceived risk's PR direct influence was found to be generally weak. However, PR plays a major role in the pre-adoption process because it's weak and direct inhibiting influence become an ""enhancer"" in the association between effort expectancy EE and the three key TAM/UTAUT constructs [performance expectancy PE, attitude ATT, and adoption intention INT]. Most importantly, the role of EE as a strong driver of PE, ATT, INT, and its significant interaction with PR highlights the unique role that both risk and EE play in the adoption process.",,"Shaikh AA,Glavee-Geo R,Karjaluoto H",,2018,39–60,10.4018/IJEBR.2018040103,https://doi-org.proxy.bnl.lu/10.4018/IJEBR.2018040103;http://dx.doi.org/10.4018/IJEBR.2018040103,Journal Article
Information Security Education and Self-Perception of Privacy Protection Risk in Mobile Web in Obstetrics Students from Peru,"The objective of the study was to determine the information security education topics developed in the training of obstetrics students and their relationship with the self-perception of privacy protection risk in mobile web during the COVID-19 pandemic at the Santiago Antúnez of Mayolo National University (UNASAM) (Huaraz-Peru). A correlational cross-sectional investigation was developed, with 164 obstetric students. The information was collected through a questionnaire applied online between November and December 2020, having determined its validity and reliability. The Chi squared statistical test (p < 0.05) was used, while the information processing was carried out using the SPSS program. It was determined that 61.6% of obstetric students perceived a high risk in the protection of their privacy in mobile web during the development of their activities in the academic cycle 2020-I. Likewise, it was evidenced that the majority of students stated that they had not developed the topics consulted with regard to information security education during their virtual studies in obstetric, especially with regard to the recommendations for the use of passwords (83.5%), privacy protection strategies (81.1%) and data management through the creation of backups (79.9%), showing a statistically significant relationship with the self-perception of privacy protection risk in mobile web (p < 0.05). It was concluded that the low development of information security education topics in the training of obstetric students is related to the self-perception of high risk in the privacy protection in mobile web during the COVID-19 pandemic.",,"Olaza-Maguiña AF,De La Cruz-Ramirez YM",,2021,32–43,10.1007/978-3-030-83164-6_3,https://doi-org.proxy.bnl.lu/10.1007/978-3-030-83164-6_3;http://dx.doi.org/10.1007/978-3-030-83164-6_3,Conference Paper
"Participatory Sensing Platform Concept For Wildlife Animals In The Himalaya Region, Nepal","Human Computer Biosphere Interaction (HCBI) is a relatively new academic discipline that acts as a critical juncture between the conservation biology and the Information, Communication and Technology (ICT). HCBI domain exploits the capabilities of the repertoire of available technological tools to remotely sense data from difficult geographical terrains in a secure and cost-effective manner. In this perspective paper, we highlight some of the bio-acoustic technologies that we have been using for our research in Fukushima prefecture, Japan. Learning from our experience in Fukushima, we provide our preliminary viewpoint on the possibility of incorporating HCBI research in Manang, Nepal. Our impressions are largely based on the site visit to Manang and informal interaction with locals and conservation specialists. The preliminary feasibility study will prove useful in future as we plan a full-fledged ICT based animal conservation study to assess how the application of ICT tools for wildlife monitoring can contribute to the economic empowerment of locals in Manang who depend on subsistence farming. In summary, this paper provides a preliminary overview of the potentiality of technology transfer from Japan to the remote hilly areas in Nepal for wildlife conservation by employing ICT tools and participatory sensing approach.",,"Shimotoku D,Yuan T,Parajuli LK,Kobayashi HH",,2022,87–98,10.1007/978-3-031-05431-0_6,https://doi-org.proxy.bnl.lu/10.1007/978-3-031-05431-0_6;http://dx.doi.org/10.1007/978-3-031-05431-0_6,Conference Paper
Detecting Voter Fraud in an Electronic Voting Context: An Analysis of the Unlimited Reelection Vote in Venezuela,"Between December 2007 and February 2009, Venezuelans participated twice in constitutional referenda where the elimination of presidential term limits was one of the most salient proposals. Assuming voter preferences did not change significantly during that period, the 'repeated' character of these elections provide us with an excellent opportunity to apply forensic tools designed to detect anomalies and outliers in election returns in elections where electronic voting technologies were used. Similar tools were first applied by Myagkov et al. ([20], [21], [22], [23]) to the study of electoral fraud in Russia and Ukraine, and were effective in the isolation of potential cases of manipulation of electoral returns. The case of Venezuela is different because there exists no widespread agreement about the integrity or otherwise fraudulent nature of national elections, and because it is a nation where electronic voting technologies are used. Unless electoral fraud takes place in exactly the same manner in each election, an analysis of the 'flow of votes' between elections can be used to detect suspicious patterns in electoral returns. Although we do not find evidence of pervasive electoral fraud compared, for instance, to the Russian case, our analysis is useful to detect polling places or regions deviating considerably from the more general pattern.",,"Levin I,Cohn GA,Ordeshook PC,Alvarez RM",,2009,4,,,Conference Paper
Proceedings of the International Conference on Information Technology & Systems (ICITS 2018),"This book includes a selection of articles from the 2018 International Conference on Information Technology & Systems (ICITS 18), held on January 10 12, 2018, at the Universidad Estatal Pennsula de Santa Elena, Libertad City, Ecuador. ICIST is a global forum for researchers and practitioners to present and discuss recent findings and innovations, current trends, lessons learned and the challenges of modern information technology and systems research, together with their technological development and applications. The main topics covered include information and knowledge management; organizational models and information systems; software and systems modeling; software systems, architectures, applications and tools; multimedia systems and applications; computer networks, mobility and pervasive systems; intelligent and decision support systems; big data analytics and applications; humancomputer interaction; ethics, computers & security; health informatics; and information technologies in education.",,"Rocha A,Guarda T",,2018,,,,Book
A View from the Other Side: Understanding Mobile Phone Characteristics in the Developing World,"Mobile devices are becoming increasingly dominant in the developing world. However, there is little insight into the characteristics of devices being used in such regions. Using a dataset of 0.5 million subscribers from one of the largest cellular operators in Pakistan, we analyze the characteristics of cell phones based on different features (e.g., CPU, memory, and cellular interface). We identify potential device-level bottlenecks for Internet access and analyze the security implications of the phones being used. To aid the analysis of cell phones, we propose abstractions (e.g., connectivity, capacity, and device security) and cluster phones based on these abstractions. Our analysis reveals interesting insights for improving mobile web performance.",,"Ahmad S,Haamid AL,Qazi ZA,Zhou Z,Benson T,Qazi IA",,2016,319–325,10.1145/2987443.2987470,https://doi-org.proxy.bnl.lu/10.1145/2987443.2987470;http://dx.doi.org/10.1145/2987443.2987470,Conference Paper
On Demand Radio Frequency Identification Based Vehicle Tracking System,"Over the years, many systems have addressed the problem of location sensing. In the past, geographic positioning systems (GPS) have been widely used to track moving objects located outside environments. These systems have several problems such as operational, environmental and high cost. The fixed GPS infrastructure causes several problems in wireless systems. Thus, GPS is considered not a suitable solution for the fixed environment. Due to this, there is a need for the system that can be replaced with less effort to meet future needs. Thus, the purpose of this paper is to discuss the available wireless technologies like radio frequency identification (RFID) techniques and mobile ad hoc sensor network. In doing so, the application of these technologies for remote objects information acquisition and tracking of moving objects is discussed. Further, the authors attempt to develop a communications setup for highways of Pakistan by using RFID and wireless sensor networks techniques to build a network that can be used for object tracking and information acquisition of moving vehicles on highways. This system may be used for variety of purposes especially for security enhancements at highways.",,"Memon S,Khoumbati K,Shaikh A",,2010,34–43,10.1504/IJBIS.2010.034003,https://doi-org.proxy.bnl.lu/10.1504/IJBIS.2010.034003;http://dx.doi.org/10.1504/IJBIS.2010.034003,Journal Article
Universities of the Kyrgyz Republic on the Web: Accessibility and Usability,"Today the Internet is the easiest way to find information about any kind of organization, and the first impression about an organization is almost always based on its Web site. This study investigated whether the Web sites of the universities in the Kyrgyz Republic comply with prevailing standards of accessibility and usability and whether these qualities depend on location and type of ownership of the universities. The analysis was conducted using online evaluation tools. Based on the data collected, the hypotheses were further tested using the SPSS statistical package. The results show a low usability rating for the vast majority of the universities' Web sites. For 90.47 % of the Web sites upload time exceeds 30 s; 52.38 % of the Web sites have broken links; and 100 % have browser compatibility problems. The results of accessibility tests show low compliance with W3C-WCAG 1.0: error rates for Priority 1, 2, and 3 checkpoints of 83.33, 92.85, and 95.24 %, respectively. The results obtained and the results of an independent t test indicate that most of the issues of all Web sites tested are not of a technical nature, and occur mainly due to human factors related to Web application development.",,"Ismailova R,Kimsanova G",,2017,1017–1025,10.1007/s10209-016-0481-0,https://doi-org.proxy.bnl.lu/10.1007/s10209-016-0481-0;http://dx.doi.org/10.1007/s10209-016-0481-0,Journal Article
GlacierNet Variant for Large Scale Glacier Mapping,"Climate change impact is profoundly visible in recent decades including its effect on the health of the mountain glaciers. Accelerated glacier recession patterns observed globally are leading to consequences like sea level rise, water security and glacier-related hazards. Therefore, it is important to monitor and understand these glacier changes. Detection of accurate glacier boundary, which is the basic input of many glacier analysis, remains a challenge even after many years of research on conventional remote sensing methods or machine learning methods. A deep learning based approach named as GlacierNet has been developed to exploit the convolutional neural network (CNN) segmentation model to accurately outline the debris-covered glacier (DCG) ablation zones in regional scope. To improve the approach, the performance of GlacierNet?s CNN is compared with several advanced CNN segmentation models, including Mobile-UNet, Res-UNet, FCDenseNet, R2UNet, and DeepLabV3+, to identify the most salient features that could contribute to the DCG segmentation accuracy.Based on the evaluation, we developed the GlacierNet2 that is an enhanced version of our GlacierNet, that incorporates deep learning, image processing, and remote sensing technologies and hydrology science. It is observed that the GlacierNet2 ameliorates the estimation of the DCG ablation region also called the ablation zone and reaches the high level of intersections over union (IOU) score of 0.8839. The important component of any glacier mapping is to include both accumulation and ablation zone. Consequently, the newly added capacity of the enhanced approach is to map the snow-covered accumulation zone (SCAZ). The experimental evaluations demonstrate that the proposed model can provide complete glacier (both accumulation and ablation zone) outlines at regional scales with an overall IOU score of 0.8619. Also, we design a large-scale mapping strategy to progressively enhance the network familiarity to varied types of glaciers via systematically repeating the training process. This strategy allows the network to delineate a large number of glaciers while only requiring a small proportion of initial training data. Thus, resulting in a significant drop in labor and expert intervention, which are required for selecting and labeling the training data. Our results show a successful and accurate generation of glacier boundaries with an intersection over union (IOU) score of 0.8115 in the Karakoram and parts of western Himalaya and an IOU of 0.7525 in the Nepal Himalaya. Our work outlines how future efforts of large and global scale mapping can be developed to monitor and analyze glacier dynamics.",,"Xie Z,Theus Aspiras,Eric Balster,Umesh Haritashya",,2022,,,,Ph.D. Thesis
The Blend of Credit Scoring Model for Individual in the Dmaic Process for Reducing Non-Performing Loan Risk,"Non-performing loan (NPL) is the main threat for all financial institutions. In order to improve loan approval process and reduce the risk of NPL, this research proposes an application of six sigma and credit scoring model. Six Sigma is an outstanding tool for process improvement by reducing defects in the process in manufacturing and service industries. Credit scoring model is a statistical model that aid in the decision making for the bank and other financial institution whether they should approve or reject the loan application. Six Sigma offers value by reducing defects and Credit scoring model can enhance credit lending policy. The implementation of Six sigma and Credit scoring model in bank loan approval process is a new topic and few literatures have studied in this area. The objectives of this research are to identify factors causing NPL and propose framework using Six Sigma and Credit scoring model to improve bank loan process and enhance the credit lending policy to reduce the risk of NPL. Case study of a bank in Cambodia is illustrated.",,"Thavarith V,Liangrokapart J",,2019,195–202,10.1145/3335550.3335583,https://doi-org.proxy.bnl.lu/10.1145/3335550.3335583;http://dx.doi.org/10.1145/3335550.3335583,Conference Paper
IWCMC '07: Proceedings of the 2007 International Conference on Wireless Communications and Mobile Computing,"On behalf of the Technical Program Committee, I welcome you all to the ACM International Wireless Communications and Mobile Computing Conference (ACM IWCMC 2007) in Turtle Bay Resort, Honolulu, Hawaii! I'm delighted that this year's ACM IWCMC accomplishes its goal under our conference theme ""The Future is Now---The New Era of Wireless Communications and Mobile Computing and Networking Technologies"" and continues its tradition of providing the premier forum for presentation of research results and experience reporting on the cutting edge research in the general areas of the wireless communications and mobile computing.This year, we received about 300 submissions from 32 countries. All papers received rigorous peer reviews from our Technical Program Committee (TPC), comprised of 41 Symposia Chairs/Co-Chairs and a total of 200 TPC members from academia, government laboratories, and industries. We also invited more than 470 external expert reviewers from all over the world. After carefully examining all the received review reports, the ACM IWCMC 2007 TPC finally selected 119 high-quality papers for presentation at the conference and publication in ACM IWCMC 2007 proceedings. The accepted papers come from United Kingdom, Canada, Germany, Australia, Taiwan, Korea, China, India, Japan, Portugal, Finland, Egypt, France, Ireland, Pakistan, Spain, Brazil, Italy, Iran, Norway, Sweden, Chile, Singapore, and the United States.The conference program starts each day with a keynote speaker given by the world-class leaders in the areas -- Dr. Robert E. Kahn, Professor Vijay K. Bhargava, Professor Aggelos K. Katsaggelos, highlighting the latest research trends in the wireless communications, mobile computing, and networks. A total of 27 technical sessions, organized in three parallel tracks, from the core of the technical program. This year, the technical sessions reflect the continued and growing interests in a wide range of spectrum, including wireless communications and networks, cross-layer design and optimization, mobile computing, wireless sensor networks, network security, information theory and applications.",,,,2007,,,,Book
Landslide Detection Based on Contour-Based Deep Learning Framework in Case of National Scale of Nepal in 2015,,,"Yu B,Chen F,Xu C",,2020,,10.1016/j.cageo.2019.104388,https://doi-org.proxy.bnl.lu/10.1016/j.cageo.2019.104388;http://dx.doi.org/10.1016/j.cageo.2019.104388,Journal Article
A Virtual Reality Exposure Therapy Application for Iraq War Post Traumatic Stress Disorder,"Post Traumatic Stress Disorder (PTSD) is reported to be caused by traumatic events that are outside the range of usual human experiences including (but not limited to) military combat, violent personal assault, being kidnapped or taken hostage and terrorist attacks. Initial data suggests that 1 out of 6 Iraq War veterans are exhibiting symptoms of depression, anxiety and PTSD. Virtual Reality (VR) exposure treatment has been used in previous treatments of PTSD patients with reports of positive outcomes. The aim of the current paper is to present the rationale, technical specifications, application features and user-centered design process for the development of a Virtual Iraq PTSD VR therapy application. The VR treatment environment is being created via the recycling of virtual graphic assets that were initially built for the U.S. Army-funded combat tactical simulation scenario and commercially successful X-Box game, Full Spectrum Warrior, in addition to other available and newly created assets. Thus far we have created a series of customizable virtual scenarios designed to represent relevant contexts for exposure therapy to be conducted in VR, including a city and desert road convoy environment. User-Centered tests with the application are currently underway at the Naval Medical Center-San Diego and within an Army Combat Stress Control Team in Iraq with clinical trials scheduled to commence in February 2006.",,"Pair J,Allen B,Dautricourt M,Treskunov A,Liewer M,Graap K,Reger G",,2006,62–72,10.1109/VR.2006.23,https://doi-org.proxy.bnl.lu/10.1109/VR.2006.23;http://dx.doi.org/10.1109/VR.2006.23,Conference Paper
"New Trends in Information and Communications Technology Applications: Third International Conference, NTICT 2018, Baghdad, Iraq, October 24, 2018","This book constitutes the refereed proceedings of the Third International Conference on New Trends in Information and Communications Technology Applications, NTICT 2018, held in Baghdad, Iraq, in October 2018. The 18 papers presented were carefully reviewed and selected from 86 submissions. The papers are organized in topical sections, namely: Computer networks; system and network security; machine learning; intelligent control system; communication applications; computer vision; and e-learning.",,"Al-mamory SO,Alwan JK,Hussein AD",,2018,,,,Book
A Sharable Cloud-Based Pancreaticoduodenectomy Collaborative Database for Physicians: Emphasis on Security and Clinical Rule Supporting,"Background: Pancreaticoduodenectomy (PD) is a major operation with high complication rate. Thereafter, patients may develop morbidity because of the complex reconstruction and loss of pancreatic parenchyma. A well-designed database is very important to address both the short-term and long-term outcomes after PD. Objective: The objective of this research was to build an international PD database implemented with security and clinical rule supporting functions, which made the data-sharing easier and improve the accuracy of data. Methods: The proposed system is a cloud-based application. To fulfill its requirements, the system comprises four subsystems: a data management subsystem, a clinical rule supporting subsystem, a short message notification subsystem, and an information security subsystem. After completing the surgery, the physicians input the data retrospectively, which are analyzed to study factors associated with post-PD common complications (delayed gastric emptying and pancreatic fistula) to validate the clinical value of this system. Results: Currently, this database contains data from nearly 500 subjects. Five medical centers in Taiwan and two cancer centers in Mongolia are participating in this study. A data mining model of the decision tree analysis showed that elderly patients (>76 years) with pylorus-preserving PD (PPPD) have higher proportion of delayed gastric emptying. About the pancreatic fistula, the data mining model of the decision tree analysis revealed that cases with non-pancreaticogastrostomy (PG) reconstruction - body mass index (BMI)>29.65 or PG reconstruction - BMI>23.7 - non-classic PD have higher proportion of pancreatic fistula after PD. Conclusions: The proposed system allows medical staff to collect and store clinical data in a cloud, sharing the data with other physicians in a secure manner to achieve collaboration in research.",,"Yu HJ,Lai HS,Chen KH,Chou HC,Wu JM,Dorjgochoo S,Mendjargal A,Altangerel E,Tien YW,Hsueh CW,Lai F",,2013,488–497,10.1016/j.cmpb.2013.04.019,https://doi-org.proxy.bnl.lu/10.1016/j.cmpb.2013.04.019;http://dx.doi.org/10.1016/j.cmpb.2013.04.019,Journal Article
"""We Even Borrowed Money From Our Neighbor"": Understanding Mobile-Based Frauds Through Victims' Experiences","Mobile-based scams are on the rise in emerging markets. However, the awareness about these scams and ways to avoid them remains limited among mobile users. We present a qualitative analysis of the dynamics of mobile-based fraud (specifically, SMS and call-based fraud) in Pakistan. We interviewed 96 participants, including different stakeholders in the mobile financial ecosystem: 71 victims of mobile-based scams, seven non-victims, 15 mobile money agents, and three officials from regulatory agencies that investigate mobile-based fraud. Leveraging the perspectives from these stakeholders and analyzing mobile-based fraud with a four-step social-engineering attack framework, we make four concrete contributions: First, we identify the nuances as well as specific tactics, methods, and resources that fraudsters use to scam mobile users. Second, we look at other actors, beyond the victim and the adversary, involved or affected by fraud and their roles at each step of the fraud process. Third, we discuss victims' understanding of mobile fraud, their behavior post-realization, and their attitudes toward reporting fraud. Finally, we discuss possible points of intervention and offer design recommendations to thwart mobile fraud, including addressing the vulnerabilities discovered in the ecosystem, utilizing existing actors to mitigate the consequences of these attacks, and realigning the design of fraud reporting mechanisms with the sociocultural practices.",,"Razaq L,Ahmad T,Ibtasam S,Ramzan U,Mare S",,2021,,10.1145/3449115,https://doi-org.proxy.bnl.lu/10.1145/3449115;http://dx.doi.org/10.1145/3449115,Journal Article
Critical Risk Path Method: A Risk and Contingency-Driven Model for Construction Procurement in Complex and Dynamic Projects,"Existing approaches to risk management in construction procurement primarily dwell on strategies designed for commonly identifiable risk factors in typical project environments. Commonly identifiable risk factors would include too early or late material delivery-a condition typically ameliorated by implementing a Just In Time JIT plan; inferior construction materials typically mitigated by employing trusted vendors; or ineffective contractors primarily avoided by the use of experienced contractors. The purpose of this paper is to present a coherent model for procurement risk management for construction and infrastructure development projects within the context of dynamic project environments-complex, or chaotic. For the purpose of this study, a critical risk path activity is one in which a delay of activity completion not only leads to project delay, but does so in a manner that may be fatal to project or at best, far greater than the actual delay. The study incorporates observations and theory with practical application for improving initiatives by emergency infrastructure development response organizations such as FEMA Federal Emergency Management Agency and USACE US Army Corps of Engineers in the United States, the NEMA National Emergency Management Agency in Nigeria, or ANDMA Afghanistan National Disaster Management Authority etc. This study presents risk response plans aimed at improving the potential occurrence of positive risk aspects while reducing, or eliminating the same for negative risk occurrences. This study explored material, equipment, and skilled labor procurement strategies related to project risk management from the perspectives of scheduling, cost, and quality-three factors often referred to as the triple project constraints. It identified gaps within specific national and multinational organizations' approaches, and provided detailed recommendations for process improvements from the procurement management perspective to ensure the potential for successful project outcomes in unstable project conditions.",,"Howard C,Iromuanya C,Hargiss KM",,2013,61–73,10.4018/jsita.2013040105,https://doi-org.proxy.bnl.lu/10.4018/jsita.2013040105;http://dx.doi.org/10.4018/jsita.2013040105,Journal Article
"Determinant of Food Security on Upland Agriculture Households in Paletwa Township, Chin State of Myanmar","This study aims to determine food security on upland agriculture households in rural area. Food security is concerned with the first two main goals of Sustainable Development Goals, No Poverty and Zero Hunger. Myanmar is Food Insecure State that showing 14.2% that is 7.7 million of 51 million population. Chin state is least developing and Paletwa is poorest out of 324 townships. Research is applied by both qualitative and quantitative approaches. 3 Villages and 1 Quarter are chosen for qualitative method and 141upland agriculture households are selected for field survey using random sampling method. The data are analyzed by logistic regression in SPSS 17 to determine food security. Age, education, schooling years of household head, size, second occupation and no. of working people in households are socio-economic determinant and own food production and fruit access are food security determinant.",,"Lwin S,Poungchompu S",,2017,44–53,10.4018/IJKSR.2017040104,https://doi-org.proxy.bnl.lu/10.4018/IJKSR.2017040104;http://dx.doi.org/10.4018/IJKSR.2017040104,Journal Article
"IC4E '18: Proceedings of the 9th International Conference on E-Education, E-Business, E-Management and E-Learning","The 9th International Conference on E-Education, E-Business, E-Management and E-Learning (IC4E) was successfully held in San Diego, USA in January 11-13, 2018. IC4E provides a platform for scientists, engineers and technologists who work in all aspects of E-Education, E-Business, E-Management and E-Learning. IC4E 2018 received 43 submissions. Among rigorous peer review, we accepted 24 papers for presentation. These papers are contributed by researchers in many different countries, including Japan, South Korea, China, Taiwan, Hong Kong, Thailand, Philippines, Pakistan, Saudi Arabia, and Nigeria. These papers cover the topics range from knowledge management, personalized learning, micro-lecture construction, security policies, collaborative learning, mobile learning, social media, student behavior tracking, and employment issues.",,,,2018,,,,Book
Smart Agriculture for Sustainable Food Security Using Internet of Things (IoT),"Internet of Things (IoT) is being used in various parts of human life (domestic and commercial) to provide ease in living, safety, increase productivity, monitoring, and resource optimization in various industries. Agriculture is one of them, where IoT and robots are being used before and after the cultivation process, from preparing land for cultivation to supplying them to the consumer market. These domains include crop monitoring, smart irrigation, pest monitoring, and smart pest control, harvesting, and safely supplying them in the consumer market by maintaining the quality and integrity of the final product. Pakistan is an agricultural country, where it stands in terms of advanced agriculture technology. In this review, we discussed the major IoT ecosystem components. What are the most practiced smart agriculture techniques and their benefits and some widely used applications of IoT in agriculture? Through this overview, we are trying to highlight the potential of IoT in agriculture for sustainable food security for Pakistan.",,"Qureshi T,Saeed M,Ahsan K,Malik AA,Muhammad ES,Touheed N,Islam SK",,2022,,10.1155/2022/9608394,https://doi-org.proxy.bnl.lu/10.1155/2022/9608394;http://dx.doi.org/10.1155/2022/9608394,Journal Article
The Presence of the Turkish Private Sector in the Kurdistan Region of Iraq,"How do practices guiding the engagement of the international private sector in fragile and conflict-affected states emerge, and who are the important actors and institutions in this process? Using a human security framework, this thesis seeks to apply critical security studies by taking a closer look at the role of the Turkish private sector in the Kurdistan Region of Iraq. This thesis argues that the international private sector can be a vital tool to enhance human security, in particular economic security, in fragile and conflict-affected states. However, without a regulatory environment for the private sector to follow ""best practices,"" it is largely at the discretion of each business to adopt measures to enhance human security. In this light, this thesis presents the conditions that either enhance or constrain economic security following the intervention of the international private sector. The central findings suggest the Turkish private sector enhanced certain economic security areas such as infrastructure recovery and restoration of access to basic services, the dismantling of a war economy and illegal economic networks, and expansion of opportunities for people through training, skills development and empowerment. The central findings also suggest the Turkish private sector constrained economic security in the areas of job creation or to the establishment of public and private sector employment, wage employment and self-employment. The impact of the Turkish private sector on public-private relations appeared to have had a mixed impact. Moreover, there were some economic security conditions such as basic income and poverty alleviation, rehabilitation and diversification of the agriculture sector, development of productive activities for ex-combatants, returnees and impoverished groups, provision of microfinance opportunities, clarification of property rights, macroeconomic development, and provision of well-coordinated, predictable, and multifaceted aid, where the Turkish private sector appeared to have had little impact if any at all.",,Fidan CB,,2016,,,,Ph.D. Thesis
Prototype of an Alignment Model of the Ministry of Telecommunications and the Information Society to a Public Organization in Ecuador,"This work contains information on technological advances in various public institutions in Ecuador, to improve the quality of services to users from government management. The objective was to design a prototype that is an alignment model for the Ministry of Telecommunications and the information society to a public organization in Ecuador; for this reason, it was important to work with a quantitative methodology that allowed to objectively know the management and applicable strategies and clarified its usefulness in the Ecuadorian context. In order for this alignment to be possible, it was proposed to comply with the decrees and laws, which allow the ease of communication between public and private companies, as well as strengthen citizen participation, support communication activity through access to public data, which values and respects diversity. It turned out that the ITIL model together with linear programming are the most suitable options to allow technology to be managed with good practices in public institutions. In this way, it was possible to conclude that state companies must align themselves, making good use of the information society, respecting the theory of public service, community communication, cybernetic theory, universal and affordable universal access to ICT, to all citizens, in order to provide care to users with quality and warmth based on the country's public policies.",,"Toapanta SM,Canales MJ,Rojas JG,Gallegos LE",,2020,58–64,10.1145/3404663.3404676,https://doi-org.proxy.bnl.lu/10.1145/3404663.3404676;http://dx.doi.org/10.1145/3404663.3404676,Conference Paper
Change Detection Analysis Using Sentinel-1 Satellite Data with SNAP and GEE Regarding Oil Spill in Venezuela,"In Venezuela, according to a report by the National Aeronautics and Space Administration (NASA) dated September 2021, up to 50,000 oil spills at sea have been monitored in the 2010–2016-time frame. In the current two-year period, the situation does not seem to have changed: the state refinery of the Venezuelan oil company Petróleos de Venezuela, S.A. (PDVSA), located near El Palito (Carabobo), is estimated to have been responsible for nearly 100,000 barrels of oil spilled in just one year. The aforementioned spills, with the intensification of extraction, transport and storage operations, have given rise to a greater number of accidents resulting in uncontrolled dispersion of material, endangering local marine-coastal ecosystems. The one that took place in July 2020 stands out, reconstructed a posteriori using satellite images that have highlighted its geo-environmental impact. Remote sensing has played a fundamental role in identifying and monitoring the spread of hydrocarbons; in the present study, the same event was analysed using Sentinel-1 Synthetic Aperture Radar Image (SAR) Change Detection techniques. The use of the desktop software SeNtinel Application Platform (SNAP) of the European Space Agency (ESA) made it possible to quantify the ocean surface affected by the phenomenon under analysis; at the same time, an algorithm was formulated within the cloud platform Google Earth Engine (GEE) which confirmed the same outputs but more quickly and allowed the implementation of an algorithm that exploits the statistical concept of value of Otsu threshold. The results obtained were subsequently compared with other results extrapolated through automatic methodologies developed by ESA, which supported a better accuracy of the procedures used in this study.",,"Caporusso G,Gallo C,Tarantino E",,2022,387–404,10.1007/978-3-031-10545-6_27,https://doi-org.proxy.bnl.lu/10.1007/978-3-031-10545-6_27;http://dx.doi.org/10.1007/978-3-031-10545-6_27,Conference Paper
ICEGOV '13: Proceedings of the 7th International Conference on Theory and Practice of Electronic Governance,"The 7th International Conference on Theory and Practice of Electronic Governance, ICEGOV2013, took place in Seoul, Republic of Korea from 22 to 25 October 2013. The conference was organized under the patronage of the Ministry of Security and Public Administration of the Republic of Korea (MOSPA) by the National Information Society Agency and by Macao-based Center for Electronic Governance at United Nations University International Institute for Software Technology (UNU-IIST) as the founder and organizer of the ICEGOV series. The conference took place under the theme ""Beyond 2015"" Smart Governance, Smart Development"". It was co-located with the Global e-Government Forum, organized by MOSPA in collaboration with United Nations Department of Economic and Social Affairs (UNDESA).The ICEGOV series focuses on the use of technology to transform relationships between government and citizens, businesses, civil society and other arms of government (Electronic Governance). Established in 2007, the series looks beyond the traditional focus on technology-enabled transformation in government (Electronic Government) towards new forms, new paradigms, and new foundations for technology-enabled governance, collaboration and sustainable development. ICEGOV is a platform where researchers, policy-makers and practitioners meet; a platform where theories are tested, insights are shared and experiences are reported; a platform for network- and capacity-building where keynote lectures and paper sessions are complemented by plenary discussions, town hall debates and poster exhibitions; a platform for international dialogue attended by participants from developing, developed and transition countries, from the United Nations system, and from many academic, governmental, non-governmental and private sector organizations. Since its establishment, the series has traveled globally from Macao (ICEGOV2007), through Cairo (ICEGOV2008), Bogota (ICEGOV2009), Beijing (ICEGOV2010), Tallinn (ICEGOV2011) and Albany (ICEGOV2012), to Seoul (ICEGOV2013) all generating significant local interest and stakeholder engagement.The program of ICEGOV2013 was built upon contributions from researchers and practitioners from around the world. In response to the call for papers, the conference received 133 papers from 54 countries and economies. The papers were evaluated in five categories: 1) Completed Research Papers providing the outcomes of complete research in one or more aspects of EGOV, with proven capability to advance the state of research in the field, limited to 10 pages; 2) Ongoing Research Papers providing the outcomes of ongoing research in one or more aspects of EGOV, with potential capability to advance the state of research in the field, limited to 4 pages; 3) Completed Experience Papers describing completed experience concerning EGOV policy or practice innovations, with proven capability to advance the state of practice in the field, including critical success factors and insights on the challenges encountered and how they were addressed, limited to 10 pages; 4) Ongoing Experience Papers describing ongoing experience concerning EGOV policy and practice innovations, with potential capability to advance the state of practice in the field, including critical success factors and insights on the challenges encountered and how they are being addressed, limited to 4 pages; and 5) Poster Papers presenting novel ideas and initiatives with potential to advance the state of research or state of practice in the field, limited to 2 pages. In total, 43 Completed Research Papers, 45 Ongoing Research Papers, 17 Completed Experience Papers, 21 Ongoing Experience Papers and 8 Posters were received. After anonymous peer-review process carried out by the members of the Program Committee at least three independent reviews were obtained for each submission as a basis for acceptance decisions: 13 submissions were accepted as Completed Research Papers, 8 as Completed Experience Papers, 29 as Ongoing Research Papers, 11 as Ongoing Experience Papers and 21 as Poster Papers. All accepted submissions, revised to address review comments, and presented at the conference within 6 paper tracks, 11 thematic sessions and one poster session, are included in this volume. Among them, like the last three ICEGOV conferences, the authors of selected papers were invited to submit extended versions of their papers for possible publication in the special issue of Government Information Quarterly, Elsevier.Based on the submitted and invited contributions and continuing the ICEGOV tradition, ICEGOV2013 featured a rich academic, capacity-building and network-building program comprising keynote lectures, plenary discussions, town hall debates, paper tracks, thematic sessions and the doctoral colloquium and poster exhibition. The program engaged individuals from over 60 countries and economies as authors, reviewers, committee members or resource persons. The details of the program are provided below.The conference included six keynote lectures on various aspects of Electronic Governance (EGOV), conducted by distinguished experts and practitioners in the area: 1) Park Chan Woo, Vice-Minister of Security and Public Administration of the Republic of Korea; 2) Alikhan Baimenov, Chairman of the Agency for Civil Service Affairs of the Republic of Kazakhstan; 3) Moon Suk Ahn, Chair Professor of e- Government, Korea University, Republic of Korea; 4) Mohammed Ali Al, Chief Executive Officer, e-Government Authority, Kingdom of Bahrain; 5) Henk G. Sol, Professor of Business and ICT and Founding Dean, University of Groningen, Netherlands; and 6) Edwin Lau, Head of Division, Reform of the Public Sector, Organization for Economic Co-operation and Development (OECD).Three plenary sessions followed the keynote lectures on the second, third and fourth day of the conference, focusing on specific questions of interest to the EGOV research and policy community:1. Are international EGOV rankings having a mobilizing or distracting influence on development? Chaired by Tomasz Janowski, Head of the Center for Electronic Governance at UNU-IIST and attended by: Vincenzo Aquaro, Chief of E-Government Branch, Division for Public Administration and Development Management, UNDESA; Bikesh Kurmangaliyeva, Deputy Chairwoman of the Board of ""Zerde"" National CT Holding, Kazakhstan; Mohammed Ali Al Qaed, CEO of eGovernment Authority, Kingdom of Bahrain; Mesfin Belachew Tefera, Technical Advisor to the Minister, Ethiopian Ministry of Communication and Information Technology; and Saleem Zoughbi, Former Regional ICT Advisor, UNESCWA and consultant for UNU-IIST.2. Who should drive smart conversations for sustainable development experts, citizens or politicians? Chaired by Marijn Janssen, Professor of ICT and Governance at Technology, Policy and Management Faculty, Delft University of Technology, Netherlands and attended by: Sunil Choenni, Head, Department of Statistical Information Management and Policy Analysis, Research and Documentation Centre (WODC), Dutch Ministry of Security and Justice; Harekrishna Misra, Professor in IT and Systems at the Institute of Rural Management Anand (IRMA), India; Henk G.Sol, Professor of Business and ICT and Founding Dean, Faculty of Economics and Business, University of Groningen, Netherlands; and Evgeny Styrin, Senior Research Analyst and Associate Professor, National Research University Higher School of Economics, Russia.3. Is a common set of e-government principles, applicable to all countries and contexts, possible? Chaired by Samuel Chan, Member of Executive Committee, Macao Science and Technology Development Fund, Macao SAR Government and attended by: Wojciech Cellary, Head of the Department of Information Technology, Poznan University of Economics, Poland; Sharon Dawes, Senior Fellow, Center for Technology in Government, University at Albany, USA; Edwin Lau, Head of Division, Reform of the Public Sector, Public Governance and Territorial Development Directorate, Organization for Economic Co-operation and Development; and Jeremy Millard, Associate Research Fellow, Brunel University, UK.Three town hall debates took place in the afternoons of the first, second and third days of the conference. They focused on three salient questions for the EGOV research and policy community:1. Catalyzing Smart Transformation: What Makes Governments Smarter? Chaired by Samia Melhem, Lead Policy Specialist, Transform Practice, Chair, eDevelopment Community of Practice, Transport, Water and ICT, Sustainable Development Network, World Bank Group; and Oleg Petrov, Senior Program Officer, ICT, World Bank; and attended by: Jabiri Kuwe Bakari, CEO, e-Government Agency, Tanzania; Rajendra Kumar, Senior Officer, Indian Administrative Service and Joint Secretary (e-Governance), Department of Electronics and Information Technology, Government of India; Bikesh Kurmangaliyeva, Deputy Chairwoman of the Board of ""Zerde"" National CT Holding, Kazakhstan; Margareta Petrusevschi, Knowledge and Learning Coordinator, e-Government Centre, Government of the Republic of Moldova; James Saaka, Executive Director, National Information Technology Authority, Uganda; Mesfin Belachew Tefera, Technical Advisor to the Ethiopian Minister of Communication and Information Technology; and Jeongwon Yoon, Executive Director, National Information Society Agency, Korea. This town hall was organized by the World Bank.2. Is Good Governance a Pre-Condition or a Consequence of the Development of Knowledge Societies? Chaired by Andrea Cairola, Adviser for Communication and Information, UNESCO Office Beijing, Cluster Office to the Democratic People's Republic of Korea, Japan, Mongolia, People's Republic of China and Republic of Korea; and attended by: Johanna Ekua Awotwi, Director of Research and ICT Operations, Centre for e-Governance, Accra, Ghana; Antonio Cordella, Lecturer in Information Systems, London School of Economics and Political Sciences, UK; Marco Peres, Director, Observatory for Society, Technology and Government Information, University Externado of Colombia, Colombia; Margareta Petrusevschi, Knowledge and Learning Coordinator, e-Government Centre, Government of the Republic of Moldova, Moldova; and Jeongwon Yoon, Executive Director, National Information Society Agency, Republic of Korea. This town hall was organized by the UNESCO Information for All Programme.3. Striking the Balance of Security, Privacy and Openness: To Open or Not To Open? Chaired by Theresa Pardo, Director of the Center for Technology in Government, University at Albany, USA, and attended by: Sharon Dawes, Senior Fellow, Center for Technology in Government, University at Albany, USA; Ramon Gil-Garcia, Research Director, Center for Technology in Government, University at Albany, USA; Louise Thomasen, independent consultant and expert in EGOV and technology, Denmark; and Lei Zheng, Assistant Professor, Department of Public Administration, Fudan University, China.The program included six paper tracks, chaired by leading international experts in the corresponding areas, comprising presentations of three to six accepted papers: 1) Building Smart Government chaired by Theresa Pardo, Director of the Center for Technology in Government, University at Albany, USA and Gabriel Puron Cid, Professor at the Centre of Research and Teaching in Economic Sciences, Mexico; 2) Governing through Networks chaired by Sehl Mellouli, Associate Professor at Laval University, Canada and Adegboyega Ojo, Research Fellow and Leader of E-Government Group at INSIGHT, National University of Ireland, Ireland; 3) Policy and Governance Innovation chaired by Natalie Helbig, Senior Research Associate at the Center for Technology in Government, University at Albany, USA and Marijn Janssen, Professor in ICT and Governance at the Delft University of Technology, Netherlands; 4) Smart Governance for Smart Industries chaired by Wojciech Cellary, Professor and Head of the Department of Information Technology at the Poznan University of Economics, Poland and Antonio Cordella, Lecturer at the London School of Economics and Political Sciences, UK; 5) Smart Governance for Smart Societies chaired by Jeremy Millard, Associate Research Fellow at the Brunel University, UK; and 6) Ethics, Transparency and Accountability chaired by Jeanne Holm, Chief Knowledge Architect at the NASA Jet Propulsion Laboratory, USA. Each track took place across the whole duration of the conference, with tutorial introduction to the topic of the track organized on the first day, presentations of accepted papers on the second or third day, and workshop-style discussion on the last day.Complementing the paper tracks, 11 thematic sessions were organized and chaired by industrial, academic, government and international organizations active in the theme of the session, comprising presentations of up to four accepted papers: 1) EGOV for Development chaired by Nag Yeon Lee, ICT Consultant and Instructor for e-Government on behalf of the Asia Pacific Center on ICT for Development, United Nations Economic and Social Commission for Asia Pacific; 2) National Data Policies chaired by Zhanat Zhakhmetova, Head of the Office of State Informatization Policy, Department of State Information Technology Policy, on behalf of the Ministry of Transport and Communications, Republic of Kazakhstan; 3) Governing Ageing Society chaired by Toshio Obi, Professor, Institute of e-Government on behalf of Waseda University, Japan; 4) Governing Smart Cities chaired by Yoon Chang So, Smart Cities Country Leader, IBM Korea on behalf of IBM; 5) Open Government Data Impact chaired by Edwin Lau, Head of Division, Reform of the Public Sector, Public Governance and Territorial Development Directorate, on behalf of the Organization for Economic Co-operation and Development; 6) Interoperability Governance chaired by Jung Sik Hwang, Platform Strategy Lead at Microsoft Korea on behalf of Microsoft; 7) Government on Social Media chaired by Jeanne Holm, Evangelist, Data.Gov and Chief Knowledge Architect at the Jet Propulsion Laboratory, NASA on behalf of the World Wide Web Consortium; 8) Innovative EGOV Applications chaired by Oleg Petrov, Senior Program Officer, ICT, World Bank on behalf of the World Bank; 9) Participatory Government chaired by Bernd Friedrich, Head of the Information and Communications Technologies for Development Project at Deutsche Gesellschaft für Internationale Zusammenarbeit (GIZ) GmbH, Germany on behalf of GTZ; 10) Mobile Governance chaired by Nestor Eduardo Fajardo Infante, Advisor for Research, Development and Innovation, Ministry of Information Technology and Communication, on behalf of the Government of Colombia; and 11) Open Data Ecosystem chaired by Jeanne Holm, Evangelist, Data.Gov and Chief Knowledge Architect, Jet Propulsion Laboratory, NASA on behalf of the U.S. Government and Data.Gov.The program also included poster exhibition, organized in the reception style to allow authors to present their ongoing work, receive feedback and engage in discussions and networking; and an interactive doctoral colloquium, jointly organized by the Center for Electronic Governance at UNU-IIST, Macao, University of Groningen, Netherlands and Chuo University, Japan. The colloquium provided doctoral students from different disciplines an opportunity to discuss a variety of EGOV topics and methods related to their research work, dissertations and career plans. The colloquium was co-chaired by Elsa Estevez, Academic Program Officer, United Nations University International Institute for Software Technology, Macao; Hiroko Kudo, Professor of Public Policy and Public Management, Faculty of Law, Chuo University, Japan; and Henk G. Sol, Professor of Business and ICT and Founding Dean, Faculty of Economics and Business, University of Groningen, Netherlands; and attended by Adegboyega Ojo, Research Fellow and Leader of E-Government Group at the INSIGHT Center for Data Analytics, National University of Ireland, Ireland as invited academic.The conference awarded best paper titles in Best Research Paper and Best Experience Paper categories. The selection was carried out jointly by Elsa Estevez as the ICEGOV2013 Awards Chair, and Tomasz Janowski and Jeanne Holm as the ICEGOV2013 Program Chairs. Three papers were nominated to the Best Experience Paper award: 1) A Reputation Based Electronic Government Procurement Model by Hichem Klabi, Sehl Mellouli and Monia Rekik; 2) Government 3.0 in Korea: Fad or Fashion? byTaewoo Nam; and 3) Secure ID Management for Social Security and Tax Number System by Hisao Sakazaki, Dan Yamamoto, Akihiro Sugimoto and Shinji Hirata. The winner in this category was ""A Reputation Based Electronic Government Procurement Model"" by Hichem Klabi, Sehl Mellouli and Monia Rekik. Three papers were also nominated to the Best Research Paper award: 1) Harnessing the Duality of e-Participation Social Software Infrastructure Design by Lukasz Porwol, Adegboyega Ojo and John Breslin; 2) When Food Quality Control in China Meets Mobile and Wireless Technology: Interactions, Potentials and Pitfalls by Shuhua Liu; and 3) Cross-departmental Collaboration in Government One-Stop Center: Factors and Performance by Xinping Liu. The winner in this category was ""Harnessing the Duality of e-Participation Social Software Infrastructure Design"" by Lukasz Porwol, Adegboyega Ojo and John Breslin.Many people and institutions contributed to the organization of ICEGOV2013. We wish to thank the official patron of ICEGOV2013, the Ministry of Security and Public Administration of the Republic of Korea for endorsing and supporting the conference. Our sincere thanks go to the National Information Society Agency, Republic of Korea (NIA) as the local organizer of the conference, particularly to Jeongwon Yoon for his vision and leadership, and to Dohyoon Kim and the whole team in NIA for their hard work and dedication to making the combined ICEGOV2013 and Global e-Government Forum event successful. We wish to express our most sincere thanks to the key sponsors Macao SAR Government and Macao Foundation and the sponsor Electronic Government of the Republic of Kazakhstan whose generous contributions allowed many academics and practitioners from developing countries to attend the conference. Special gratitude is due to Macao SAR Government, its Public Administration and Civil Service Bureau, and Macao Foundation for continuing support to the ICEGOV conference series and the origin of the series e-Macao Program. We also wish to thank ICEGOV2013 partners for their presence, support and in-kind contributions: Brunel University, London, UK; Center for Technology in Government, University at Albany, USA; Data.Gov, U.S. Government; German Cooperation, Deutsche Zusammenarbeit and Deutsche Gesellschaft fur Internationale Zusammenarbeit, Germany; IBM; Information and Communication Technologies, World Bank; Microsoft; Ministry of Information Technology and Communication, Colombia (MINTIC); Organization for Economic Co-operation and Development; Poznan University of Economics, Poland; The Insight Centre for Data Analytics, National University of Ireland, Ireland; The Science and Technology Development Fund, Government of Macao SAR, Macao; UNESCO Information for All Programme; United Nations Asian and Pacific Training Centre for Information and Communication Technology for Development; Vive Digital Programme, MINTIC, Colombia; Waseda University, Japan; and the World Wide Web Consortium. We also wish to express our thanks to ACM Press for publishing the ICEGOV2013 conference proceedings. We are most grateful to the whole Advisory Committee for supporting the conference and to all members of the Program Committee and additional reviewers for their efforts to carry out quality reviews and to help build a strong conference program. We thank keynote speakers; organizers, chairs and moderators of the plenary sessions, town hall debates, paper tracks, thematic sessions, the doctoral colloquium, and the poster session; and all panelists and speakers for their intellectual contributions. Last but not least, we are most thankful to all authors for their efforts in preparing, submitting and presenting papers at ICEGOV2013.We hope that ICEGOV2013 will further contribute to building, growing and connecting global EGOV research, policy and practice communities, able to cross not only national and regional but also institutional and thematic borders, and that the contacts, discussions and ideas initiated in Seoul in October 2013 will continue well after the conference and towards ICEGOV2014 in Guimaraes, Portugal.",,,,2013,,,,Book
Smart House: Artificially Intelligent Home Automation System,"Technologies like Home Automation Systems are still under considerations and have not been developed up to the level of ultimate maturity. Targeting the challenges of the proposition of a home automation system capable of controlling house's electric appliances and providing efficient security system, a thorough research has been conducted in fields of System Automation, Hardware Engineering, Software Engineering, Human Machine Interaction, Mobile Programming, Produce Line Architectures, Software Testing and Data Management Systems. Taking help from reviewed research and using our personal research and development experiences, we have proposed an approach i.e. Smart House. This book discusses in detail the way to achieve the set goals of this research along with the detailed presentation of system requirements, proposed approach, hardware and software designs, implemented prototype, testing, deployment, usage, benefits and limitations. The presented research and development has been performed as the part of author's bachelor project submitted in Fall 2002 at Punjab Institute of Computer Science, Faculty of Information Technology, University of Central Punjab, Pakistan.",,"Ahmed Z,Ali M",,2011,,,,Book
Analysis and Assessment of Islamic State's Military Strategy in Iraq (2011-2015),"Because many militant groups from the Islamist political landscape and beyond have suffered extinction, survival of insurgent groups in a context of insecurity and rivalry is not a fact. In 2010, the Islamic State in Iraq was near extinction and considered as defeated by a myriad of enemies regrouping 400 000 fighters (the US and Iraqi forces and militias). Four years later, it was able to control a significant part of the Iraqi and Syrian soil, outperform all Islamist groups in history, proclaim a caliphate and export its model, which will have long-lasting consequences at regional and international levels. This thesis seeks to explain the group's resurgence from 2011 to 2015 by adopting a provincial perspective and with the theoretical framework of the indirect approach. By introducing a categorisation of operations based on their confrontational nature, this investigation tries to understand IS' military effort in Iraq from a quantitative and qualitative perspective. In addition, a study of its relations with challenging social structures (tribes and insurgent groups) and an analysis of the group's propaganda frames give us the possibility to determine how the group introduced more flexibility in its overall strategy and articulated a particular discourse in order to attract deprived Sunni Iraqis during the 2012 Iraqi protests. The main contribution of using this model is to explain IS' past resurgence and enrich the existing literature with a complementary explanation of the group tactics and rapid morphing from a guerrilla to conventional warfare. This research project possesses the following creative elements: it provides a detailed and in-depth account of Islamic State's strategy, applies theoretical frameworks from security studies to it and offers a better understanding of the group ́s political behaviour by analysing its interactions with a range of actors ranging from its social incubator to competitive social structures and ideological rivals. It aims to expand on the idea of the Islamic State as an insurgent group that has adopted a repertoire of different strategies to establish an expansive caliphate by closely examining its adoption of the indirect approach and its execution at the operational levels of war.",,Laghmari M,,2020,,,,Ph.D. Thesis
ROBOTMAN: Security Robot for Human-Robot Interaction Inside Malls,"The market for service robot is rapidly growing nowadays to interact in a direct manner with humans. In the future we expect that robots will be able to provide a variety of services for humans. The concept of humanoid robot working as security guards for public places has grown in recent years. The security job is presented as a difficult activity that requires a significant physical wear on the average individual. Additionally, the demand of security services are growing around the world but companies that provide the services do not have enough trained personal to satisfy this opportunity.Thus, a security robot was developed in order to perform patrols during the night, while functioning as a platform for human-robot interaction during the day in indoors. One of the goals is to improve the welcoming of visitors to the mall using a robot that can also provide security and give information about the mall to the customers. A security company provided the key information, knowledge and guidance regarding the activities that a security guard is required to perform when working inside a mall. This work was developed with government funding and the collaboration between industry and university.We designed a stable and aesthetically pleasing security humanoid robot that is able to not only monitor a specific area, but to welcome visitors, provide information, help people, and improve their visit to a mall located in Peru. The design process is presented in Fig. 1a and preliminary tests were performed inside a mall Fig. 1.b. The robot has several sensors such as security cameras, depth camera, proximity sensors and a LIDAR to avoid obstacles, also actuators to move the arms and head to show expressions, LED eyes to represent emotional states, microphones and a pleasant voice to improve interaction with the public.The results reveal that the robot is a helpful tool for the security guards to improve their work and also satisfy at some level the expectations of the customers. The robot's appearance fulfills its objective of inviting people to interact with it, in this way the robot achieves its role of informative agent. The customers were able to interact through its interactive screen and the remote application for telecommunication with an external human agent that work for the mall and answer all their questions projecting his voice through the robot.We conclude that social robots could improve the life of the people not only in their homes, but also in open spaces where security and attention to costumers is needed. It's still a long way to the robot to perform as a human security guard but in the moment we present a helpful tool that can support their activities remotely. This technology is not aiming to replace humans, but to improve his performance in their job allowing them to cover large and remote areas.",,"López JA,Cuéllar F",,2017,410,10.1145/3029798.3036653,https://doi-org.proxy.bnl.lu/10.1145/3029798.3036653;http://dx.doi.org/10.1145/3029798.3036653,Conference Paper
Front Matter,"In the great digital era, we are witnessing many rapid scientific and technological developments in human-centered, seamless computing environments, interfaces, devices, and systems with applications ranging from business and communication to entertainment and learning. These developments are collectively best characterized as Active Media Technology (AMT), a new area of intelligent information technology and computer science that emphasizes the proactive, seamless roles of interfaces and systems as well as new media in all aspects of digital life. An AMT based computer system offers services that enable the rapid design, implementation, deploying and support of customized solutions.The first International Conference on Active Media Technology (AMT01) was held in Hong Kong in 2001, the second International Conference on Active Media Technology (AMT03) was held in Chongqing, China in May 29--31 of 2004, and the third International Conference on Active Media Technology (AMT05) was held in Kagawa, Japan in May 2005. The 4th International Conference on Active Media Technology (AMT06) follows the success of AMT01, AMT03 and AMT05.AMT06 is the leading International Conference focusing on Active Media Technology. It aims to bring together researchers from diverse areas, such as Web intelligence, data mining, intelligent agents, smart information use, networking and intelligent interface. It also encourages collaborative research in these areas to provide best services for enabling the rapid design, implementation, deploying and support of customized solutions.The conference includes the following topics:• Active Computer Systems and Intelligent Interfaces• Adaptive Web Systems and Information Foraging Agents• Web mining, Wisdom Web and Web Intelligence• E-Commerce and Web Services• Data Mining, Ontology Mining and Data Reasoning• Network, Mobile and Wireless Security• Entertainment and Social Applications of Active Media• Agent-Based Software Engineering and Multi-Agent Systems• Digital City and Digital Interactivity• Machine Learning and Human-Centred Robotics• Multi-Modal Processing, Detection, Recognition, and Expression Analysis• Personalized, Pervasive, and Ubiquitous Systems and their Interfaces• Smart Digital Media• Evaluation of Active Media and AMT Based SystemsAMT06 is sponsored by the IEEE Systems, Man, and Cybernetics Society and Queensland University of Technology. It attracted 123 submissions from 19 countries and regions: Algeria, Australia, China, Canada, England, Finland, France, Hong Kong, India, Japan, Korea, New Zealand, Pakistan, Poland, Republic of Korea, Taiwan, United Arab Emirates, United Kingdom, and United States of America. The review process was rigorous. Each paper was reviewed by two reviewers at least, and most of them reviewed by three reviewers.The Program Committee accepted 39 regular papers (the approximate acceptance rate is 32%), 33 short papers (the approximate acceptance rate is 39%) and 9 industry/demonstration papers.We would like to thank the members of Program Committee and Organization Committee and reviewers who contributed to the success of this conference.Yuefeng Li, Mark Looi and Ning Zhong, 17 March 2006",,,,2006,i–xvi,,,Conference Paper
MODIS-Based Remote-Sensing Monitoring of the Spatiotemporal Patterns of China's Grassland Vegetation Growth,"China has abundant grassland resources approximately 400 million ha of natural grasslands, which account for 41.7% of China's total area. Grasslands are an important base for boosting the development of China's livestock husbandry economy and maintaining China's ecological security. Using Moderate Resolution Imaging Spectroradiometer MODIS remotely sensed data, this study developed a grassland vegetation growth index that ranked the magnitude of grassland vegetation growth indices across a wide variety of field experiments. This study applied the grassland vegetation growth index to conduct remote-sensing monitoring of the spatiotemporal status of China's grassland vegetation growth in 2008. We found that the vegetation growth of China's grassland was classified as ‘good’ in 2008. The areas of grassland with desirable vegetation growth accounted for 38.47% of China's monitored grassland areas, and the areas with less desirable vegetation growth accounted for 22.85%. Additionally, the good vegetation growth was stable within each 10 day study period in 2008. The vegetation growth reached a balance in early June. After early September, the proportion of grasslands with desirable vegetation growth declined, and the proportion of grasslands with balanced and less desirable growth increased. The regions with less desirable vegetation growth mainly included the middle and eastern regions of Inner Mongolia, the northern region of Xinjiang, and most parts of Heilongjiang. The regions with desirable vegetation growth were mainly distributed in the north of Tibet, the southwest of Qinghai, the west of Inner Mongolia, Gansu, Ningxia, Shanxi, and the northwest of Liaoning. The remote-sensing monitoring of the spatiotemporal patterns of China's grassland vegetation growth in the present study revealed the overall vegetation growth status of China's grassland on a broad scale. These findings could provide a helpful scientific basis for understanding China's grassland vegetation conditions and the management and regulation of grassland livestock husbandry.",,"Xu B,Yang XC,Tao WG,Miao JM,Yang Z,Liu HQ,Jin YX,Zhu XH,Qin ZH,Lv HY,Li JY",,2013,3867–3878,10.1080/01431161.2012.762696,https://doi-org.proxy.bnl.lu/10.1080/01431161.2012.762696;http://dx.doi.org/10.1080/01431161.2012.762696,Journal Article
ACSC '08: Proceedings of the Thirty-First Australasian Conference on Computer Science - Volume 74,"The Australasian Computer Science Conference (ACSC) series is an annual forum, bringing together research sub-disciplines in Computer Science. The meeting allows academics and researchers to discuss research topics as well as progress in the field, and policies to stimulate its growth. This volume contains papers presented at the Thirty First ACSC in Wollongong, NSW, Australia. ACSC 2008 is part of the Australasian Computer Science Week which ran from Jan 22nd to 25th, 2008.The ACSC 2008 call for papers solicited contributions in all areas of computer science research. This years conference received submissions from Australia, New Zealand, China, France, India, Iran, Jamaica, Jordon, Malaysia, Pakistan, South Africa, Turkey, UK, and Taiwan. The topics addressed by the submitted papers illustrate the broadness of the discipline. The authors categorised their submissions into one or more of the following topics:- Algorithms (9 papers)- Artificial Intelligence (7 papers)- Communications and Networks (4 papers)- Computer Architecture (2 paper)- Computer Vision (4 papers)- Databases (5 papers)- Distributed Systems (6 papers)- E-Commerce (4 papers)- Formal Methods (6 papers)- Graphics (6 papers)- High Performance Computing (7 papers)- Human-Computer Interaction (8 papers)- Mobile Computing (6 papers)- Multimedia (1 paper)- Object Oriented Systems (3 papers)- Ontologies (1 paper)- Operating Systems (5 papers)- Programming Languages (4 papers)- Robotics (1 paper)- Scientific Computing (5 papers)- Security and Trusted Systems (5 papers)- Simulation (6 papers)- Software Engineering (5 papers)- Speech (1 paper)- Theory (3 papers)- Visualization (6 papers)- Web Services (3 papers)The programme committee consisted of 28 highly regarded academics from around the globe, including Australia, Brazil, Canada, Japan, New Zealand, Singapore and USA. All papers were sent to at least three programme committee members for review and every effort was made to obtain at least three reviews. Of the 47 papers submitted, 16 were selected for presentation at the conference.The programme committee invited Professor Joxan Jaffar, to give a keynote on Constraint Logic Programming for Program Analysis. Professor Jaffar has recently completed a stint as Dean of the School of Computing from 2001-2007 at the National University of Singapore. His interests are in programming languages and applications, with emphasis on the logic and constraint programming paradigms. Amongst his main contributions are the principles of constraint logic programming, and the widely-used CLP(R) system. The committee also invited Dr Benjamin Burton and Associate Professor Ewan Tempero to give invited talks. Dr Burtons talk was entitled Informatics Olympiads:Challenges in Programming and Algorithm Design. Associate Professor Temperos talk is entitled On Measuring Java Software.",,,,2008,,,,Book
CSTST '08: Proceedings of the 5th International Conference on Soft Computing as Transdisciplinary Science and Technology,"Soft Computing (SC) has an evolving collection of methodologies, which is aimed to exploit tolerance for imprecision uncertainty, and partial truth to achieve robustness, tractability, and low cost. SC provides attractive opportunity to represent the ambiguity in human thinking with real life uncertainty. Fuzzy logic (FL), Neural Networks (NN), and Evolutionary Computation (EC) were the core methodologies of soft computing. Later chaos computing, fractal theory, wavelet transformation, cellular automaton, percolation models, and immune network theory were added to enhance soft computing. However, they should not be viewed as competing with each other, but synergistic and complementary, instead. SC was actually the combination or fusion of each methodology which yielded new computational capabilities (hybrid systems). Soft computing is currently causing a paradigm shift (breakthrough) in science and technology.The stage for the Fifth IEEE/ACM International Conference on Soft Computing as Transdisciplinary Science and Technology (CSTST'08) has been set. This edition is dedicated to commemorate the memory of Professor Yasuhiko Dote, Founding Chair of WSTST series of meetings. In essence, CSTST'08 is built on the success of the previous four events held in Muroran, Japan namely the IEEE International Workshop on Neuro Fuzzy Control, in 1993; IEEE International Workshop on Soft Computing in Industry, in 1996, the IEEE International Workshop on Soft Computing in Industry, in 1999 and International Workshop on Soft Computing as Transdisciplinary Science and Technology (WSTST'2005). CSTST'08 is hosted by University of Cergy Pontoise, France and is technically co-sponsored by IEEE Systems Man and Cybernetics Society, ACM SIGAPP (French Chapter), IEEE French Section, World Federation on Soft Computing, European Society for Fuzzy Logic, Technology and International Fuzzy Systems Association, and AFIHM - French Association of Human Computer Interaction. On behalf of the CSTST'08 program committee, we wish to extend a very warm welcome to this edition in Cergy-Pontoise/Paris, France. The conference program committee has organized an exciting and invigorating program comprising presentations from distinguished experts in the field, and important and wide-ranging contributions on state-of-the-art research that provide new insights into current cutting edge results on ""Soft Computing as Transdisciplinary Science and Technology"".This year, we received over 212 regular submissions and we are really gratified by the international diversity of this conference: authors of submitted work hail from no less than 30 countries including Vietnam, Egypt, Bulgaria, Turkey, Russia, Netherlands, Austria, Malaysia, Sweden, Croatia, Kuwait, Cyprus, Belgium, Estonia, Latvia, Lebanon, Macedonia, Singapore, Argentina, United Arab Emirates, Thailand, Ukraine, Hungary, Ireland, Czech, Republic, Spain, Norway, Taiwan, Canada, Libya, Romania, Mexico, Greece, Brazil, Pakistan, Germany, Australia, Tunisia, India, United States of America, Italy, Korea, Poland, Algeria, Japan, United Kingdom, Iran, China, Portugal, and France. The technical program of CSTST'08 conference comprises of 62 papers. The conference program committee had a very challenging task of choosing high quality submissions. Each paper was peer reviewed by at least three or more independent referees of the program committee and the papers were selected based on the referee recommendations. The papers offer stimulating insights into emerging intelligent technologies and their applications in Internet security, chance discovery, humanized computational intelligence, web intelligence, data mining, image processing, swarm intelligence, optimization and so on.",,,,2008,,,,Book
Factors Affecting Internet Banking Adoption among Internal and External Customers: A Case of Pakistan,"This study investigates the determinants which attract the customers to adopt internet banking in Pakistan by employing internal and external customers, on the sample size of 210 for internal and 151 for external respondents through using the survey research instrument questionnaire. The confirmatory factor analysis with multiple regressions technique has been applied. The result of regression analysis shows that perceived usefulness PU, information of internet banking INF, perceived risk PR, security and privacy SP shows more influence to increase the intention of external customers to adopt internet banking services while government support GS provide more influence for the internal customers in adoption of internet banking services. This study proves that external customers can be more emphasised, if they believe in convenience in adopting the services. It is recommended that banks should take some consideration to apply internet banking by delivering the information in an easiest way, provide more usefulness and benefits and also minimise the fraud as providing more security and privacy. This will help the bank to increase their profit by reducing its cost, time saving and retain more potential users.",,"Raza SA,Hanif N",,2013,82–96,10.1504/IJEF.2013.051746,https://doi-org.proxy.bnl.lu/10.1504/IJEF.2013.051746;http://dx.doi.org/10.1504/IJEF.2013.051746,Journal Article
Essays on Resilience Measurement,"Robust measurement is key to the design and targeting of resilience-building interventions. Yet, conventional approaches to resilience measurement are often ill-suited to the needs of development and humanitarian stakeholders, proving costly, timeconsuming and difficult to coordinate. In this thesis I explore the use, validity and viability of an alternative suite of approaches: subjective measures of resilience. I start by clarifying the conceptual distinctions between subjectivity and objectivity as they relate to resilience measurement, before introducing a continuum that highlights the strengths and weaknesses of different types of approaches. I then develop a new perception-based measure, coined the Subjectively self-Evaluated Resilience Score (SERS). Using a large household survey in Northern Uganda, I provide like-for-like comparisons between SERS and a conventional objective approach to resilience measurement. While I show that the two measures are moderately correlated, they differ notably in associations with key socio-economic traits. In order to further probe the validity of subjective measures, I examine whether SERS is sensitive to external shocks. Using mobile phones to conduct remote interviews I assemble a novel high-frequency panel survey on resilience. Here I reveal how perceived levels of resilience fluctuate in the aftermath of seasonal flooding in Eastern Myanmar: dropping sharply in the first few months, before slowly converging over the course of a year. I also compare the impact of flood exposure across different socio-economic groups, revealing how female-headed households are hardest hit. Lastly, using the same site in Myanmar, I look more closely at the temporal dynamics of resilience. Insights from an extended panel provide quantitative evidence of intra-annual variation in levels of resilience. Here I find consistent non-linear associations between subjectively-evaluated scores and changes in seasonality and weather. Findings also point to potential resilience thresholds and tipping points. Weighed together, these results: challenge core assumptions in the resilience literature; highlight the potential of subjective measures; and point to the need for greater diversity of resilience evidence.",,Jones L,,2020,,,,Ph.D. Thesis
Adaptability of Backcasting for Sustainable Development: A Case Study from Nepal,"To cope with problems like climate change, lack of food security, and poverty, a more reasonable use of existing resources is needed. Hence, a transition towards a sustainable behavior in the industrial as well as the developing countries is of core importance. Transition management and backcasting are two methodologies that have been developed mainly in the Netherlands to achieve this behavioral change. This paper examines in a case study, in a small village in the mid-hills of Nepal, whether these methodologies are also applicable in a developing country. Moreover it analyzes which adjustments are needed to achieve good outcomes. First results show that this methodology seems to be appropriate to trigger a change in thinking towards long-term considerations amongst the small scale farmers. Long-range thinking and future envisioning can stimulate investments in technologies that tend to be sustainable and guarantee a more stable return in the long run. Compared to programs in Europe, instructors should adjust time frame and workshop design.",,"Wieners E,Neuburger M,Schickhoff U",,2015,16–27,10.4018/ijabim.2015070102,https://doi-org.proxy.bnl.lu/10.4018/ijabim.2015070102;http://dx.doi.org/10.4018/ijabim.2015070102,Journal Article
"Multi-Factor, Multi-State, Multi-Model Scenarios","Decision-makers aiming to improve food security, livelihoods and resilience are faced with an uncertain future. To develop robust policies they need tools to explore the potential effects of uncertain climatic, socioeconomic, and environmental changes. Methods have been developed to use scenarios to present alternative futures to inform policy. Nevertheless, many of these can limit the possibility space with which decision-makers engage. This paper will present a participatory scenario process that maintains a large possibility space through the use of multiple factors and factor-states and a multi-model ensemble to create and quantify four regional scenarios for Southeast Asia. To do this we will explain 1) the process of multi-factor, multi-state building was done in a stakeholder workshop in Vietnam, 2) the scenario quantification and model results from GLOBIOM and IMPACT, two economic models, and 3) how the scenarios have already been applied to diverse policy processes in Cambodia, Laos, and Vietnam. We developed 4 multi-factor, multi-state socio-economic scenarios for Southeast Asia.Diverse scenarios provide wide possibility space for testing of robust policies.Quantifying scenarios in multiple models is challenging but increases scenario robustness.Using multiple model increases the scenario possibility space.",,"Mason-D'Croz D,Vervoort J,Palazzo A,Islam S,Lord S,Helfgott A,Havlík P,Peou R,Sassen M,Veeger M,van Soesbergen A,Arnell AP,Stuch B,Arslan A,Lipper L",,2016,255–270,10.1016/j.envsoft.2016.05.008,https://doi-org.proxy.bnl.lu/10.1016/j.envsoft.2016.05.008;http://dx.doi.org/10.1016/j.envsoft.2016.05.008,Journal Article
Author Profiling on Bi-Lingual Tweets,"The task of author profiling aims to distinguish the author’s profile traits from a given content. It has got potential applications in marketing, forensic analysis, fake profile detection, etc. In recent years, the usage of bi-lingual text has raised due to the global reach of social media tools as people prefer to use language that expresses their true feelings during online conversations and assessments. It has likewise impacted the use of bi-lingual (English and Roman-Urdu) text in the sub-continent (Pakistan, India, and Bangladesh) over social media. To develop and evaluate methods for bi-lingual author profiling, benchmark corpora are needed. The majority of previous efforts have focused on developing mono-lingual author profiling corpora for English and other languages. To fulfill this gap, this study aims to explore the problem of author profiling on bi-lingual data and presents a benchmark corpus of bi-lingual (English and Roman-Urdu) tweets. Our proposed corpus contains 339 author profiles and each profile is annotated with six different traits including age, gender, education level, province, language, and political party. As a secondary contribution, a range of deep learning methods, CNN, LSTM, Bi-LSTM, and GRU, are applied and compared on the three different bi-lingual corpora for age and gender identification, including our proposed corpus. Our extensive experimentation showed that the best results for both gender identification task (Accuracy = 0.882, F1-Measure = 0.839) and age identification (Accuracy = 0.735, F1-Measure = 0.739) are obtained using Bi-LSTM deep learning method. Our proposed bi-lingual tweets corpus is free and publicly available for research purposes.",,"Ashraf MA,Nawab RM,Nie F,Pinto D,Singh V,Perez F",,2020,2379–2389,10.3233/JIFS-179898,https://doi-org.proxy.bnl.lu/10.3233/JIFS-179898;http://dx.doi.org/10.3233/JIFS-179898,Journal Article
Simulation of Vehicular Network Use in Emergency Situations and Security Applications on a Pakistan Highway,"VANETs (vehicular ad hoc networks), which are revolutionary techniques to enhance road safety, can be used to broadcast information about dangerous traffic conditions or accidents. However, distributing important information for driver safety and well-being has strict time and reliability requirements. This is because messages must be received by all cars involved in a potentially dangerous scenario for proper precautions to be taken to avoid the problem from materializing or intensifying. Because of the deterioration in conventional wireless communication system performance, ensuring that such requirements are met is a serious concern. To validate the concept before the actual installation of such systems and their absorption into the vehicle sector, it is therefore critical to employ simulation methodologies that are both reliable and thorough. This piece consists of large-scale, realistic security simulation research of an emergency situation based on actual road traffic data acquired on a Pakistan route. The study’s findings are detailed in the following paragraphs. Aspects such as the incorporation of fixed communication units along a stretch of roadway and the performance of the vehicular network notifying all vehicles engaged in the various accident scenarios modeled on the same stretch of highway were evaluated. Both of these characteristics were designed to increase safety and security applications. After doing the investigation, it was observed that when fixed communication units are incorporated into the network infrastructure, there is a shorter delay in receiving the accident notification. This was the conclusion made after reviewing the findings. Drivers of vehicles located closer to the accident site will be able to respond in a timely and safe manner as a result of this improvement in network performance, and drivers security of vehicles located further away will have the option of exiting the highway to avoid potential congestion caused by increased road traffic.",,"Al-Douri AT,Mohammed Kadhim N,Mohamad AA,Abeyie M,Azeem I",,2022,,10.1155/2022/2902263,https://doi-org.proxy.bnl.lu/10.1155/2022/2902263;http://dx.doi.org/10.1155/2022/2902263,Journal Article
Strategic Environmental Assessment for Better Flood Risk Planning in Pakistan / Strategische UmweltprüFung Zur Verbesserung Der Hochwasserschutzplanung in Pakistan,"In this thesis, a possible protocol has been developed using Strategic Environmental Assessment (SEA) in Pakistan for better flood management planning. The aim of the study was to streamline the environmental concerns into higher level planning. The protocol takes into account the strengths and weaknesses of the existing institutional and legal structures for easy capability. The study focused on assessing Pakistan's readiness to make use of SEA in planning especially for flood management in Indus River. The study includes review and analysis of national and international literature, governmental documents and National Experts Survey. The development of the proposal is based on the floods and floods management issues related with Indus River, current flood management practices, associated issues, and need for SEA. In particular, the recently drafted 'Rules on SEA' (2014) in Pakistan was followed to discuss the basic requirements for developing SEA-Report contents for flood management plans. In addition, the reference for SEA application in flood management sector has been made from international practices and experience. Flood management planning system and environmental assessment system of Pakistan have been analyzed to identify strengths and short-comings to justify and develop basis for the research. The thesis comprised of five chapters. Chapter-1 introduces research subject and research methodology. Pakistan's readiness to use SEA including an overview of the existing policy framework, and institutional setup related to Flood Management is described in Chapter-2. The country's experience in using an environment assessment tools like EIA and SEA is discussed in Chapter-3. The findings of Chapter-4 include proposal or possible SEA-Protocol for idealized but easy-to-follow SEA-Report contents within the legal and administrative context of the country. Finally, conclusions, recommendations and subjects for future research have been described in Chapter-5.",,Hameed K,,2017,,,,Ph.D. Thesis
"Mapping the Diversity of Agricultural Systems in the Cuellaje Sector, Cotacachi, Ecuador Using ATL08 for the ICESat-2 Mission and Machine Learning Techniques","The mapping of cropland helps to make decisions due to the intensification of its use, where the conditions of the crops change due to climatic variability and other socio-economic factors. In this way, the implementation of modern sustainable agriculture is essential to prevent soil degradation as measures to guarantee food security, propose sustainable rural development and protect the provision of different ecosystem services associated with the soil. NASA’s Ice, Cloud, and Land Elevation Satellite-2 (ICESat-2) launched September 15, 2018, offers new possibilities for the mapping of global terrain and vegetation. An additional science objective is to measure vegetation canopy height as a basis for estimating large-scale biomass and biomass change. The Advanced Topographic Laser Altimeter System (ATLAS) instrument on-board ICESat-2 utilizes a photon-counting LIDAR and ancillary systems (GPS and star cameras) to measure the time a photon takes to travel from ATLAS to Earth and back again and to determine the photon’s geodetic latitude and longitude. ICESat-2 ATL08 (Along-Track-Level) data product is developed for vegetation mapping with algorithms for along-track elevation profile of terrain and canopy heights retrieval of the from ATLAS point clouds. Thus, this study presents a brief look at the ATL08 product highlight the broad capability of the satellite for vegetation applications working with data of study area Seis de Julio de Cuellaje (SDJC), province of Imbabura, Ecuador. The study used Normalized Difference Vegetation Index (NDVI) by the year 2020 time-series at 30 m resolution by employing a Machine Learning (ML) approach. The results of this research indicate that the ATL08 data from the ICESat-2 product provide estimates of canopy height, show the potential for crop biomass estimation, and a machine learning land cover classification approach with a precision of 95.57% with Digital Elevation Model (DEM) data.",,Fernando G,,2021,170–181,10.1007/978-3-030-87013-3_13,https://doi-org.proxy.bnl.lu/10.1007/978-3-030-87013-3_13;http://dx.doi.org/10.1007/978-3-030-87013-3_13,Conference Paper
Prediction of Wheat Production Using Machine Learning Algorithms in Northern Areas of Pakistan,,,"Ahmed MU,Hussain I",,2022,,10.1016/j.telpol.2022.102370,https://doi-org.proxy.bnl.lu/10.1016/j.telpol.2022.102370;http://dx.doi.org/10.1016/j.telpol.2022.102370,Journal Article
"Heritage Is a Struggle: Music, Neoliberal Logics, and the Practice of Intangible Cultural Heritage in Peru","This dissertation examines the ways neoliberal logics permeate local ideas about musical safeguarding as part of application processes in Peru to nominate expressive practices to the United Nations Educational, Scientific and Cultural Organization (UNESCO) Representative List of the Intangible Cultural Heritage of Humanity. In these collaborative endeavors, the multiple ideas that stakeholders have about musical significance and safeguarding must interact with the standardized concepts and requirements of the Intangible Cultural Heritage (ICH) global framework, which is based on neoliberal logics. Participants in the processes develop strategies for navigating the complex bureaucratic layout of ICH nominations, while they negotiate and compete with each other to achieve forms of musical safeguarding that align with their particular knowledge, needs, and agendas. I argue that this ICH safeguarding mechanism furthers the penetration of neoliberal logics into local understandings of community-based musical practices, as practitioners, state actors, and other stakeholders adhere them to a cultural-value system based on exchange value and cultural expediency and identify opportunities to advance their musical and non-musical agendas. This research is based on an ethnographic study of four application processes of Peruvian musical practices to the UNESCO Representative List, focusing on the actions and decisions of the participants in these applications and ICH safeguarding actions. I explore the inner dynamics of these collaborative endeavors in order to reveal: 1) the way in which neoliberal ideas about ICH safeguarding interacted with local ideas about musical significance; 2) the local and national agendas attached to musical practices and their safeguarding; 3) the strategies taken by the participants to advance their particular agendas while complying with the institutional requirements of this UNESCO mechanism; and 4) the possibilities that this state of affairs might entail for grassroots practitioners.",,"Chocano R,León J,Avellaneda C",,2020,,,,Ph.D. Thesis
Factors Affecting E-Commerce Adoption Among Smes : A Case Study Investigation of a Developing Economy - Pakistan,"The main objective of this study is to increase the knowledge base on the factors affecting adoption of e-commerce in small and medium-sized enterprises (SMEs). As such the study evaluates and extends the Technology-Organisation-Environment (TOE) framework developed by Tornatzky and Fleischer (1990) by adding an individual context. A qualitative interpretative research approach is adopted with the case study as the research plan, and the philosophical approach adopted is predominantly epistemology and methodology. The context within which this study is set is SMEs within the developing economy of Pakistan. Eight case studies were chosen with three participants from each; giving a total of 24 participants. At the time of data collection, all participants were either owner-manager/chief executive officers or managers within departments. Qualitative data were collected by applying data triangulation techniques including face-to-face and telephone interviews, direct observation, documentary evidence and website content. Data were then analysed using thematic techniques. Using the extant literature and the data collected in this study (particularly in relation to the extended individual context), it is found that the following factors in particular influence the adoption (or not) of e-commerce within the participating Pakistani SMEs: (i) availability and quality of the latest ICT infrastructure units, (ii) national online readiness, (iii) internet speed, (iv) online payment security and data privacy mechanism, (v) power outages, (vi) organisational size and structure, (vii) website availability and ease of payment facility, (viii) inadequate financial and ICT skilled expertise (HR), (ix) traditional business selling methods, (x) consumer preference for cash on-delivery payments, (xi) lack of consumer confidence in e-commerce, (xii) technological awareness and education in society, (xiii) lack of government and local business institutes' support and (xiv) owner-managers and senior management characteristics. This study contributes to the knowledge base for better understanding the adoption factors of e-commerce and supports the use of the extended TOE model. The qualitative results are useful not only for SMEs themselves, but also for policymakers, governments and local business institutes, and future researchers. In terms of the limitations of the study, the study is limited as it concerns only SMEs in three cities in Pakistan (Islamabad, Lahore and Karachi). Other qualitative and quantitative studies could include SMEs from different rural areas of Pakistan. In addition, it would be interesting to compare the results with those of other developing economies in the Southeast region.",,Nazir M,,2019,,,,Ph.D. Thesis
Automated Generation of Counterterrorism Policies Using Multiexpert Input,"The use of game theory to model conflict has been studied by several researchers, spearheaded by Schelling. Most of these efforts assume a single payoff matrix that captures players’ utilities under different assumptions about what the players will do. Our experience in counterterrorism applications is that experts disagree on these payoffs. We leverage Shapley’s notion of vector equilibria, which formulates games where there are multiple payoff matrices, but note that they are very hard to compute in practice. To effectively enumerate large numbers of equilibria with payoffs provided by multiple experts, we propose a novel combination of vector payoffs and well-supported ϵ-approximate equilibria. We develop bounds related to computation of these equilibria for some special cases and give a quasipolynomial time approximation scheme (QPTAS) for the general case when the number of players is small (which is true in many real-world applications). Leveraging this QPTAS, we give efficient algorithms to find such equilibria and experimental results showing that they work well on simulated data.We then built a policy recommendation engine based on vector equilibria, called PREVE. We use PREVE to model the terrorist group Lashkar-e-Taiba (LeT), responsible for the 2008 Mumbai attacks, as a five-player game. Specifically, we apply it to three payoff matrices provided by experts in India--Pakistan relations, analyze the equilibria generated by PREVE, and suggest counterterrorism policies that may reduce attacks by LeT. We briefly discuss these results and identify their strengths and weaknesses from a policy point of view.",,"Sawant A,Dickerson JP,Hajiaghayi MT,Subrahmanian VS",,2015,,10.1145/2716328,https://doi-org.proxy.bnl.lu/10.1145/2716328;http://dx.doi.org/10.1145/2716328,Journal Article
Flexible Robotic Teleoperation Architecture Under IEC 61499 Standard for Oil & Gas Process,"One of the main characteristics of oil extraction stations is the latent danger present in the installations due to the nature of the procedures done and the type of raw material used. In Ecuador, another specific complication is presented in this type of stations due to their geographical locations. Teleoperation allows a human operator to transmit his abilities and capabilities into specialized robotic elements capable of replicating them. This allows a person to successfully execute manipulation and transporting tasks in dangerous environments with limited access, from a secure place located in a far distance. However, for the implementation of teleoperated systems, it is necessary to ensure the execution of operative and communication tasks in real time. This can be obtained with the use of advanced and high performance devices and improved communication protocols. The aim of this paper is to propose the Robotic Technology Transfer (RTT) of a teleoperation system developed in academic research laboratories to the industrial oil extraction field. It allows an operator to control a mobile manipulator to perform inspection and maintenance tasks in a Drilling Rig from an oil extraction station, basing its work on the use of the industrial automation standard IEC-61499 and the MQTT protocol for communications.",,"Garcia CA,Naranjo JE,Campana LA,Castro M,Beltran C,Garcia MV",,2018,1269–1272,10.1109/ETFA.2018.8502520,https://doi-org.proxy.bnl.lu/10.1109/ETFA.2018.8502520;http://dx.doi.org/10.1109/ETFA.2018.8502520,Conference Paper
Digital Competencies for Developing and Managing Digital Libraries,"PurposeThe purpose of this study was to explore the essential digital competencies for developing and managing digital libraries. The study identified useful training programs for university librarians to acquire digital competencies. It examined their digital competencies for developing and managing digital libraries in universities of Pakistan. This study also evaluates their digital knowledge in applying security measures to protect digital contents.Design/methodology/approachThe quantitative research method was used to conduct this study. Research questions and hypothesis were developed to achieve the objectives. In-depth review of related literature was conducted to draft a list of essential digital competencies for developing and managing digital libraries. It was circulated among the panel of experts to get their valuable feedback to make a final list of digital competencies for developing and managing digital libraries. A questionnaire was developed to measure the status of digital competencies of university librarians in Pakistan. It was pre-tested on 20 respondents before applying to the whole population. SPSS software was used to analyze data. Descriptive and inferential statistics were applied to achieve results.FindingsThe findings of the study showed that digital competencies for developing and managing digital libraries fall into three main categories: digital competencies for developing digital libraries; digital competencies for managing digital libraries; and digital competencies to protect digital contents. The results revealed that training programs offered by Higher Education Commission HEC, library associations, library schools, in-house trainings, use of online tutorials and trainings offered by skilled professionals are highly important and useful for university librarians to acquire digital competencies. The study concluded that the university librarians working in HEC recognized universities in Punjab province possess basic level of digital competencies for developing and managing digital libraries. Their digital competencies vary on the basis of their university type, i.e. public and private sector.Research limitations/implicationsThis study measures digital competencies of university librarians in Pakistan. The study has practical implications for librarians, library schools, library associations, university libraries and HEC.Practical implicationsThis study has practical implications for librarians, information professionals, libraries and library schools. The results are useful for librarians to get knowledge of digital competencies which are essential for developing and managing digital libraries and protecting digital contents. They can develop their digital competencies in identified areas. This study has identified useful training programs for university librarians for acquiring digital competencies. The university librarians should use these programs to gain needed digital skills.Social implicationsLibrarians can get knowledge of digital competencies for developing and managing digital libraries to face the challenges of digital age.Originality/valueIn contrast to previous research work on investigating computer skills, information and communication technology skills, technological skills and general digital skills, this study particularly identifies the essential digital competencies for developing and managing digital libraries. It helps library and information science schools, library associations, training groups and university libraries to offer adequate training opportunities in identified areas to meet the challenges of the digital age.",,,,2017,573–597,10.1108/EL-06-2016-0133,https://doi-org.proxy.bnl.lu/10.1108/EL-06-2016-0133;http://dx.doi.org/10.1108/EL-06-2016-0133,Journal Article
Observing Gender Dynamics and Disparities with Mobile Phone Metadata,"We explore the extent to which gender disparities in Pakistan are reflected in the anonymized mobile phone logs of millions of Pakistani residents. Our analysis uses data capturing the communications behavior of several million individuals, for whom we observe the gender, but no additional demographic or personally identifying information. Here, we focus on validating aggregate regional patterns, correlating metrics derived from the mobile phone logs with socioeconomic statistics collected from more traditional sources. In these preliminary results, we observe a statistically significant relationship between districts with relatively high rates of female mobile phone penetration and districts that report high levels of gender parity in traditional surveys. However, this relationship is not uniform, and less developed regions exhibit a weaker correlation. We interpret these findings as suggestive evidence that such data can provide a novel perspective on gender dynamics in developing countries.",,"Reed PJ,Khan MR,Blumenstock J",,2016,,10.1145/2909609.2909632,https://doi-org.proxy.bnl.lu/10.1145/2909609.2909632;http://dx.doi.org/10.1145/2909609.2909632,Conference Paper
Poster: Optimal Path Finding for Emergency Cases on Android,"This paper intends to develop the optimal path finding for emergency cases on mobile devices. According to the weak road network infrastructure of Myanmar, there are some difficulties for Emergency Vehicles. In some townships, there are narrowed roads which are not wide enough to enter the vehicles and closed roads which are not passed through the other streets. In the emergency cases (e.g. Accident or Fire), the drivers mistakenly choose these roads, it can cause problems and delays. The main objective of this system is to find the optimal routes between incident site and emergency services without delay caused by closed and narrowed roads. The system uses Multiple Sources Single-Destination (MSSD) Algorithm using node exclusion to calculate optimal route. Our proposed system significantly solves to find the accident location and locate the closest emergency services by using the real-time technology (GPS/GSM).",,"Phyo KZ,Sein MM",,2016,71,10.1145/2938559.2948851,https://doi-org.proxy.bnl.lu/10.1145/2938559.2948851;http://dx.doi.org/10.1145/2938559.2948851,Conference Paper
Comparative Analysis of Terrorists’ Objectives Hierarchies,"To develop effective counterterrorism strategies, it is important to understand the capabilities and objectives of terrorist groups. Much of the understanding of these groups comes from intelligence collection and analysis of their capabilities. In contrast, the objectives of terrorists are less well understood. In this article, we describe a decision analysis methodology to identify and structure the objectives of terrorists based on the statements and writings of their leaders. This methodology was applied in three case studies, resulting in the three objectives hierarchies of al-Qaeda, Islamic State of Iraq and the Levant (ISIL), and Hezbollah. In this article, we propose a method to compare the three objectives hierarchies, highlight their key differences, and draw conclusions about effective counterterrorism strategies. We find that all three terrorist groups have a wide range of objectives going far beyond the objective of killing and terrorizing people in the non-Muslim world. Among the shared objectives are destroying Israel and expelling Western powers from the Middle East. All three groups share the ambition to become a leader in the Islamic world. Key distinctions are the territorial ambitions of ISIL and Hezbollah versus the large-scale attack objectives of al-Qaeda. Objectives specific to ISIL are the establishment of a caliphate in Iraq and Syria and the re-creation of the power of Sunni Islam. Hezbollah has unique objectives related to the establishment of a Palestine State and to maintain the relationship with and support of Iran and Syria. Al-Qaeda’s objectives remain focused on large-scale attacks in the West. We also note a recent shift to provide support for small-scale attacks in the West by both al-Qaeda and ISIL. Our method can be used for comparing objectives hierarchies of different organizations as well as for comparing objectives hierarchies over time of one organization.",,"Siebert JU,von Winterfeldt D",,2020,97–114,10.1287/deca.2019.0400,https://doi-org.proxy.bnl.lu/10.1287/deca.2019.0400;http://dx.doi.org/10.1287/deca.2019.0400,Journal Article
Mainstreaming Disaster Risk Reduction in Nepal : The Rhetoric and the Reality,"To address the growing frequency and intensity of disasters a global effort is underway to change the dominant approach to disaster policy from disaster response to integrating disaster risk reduction (DRR) throughout development activities. Research into how DRR policy progresses in a government context is lacking. Using a qualitative case-study approach this research examines how the global policy prescription of mainstreaming disaster risk reduction (DRR) is unfolding within the Government of Nepal. In particular, this research a) challenges the rhetoric of substantive policy change that underpins the concept of mainstreaming and b) questions its efficacy as a neoliberal post-New Public Management policy tool given that the disaster vulnerability literature implicates neoliberalism as a driver of disaster risk. Finding change to be the dominant theme throughout the research, it applies theories and frameworks from the policy paradigm change literatures (e.g. Advocacy Coalition Framework, social learning and paradigm policy change) to explain what was found in the Nepal case-study. Eight months of fieldwork took place throughout 2014-2016. In total, eighty-eight in-depth interviews were conducted with bureaucrats and political party members at the central, district, and local levels. This research advances the disaster vulnerability scholarship through its critique of neoliberal policy discourse and its application of policy change literature. It is argued that the concept of mainstreaming fits the criteria of a neoliberal buzzword; the findings of this research demonstrate why this is problematic. The lead ministry responsible for disaster management appropriated the global policy rhetoric of mainstreaming DRR in order to minimize any substantive policy change that the DRR agenda promotes. Despite this, evidence is also found of a growing awareness and advocacy of DRR within the Government of Nepal. This is suggestive of an advocacy coalition starting to develop, which is being built through social learning. The role of individual bureaucrats and political party members, rather than a centralised legalistic approach, is found to be fundamental to changing the disaster response policy paradigm. This research calls attention to the need to critically analyse how top-down global DRR policy prescriptions are interpreted by nation-states. Empty and hollow global policy buzzwords are easily translated into a rhetoric that does not match with the reality of the governing and the policy environment.",,Walsh S,,2017,,,,Ph.D. Thesis
SIN '15: Proceedings of the 8th International Conference on Security of Information and Networks,"Dear colleagues and friends, we invite you to the 8th International Conference on Security of Information and Networks (SIN 2015), which is held in Sochi, one of the most beautiful cities in Russia. In 2014, Sochi held remarkable Winter Olympic Games. You will be inspired by warm Black Sea, high Caucasus, and interesting discussions of information security problems. The conference of SIN series is held in Russia for the second time. It is hosted by Southern Federal University (SFedU), in particularly by its following subdivisions: the Institute of Computing Technologies and Information Security, the Department of IT-Security, and South-Russian Regional Educational and Research Center for IT-Security Problems. The honorary chair of the conference is Dr. Marina Borovskaya, the rector of SFedU.The 8th International Conference on Security of Information and Networks is an international forum for presentation of theoretical and applied results in the area of information and network security. SIN 2015 continues the tradition of fruitful meetings of SIN series in Famagusta, Taganrog, Sydney, Jaipur, Aksaray, and Glasgow. Future conferences are scheduled to take place in the USA (2016) and Australia (2017). The program of SIN 2015 includes invited and selected peer-reviewed talks, as well as special sessions on theory and practice of information security. The papers will be published in the digital library of Association of Computing Machinery (ACM) and indexed by Scopus. Just like previous events of SIN series, SIN 2015 is carried out in technical cooperation with Special Interest Group on Security Audit and Control of ACM. The Program Committee received 92 paper proposal, of which 34 were accepted as full papers and 29 were accepted as short papers, position statements and fast abstracts. Geographical distribution of the participants is quite diverse. There are 37 representatives of 16 Russian universities, including Southern Federal University, National Research Nuclear University MEPhI, Bauman Moscow State Technological University, Kazan National Research Technological University, Peter the Great St. Petersburg Polytechnic University, Novosibirsk State University, Omsk State University of Technology, Siberian State Aerospace University, Samara State University of Technology, etc. Nearly thirty participants of the conference represent universities and research institutions of Belgium, China, Colombia, Germany, India, Iran, Italy, Luxembourg, Pakistan, Tunisia, Turkey, the United Kingdom, and the USA.",,,,2015,,,,Book
Assessment of Climate Change Impact on Vegetation Using Satellite Earth Observations,"Understanding how vegetation growth and seasonality will change as a result of climate change is imperative for the development of targeted mitigation strategies by governments and non-governmental organizations (NGOs). Iraq is a country that is teetering precariously under the current climate regime and political instability. However, little environmental research has been carried out, and as such relatively little is known about the interaction between Iraq's vegetation and climate. The United Kingdom (UK), although a more economically and politically stable country, too, has areas that will be affected by future climate change, including increased risk of flooding and extreme temperatures and sea level rise. The main aim of this thesis is to develop a robust model that provides high-resolution data, which can predict future phenological growth in regions with different climate regimes (Iraq and the UK). This thesis sets out to test a modified Growing Season Index (GSI) model and to answer the following questions: RQ1) What is the relationship between the Normalized Difference Vegetation Index (NDVI) from the MODerate-resolution Imaging Spectroradiometer (MODIS) and two climatic factors (precipitation and air temperature)?: RQ2) Does the use of precipitation as a variable make the GSI phenological model more robust? And RQ3) Can the model be used to predict future phenological changes? The study areas (Iraq and the UK) contain a number of climatic and environmental zones. The proposed model is tested across the whole of Iraq and in three climatically and environmentally different regions: Sulaymaniyah, Wasit and Basrah. The model is also tested in the UK for comparison with Iraq. First, the relationship was investigated between the MODIS NDVI and two climatic variables, precipitation and air temperature, over the last decade (RQ1). The results show that there is a strong link between temporal patterns of NDVI and precipitation, and a weak link with air temperature, thus indicating that precipitation is the primary factor in germination with air temperature acting as a secondary driver. Further, an extant phenological model, the GSI, is modified by adding the new precipitation variable, to better quantify relationships between weather and vegetation canopy dynamics across the various semi-arid regions of Iraq (RQ2). It is found that the correlations are more robust with the modified model. The model is then used to test the applicability of the GSI model in predicting future phenological changes (RQ3) using climate change scenario datasets for the period (1951-2098). The results show that the modified GSI model performs well in predicting future phenological changes. The model is tested across the UK for comparison with Iraq. The results show that the modified model is far more robust when the new variable of precipitation is added. It performs well in comparison with the past NDVI datasets. It also simulates well in comparison with other climate scenario models and can be confidently used to predict future climate change, particularly in areas with insufficient infrastructure and political stability leading to a dearth of ground survey data. In addition, the thesis investigates the duration of maturity of the vegetation and monitoring wheat cropland growth in a specific region of the UK (Duxford) over a limited time span, using a new remote sensing dataset (Sentinel S-1 images) and applying the Differential Interferometry Synthetic-Aperture Radar (DInSAR) technique, with a comparison with MODIS NDVI data. The results show that there is real potential in using the DInSAR technique and remotely sensed data (MODIS dataset) to estimate the crop height and to calculate the area of crop distribution.",,Daham AM,,2020,,,,Ph.D. Thesis
Challenges and Solutions Implementing an SMS Text Message-Based Survey CASI and Adherence Reminders in an International Biomedical HIV PrEP Study (MTN 017),,,"Brown W,Giguere R,Sheinfil A,Ibitoye M,Balan I,Ho T,Brown B,Quispe L,Sukwicha W,Lama JR,Carballo-Diéguez A,Cranston RD",,2018,78–86,10.1016/j.jbi.2018.02.018,https://doi-org.proxy.bnl.lu/10.1016/j.jbi.2018.02.018;http://dx.doi.org/10.1016/j.jbi.2018.02.018,Journal Article
Improving Design & Usability of Interactive Vulnerability Mapping Tools for Global Health Preparedness,"The ability of organizations and governments to anticipate disease outbreak risks and respond to emergent threats, commonly known as global health preparedness, presents both a challenging opportunity and an urgent imperative for public health informatics interventions. An example is the need to address the public health risks of vector-borne and zoonotic disease (VBZD) outbreaks, as understanding and preparing for such multifactorial events involves the careful integration of human, animal, entomological, environmental, and infrastructure data. The integration, presentation, and understanding of this data, and associated risks, demands usable tools and technology. Visualization can be a useful way to apply systems thinking to such problems. Unfortunately, existing visualization tools frequently do not assess whether they meet the needs of their users and do not incorporate best practices championed by human centered design (HCD). In my dissertation research, I propose design recommendations for visualization tools to help decision makers in global health preparedness identify spatial areas that are vulnerable to outbreaks, meaning better awareness in areas at a relatively high risk for VBZD outbreaks and a lower capacity to contain spread.Spatial Systems for Decision Support (SSDS) are a type of visualization tool that enable public health practitioners to make critical decisions informed by timely access to pertinent, analyzed data. In my research, I propose a new type of SSDS, interactive vulnerability mapping tools. This new tool can provide critical, rapid support to decision makers and practitioners in global health. Decision makers include epidemiologists, public health planners, vector control specialists, and directors, each of whom might use this information to allocate vaccine resources or plan intervention activities to high-risk regions.In my dissertation research, I have applied principles of human-centered design (HCD) and data visualization to design and evaluate the usability of interactive vulnerability mapping tools for dengue vulnerability in Peru (Aim 1) and Rift Valley fever vulnerability in Kenya (Aim 2). To situate my Aims 1 and 2 in the context of existing literature, I conducted a scoping review of interactive vulnerability mapping tools for VBZD preparedness (Aim 3) that describes current literature by characterizing data, users, technology, and use cases. I then compare findings from Aims 1 and 2 to the existing literature to identify gaps and inform design recommendations for future work. This work contributes: 1) usable interactive vulnerability mapping tools designed with public health decision makers in Peru and Kenya; 2) empirical data on the design, data visualization preferences, usability, and acceptance of interactive vulnerability mapping tools for VBZD vulnerability in global health settings; and 3) design recommendations for interactive mapping tools for VBZD informed by a scoping review of the literature and findings from Aims 1 and 2. This research will advance the fields of global health and pandemic preparedness, human computer interaction, and data visualization. It provides evidence to suggest that interactive vulnerability mapping tools hold the potential to more effectively prepare for and prevent VBZD outbreaks when they are designed and evaluated with purposeful user engagement.",,"Snyder LE,Peter Rabinowitz,Nancy Puttkammer,Uba Backonja,Chris Adolph",,2021,,,,Ph.D. Thesis
Aluminum Foil Satellite Dishes and a Millennium of Experience: Sustainability in the High Andes,"This address will describe an ICT research project that is context specific and achieved economic and social turnarounds where other ICT projects have failed. The message for computer science educators and professionals is that desired impact has less to do with science and technology and more to do with understanding context and culture. Evaluating implementation options to advance educational and social needs is applying intelligence to technology. Technology without context is a chasm.Literature on contextual relevance such as Habermas, Friere, Husserl, Gadamer, Borgman, abounds. However the absence of minorities in our computer classes, the overarching business use of technology to automate historic processes and the obsession with development of new technologies in the abstract without considering their applications indicate that our profession is slow to grasp this.The ancient Incan culture, through the Quechuan people of Antabamba Peru, a remote indigenous society high in the Andean Mountains has over 700 years of proven social, environmental and economically sustainable practice. Until only 10 years ago Antabamba was a time capsule which was isolated from the world by several days walk from the nearest road. When the road was built in 1995 the multinational products, television, marketing and western philosophies of business practice soon followed. Within 10 years the population of Antabamba was worse off than in anytime in the previous 700 years and risked losing what the developed world is in search of, sustainable practice.Starting in 2003 the Unitec project spent a year learning what had underpinned this ancient culture. Yesterdays wireless technologies, internet, web design, No. 8 wire, aluminum foil satellite dishes and some basic tools were grounded in the traditional Incan methodologies of sharing, learning and understanding. Unparalleled results were achieved. Together with the local communities, the Unitec project developed a methodology called ""Community Centric Empowerment"" (CCE) which has been attributed by OSIPTEL, the Telecommunications Authority in Peru and the Latin American telecommunication council representative as the deciding factor that has separated this project from other ""telecenter"" projects in Latin America. Additional studies focusing on the ability of ICT to reduce poverty and exploitation in third world countries by FITEL, the Rural development wing of OSIPTEL in Peru, support the notion of the importance of how, rather than what, when it comes to ICT use for poverty reduction (Bossio 2005) (Newman 2006). These studies showed the usage patterns and impact of the Unitec project to be quite distinctive compared with any other poverty alleviation project using ICT.In keeping with the phenomenological methodology of the initial study, this address will describe the story of the Peruvian project to demonstrate to ICT educators and professionals that how we implement ICT is as important as what we implement, when social and economic sustainability are our objectives. It lays down a challenge to ICT educators and professionals to reconsider the priorities in our teachings and philosophies.",,"Young A,Muller L",,2006,2,10.1145/1140124.1140126,https://doi-org.proxy.bnl.lu/10.1145/1140124.1140126;http://dx.doi.org/10.1145/1140124.1140126,Conference Paper
Aluminum Foil Satellite Dishes and a Millennium of Experience: Sustainability in the High Andes,"This address will describe an ICT research project that is context specific and achieved economic and social turnarounds where other ICT projects have failed. The message for computer science educators and professionals is that desired impact has less to do with science and technology and more to do with understanding context and culture. Evaluating implementation options to advance educational and social needs is applying intelligence to technology. Technology without context is a chasm.Literature on contextual relevance such as Habermas, Friere, Husserl, Gadamer, Borgman, abounds. However the absence of minorities in our computer classes, the overarching business use of technology to automate historic processes and the obsession with development of new technologies in the abstract without considering their applications indicate that our profession is slow to grasp this.The ancient Incan culture, through the Quechuan people of Antabamba Peru, a remote indigenous society high in the Andean Mountains has over 700 years of proven social, environmental and economically sustainable practice. Until only 10 years ago Antabamba was a time capsule which was isolated from the world by several days walk from the nearest road. When the road was built in 1995 the multinational products, television, marketing and western philosophies of business practice soon followed. Within 10 years the population of Antabamba was worse off than in anytime in the previous 700 years and risked losing what the developed world is in search of, sustainable practice.Starting in 2003 the Unitec project spent a year learning what had underpinned this ancient culture. Yesterdays wireless technologies, internet, web design, No. 8 wire, aluminum foil satellite dishes and some basic tools were grounded in the traditional Incan methodologies of sharing, learning and understanding. Unparalleled results were achieved. Together with the local communities, the Unitec project developed a methodology called ""Community Centric Empowerment"" (CCE) which has been attributed by OSIPTEL, the Telecommunications Authority in Peru and the Latin American telecommunication council representative as the deciding factor that has separated this project from other ""telecenter"" projects in Latin America. Additional studies focusing on the ability of ICT to reduce poverty and exploitation in third world countries by FITEL, the Rural development wing of OSIPTEL in Peru, support the notion of the importance of how, rather than what, when it comes to ICT use for poverty reduction (Bossio 2005) (Newman 2006). These studies showed the usage patterns and impact of the Unitec project to be quite distinctive compared with any other poverty alleviation project using ICT.In keeping with the phenomenological methodology of the initial study, this address will describe the story of the Peruvian project to demonstrate to ICT educators and professionals that how we implement ICT is as important as what we implement, when social and economic sustainability are our objectives. It lays down a challenge to ICT educators and professionals to reconsider the priorities in our teachings and philosophies.",,"Young A,Muller L",,2006,2,10.1145/1140123.1140126,https://doi-org.proxy.bnl.lu/10.1145/1140123.1140126;http://dx.doi.org/10.1145/1140123.1140126,Journal Article
"Customizable, Scalable and Reliable Community-Based Mobile Health Interventions","In pursuance of the Millennium Development Goals (MDGs) set by United Nations in 2000, both Community Based Participatory Research (CBPR) and Mobile Health (mHealth) have proved to be a great tool for advancements in patient monitoring, emergency care and community empowerment. Rapid proliferation of mobile telephony in low income, rural and underserved populations in the absence of other information and communication technology media have prompted the interests of researchers in public health sector. Exploiting mobile communication has resulted in formulation of a dependable and effective socio-technical ecosystem for public health. Whereas, involving academic researchers and community partners to collaborate and develop social and computational models, Community Based Participatory Research (CBPR) approach targets building communication, trust and capacity, with the final goal of increasing community participation in the research process. CBPR is a collaborative approach to research which equitably involves all partners in the research process for betterment of the targeted community. In this paper we present a conceptual and implementation architecture for conducting mHealth assisted community-based interventions. The framework allows CBPR partners to customize the system and design interventions around locale, technology, geographic, scale, and nonetheless social and cultural aspects. We also present the design of our planned intervention addressing prenatal monitoring of underserved populations in the Andean regions of Peru.",,"Kaushik B,Brunette MJ,Fu X,Liu B",,2014,43–48,10.1145/2633651.2633659,https://doi-org.proxy.bnl.lu/10.1145/2633651.2633659;http://dx.doi.org/10.1145/2633651.2633659,Conference Paper
GAPs: Geospatial Abduction Problems,"There are many applications where we observe various phenomena in space (e.g., locations of victims of a serial killer), and where we want to infer “partner” locations (e.g., the location where the killer lives) that are geospatially related to the observed phenomena. In this article, we define geospatial abduction problems (GAPs for short). We analyze the complexity of GAPs, develop exact and approximate algorithms (often with approximation guarantees) for these problems together with analyses of these algorithms, and develop a prototype implementation of our GAP framework. We demonstrate accuracy of our algorithms on a real world data set consisting of insurgent IED (improvised explosive device) attacks against U.S. forces in Iraq (the observations were the locations of the attacks, while the “partner” locations we were trying to infer were the locations of IED weapons caches).",,"Shakarian P,Subrahmanian VS,Sapino ML",,2011,,10.1145/2036264.2036271,https://doi-org.proxy.bnl.lu/10.1145/2036264.2036271;http://dx.doi.org/10.1145/2036264.2036271,Journal Article
Breast Cancer Prognosis from Patient Profiling by SFM,"Breast cancer is the one of the common cancer types for woman society in Myanmar as well as in the world. Identifying recurrence and breast cancer patients profiling in terms of breast cancer recurrence-related data and breast cancer patient characteristics provide new insights into the complexity and causes of breast cancer recurrence. To estimate the probability of recurrence given the patient's symptoms, the statistical model is one of the current prognosis techniques. This approach offers reliable conclusions but lacks explanatory power in a human readable form, i.e. no obvious qualitative chain of inference to the conclusion. To address this issue, we investigate the exploitation of the frequent pattern as an underlying technique for this purpose. As a result, this approach can be very efficiently applied in this domain.",,"Oo KM,Thein NL",,2006,129–133,,,Conference Paper
Spies in the Sky: Surveillance Satellites in War and Peace,"In Spies in the Sky' Patrick Norris responds to the occasion of the 50th Anniversary of the dawn of the Space Age the launch of Sputnik 1 with a review of the most important historical applications of space science for the benefit of the human race during that half century, focusing particularly on the prevention of nuclear war. The author addresses the oft quoted conclusion that the Moon landings and the race to the Moon between the two superpowers were a side effect of the Cold War, by describing what he believes was the more important event the use of satellites by military to prevent the Cold War becoming a hot war. In developing the story the author casts a spotlight on a little-known aspect of the Space Age, namely the military dimension. Today military satellites represent 25 percent of all satellites in orbit, and they are just as important now in preventing regional nuclear war as they were in preventing global Armageddon more than 30 years ago. Beginning with a discussion of Sputnik 1, and the impact of its launch, both on the Soviets and on the West, the book continues to show the social, economic and scientific benefits of satellites today in our daily lives some 50 years later. The author introduces the concept of the Cold War nuclear stand off and mutually assured destruction and shows how spy satellites developed, and the problems of using them to verify arms limitation treaties. He identifies the significance of the ABM Treaty and of SALT and demonstrates how satellites were used to underpin such agreements. He then discusses fringe nuclear powers, such as the UK, France and China and the concept of nuclear non-proliferation. He concludes by looking at the regional tensions of today, including Israel and Arabic nations, India and Pakistan and the threat posed by North Korea, and looks ahead to what the future holds.",,Norris P,,2007,,,,Book
Determining the Factors Affecting the Accuracy of Effort Estimates for Different Application and Task Types,"An important asset in the skill set of any software project manager is the ability to somewhat accurately estimate the effort required to develop a software application. Acquiring this asset, however, requires a thorough understanding of the factors that may affect the accuracy of these estimates. This paper presents the results of an empirical study conducted to determine the causes of variation in the accuracy of effort estimations for different application and task types. A Pakistani software house that specializes in developing financial transaction processing applications is chosen for this empirical study. Actual and estimated values for software development effort are gathered and analyzed for four different types of applications--web-based, database, parallel processing, and telephony--each having six different types of tasks i.e. business-development, new features, usability, security, support, and performance. Over 1000 data points are considered. Analysis of the results reveals, for instance, that the effort for web-based applications is mostly underestimated while the effort for telephony applications is mostly overestimated. The underestimation in web-based applications is usually due to a failure to account for the learning curve associated with rapidly changing web technologies while the overestimation in telephony applications is usually due to a failure to account for the usage of third-party components.",,"Bukhari S,Malik AA",,2012,41–45,10.1109/FIT.2012.16,https://doi-org.proxy.bnl.lu/10.1109/FIT.2012.16;http://dx.doi.org/10.1109/FIT.2012.16,Conference Paper
Facing the Archaeological Looting in Peru by Using Very High Resolution Satellite Imagery and Local Spatial Autocorrelation Statistics,"In many countries of Southern America, Asia and Middle East clandestine excavations affect more than other man-made and natural risks archaeological heritage. Direct and aerial surveillance are not always suitable for protection and monitoring sites of cultural interest. This favoured the use of Very high resolution satellite data for the detection of looting pits.This paper is focused on results we obtained from ongoing research focused on the use of VHR satellite images and spatial autocorrelation statistics, such as Moran's I, Geary's C, and Getis-Ord Local Gi index, for the identification and monitoring of looting.A time series of satellite images (QuickBird-2 and World-View-1) has been exploited to analyze and monitor archaeological looting in Cahuachi, a large Ceremonial Centre built by the Nasca Civilization in Southern Peru. The spatial autocorrelation statistics enabled us to extract spatial anomalies linked to illegal excavations and to recognize and quantitatively characterize looting patterns over the years.The results obtained encourage the application of satellite by means of cluster analysis techniques for the monitoring of archaeological sites.",,"Lasaponara R,Masini N",,2010,254–261,10.1007/978-3-642-12156-2_19,https://doi-org.proxy.bnl.lu/10.1007/978-3-642-12156-2_19;http://dx.doi.org/10.1007/978-3-642-12156-2_19,Conference Paper
Nitrogen Mustard Induces Early Changes in Skin Protein Expression: Potential Targets for Therapeutic Intervention,"Exposure to mustard gas is a current issue. Recently there has been resurgence in chemical warfare attacks, specifically in the Middle East. In August 2015, artillery shells were fired at Isnibil, a village east of Marea, Syria leaving 23 individuals hospitalized with signs of poisonous mustard gas exposure. Mustard gas attacks in Iraq and Syria resulted in victims presenting with respiratory problems, irritation to the eyes, vomiting, and damage to the skin which included blisters and burns. Skin barrier integrity is essential to human health and wellbeing. The stratum corneum, the outermost layer of the skin, is crucial for the body's defense against environmental toxins. Disruptions, which occur following chemical exposures, are associated with delayed wound healing and chronic wounds. Nitrogen mustard (NM, bis (2-chloroethyl) methylamine, mechlorethamine), an analog of the chemical warfare agent sulfur mustard (SM, bis (2-chloroethyl) sulfide), is a bifunctional alkylating agent that can induce oxidative stress, DNA damage and inflammation resulting in extensive skin damage. Since sulfur mustard is both lipophilic and volatile, dermal exposure can be localized using vapor cup models. In contrast, NM is hydrophilic; thus, direct application in solvents results in spreading over a relatively large area of skin. This makes quantification of tissue damage difficult to assess. Key to elucidating the mechanism of action of mustards and testing potential countermeasures is the ability to generate reproducible injury in localized areas of the skin in experimental animal models. Despite extensive research, mechanisms underlying the chronological events of NM induced skin injury are not clearly understood, which makes it difficult to develop effective treatments for mitigating vesicant induced damage to the skin. Further understanding the effects of mustards on the skin will help determine potential therapies that can be used to mitigate toxicity. My proposed project focuses on chemical warfare agents and how alterations in oxidative stress and DNA damage proteins can alter skin re-generation following exposure. More specifically, it aims to examine the early chronology of NM damage on skin epithelium. The proposed studies will use a modified semi-occlusive patch test model developed in our laboratory to study skin injury following NM exposure to elucidate the underlying mechanism of action for the development of potential therapeutics.",,"Wahler G,Chen S,Guo G,Heck D",,2020,,,,Ph.D. Thesis
The Internet Connection: System Connectivity and Configuration,"From the Book: This book explains how to connect your computer or network to the world's largest computer network and community of computer users: the Internet. The Internet The Internet is the largest computer network in the world, consisting of more than 13,000 networks and more than 1,776,000 machines as of July 1993. It has been growing approximately 100 percent annually for the last five years. The TCP/IP protocols used in the Internet are very capable, but are not plug and play. The pool of knowledgeable TCP/IP engineers is not growing as fast as the Internet itself. This book addresses that gap in knowledge. There is no Internet, Inc. to call for service like the old telephone monopoly. The Internet is a worldwide decentralized distributed cooperative interconnection of numerous underlying technologies and organizations with no overall goals, management, or pricing structure. Like the current telephone system, there are many suppliers of Internet connectivity and services, some competing, some complementary. Just as the local telephone company usually does not do the wiring inside your house, your Internet connectivity provider will not set up the environment inside your local host machine or your local network. This book tells you how to do it yourself. The Internet is not like television. When you join the Internet, you become a participant, able to post mail and news; to publish files, documents, and software; and to make your machines directly accessible to others, if you wish. Your machine or network becomes a part of the distributed Internet mail system, and canbecome a server of many other kinds of information. This book tells you how to join the Internet community. The Book This book shows how to connect to the Internet, step by step, from finding a connection through registering a domain and a network number, through configuring your TCP/IP protocols, to running your own domain server and setting up your mail and news systems. Security techniques are described, for use either with or without a router. The most common new Internet services, netfind, archie, WAIS, and gopher, are covered. Access information for network connectivity providers, for domain and IP network number registries, and for other books, is included. Much of the material in this book is applicable to any software platform, because it is about the TCP/IP protocols, which were designed to work with any platform. Singleprocess personal computer operating systems such as MS-DOS and MacOS are most frequently used as clients of network services. The book includes information on where to get TCP/IP software packages for IBM compatibles and Macintoshes. Multi-process operating systems such as UNIX commonly run both clients and servers. Most of the detailed information on the book on setting up and configuring network application servers is about UNIX software. This book is about setting up communications between your host or network and the Internet. That is, it is about communications with the outside world. We must address some internal LAN issues in dealing with external connectivity, but we avoid discussion of issues solely related to LANs, just as we avoid discussion of issues of system administration, unless they also are related to external connectivity. The book includes brief overviews of Internet services and protocols, and it briefly describes what the Internet is and is not, and how it differs from other networks. However, we assume the reader already knows about those other networks, knows about Internet services, and already wants to connect to the Internet. This book shows how to do that. Organization The book begins with two overview chapters, about services and networks. Chapter 1, Internet Services gives a motivational overview of what you can do with the Internet, and then describes the size and growth of the Internet. The bulk of the chapter describes specific Internet services, their facilities and advantages, and the TCP/IP protocols that support them on the Internet. Chapter 2, The Internet and Other Networks gives an overview of the history, protocols, and politics of the Internet and other networks, such as FidoNet, UUCP, BITNET, USENET, that together form the global Matrix of computers that exchange electronic mail. These contextual chapters set the stage and define the terms for the rest of the book. If you are already familiar with the Internet, you may want to skip forward to the other chapters, but there is an amazing amount of disinformation about the Internet in circulation, and these chapters are short and, we hope, accurate. Before you can use the Internet you have to decide how to connect, and you may need to register organizational names and network addresses. Chapter 3, Types of Internet Access categorizes types of access to the Internet, ranging from public hosts to direct fiber optic connections at hundreds of megabits per second. The chapter includes very brief refresher on protocol layering models and Internet protocol layers. Chapter 4, Registering Domain Names and IP Numbers tells exactly how to register a domain name and a network number, and where to get the registration forms by electronic mail, or on paper or CD/ROM. The rest of the chapters show how to set up Internet services, and are presented approximately in the order you are likely to need the services they describe. Chapter 5, Setting Up IP Chapter 6, Setting Up the Domain Name System Chapter 7, Setting Up Internet Electronic Mail Chapter 8, Setting Up USENET News Chapter 9, Security Issues Chapter 10, Setting Up Resource Discovery Services These chapters do not attempt to describe all possibilities in great generality (we've already done that in another book, Practical Internetworking with TCP/IP and UNIX). Instead, they give the short and direct path to getting what you're most likely to need set up as quickly and painlessly as possible. The appendices provide names and addresses for sources of information. Appendix A, Internet Providers lists Internet providers, from public login hosts to dialup and direct IP connectivity providers. Appendix B, Registration Templates includes the actual text of example registration templates for domains and IP network numbers, and the addresses to send them to. Appendix C, Software and Other Information tells where to get the software (often over the Internet itself; sometimes for free; sometimes from commercial suppliers). Appendix D, Further Reading is a brief reading list of books about the Internet and other networks. There is a brief glossary, and a brief index. The cover shows a view of the world from above the north pole, with each of four networks glowing in its own color light. Similar maps appear on four of the endpapers,* showing the whole world, Eurasia, and most of Canada and the United States. As the legends indicate, wide orange ellipses are for UUCP, tall violet ellipses are for FidoNet, blue squares are for BITNET, EARN, and other NJE networks, and green circles are for the Internet; these four networks are the largest distributed networks in the Matrix, and they are described briefly in Chapter 2. The size of an icon indicates the number of host computers near the center of the icon. For example, the map of Eurasia shows the Internet green as the most prevalent in the north and west of Europe, and BITNET (or other NJE network) blue as the most widespread in the middle east. In eastern Europe, Internet green fades into FidoNet and UUCP violet and orange in central Asia, until east Asia suddenly shows all four networks again. However, the Internet is following behind those two access networks, and green Internet circles are visible in Talinn, St. Petersburg, Kiev, Moscow, Novosibirsk, New Delhi, Bombay, and Accra, Ghana. The fourth endpaper shows growth rates of the Internet alone in each country of the world. Much of the world is already connected, from Antarctica to Siberia, from Greenland to Ecuador, from Australia to Austria. The newest countries are growing the fastest, but even the longest connected and most densely networked countries are adding new hosts at exponential rates. Readers This book is for readers who know they want a connection to the Internet, not to a different network. It is for anyone who wants to connect a single machine or a network to the Internet. Such a machine might be in someone's house or office, in a company or a university. Such a network might be in a company office or a university department. Managers and executives can use the book to get a good idea of what is really involved in setting up an Internet connection. Technical people can use the book to actually set up a connection. In 1993 more than a million new machines and ten thousand more networks are expected to connect to the Internet. That is more people confused by technology than all the TCP/IP consultants in the world can help directly. This book can assist many of those people in doing their own basic IP connection configuration. In 1994, two million more machines and twenty thousand more networks are expected. These numbers are just for the Internet proper. There are probably at least as many machines in private IP networks inside companies, and more are forming all the time. Many of these enterprise networks then connect to the Internet, either as full participants, or through one of the kinds of firewalls described in this book. Every company or department considering making such a connection has at least one potential reader. Every engineer involved in setting up the connection, the engineer's manager, and many of the engineer's users, are potential readers. The person wanting to connect may already have electronic mail access to some other network, such as UUCP, FidoNet, or BITNET, or may be a complete newcomer to wide area networking. Even if you work in a place with many network experts, it is very easy to spend a lot of time finding the right person to ask for basic information about a variety of topics. This book answers most of the basic questions, and points you at sources for appropriate registrars, software, and vendors. The book is aimed more at newcomers, but will also be of use to engineers familiar with LANs who want the quick path to setting up wide area Internet services. Most specific details are drawn from the Internet in the United States. Most details are the same in other countries. Where there are major differences, we describe them. The Internet itself is still mostly (60%) in the United States, but already reaches at least 50 other countries, and is growing even more rapidly in some of them than the overall 100% annual growth rate. Thus readers of this book may be anywhere in the world. Terminology and Typography This book is written in American English. We have avoided idioms that might be hard for other English readers to understand. We have also avoided overly formal or academic phrasing, while attempting to maintain clarity. Jargon words and important terms are defined in the glossary, and also appear in the index. Glossary definitions are necesarily brief, and isolated. Definitions of the same terms in the text are often longer, and are always given in context, so you may want to use the index to locate these embedded definitions, as well. Defined terms, whether words, phrases, or acronyms, appear in boldface where they are defined in the text, and sometimes in other places where they are important. Most networking concepts depend on other networking concepts, leading to circular definitions. For this reason, many terms are introduced briefly at the outset, and defined more properly later. For example, major network services are introduced in Chapter 1 and many of them are defined or discussed in more detail in later chapters. Similarly, basics of packet switching are explained in the ARPANET section in Chapter 2, leading into the discussion of network protocol layering in Chapter 3, the discussion of routing in Chapter 5, and the discussion of types of gateways in Chapter 9. Acronyms present some interesting issues. Traditionally in English an acronym is a short sequence of letters, pronounceable or not, derived from a sequence of words, perhaps by taking the first letter of each word. In computing and networking jargon acronyms are not always derived from a longer form; often the reverse is true. That is, frequently an interesting, euphonic, or punning acronym is constructed and later an expansion is produced to fit it. Such expansions serve merely as mnemonics and rationalizations, not as derivations. In addition, many network protocols and networking organizations are known almost exclusively by their acronyms, not by a longer name. For these reasons, we do not follow the usual English convention of always introducing an acronym in parentheses after the longer form of the name. Sometimes we instead give the acronym first, followed by the longer form in parentheses. In addition, it is customary in English to introduce an acronym along with its longer form once only in a text. We also do not follow this custom, since readers may not be reading this book in strict page number order, and even readers who do may easily have forgotten a complicated acronym introduced at the beginning of the book by the time it recurs in a later chapter. Instead we reintroduce an acronym whenever we think it might have been forgotten. All acronyms that are given in the text with an expansion appear in the index, and most also appear in the glossary. Network protocol names are usually given in uppercase, whether they are acronyms or not, as in RLOGIN. UNIX commands are given in lowercase and italics, as in rlogin. Pathnames are given in italics, as in /etc/hosts.equiv.",,"Quarterman JS,Mitchell SC,Smoot CM",,1993,,,,Book
BCEAP - A Blockchain Embedded Academic Paradigm to Augment Legacy Education through Application,"Education plays an important role in the economic and social progress of any community. Currently, the higher education system needs immense of enhancements for fulfilling essential needs of the productive and beneficial educational environment. The major challenges faced by the higher education system is the verification of student's data at the time of admission in the university. Current admission system requires a lot of time along with an excess of human resources. there is no mechanism for the authentication of degrees and educational certificates in Pakistan. In this paper, the proposed system based on blockchain technology. This system can validate and verify degree/certificates. The proposed system can verify and validate the student's educational record from respective educational stakeholders like HEC, PEC, IBCC. It allows students to apply for admission using the single platform at a few clicks. Using blockchain, this system is secure, tamper-resistant, time saving, efficient and reliable.",,"Ghaffar A,Hussain M",,2019,,10.1145/3341325.3342036,https://doi-org.proxy.bnl.lu/10.1145/3341325.3342036;http://dx.doi.org/10.1145/3341325.3342036,Conference Paper
Understanding Gender Bias: Differences in Tech Stereotypes According to the Socio-Economic Background of Girls,"Of all the countries that belong to OCDE, the Latin American countries have the highest levels of inequality. Chile is among them, with scores similar to Bolivia and Guatemala [10]. Also, the number of women living in poverty is higher than that of men [4]. Women’s economic context is essential for their families, as 90% of single-parent families are supported by women [12]. One way of achieving economic development may be choosing a career in technology, as tech jobs are among the highest-paid in the country [9]. Also, they are flexible, allowing women to balance work and family, and have been proven to promote social mobility and country economic growth [2, 7]. However, there is a well-known gender gap in technology; for example, only 24% of computer science students are women in Chile [9]. To inspire women to have a computer science career, interventions should be undertaken while they are girls, by addressing stereotypes that influence their attitude towards technology [6]. These stereotypes are influenced by the context in which girls grow [5]; in particular their socio-economic context [3]. Therefore, it may be essential to understand the context of girls, and their thoughts towards tech stereotypes, to create better computer science education interventions. We conducted a preliminary interview study with sixth grade girls, since at this age, stereotypes can still be challenged [1, 11], while the opportunities to challenge stereotypes decrease from the eighth grade on [8]. The research question of this study is whether there are different stereotypes regarding technology among girls with different socio-economical levels. It has been hypothesized that there will be different stereotypes among these girls. Twelve interviews were done: 6 with girls from low vulnerability contexts, and 6 with girls from high vulnerability contexts. The interview data were transcribed and analyzed using grounded theory under Charmaz’s perspective. The results from this preliminary study were that girls from a high vulnerability context have a negative attitude towards technology mainly because of misconceptions regarding technology, e.g. what it does and how to work with it. On the other hand, girls from low vulnerability contexts have stereotypes in which tech careers were considered to be manly, and they also had concerns about family-work balance. With this information, a semi-structured interview has been developed to apply to girls from low and high vulnerability contexts and analyzed with ground theory. Further, with this qualitative information, a quantitative tool will be developed. A national survey will be created to determine if these different stereotypes are also present in the larger population of girls. With this information, better computer science education interventions may be created, especially focused on high vulnerability contexts, considering the particular stereotypes that these girls have regarding tech stereotypes, that keep them away from computer science careers.",,"Vergara K,Herskovic V,Guerrero P",,2022,55–56,10.1145/3501709.3544289,https://doi-org.proxy.bnl.lu/10.1145/3501709.3544289;http://dx.doi.org/10.1145/3501709.3544289,Conference Paper
The Adaption of CMMI for an In-House Software Development Department,"Seminar paper from the year 2017 in the subject Computer Science - Applied, grade: 3.2, Virtual University of Pakistan, language: English, abstract: This paper is a basic work for introducing the CMMI for in-house software development department. The purpose of this paper is to develop a ground and a clear understanding for the organizations who are willing to adapt the best practices in general for increasing the functional and technical efficiency at their in-house software development department. Normally the CMMI is implemented at organization level in the software development, software engineering, system engineering or system security organizations. The main theme of this paper is to attain CMMI level-2 in software development department. Once the best practices become common in an organization at departmental level, that opens new horizons to build high level understanding of more mature and simplified set of processes that leads toward the organizational maturity which covers the set of overall process areas across the organization. There are various factors involve that limits an organization to adopt a process improvement model. Usually the new emerging organizations having the staff who have previous experience in highly functioning organizations plan to adapt the process improvement models. It is also a dilemma that small and medium size organizations could not establish successful implementation of Software improvement process models because such organizations work in limited resources and restrict time frame [1]. It is more hard to adapt the CMMI at departmental level because main focus of such an organization is towards the productivity or the main stream of the business. Specifically, this paper will provide the structural process and case study of software development department of an organization having very diversified functional and financial dimensions to improve functional efficiency from poorly controlled activities to a managed environ",,Shabbir R,,2017,,,,Book
Website Usability Analysis of Non Profit Organizations: A Case Study of Pakistan,"Nonprofit organizations are an important pillar of any society, which specifically serve under privileged sections of society. Modern Information and communication technologies have huge potential to benefit the working processes of such organizations but deploying software application in such settings is quite challenging. These challenges emerge due to shortage of skilled employees, limited funds and weak organizational structures. In order to further understand the technological implications in such settings this paper explores the websites of different nonprofit organizations of Pakistan to understand usability problems. In order to gather empirical data, it prepared questionnaire mainly focusing on Jakob Nielson's heuristics. These questionnaires were distributed to final year undergraduate students taking a Human Computer Interaction module. The findings highlight serious usability issues in these websites. These findings are helpful for the nonprofit organizations to improve these websites for better information access.",,"Saeed S,Shabbir S",,2014,70–83,10.4018/ijpada.2014100105,https://doi-org.proxy.bnl.lu/10.4018/ijpada.2014100105;http://dx.doi.org/10.4018/ijpada.2014100105,Journal Article
Impact of Digital Marketing on Consumers' Impulsive Online Buying Tendencies With Intervening Effect of Gender and Education: B2C Emerging Promotional Tools,"We are living in the digital age where consumers have become more elegant, and their buying intention is radically transforming from traditional to online buying behavior. This study investigates the impact of digital marketing (DM) tools on consumers' online impulsive buying tendencies (OBIT), i.e., effective (AD) and cognitive tendencies (CD) with intervening role of the gender (GDR) and education-level (EL). Four hundred surveys were randomly distributed to online shoppers in Pakistan. SEM was applied to test the proposed relationships and findings revealed a positive association between DM and consumers' OBIT. The comprehensive examination affirmed the positive interrelationships of sub-dimensions of DM on consumers' OBIT, i.e., AD and CD. It is further revealed that GDR and EL did not moderate the relationships between DM and OBIT. This study furnishes insights on how advertisers can exploit such platforms to achieve OBIT and creating effective relationships in today's digital age. This study demonstrates certain directions for academicians and practitioners.",,"Waheed A,Wang D,Sarwar-A Alam MD",,2019,44–59,10.4018/IJEIS.2019070103,https://doi-org.proxy.bnl.lu/10.4018/IJEIS.2019070103;http://dx.doi.org/10.4018/IJEIS.2019070103,Journal Article
Quality Model for the Selection of Floss-Based Issue Tracking System,"The complexity of the issue tracking systems (ITS) which meet the requirements of the Infrastructure Technology Information Library (ITIL) encumbers their selection. In addition, we have to consider some other variables, such as the wide range of tools, their functionality level and their costs. Regarding the cost of the ITS, nowadays the use of ITS based on Free/Libre Open Source Software (FLOSS) is an increasing trend. Therefore, the purpose of this article is to present a model to evaluate the quality characteristics of the FLOSS-ITS according to ITIL recommendations. This quality model is aimed at selecting the best of the available tools. The set of characteristics evaluated is presented based on the product perspective of the Software Quality Systemic Model (MOSCA). This perspective is inspired on the ISO/IEC 9126 standard and Dromey's quality model. Also it was applied to a case study -a venezuelan company-, which is willing to exploit and extend the capabilities of this type of tools to its customers. The case study allowed us to establish the FLOSS-ITS's quality requirements: functionality, reliability and usability. Lastly, validation of the model through its application to three FLOSS-ITS tools is presented.",,"Raffoul E,Domínguez K,Pérez M,Mendoza LE,Grimán AC",,2008,43–49,,,Conference Paper
Bracken Fern Frond Status Classification in the Andes of Southern Ecuador: Combining Multispectral Satellite Data and Field Spectroscopy,"In the anthropogenic fire-disturbed ecosystem of the San Francisco Valley in the Andes of southeastern Ecuador, dense stands of an aggressive invasive weed, the southern bracken fern Pteridium arachnoideum and Pteridium caudatum, dominate the landscape. To secure sustainable land management in the region, a comprehensive understanding of bracken spatial-distribution patterns and life cycle dynamics is crucial. We investigated the possibility of detecting bracken-infested areas and frond status live, fungi-infected, and dead by means of a high-resolution QuickBird scene from October 2010 and spectral signatures based on field spectroscopy. After image pre-processing, a two-step classification procedure first delineates the bracken-infested area by means of a maximum-likelihood hard classification. The probability-guided unmixing classifier with field-derived end-members is applied in the second step to obtain the fractional cover of the different frond statuses per pixel. The results showed that the areas infested by bracken could be distinguished from the other land-cover classes with high accuracy overall accuracy of 0.9973. Also, the three frond statuses could be accurately classified at the sub-pixel level. The ‘dead’ class was the dominant frond status at the time of image acquisition October 2010. We conclude that the extreme dry spell in October 2010 was particularly responsible for this dominance.",,"Curatola Fernández GF,Silva B,Gawlik J,Thies B,Bendix J",,2013,7020–7037,10.1080/01431161.2013.813091,https://doi-org.proxy.bnl.lu/10.1080/01431161.2013.813091;http://dx.doi.org/10.1080/01431161.2013.813091,Journal Article
Rescue Command Communication Systems and Emergency Management Platform in Mine Based on Internet of Things,"In view of several problems existing among current rescue command communication and emergency management systems in mine, such as these systems have fewer supporting technologies and functions, has lower level of informatization and networking, has weaker technology extensibility due to the C/S software architecture, has more difficulty in remote online maintenance and upgrade in the future etc. a rescue command communication system and emergency management platform in mine based on Internet of Things are put forward. This system integrate the Internet of Things, wireless Mesh network, optical fiber and satellite communications platform comprehensively, as well as adopts B/S architecture, data mining database and remote database shared access modern information and other key technologies in order to realize the functions including disaster incident management, rescue command, visual management of disaster sites, rescue team and equipments management and emergency rescue plan management, etc. Strong extensibility and practicability were proved through the practical applications in a coal mine in Inner Mongolia and also proved by some training tests in a national rescue team in Heilongjiang province. These features of this system not only contribute to the informatization and networking of emergency command and rescue team management in the coal mine and rescue team, but also offer important reference value for other mine rescue teams at home or abroad to improve their ability on emergency command and management.",,"Liu C,Song W,Guo D,Wang L",,2013,17–22,10.1109/ITA.2013.10,https://doi-org.proxy.bnl.lu/10.1109/ITA.2013.10;http://dx.doi.org/10.1109/ITA.2013.10,Conference Paper
Low Voltage Digitally Controlled Impedance Based Energy Efficient Vedic Multiplier Design on 28nm FPGA,"Low Voltage Digitally Controlled Impedance (LVDCI) is an I/O standard available on FPGA. This design is LVDCI IO standard based Energy Efficient Vedic Multiplier Design on FPGA. Selection of IO standard play an important role in power dissipation of design. Therefore, we are going to select the most energy efficient IO standards in LVDCI family for Vedic Multiplier. This Vedic multiplier design is a part of project of Vedic arithmetic circuits. The final deliverable of this project is Vedic Processor by merging both concepts of Veda, first book of this world, and the latest technology of this world. In order to test thermal aware design, we want to see that how does an electronic device behave if we change the temperature of surrounding in which it is working. For that purpose we have taken temperatures of four different regions. Furnace Creek Ranch is area of North America recorded the highest temperature of the world that is 56.7°C [1]. Approximately, 53.5°C is the maximum temperature recorded in Mohenjo-Daro situated in Sindh Pakistan [1]. We have also taken median temperature of Delhi i.e. 40°C and standard normal temperature i.e. 21°C. We are operating Vedic Multiplier with the four different temperature and different LVDCI IO standard and observe device performance, and power dissipation. When we use 28nm FPGA under room temperature of 40°C, there are 93.42%, 92.6%, 93.99%, 93.59% and 89.79% reduction in total power dissipation of Vedic multiplier using LVDCI 15, LVDCI 18, LVDCI DV2 15, LVDCI DV2 18 and HSLVDCI 15 respectively. Similarly, when we use 28nm FPGA, there is approximately 90-96% reduction in leakage power dissipation of Vedic multiplier using different LVDCI and different temperature. There is no change in I/O power with change in temperature for uniform IO standard. When we use different IO standard of LVDCI family, there is significant reduction in leakage power. FPGA based on 28 nm technology is more energy efficient than 40 nm technology based FPGA.",,"Goswami K,Pandey B,Jain A,Singh D",,2014,951–955,10.1109/CICN.2014.201,https://doi-org.proxy.bnl.lu/10.1109/CICN.2014.201;http://dx.doi.org/10.1109/CICN.2014.201,Conference Paper
"Fostering Anticipatory Action Via Social Protection Systems: A Case Study of the Climate Vulnerability of Flood-Exposed Social Security Allowance Beneficiaries in Bardiya District, Nepal","Rationale – Climate disasters represent a significant and growing proportion of the humanitarian burden and are a key factor in increasing poverty and insecurity. A myriad of studies demonstrate that aid delivered in an ex-ante fashion can be effective in mitigating losses of life, assets and livelihoods associated with climate hazards. This inquiry supplements the nascent body of research and empirical evidence base pertaining to the building of anticipatory capacity into large-scale national systems, namely via linking a Forecast-based Financing mechanism to an existing social protection system.Research question – Using the case of flood disasters in Bardiya district, Nepal, the research inquired the following: How can social protection be combined with Forecast-based Financing in order to optimise anticipatory humanitarian relief for climate-related disasters?Sub-questions – Research sub-questions guided the inquiry: (1) To what extent are current social protection beneficiaries exposed to climate-related disasters? (2) What is the specific climate vulnerability of social protection beneficiaries? (3) What are the anticipatory relief needs of climate vulnerable social protection beneficiaries?Methodology – Grounded in empirical research via the conduct of a qualitative single case study, the inquiry adopted a conceptual perspective and an exploratory design. A remote data collection strategy was applied, which included (1) a thorough desk review of key scientific literature and secondary data provided by in-field humanitarian organisations; and (2) semi-structured interviews with key informants.Key findings – The data demonstrated that the exposure of social protection beneficiaries to flood hazards is comparable to the general population. Nevertheless, an elevated climate vulnerability is evident secondary to an increased sensitivity and diminished adaptive capacity. The flood anticipatory relief needs/preferences identified include cash-based assistance, food provisions, evacuation assistance and/or enhanced Early Warning Systems.Conclusion – The research supports the utilisation of the proposed conceptual model for an integrated social protection and Forecast-based Financing mechanism, inclusive of vertical and horizontal expansion, in order to effectively identify the most climate vulnerable groups and to guide the provision of targeted anticipatory actions. The mechanism is optimised when a people-centred approach is utilised, with reference to the idiosyncratic, lifecycle and corresponding intersectional vulnerabilities of the targeted population. These findings will contribute to prospective programming in Nepal; additionally, the extent to which they can be generalised will be informed by future applied efficacy studies and comparative analyses with research from differing contexts.",,Desroches S,,2020,,,,Ph.D. Thesis
JaʿFar al-Khuldundefined (d. 348/959) and the Early Sufi Textual Tradition: Text-Critical and Computational Approaches,"Similar to many religious and social movements that arose during the first four Islamic centuries, there exists a considerable temporal gap between the hypothesized emergence of Sufism in Iraq in the third/ninth and fourth/tenth centuries and systematic documentation thereof, a situation that presents serious challenges to the study of this period of Sufi activity. This dissertation presents new textual and methodological resources to analyze the formation of early Sufism as a new religious movement, with special reference to the Baghdadi Sufi Jaʿfar al-Khuldundefined (d. 348/959). Although al-Khuldundefined has long been recognized as heir to important early Sufis and a chronicler of their exploits, his considerable textual legacy has never been systematically analyzed or contextualized. The diverse body of texts associated with him spans the genres of geography, biography, Prophetic undefinedadundefinedth, philosophical and doctrinal treatises, and advice literature. The structured nature of this data, which is largely unique to the field of Arabo-Islamic bibliography, makes it possible to apply and extend the types of text-critical and computational modes of investigation that have been proposed to analyze earlier phases of the development of the early Arabic textual corpus. Such analyses offer new insights into the structure of the early Sufi movement, including its intellectual bases, the membership of the early community, the nature of in-group relations, and the formation of a group-level alliance with the Shafiʿi legal school. This dissertation thus makes an original contribution to the study of early Sufism by expanding the archive of available texts for this period, as well as delineating key organizational features of the early Sufi movement. Furthermore, it demonstrates the complementarity of computational methods with traditional scholarly approaches to Arabic texts and Islamic prosopographical material.",,"Farrell J,Kevin Corrigan,L Cornell R",,2021,,,,Ph.D. Thesis
CEA '13: Proceedings of the 5th International Workshop on Multimedia for Cooking & Eating Activities,"It is our pleasure to welcome you to the ACM Multimedia 2013 Workshop on Multimedia for Cooking and Eating Activities (CEA'13), the fifth of its series, held in Barcelona, Catalonia, Spain.Cooking and eating have been the most fundamental activities of humankind from ancient days, which affect various aspects of human life such as health, dietary, human communication, safety of food, entertainment, culinary art, welfare, and so on. However, many people who cook at home require supports for cooking because it requires experience and knowledge. They may also need support for food-logging and menu planning for the health of their family. Needless to say, support for a good and enjoyable dinner would improve the quality of life. On the other hand, systematic cooking and eating support for the elderly and/or physically challenged people are significantly important.The call for papers attracted 21 submissions from France, Italy, Japan, Pakistan, Spain, Switzerland, Turkey, United Kingdom and United States. Each paper was carefully reviewed by three reviewers in related fields, and we decided to accept thirteen papers based on the reports. Three of the accepted papers will be presented in the form of oral presentation, which includes one Best Paper Award winning paper, established last year. The others will be presented in the form of short oral and poster presentation to secure more time for discussions, in order to improve the works to an even higher level. These presentations are about the recent technologies in the fields of Application for cooking and eating support, Menu planning, dietary management, and foodlogging, Analysis and utilization on cooking recipe, Analysis of cooking video, Analysis of Web contents on cooking and eating activities, Cooking archiving and recognition, Learning contents creation for cooking, Recipe text / image / video retrieval and recommendation, Ubiquitous environment and interface in kitchen, and so on.As for the invited talk, we will invite Professor Amélie Cordier of Laboratoire d'Informatique en Image et Systèmes d'information (LIRIS), France. In her talk, she will provide an overview of the challenges that cooking raises for computer science and information technology. She focuses on many issues such as representation of cooking recipes, suggestions, recommendations, dietary practices, personalization, social side, creativity.",,,,2013,,,,Book
Front Matter,"This festschrift celebrates the life of a remarkable man.Rob Milne died while climbing Mount Everest early on 5th June 2005 Nepal Time. He was 48. He is survived by his wife Val and his two children Alex and Rosemary.His untimely death was a tragedy, but Rob packed 96 years of living into his 48 years of life. In any one of his three “careers” ---as a hi-tech entrepreneur, as an AI researcher and as a mountaineer ---his achievements would have been enough for most ordinary mortals. But Rob combined world-class success in all of them. This book covers all these facets of his life. Each chapter has been contributed by one or more of his close collaborators as their tribute to Rob and to his legacy.Rob's ascent of Everest was to have been the culmination of a lifetime's ambition to climb the highest summits in each of the world's seven continents. Everest was the last of these seven summits. He was only 400 metres from the top when he died from a sudden and massive heart attack. He had been an ambitious and successful mountaineer since his childhood in Colorado. As Val, said in a radio interview, “Rob died at the top, doing what he loved”. This was true not just of his mountaineering, but in all the spheres of his life.I first met Rob in 1978 in Boston, Massachusetts. He was just finishing his undergraduate degree at MIT and had applied to be a PhD student at Edinburgh under my supervision. I was visiting MIT that Summer, so we met to discuss his research project. I was quickly introduced to his climbing expertise. He showed me how to climb a vertical brick wall using the gaps in the bricks as hand and foot holds. He invited me to try; I declined. He came to Edinburgh that Autumn to work on machine understanding of mechanics problems written in English as part of our Mecho project: one of the first non-US expert systems. In 1980, Rob initiated his project to conquer the seven summits by climbing Denali (Mt McKinley) in Alaska. I remember getting vertigo just by reading his subsequent article in our University Bulletin. Rob met Val at Edinburgh, and they married in 1981.Climbing Denali required determination and persistence. Rob exhibited these qualities in everything he did, which is why he achieved so much. But it wasn't always the best approach. When it came to his PhD viva, Rob somehow got the misconception that to concede to his examiners on any point, however minor, would destroy his chances. He, therefore, fought every step of the way. The viva lasted eight hours! He obtained his PhD in 1983.In 1982Rob returned to the USAwhere he worked first at the Wright-Paterson Airforce Base and then at the Pentagon. At the Pentagon, he introduced AI research into the US Army by founding the Army AI Center, in which he was the Chief AI scientist. Returning to Scotland in 1986, he founded Intelligent Applications: one of the first non-US expert system companies. After experimenting with various AI products, IA focused on turbine monitoring with its 'Tiger' product.Most entrepreneurs running innovative, hi-tech companies have little time for extracurricular activity, but Rob found time both for his mountaineering and his AI research. He continued to publish in the top journals and conferences, authoring over 75 papers on knowledge-based systems, data-mining, qualitative reasoning, etc. He was a popular speaker, giving many invited talks and tutorials at major conferences. He acted as a bridge between academia and industry, for instance, frequently talking to academics on the technology transfer process.Rob was a natural leader; he tirelessly and selflessly gave his time to help organise the activities in which he was involved. For instance, in both the British and European AI communities, he regularly served on conference committees, having been chair of both the British Computer Society's Specialist Group on AI Conference and of the European Conference on Artificial Intelligence. He was an officer of both organisations, including being the President of ECCAI 2000-04. He was also the inspiration behind bringing IJCAI-05 to Edinburgh, being the Local Arrangements Chair until his death. Rob played a key part in setting up the European Network of Excellence MONET (Model Based and Qualitative Systems), and in a second phase its Task Group BRIDGE (Bridging AI and Control Engineering model based diagnosis approaches) that focused on diagnosis.Rob played an active role in the Scottish Software Federation (which merged to form ScotlandIS in 2000): the industry bodies for IT and software companies in Scotland. Rob was a director of each organisation 1997--2002. He was a mentor to a number of start-up companies and guided other entrepreneurs in their efforts to establish successful businesses. In recognition of his academic and industrial achievements, he was elected a Fellow of the Royal Society of Edinburgh in 2003. He was also active in Scottish Mountaineering Club, being Convener of the publications sub-committee and co-authoring a book on the Corbetts (the 219 Scottish hills between 2500ft and 3000ft high). In 1997, Rob became only the 1860th person to have climbed all the Munros (the 284 Scottish mountains over 3000ft high). He was a keen winter climber, helping to establish a number of high-grade new climbs throughout Scotland.Rob summed up his attitude to life in a radio interview, by saying that it was important to wake up every morning with an exciting challenge in mind. He always set himself ambitious goals then attained them by persistence and determination. Ambition, persistence and determination are qualities sometimes associated with people who are difficult to get on with. Not so Rob. He was one of the most pleasant and easy-going people it has been my pleasure to work with. We already miss him.Alan Bundy, May 25, 2006",,,,2006,i–vii,,,Conference Paper
Spatio-Termporal Reasoning about Agent Behavior,"There are many applications where we wish to reason about spatio-temporal aspects of an agent’s behavior. This dissertation examines several facets of this type of reasoning. First, given a model of past agent behavior, we wish to reason about the probability that an agent takes a given action at a certain time. Previous work combining temporal and probabilistic reasoning has made either independence or Markov assumptions. This work introduces Annotated Probabilistic Temporal (APT) logic which makes neither assumption. Statements in APT logic consist of rules of the form “Formula G becomes true with a probability [L,U] within T time units after formula F becomes true” and can be written by experts or extracted automatically from historical data. In this dissertation, we explore the problem of entailment, specifically what is the probability that an agent performs a given action at a certain time based on a set of such rules. We show this problem to be coNP-hard (in the complexity class coNP under some natural assumptions) and present several sets of linear constraints for solving this problem exactly. We then develop a sound, but incomplete fixpoint operator as a heuristic for such queries. This approach was implemented and tested on 23 different models automatically generated from several datasets. The operator quickly converged to produce tight probability bounds for the queries.Second, agent behavior often results in “observations” at geospatial locations that imply the existence of other, unobserved, locations we wish to find (“partners”). In this dissertation, we formalize this notion with “geospatial abduction problems” (GAPs). GAPs try to infer a set of partner locations for a set of observations and a model representing the relationship between observations and partners for a given agent. This dissertation presents exact and approximate algorithms for solving GAPs as well as an implemented software package for addressing these problems called SCARE (the Spatio-Cultural Abductive Reasoning Engine). We tested SCARE on counter-insurgency data from Iraq, attempting to locate enemy weapons caches (partners) based on attacks (observations). On average, SCARE was able to locate weapons caches within 690 meters of actual sites. Additionally, we have considered a variant of the problem where the agent wishes to abduce regions that contain partner points. This problem is also NP-hard. To address this issue, we develop and implement a greedy approximation algorithm that finds small regions which contain partner points - on average containing 4 times as many partners as the overall area.We then provide an adversarial extension to GAPs as follows: given a fixed set of observations, if an adversary has probabilistic knowledge of how an agent were to find a corresponding set of partners, he would place the partners in locations that minimize the expected number of partners found by the agent. In a complementary problem, the agent has probabilistic knowledge of how an adversary locates his partners and wishes to maximize the expected number partners found. We show that both of these problems are NP-hard and design schemes to find approximate solutions - often with theoretical guarantees. With our implementation, we demonstrate that these algorithms often obtain excellent solutions.We also introduce a class of problems called geospatial optimization problems (GOPs). Here the agent has a set of actions that modify attributes of a geospatial region and he wishes to select a limited number of such actions (with respect to some budget) in a manner that either causes some goal to be true (goal-based GOPs) and/or maximizes a benefit function (benefit-maximizing GOPs). Additionally, there are certain combinations of actions that cannot be combined. We show NP-hardness (membership in NP under reasonable assumptions) as well as provide limits of approximation for these problems. We then develop sets of integer constraints that provide an exact solution and provide an approximation algorithm with a guarantee. (Abstract shortened by UMI.)",,Shakarian P,,2011,,,,Ph.D. Thesis
"Testing Foundational Tenets of Stable Isotope Ecology Analyses in Neotropical Mammalian Communities, and Implications for Terrestrial Paleoecology","Stable isotope analyses are powerful tools for reconstructing ancient ecologies and ecosystems, as they are independent of morphology and directly reflect dietary ecology. The application of stable isotope analyses, however, is not without limitations, as determination of food web dynamics using these methods often relies on poorly tested assumptions. The guiding thread of this thesis is the testing of foundational cornerstones on which these methods rely, in order to validate the suitability of applying these techniques to different mammalian clades, and to more reliably and confidently interpret the isotopic signals preserved in extinct organisms.The first chapter of this thesis tests the validity of an important assumption behind the interpretation of stable carbon isotope analyses for understanding diet in terrestrial mammalian herbivores: if, as assumed for almost two decades, mammalian bioapatite δ13C is enriched by 14‰ relative to dietary δ13C. By analyzing new isotopic data from a never before assessed herbivorous group spanning a broad range of body masses—sloths (Xenarthra, Mammalia)— and other mammals with experimentally controlled or observationally known diets, I discovered considerable variation in diet–bioapatite δ13C enrichment among mammals. Statistical tests (ordinary least squares, quantile, robust regressions, Akaike information criterion model tests) documented independence from phylogeny, and a previously unrecognized strong and significant correlation of δ13C enrichment with body mass for all mammalian herbivores. A single-factor body mass model outperformed all other single-factor or more complex combinatorial models evaluated, including for physiological variables (metabolic rate and body temperature proxies), and indicated that body mass alone predicts δ13C enrichment. These analyses, spanning more than 5 orders of magnitude of body sizes, yield a size-dependent prediction of isotopic enrichment across Mammalia and for distinct digestive physiologies, permitting reconstruction of foregut versus hindgut fermentation physiologies for fossils and refined mean annual paleoprecipitation estimates based on δ13C of mammalian bioapatite.Second, I sought to evaluate the existing paradigm governing identification of closed canopy rainforests in the fossil record using mammalian δ13C data: the presence of mammals with dietary δ13C <-31‰, which has only been observed in closed canopy rainforests in Equatorial Africa, the only other tropical ecosystem sampled extensively. This chapter provides a characterization of δ13Cbioapatite, δ13Chair and δ15Nhair of a modern mammalian community in western Amazonia, in Peru, to test if the isotopic structure of mammals in this Neotropical ecosystem is similar to those in African tropical rainforests. The results indicate that despite their marked geographical and taxonomic differences, median δ13Cdiet values from closed canopy rainforests in Amazonia (-27.4‰) and equatorial Africa (-26.9‰) are not significantly different. Amazonian mammals, however, seem to exploit a narrower spectrum of dietary resources than equatorial African mammals, as depicted by the absence of highly negative δ13Cdiet values previously proposed as indicative of rainforests (<-31‰). I hypothesize that differential effects of late Pleistocene extinction may be responsible for the ecological disparities among the two rainforests, by significantly reducing evolutionary time and dietary breadth reflected in the modern Amazonian mammalian community.Finally, the third chapter of this dissertation evaluates assumptions behind δ15N amino acid compound specific analyses in order to test the controversial hypothesis of carnivory and consumption of proteins of animal origin in fossil sloths. This analytical technique relies on three main assumptions. First, that the offset between the δ15N of glutamic acid (δ15NGlx) and phenylalanine (δ15NPhe) in the organism under study will increase with increasing trophic level. Second, that the offset between δ15NGlx and δ15NPhe at the base of the food chain is relatively constant and has a value of -8.4‰ for C3 ecosystems. Third, that the trophic discrimination factor in all ecosystems (the difference in δ15NGlx relative to δ15NPhe with increasing trophic level) is 7.6‰. The results of my experiments conducted on extant xenarthrans (sloths and anteaters) with controlled diets document that only the first assumption holds true. Rather than relying on an equation with constants introducing uncertainties and that are not applicable to organisms feeding on a combination of items of different origin (e.g., C3 + C4 plants), δ15NGlx and δ15NPhe values by themselves can accurately reconstruct the trophic position of organisms. Indeed, the results on δ15NGlx and δ15NPhe herein obtained for five xenarthran species in controlled feeding experiments, combined with mammalian data available from the literature, show strong and significant correlations between these two AAs and with trophic positions. Both the TP equation and the regression analyses of δ15NGlx and δ15NPhe suggest that the Pleistocene fossil ground sloths Mylodon darwinii and Nothrotheriops shastensis were not pure herbivores as commonly presumed, but rather that they were both mixed feeders/omnivores, incorporating items of animal origin in their diets.",,Lara JV,,2020,,,,Ph.D. Thesis
A Framework for User Characterization Based on Tweets Using Machine Learning Algorithms,"Twitter having more than three billion users is one of the most commercial and popular social networking sites. Twitter permits its users to post short messages and update their status. Tweets can be seen instantly by the followers of the users and other people with no twitter accounts. So by far most of the substance posted on the twitter is publicly accessible. Enormous number of political actors used twitter, who are interested in seeking extreme motives like radicalization, mobilization and recruiting activities. Twitters is used by large number of extremist organizations for press releases, public declaration and provide confirmation or motivation of their attacks. There have been several works looking at identifying extremist content based on twitter data but user identification using tweets has not been focused enough because of publication barrier and unavailability of data. In this research, a model is proposed which characterize a user into extremist and non-extremist categories. In this approach, pre-processing is done using natural language processing techniques and feature selection is performed using bag of words model. TF-IDF and word length is applied to obtain vector or feature to measure the significance of obtained vector in the whole document. We performed a methodology using classification through NB (Multinomial naïve Bayes) naïve Bayes on crises related tweets and Kaggle dataset related to tweets published by several Islamic State of Iraq and Sham to validate our proposed model. In this paper, a novel method is discussed for user characterization based on tweets posted by them. Evaluation results show that our suggested method gives best retrieval accuracies for word length feature extraction approach.",,"Zahra K,Azam F,Butt WH,Ilyas F",,2018,11–16,10.1145/3301326.3301373,https://doi-org.proxy.bnl.lu/10.1145/3301326.3301373;http://dx.doi.org/10.1145/3301326.3301373,Conference Paper
Robust Spatial Fuzzy GMM Based MRI Segmentation and Carotid Artery Plaque Detection in Ultrasound Images,,,"Hassan M,Murtza I,Hira A,Ali S,Kifayat K",,2019,179–192,10.1016/j.cmpb.2019.04.026,https://doi-org.proxy.bnl.lu/10.1016/j.cmpb.2019.04.026;http://dx.doi.org/10.1016/j.cmpb.2019.04.026,Journal Article
A Causal Inquiry Into the Economics of Migration and Trade,"Causality is central to economics (Hoover, 2006), for it is the means to predict effects of new interventions and calculate policy counterfactuals (Heckman, 2008). However, in a complex world where ""everything depends on everything else"" (Valavanis, 1959, as quoted in Hoover, 2004), how does one go about identifying cause and effect? In some applied sciences, Randomized Control Trials provide the gold standard for this purpose (Noble Prize Committee, 2021). The use of experiments offers a satisfactory way since it gives evidence that is both controlled and reproducible (Gower, 2012). However, many questions cannot be addressed by an experiment, either due to financial, ethical or practical constraints (Noble Prize Committee, 2021). Many philosophers viewed the use of statistical methods to be a substitute for experiments, because it allows the extraction of a repeated pattern from a large collection of data (Morgan, 1990). More specifically, it gives scientists a way to deal with plurality of causes in a nonexperimental context (Morgan, 1990). Since the adoption of statistical techniques in economic modeling, it was evident that randomization is not applicable in a similar manner as is the case of controlled experiments, so regression analysis did not essentially provide a causal interpretation (Wold, 1954). To deal with this, a recent breakthrough has been the adoption of the design-based approach that is ""aimed at emulating a randomized experiment to answer a causal question using observational data"" (Noble Prize Committee, 2021). With the exception of the first chapter, this thesis applies this methodological tool, in particular; exploits a quasi-experimental variation, to answer causal questions on migration and trade.The first chapter examines the effect of religiosity on employment among migrants. The economics literature has recently been more active in examining the effect of religion on different economic outcomes. Religion is found to be an important determinant of individuals' preferences. In line with some recent work, we deal with the inherent complexity in disentangling religiosity from culture by adopting the epidemiological approach that focuses on migrants. Although an IV strategy, a design-based approach, has been used in the literature to identify the causal effect of religiosity, we decide against its use due to the limitations of IV in this context. We argue that the epidemiological approach along with a rich set of fixed effects that the survey provides allow us to come as close as possible to identifying a causal effect of religiosity on employment. We use one wave of the European Values Study that gives us a sample of 46 European countries in 2008. Our OLS estimates show a negative effect of religiosity on employment. Robustness checks are carried out that confirm the validity of our estimates. We also look into possible mechanisms that could drive this relationship. However, we do not find evidence that any of the potential variables available in our survey can be the channel through which religiosity drives employment. The second chapter, co-authored with Jean-François Maystadt and Maurizio Zanardi, moves into international trade and exploits a natural experiment to better understand the effect of transportation costs on trade. The negative effect of distance on trade is well-established in the literature. However, the debate continues on whether the observed effect is exclusively due to transportation costs or other omitted variables. We take advantage of the blockade that was imposed on the State of Qatar in 2017 to rule out the endogeneity problem and examine how the resulting rise in air transportation costs affected trade. We employ a gravity model estimated using a Poisson pseudomaximum likelihood estimator, and find an air transportation cost elasticity of trade between -0.3 and -0.5. We provide robustness checks to confirm that our results are not driven by potential contaminating factors.The third chapter, co-authored with Jean-François Maystadt and Maria Navarro Paniagua, attempts to get a better understanding of how an exogenous shock affects remittances received by households in Nepal. The blockade on Qatar in 2017 had its share of negative impact on vulnerable migrants. We exploit three waves of a panel dataset on Nepali households between 2016 and 2018. Following the shock, households who had migrants in Qatar experienced a substantial decline in remittances compared to households whose migrants were in different international destinations. We adopt a difference-in-difference approach and control for pre-embargo characteristics to rule out any confounding factors that could bias our estimates. We also show that the decline in remittances is mainly found amongst the poorest households. This result sheds light on the compounded problem of poverty since it is the poor who seem to suffer most and have least resilience in the face of shocks.",,Al-Malk A,,2022,,,,Ph.D. Thesis
Attributing Authors of Emirati Tweets,"Electronic text Author Attribution (AA) is a well known stylometry problem that attempts to infer the identity of authors of disputed electronic texts by solely analyzing the texts. This is important for various applications such as forensics and market analysis. However, currently the state of the art in author identification has never been evaluated against Emirati social media electronic texts. This is partly due to the fact that no evaluation dataset exists that is suitable for evaluating author identification methods in the domain of Emirati social media electronic texts. This paper presents the first of such evaluations, along with the release of the Khonji-Iraqi Emirati Tweets author identification evaluation dataset with 30 authors (KIT30). Additionally, novel definitions of grams are introduced, namely compound grams, which demonstrate that decision models that make use of them can achieve higher classification accuracies than the alternative case when classical definitions of grams are followed. The findings also indicate that, when suitable data representation is used, the degradation in the classification accuracy, as the space of suspect authors increases, is not necessarily as sharp as previously reported in the literature. This suggests that AA problem solvers can be significantly more scalable as previously evaluated.",,"Khonji M,Iraqi Y",,2018,206–212,10.1109/GLOCOM.2018.8647952,https://doi-org.proxy.bnl.lu/10.1109/GLOCOM.2018.8647952;http://dx.doi.org/10.1109/GLOCOM.2018.8647952,Conference Paper
Dengue Spread Information System (DSIS),"Mosquitoes are responsible for transfer of many vector-borne diseases including Malaria, Zika and Dengue. These amount to 17% of the total infectious diseases across the globe, leading to a death toll approximately 700,000 annually.Dengue is a preventable viral infection transmitted by Aedes mosquitoes. However, over the past 50 years, the number of dengue cases has increased by a whopping 30-fold. Every year an approximately 500,000 people are admitted with severe dengue, with an estimated 40,000 deaths. In several countries in south American continent and Asia, dengue is one of the leading causes of death. It is mainly found in tropical and sub-tropical regions, particularly surrounding urban and semi-urban areas. Historically, there has been an intensive increase in the number of dengue cases from 2000-2010 and, if adequately explored, essential information can be retrieved.Our work involves the development of the Dengue Spread Information System (DSIS), a geographic-health information system designed to highlight the spread of dengue cases in Iquitos, Peru, and San Juan, Puerto Rico from 1990 to 2013. The application is aimed at citizens, travelers, policymakers and researchers to analyze and interpret the change in risk factors leading to dengue outbreaks and develop essential early warning applications and policies to counter future dengue outbreaks.",,"Bhanot K,Schroeder D,Llewellyn I,Luczak N,Munasinghe T",,2020,150–159,10.1145/3418094.3418133,https://doi-org.proxy.bnl.lu/10.1145/3418094.3418133;http://dx.doi.org/10.1145/3418094.3418133,Conference Paper
Predicting Student Performance Using Advanced Learning Analytics,"Educational Data Mining (EDM) and Learning Analytics (LA) research have emerged as interesting areas of research, which are unfolding useful knowledge from educational databases for many purposes such as predicting students' success. The ability to predict a student's performance can be beneficial for actions in modern educational systems. Existing methods have used features which are mostly related to academic performance, family income and family assets; while features belonging to family expenditures and students' personal information are usually ignored. In this paper, an effort is made to investigate aforementioned feature sets by collecting the scholarship holding students' data from different universities of Pakistan. Learning analytics, discriminative and generative classification models are applied to predict whether a student will be able to complete his degree or not. Experimental results show that proposed method significantly outperforms existing methods due to exploitation of family expenditures and students' personal information feature sets. Outcomes of this EDM/LA research can serve as policy improvement method in higher education.",,"Daud A,Aljohani NR,Abbasi RA,Lytras MD,Abbas F,Alowibdi JS",,2017,415–421,10.1145/3041021.3054164,https://doi-org.proxy.bnl.lu/10.1145/3041021.3054164;http://dx.doi.org/10.1145/3041021.3054164,Conference Paper
Confronting Global Issues: A Multipurpose IR Simulation,"This article describes an international relations simulation that focuses on threats of transnational insurgent organizations, the future of the Iraqi regime, and the effect of globalization on foreign policies. It contains both the Simulation Director's Guide and the Participant's Guide. The guides explain the steps taken to run the simulation and offer a description of the process. During the game, conflicting goal-directed participants, representing specific actors in the international system, must derive and achieve foreign policy goals given military and monetary constraints. The general rules and procedures apply to simulations involving myriad international processes and actors.",,"Shellman SM,Turan K",,2006,98–123,10.1177/1046878105278924,https://doi-org.proxy.bnl.lu/10.1177/1046878105278924;http://dx.doi.org/10.1177/1046878105278924,Journal Article
Medical Scientific Output and Specialization in Latin American Countries,"“Smart specialization” allows one to identify national strengths and weaknesses within research fields and establish priorities accordingly. It may be a useful strategy for building scientific capacity in developing and peripheral countries. The objective of this paper is to characterize the scientific output and specialization of the most productive Latin American countries with focus on international collaboration and impact. We conducted a descriptive study based on the SCImago Institutions Ranking (SIR) portal, in the field of Medicine, for the period 2003–2013. The set of indicators applied was based on documents, citation, and collaboration. The results show that at the global level, Surgery, Cardiology, Oncology, Neurology, and Public Health are the most productive subjects in Medicine; in Latin America the most productive topics are Public Health, Infectious Diseases, Surgery, Neurology, and Cardiology and Cardiovascular Medicine. The most prolific countries are Brazil, Mexico and Argentina, though the ones having greater impact and more collaboration are Peru, Puerto Rico, and Argentina. The most productive and visible fields, such as Oncology, Cardiology, and Infectious Diseases, are related to major global health problems involving chronic and emerging diseases. This information could be useful to design pragmatic policies, to encourage research in key fields in order to respond better to the health needs of a given population.",,"Zacca-González G,Chinchilla-Rodríguez Z,Vargas-Quesada B",,2018,1635–1650,10.1007/s11192-018-2717-7,https://doi-org.proxy.bnl.lu/10.1007/s11192-018-2717-7;http://dx.doi.org/10.1007/s11192-018-2717-7,Journal Article
Design Thinking for the Construction of Technological Solutions in a Science Course in a Virtual Environment,"The trend of curricular reforms in recent years has been to promote the scientific learning of students, the Peruvian curriculum, through the curricular area of ​​Science and Technology, poses as one of the skills that must be developed in students is the so-called “Design and build technological solutions to solve problems in your environment”, whose products are evaluated at an institutional level for the Eureka Science and Technology Fair contest, however many of these need to be strengthened with aspects of creativity at the design level of the technologies, therefore, we set as an objective, to implement Design Thinking to develop competence in the design and construction of technological solutions to solve problems in their environment in female secondary school students in the Science and Technology course, during remote classes. Applying the intervention based on design thinking from the IDEO proposal, considering functional creativity and various cognitive scaffolds developed in electronic learning, the results were favorable, achieving significant differences in the evaluation scores obtained before and after in the first and second phase of the execution of the intervention achieving a Level of significance = 1%, the students were able to identify an alternative technological solution to the problem presented, design the alternative technological solution, implement it and go through the validation process to comply with the specifications of design and operation and evaluates and communicates the operation and impacts of its technological solution alternative.",,"Arbulú Pérez Vargas CG,Gómez Fuertes A,Reyes Pérez MD,Espino Carrasco DK,Rojas Palacios LE",,2022,3–13,10.1007/978-3-031-05675-8_1,https://doi-org.proxy.bnl.lu/10.1007/978-3-031-05675-8_1;http://dx.doi.org/10.1007/978-3-031-05675-8_1,Conference Paper
Activators of Information in the Peruvian Rural Andes: A Grounded Theory Analysis of the Dissemination of Computer-Mediated Information,"Over the last few years, governments and development agencies have provided access to information and communication technology (ICT) with the intention to improve living conditions in underserved communities around the world. Unfortunately, how the ICT tools are used and the information they convey is exploited have not received enough attention. Through a holistic-multiple case study design, this book describes how individuals living in six rural communities in the northern Peruvian Andes use (or not) computers. An interpretive analysis informed by grounded theory reveals the mechanisms by which computer-mediated information is obtained and disseminated by those who use computers. These social actors are named here as activators of information and their role in disseminating fresh and valuable information is thoroughly explained. This book will be a valuable addition to government officials, donors and professionals working in the ICT for development field. Likewise, researchers interested in the application of grounded theory will benefit from the systematic application of the method for the data analysis in an inductive fashion.",,Diaz Andrade AE,,2009,,,,Book
Algorithms for Enhancing Satellite Imagery to Discover Archaeological Finds Covered by Shadow,"Very high-resolution (VHR) images proved to be an invaluable source of information even in the archaeological domain, but sometimes shadows hinder their full exploitation. To overcome such limitation, this research proposes a workflow able to analyze shadowed zones, by processing Pléiades and World-View 2 images. The case study is the archaeological site of Maltai, in the Iraqi Kurdistan Region, which presents shadowed areas to be detected. Applying de-shadowing workflow has been tested over multispectral and panchromatic images, with different invariant color spaces. The proposed methods exploit the techniques of automatic thresholding and spectral ratio in the detection of shadow regions. This approach shows a clustering of shadow pixels for an enhanced images visualization and proves its suitability for archaeological settings.",,"Chiappini S,Di Stefano F,Malinverni ES,Pierdicca R",,2020,664–673,10.1007/978-3-030-58814-4_53,https://doi-org.proxy.bnl.lu/10.1007/978-3-030-58814-4_53;http://dx.doi.org/10.1007/978-3-030-58814-4_53,Conference Paper
The Narrative and Media Literacy as Influential Factors in the Efficacy of Programs for the Prevention of Teenage Pregnancy,"Unwanted pregnancy in Ecuador is a public health problem. Studies have shown that various psychosocial factors affect risky sexual behavior. Lack of information and poor access to sexual and reproductive healthcare service, an early start to sexual activity, impulsivity, and perceived invulnerability, among others. Interesting results have also been found on the effects that persuasive narrative produce through educational and entertainment videos for changes in attitudes and intention of risky behavior in the area of health. And separately media literacy has been studied as a factor in the level of critical skills used by teens, which makes them more critical in the face of media products that try and convince them of something, but what has not been studied is how being media competent affects their mediation of persuasive arguments in the field of communication on healthcare. The objective of this study is to determine if the level of media literacy mitigates the impact of two videos, narrative and non-narrative, how that impacts their attitude, knowledge, and intentional behavior to prevent unwanted pregnancy, and the perceived vulnerability, counter-argumentation will also be considered, transport and identification with characters as variable mitigating factors. Experimental research will be applied to adolescent women. The results are intended to be used to show the efficacy of the messages in pregnancy prevention videos are key to be considered in healthcare campaigns for sexual and reproductive health in adolescents.",,"Cabrera CG,Igartua JJ",,2016,1189–1196,10.1145/3012430.3012668,https://doi-org.proxy.bnl.lu/10.1145/3012430.3012668;http://dx.doi.org/10.1145/3012430.3012668,Conference Paper
Prediction of Risk Takers in Arterial Hypertension Patients with Data Mining Application,"Hyper arterial pressure (HAP) is a disease that kills silently because it does not produce symptoms in the early stages, making it difficult to diagnose. When it is detected, its treatment is not accessible to everyone, which affects the disease’s long-term development. Hypertension affects a large portion of the Iraqi population. In the current research paper, we have discussed how data mining can be applied to identify the status of the risk factors that affect arterial hypertension due to I10-I15 causes, evaluating the context variables disability, overwork, high-risk pregnancy, stress, high diets, and poor nutrition in the population between 50 and 64 years in the city of Baghdad. It is possible to see how data mining in large volumes of health data can generate new knowledge and thus uncover hidden patterns in the data through the development of this research. Attributes directly linked to disease prevalence can be found in data from Baghdad, Iraq, even if they are not directly linked to a specific cause. This shows that some variables are transversal to the development of the disease regardless of its categorization. Cluster analysis revealed that, even though these diseases are categorized as having different causes, they have a degree of incorrect classification of 40.71% because they present attributes with a similar behavior transversal to the disease and not the disease-specific cause for which it is categorized.",,"Alhazmi L,Alassery F,Rosales HG",,2022,,10.1155/2022/5093049,https://doi-org.proxy.bnl.lu/10.1155/2022/5093049;http://dx.doi.org/10.1155/2022/5093049,Journal Article
Training with Additional Semantic Constraints for Enhancing Neural Machine Translation,"Replacing the traditional cross-entropy loss with BLEU as the optimization objective is a successful application of reinforcement learning (RL) in neural machine translation (NMT). However, a considerable weakness of the approach is that the monotonic optimization of BLEU’s training algorithm ignores the semantic fluency of the translation. One phenomenon is an incomprehensible translation accompanied by an ideal BLEU. In addition, sampling inefficiency as a common shortcoming of RL is more prominent in NMT. In this study, we address these issues in two ways. (1) We use the annealing schedule algorithm to add semantic evaluation for reinforcement training as part of the training objective. (2) We further attach a value iteration network to RL to transform the reward into a decision value, thereby making model training highly targeted and efficient. We use our approach on three representative language machine translation tasks, including low resource Mongolian-Chinese, agglutinative Japanese-English, and common task English-Chinese. Experiments show that our approach achieves significant improvements over the strong baselines, besides, it also saves nearly one-third of training time on different tasks.",,"Ji Y,Hou H,Chen J,Wu N",,2019,300–313,10.1007/978-3-030-29908-8_24,https://doi-org.proxy.bnl.lu/10.1007/978-3-030-29908-8_24;http://dx.doi.org/10.1007/978-3-030-29908-8_24,Conference Paper
Dependency Structures for Statistical Machine Translation,"Dependency structures represent a sentence as a set of dependency relations. Normally the dependency structures from a tree connect all the words in a sentence. One of the most defining characters of dependency structures is the ability to bring long distance dependency between words to local dependency structures. Another the main attraction of dependency structures has been its close correspondence to meaning. This thesis focuses on integrating dependency structures into machine translation components including decoder algorithm, reordering models, confidence measure, and sentence simplification. First, we develop four novel cohesive soft constraints for a phrase-based decoder namely exhaustive interruption check, interruption count, exhaustive interruption count, and rich interruption constraints. To ensure the robustness and effectiveness of the proposed constraints, we conduct experiments on four different language pairs, including English-Iraqi, Spanish and Arabic, Chinese-English. The improvements are in between 0.4 and 1.8 BLEU points. These experiments also cover a wide range of training corpus sizes, ranging from 500K sentence pairs up to 10 million sentence pairs. Furthermore, to show the effectiveness of our proposed methods we apply them to systems using a 2.7 billion words 5-gram LM, different reordering models and dependency parsers. Second, to go beyond cohesive soft constraints, we investigate efficient algorithms for learning and decoding with source-side dependency tree reordering models . We propose a novel source-tree reordering model that exploits dependency subtree inside / outside movements and cohesive soft constraints. These movements and constraints enable us to efficiently capture the subtree-to-subtree transitions observed both in the source of word-aligned training data and in the decoding time. Representing subtree movements as features allows MERT to train the corresponding weights for these features relative to others in the model. Moreover, experimental results on English-Iraqi, Spanish show that we obtain improvements +0.8 BLEU and ý1.4 TER on English-Spanish and +0.8 BLEU and ý2.3 TER on English-Iraqi. Third, we develop Goodness , a novel framework to predict word and sentence level of machine translation confidence with dependency structures. The framework allows MT systems to inform users which words are likely translated correctly and how confident it is about the whole sentence. Experimental results show that the MT error prediction accuracy is increased from 69.1 to 72.2 in F-score. The Pearson correlation between the proposed confidence measure and the human-targeted translation edit rate (HTER) is 0.6 . Improvements between 0.4 and 0.9 TER reduction are obtained with the n-best list reranking task using the proposed confidence measure. Also, we present a visualization prototype of MT errors at the word and sentence levels with the objective to improve post-editor productivity. Finally, inspired by study in summarization we propose TriS , a novel framework to simplify source sentences before translating them. We build a statistical sentence simplification system with log-linear models. In contrast to state-of-the-art methods that drive sentence simplification process by hand-written linguistic rules, our method used a margin-based discriminative learning algorithm operates on a feature set. The feature set is defined on statistics of dependency structures as well as surface form and syntactic structures of sentences. A stack decoding algorithm is developed in order to efficiently generate and search simplification hypotheses. Experimental results show that the simplified text produced by the proposed system reduces 1.7 Flesch-Kincaid grade level when compared with the original text. We show that a comparison of a state-of-the-art rule-based system to the proposed system demonstrates an improvement of 0.2 , 0.6 , and 4.5 points in ROUGE-2, ROUGE-4, and Ave F 10, respectively. We present subjective evaluations of the simplified translation quality for an English-German MT system.",,Bach N,,2012,,,,Ph.D. Thesis
Earthquake Risk to Inca's Historical Constructions in Machupicchu,"The citadel of Machupicchu is probably the most famous Inca heritage site in Peru. Considering the seismically active region, this research is an attempt to perform a seismic risk analysis of the heritage structures at Machupicchu. A systematic approach is adopted for this purpose. Characteristic seismicity of the region, where these historical constructions are located, is discussed based on the seismic hazard analysis. Evaluation of the vulnerability of the structures under the prevalent earthquake hazard is another important aspect essential for risk analysis. As a first step to proper understanding of the seismic behavior of these heritage structures, typical elements of Inca construction are studied by simple analytical models to verify basic aspects of structural integrity. The possibility that peak ground acceleration corresponding to even relatively low hazard may produce instability in some structural components like gable walls was noted. In view of this preliminary result, attempt was made to identify the dynamic characteristics of typical buildings units from more detailed investigation. This forms part of the outcome from the field study program, which included microtremor measurement of free field as well as typical constructions, planned and undertaken by the authors. The results of the microtremor measurements are utilized to estimate the dynamic characteristics of the Inca stone structures. That is, the analytical results are compared with the measurements to calibrate the analytical model. Since microtremor measurements involve very small displacements, the characteristics of stones structures thus obtained correspond to elastic behavior applicable to small strain condition. Based on this scheme, an approach has been proposed to evaluate the seismic behavior and hence the seismic vulnerability of these structures. The procedure also permits identification of the probable mode of failure of the structures concerned.",,"Cuadra C,Karkee MB,Tokeshi K",,2008,336–345,10.1016/j.advengsoft.2007.01.002,https://doi-org.proxy.bnl.lu/10.1016/j.advengsoft.2007.01.002;http://dx.doi.org/10.1016/j.advengsoft.2007.01.002,Journal Article
The CAS-PEAL Large-Scale Chinese Face Database and Baseline Evaluations,"In this paper, we describe the acquisition and contents of a large-scale Chinese face database: the CAS-PEAL face database. The goals of creating the CAS-PEAL face database include the following: 1) providing the worldwide researchers of face recognition with different sources of variations, particularly pose, expression, accessories, and lighting (PEAL), and exhaustive ground-truth information in one uniform database; 2) advancing the state-of-the-art face recognition technologies aiming at practical applications by using off-the-shelf imaging equipment and by designing normal face variations in the database; and 3) providing a large-scale face database of Mongolian. Currently, the CAS-PEAL face database contains 99 594 images of 1040 individuals (595 males and 445 females). A total of nine cameras are mounted horizontally on an arc arm to simultaneously capture images across different poses. Each subject is asked to look straight ahead, up, and down to obtain 27 images in three shots. Five facial expressions, six accessories, and 15 lighting changes are also included in the database. A selected subset of the database (CAS-PEAL-R1, containing 30 863 images of the 1040 subjects) is available to other researchers now. We discuss the evaluation protocol based on the CAS-PEAL-R1 database and present the performance of four algorithms as a baseline to do the following: 1) elementarily assess the difficulty of the database for face recognition algorithms; 2) preference evaluation results for researchers using the database; and 3) identify the strengths and weaknesses of the commonly used algorithms.",,"Gao W,Cao B,Shan S,Chen X,Zhou D,Zhang X,Zhao D",,2008,149–161,10.1109/TSMCA.2007.909557,https://doi-org.proxy.bnl.lu/10.1109/TSMCA.2007.909557;http://dx.doi.org/10.1109/TSMCA.2007.909557,Journal Article
Prediction for Various Drought Classes Using Spatiotemporal Categorical Sequences,"Drought frequently spreads across large spatial and time scales and is more complicated than other natural disasters that can damage economic and other natural resources worldwide. However, improved drought monitoring and forecasting techniques can help to minimize the vulnerability of society to drought and its consequent influences. This emphasizes the need for improved drought monitoring tools and assessment techniques that provide information more precisely about drought occurrences. Therefore, this study developed a new method, Model-Based Clustering for Spatio-Temporal Categorical Sequences (MBCSTCS), that uses state selection procedures through finite mixture modeling and model-based clustering. The MBCSTCS uses the functional structure of first-order Markov model components for modeling each data group. In MBCSTCS, the suitable order K of the components is selected by Bayesian information criterion (BIC). In MBCSTCS, the estimated mixing proportions and the posterior probabilities are used to compute probability distribution associated with the future steps of transitions. Furthermore, MBCSTCS predicts drought occurrences in future time using spatiotemporal categorical sequences of various drought classes. The MBCSTCS is applied to the six meteorological stations in the northern area of Pakistan. Moreover, it is found that MBCSTCS provides expeditious information for the long-term spatiotemporal categorical sequences. These findings may be helpful to make plans for early warning systems, water resource management, and drought mitigation policies to decrease the severe effects of drought.",,"Niaz R,Almazah MM,Zhang X,Hussain I,Faisal M,Amirteimoori A",,2021,,10.1155/2021/7145168,https://doi-org.proxy.bnl.lu/10.1155/2021/7145168;http://dx.doi.org/10.1155/2021/7145168,Journal Article
Measuring the Effects of Risk and Cultural Dimensions on the Adoption of Online Stock Trading: A Developing Country Perspective,"Online stock trading OST is a growing phenomenon across countries, yet there is a sparse literature focusing on the negative utilities risks that causing the low adoption. Drawing from perceived risk theory, this article attempts to fill the gap by identifying the influential risk factors that impede the acceptance of OST in a developing country, Pakistan. The study also applies the Hofstede cultural theory to ascertain the effects of cultural moderators on investors' usage behavior UB. Based on structured questionnaire, 443 valid responses were received from current and potential investors. The model was tested using structural equation modeling through Smart-PLS. The results validate a negative and significant relationship between risk dimensions and investors' behavioral intentions BI to use OST. Especially time, financial, performance, privacy and opportunity cost risks are found having a negative impact on investors' BI. Moreover, the study finds that cultural dimensions, collectivism, and uncertainty avoidance, moderate the relationship between BI and UB.",,"Khan SU,Liu X,Khan IU,Liu C,Hameed Z",,2018,106–127,10.4018/IJEIS.2018070106,https://doi-org.proxy.bnl.lu/10.4018/IJEIS.2018070106;http://dx.doi.org/10.4018/IJEIS.2018070106,Journal Article
Elucidating Atmospheric Turbulence Across Scales in Numerical Models: Land-Atmosphere Interaction Controls of Moist Processes,"This research aims to understand the development of the atmospheric energy spectrum and energy transfer mechanisms across scales. A clear understanding of energy spectrum development and transfer mechanisms is necessary for improving the representation of multiscale processes in the atmosphere, modeling turbulence in the boundary layer, and understanding the limits of atmospheric predictability. This work consists of three parts. The first part investigates the Navier Stokes Equations (NSE) behavior under idealized conditions relevant to large-scale atmospheric turbulence. This study uses a series of direct numerical simulations (DNS) of two-dimensional (2D) with forcing applied at different scales to investigate energy transfer between the synoptic scale and the mesoscale. The DNS results, forced by a single kinetic energy source at large scales, show that the energy spectra slopes of the direct enstrophy cascade are steeper than the theoretically predicted -3 slope. Next, the presence of two inertial ranges in 2D turbulence at intermediate scales is investigated by introducing a second energy source in the meso-a-scale range. The energy spectra for the simulations with two kinetic energy sources exhibit flatter slopes closer to -5/3, consistent with the observed kinetic energy spectra of horizontal winds in the atmosphere at synoptic scales. Further, the results are independent of model resolution and scale separation between the two energy sources, with a robust transition region between the lower synoptic and the upper meso-a scales in agreement with classical observations in the upper troposphere. These results suggest the existence of mesoscale feedback on synoptic-scale predictability that emerges from the concurrence of the direct (downscale) enstrophy transfer in the synoptic scales and the inverse (upscale) kinetic energy transfer from the mesoscale to the synoptic-scale in the troposphere.The second part of this research is devoted to the characterization of atmospheric energy spectra over mountainous terrain under various environmental conditions using the Weather and Research Forecasting (WRF) model. First, a comprehensive analysis of climatology and mesoscale structure of cold air intrusions (CAIs) over the Andes shows that the events are responsible for localized heavy rainfall events (200 mm, less than 6 hours) in the mid-elevations ( 1,500) Peruvian Andes. The climatology analysis uses European Center Medium-Range Weather Forecasts (ECMWF) reanalysis, Tropical Rainfall Measurement Mission (TRMM) data products, and rain-gauge observations. This analysis emphasized characterizing year-round CAI frequency, CAI interactions with Andes topography, and their impact on orographic precipitation climatology. The results show a robust enhancement of the diurnal cycle of precipitation during CAI events in all seasons, particularly in the increase in surface rainfall rate during early morning at intermediate elevations ( 1,500m), the eastern Andes orographic maximum. This link between CAI frequency and rainfall suggests that they play an essential role in maintaining the Andes to Amazon year-round terrestrial connectivity through runoff production and transport by the river networks. Second, the next step examines the transient behavior of horizontal scaling in the vertical during the evolution of extreme orographic precipitation storms. Furthermore, a mechanistic framework to examine the implications of ongoing deforestation in the western Amazon on orographic precipitation in the tropical Andes through land-atmosphere interactions is provided. Understanding the interplay between large-scale dynamics and land-atmosphere interactions is critical to developing an effective boundary layer processes parameterizations for future numerical weather prediction models. The study includes a case over the Appalachians as middle mountains in comparison to high mountains (Andes) highlighting terrain and weather contrasts. Previous work showed that atmospheric model simulations exhibit different scaling behavior of vertically averaged horizontal wind (u, v) and moisture (q) in the mesoscales for convective (spectral slopes β −5/3) and non-convective (β −11/5) conditions. Here, the results show that β exhibits a strong diurnal cycle switching between convective and non-convective behavior following the space‐time evolution of atmospheric stability in the lower troposphere (below 700 hPa) depending on latitude, topography, landform, and the synoptic environment. Anomalous flattening of the wind and moisture spectra (i.e., spectral saturation, ∣β ∣ < 5/3) at high wavenumbers and up to 200 hPa is tied to convective activity, where and when strong vertical motions develop, corresponding to an abrupt directional switch from horizontal energy transfer to vertical energy transfer including latent heating release and parameterized microphysical processes. In the small mesoscales (<50 km), β − 5/3 at all times up to 200 hPa with nighttime steepening (β −11/5) below the orographic envelope where cold air pools form at low elevations and vertical motion weakens in the Appalachians. In the Andes, at a high elevation, the scaling behavior exhibits a stronger diurnal cycle at low levels (below 700 hPa) with significant shoaling between tropical and high latitudes. Furthermore, blocking and strong modification of regional circulations result in nighttime anisotropy at midlevels on the altitudinal profile along the North‐South topographic divide. The third part focuses on modeling turbulent fluxes near the surface, which is essential for an accurate representation of the heterogeneous surface boundary layer. Second-moment turbulent models have been widely used in numerical weather prediction models for parameterizing the planetary boundary layer (PBL). The most commonly used schemes are based on the Mellor and Yamada (1982) model, which are currently implemented to only account for the contribution of the vertical divergences of the vertical turbulent fluxes in the WRF model. Horizontal tendencies are parameterized separately based on a Smagorinsky scheme. Although this approach may be successful in coarse grid models (e.g., dx 12-2 km), the influence of horizontal gradients becomes more important when the grid spacing is smaller (less than 1 km). Recently, a full 3D PBL scheme (3DPBL) has been implemented in WRF to reconcile the representation of the vertical and horizontal turbulent mixing. The model integrates 3DPBL parameterization with a diagnostic model of the three-dimensional second-order turbulent properties of the flow in the surface layer. A set of modifications introduced to the surface parameters provides a better diagnosis of the surface layer covering different flow regimes based on anisotropy and stability conditions. The near-surface diagnostic variables are analyzed and compared with the data from the Weather Forecast Improvement Project II (WFIPII).Finally, the dissertation discusses and recommends potential directions for future research focusing on boundary layer processes.",,"Eghdami M,Bragg A,Katul G,Li W",,2020,,,,Ph.D. Thesis
A Survey of Software Estimation Techniques and Project Planning Practices,"Paper provides in depth review of software and project estimation techniques existing in industry and literature, its strengths and weaknesses. Usage, popularity and applicability of such techniques are elaborated. In order to improve estimation accuracy, such knowledge is essential. Many estimation techniques, models, methodologies exists and applicable in different categories of projects. None of them gives 100% accuracy but proper use of them makes estimation process smoother and easier. Organizations should automate estimation procedures, customize available tools and calibrate estimation approaches as per their requirements. Proposed future work is to study factors involved in Software Engineering Approaches (Software Estimation in focus) for Offshore and Outsourced Software Development taking Pakistani IT Industry as a Case Study.",,Nasir M,,2006,305–310,10.1109/SNPD-SAWN.2006.11,https://doi-org.proxy.bnl.lu/10.1109/SNPD-SAWN.2006.11;http://dx.doi.org/10.1109/SNPD-SAWN.2006.11,Conference Paper
Simulation of Vehicular Network Use in Emergency Situations and Security Applications on a Pakistan Highway,"VANETs (vehicular ad hoc networks), which are revolutionary techniques to enhance road safety, can be used to broadcast information about dangerous traffic conditions or accidents. However, distributing important information for driver safety and well-being has strict time and reliability requirements. This is because messages must be received by all cars involved in a potentially dangerous scenario for proper precautions to be taken to avoid the problem from materializing or intensifying. Because of the deterioration in conventional wireless communication system performance, ensuring that such requirements are met is a serious concern. To validate the concept before the actual installation of such systems and their absorption into the vehicle sector, it is therefore critical to employ simulation methodologies that are both reliable and thorough. This piece consists of large-scale, realistic security simulation research of an emergency situation based on actual road traffic data acquired on a Pakistan route. The study’s findings are detailed in the following paragraphs. Aspects such as the incorporation of fixed communication units along a stretch of roadway and the performance of the vehicular network notifying all vehicles engaged in the various accident scenarios modeled on the same stretch of highway were evaluated. Both of these characteristics were designed to increase safety and security applications. After doing the investigation, it was observed that when fixed communication units are incorporated into the network infrastructure, there is a shorter delay in receiving the accident notification. This was the conclusion made after reviewing the findings. Drivers of vehicles located closer to the accident site will be able to respond in a timely and safe manner as a result of this improvement in network performance, and drivers security of vehicles located further away will have the option of exiting the highway to avoid potential congestion caused by increased road traffic.",,"Al-Douri AT,Mohammed Kadhim N,Mohamad AA,Abeyie M,Azeem I",,2022,,10.1155/2022/2902263,https://doi-org.proxy.bnl.lu/10.1155/2022/2902263;http://dx.doi.org/10.1155/2022/2902263,Journal Article
Information Security Education and Self-Perception of Privacy Protection Risk in Mobile Web in Obstetrics Students from Peru,"The objective of the study was to determine the information security education topics developed in the training of obstetrics students and their relationship with the self-perception of privacy protection risk in mobile web during the COVID-19 pandemic at the Santiago Antúnez of Mayolo National University (UNASAM) (Huaraz-Peru). A correlational cross-sectional investigation was developed, with 164 obstetric students. The information was collected through a questionnaire applied online between November and December 2020, having determined its validity and reliability. The Chi squared statistical test (p < 0.05) was used, while the information processing was carried out using the SPSS program. It was determined that 61.6% of obstetric students perceived a high risk in the protection of their privacy in mobile web during the development of their activities in the academic cycle 2020-I. Likewise, it was evidenced that the majority of students stated that they had not developed the topics consulted with regard to information security education during their virtual studies in obstetric, especially with regard to the recommendations for the use of passwords (83.5%), privacy protection strategies (81.1%) and data management through the creation of backups (79.9%), showing a statistically significant relationship with the self-perception of privacy protection risk in mobile web (p < 0.05). It was concluded that the low development of information security education topics in the training of obstetric students is related to the self-perception of high risk in the privacy protection in mobile web during the COVID-19 pandemic.",,"Olaza-Maguiña AF,De La Cruz-Ramirez YM",,2021,32–43,10.1007/978-3-030-83164-6_3,https://doi-org.proxy.bnl.lu/10.1007/978-3-030-83164-6_3;http://dx.doi.org/10.1007/978-3-030-83164-6_3,Conference Paper
The Status of Information Security Systems in Banking Sector from Social Engineering Perspective,"Social Engineering Attack has recently become a real threat affecting organizations, and 53.9% of such attacks target the banking sector. Successful attacks violate privacy by breaching sensitive data, and can cause huge financial loss for organizations and individuals, alongside reputational damage for firms. Although banks invest extensive resources in cyber security, with large budgets spent on securing their hardware and software, the human factor offers numerous weaknesses that can be easily exploited, and real and pertinent security challenges remain serious threats. This paper presents an information technology governance framework applied on a Jordanian bank to protect the system from social engineering attack. We worked on a case study that mainly focuses on phishing attack, which is considered one of the most common threats in banks, and the way staff will deal with it. The results show positive improvements in staff awareness and in avoiding such types of attacks, as well as a marked increase in reporting any suspicious activity noticed by employees.",,"Hammour RA,Gharaibeh YA,Qasaimeh M,Al-Qassas RS",,2019,,10.1145/3368691.3368705,https://doi-org.proxy.bnl.lu/10.1145/3368691.3368705;http://dx.doi.org/10.1145/3368691.3368705,Conference Paper
"Kodi's Master Guide: A Step By Step Pictorial Guide On How To Download, Install & Upgrade To Kodi 18.0 On Xbox One & 17.6 On IPhone, IPad, Amazon Fire ... VPN, Mouse Toggle, Add-Ons, IPTV, SET TV..","HAVE YOU EVER THOUGHT ABOUT INSTALLING KODI ON YOUR IPAD, IPHONE, AMAZON FIRE TV STICK, FIRE TV, WINDOWS, ANDROID TV BOX, ANDROID PHONE AND XBOX ONE? Do you desire to turn your iPad, iPhone, Amazon Fire TV Stick, Fire TV, Windows, Android TV Box, Android Phone and Xbox One into an entertainment hub? Do you know that you can turn your home into an entertainment center? Do you know that you can update the Kodi on your gadget to the latest version (17.6 krypton and 18.0 leia) Do you know that you can surf online and stream thousands of TV shows, music, videos, kiddies, live shows, romance, sport documentary and a lot more for free? Do you know that with Area 51 IPTV installed and just $5 you can enjoy as much as 70 HD channels in a month? Do you know that you can enjoy maximum security, privacy and anonymity with IPVanish VPN or ExpressVPN installed on your gadget? What more? Kodis Master Guide is your ultimate guide that will usher you in a step by step method with pictures into the world of entertainment. What will you gain? With this book, you will get to learn: A simplified step by step method with pictures on how to download and install the latest version of Kodi on your iPad or iPhone, Amazon Fire TV Stick or Fire TV, Windows, Android TV Box, Android Phone and Xbox One How to upgrade to the latest version of Kodi on your Windows, Amazon Fire TV Stick or Fire TV, Android TV Box, and iPhone or iPad How to install: Neptune Rising, genesis Reborn, Covenant, Dogs Bollock and Supremacy add-ons on your device How to enable and disenable parental control to restrict or limit what people watch on you device How to install and setup ExpressVPN on your Amazon Fire TV Stick or Fire TV, iPhone or iPad, and Mac How to setup and install IPVanish on Amazon Fire TV Stick or fire TV and iPhone or iPad How to install and setup Mouse Toggle and to fixed mouse toggle to walk perfectly on your Amazon Fire TV Stick or Fire TV How to setup and install Area 51 IPTV on your Amazon fire TV Stick or Fire TV, Android Box TV and Nvidia How to setup and install SET TV on your Amazon Fire TV Stick and Fire TV to enjoy both locals and international HD channels And a lots of other secrets tips to get the best of Kodi. Grab your copy BY CLICKING THE BUY BUTTON NOW! tags: Box install download iptv, Fire TV stick kodi smart, what update upgrade book, 17.6 latest loaded media, play how dot ipad iphone jailbroken pro kodi guide, python mxq 17.6 18 smart, amazon fire tv Stick box, android manual how book, VPN Add-ons app install ultra python iptv, how what area 51 ip mx3 echo dot box install download hd, raspberry pi krypton what, how upgrade update adds kodi Ray Phillips, installing exodus on new kodi 17 krypton STEVEN MARK, how to install kodi on firestick Alex Silver, how to watch tv without cable Stephen Lovely, how to install kodi on fire stick Steve Wright, exodus on kodi 17 1 krypton KEN ADAMS, kodi, kodi manual, kodi for dummies, kodi book for firestick, kodiak point, kodi for dummies 2017, kodi books, kodi app for fire tv stick, kodi android, kodi for firestick, kodi fire stick, kodi for android, kodi guide, kodi how to book, kodi heart, kodi instructions for fire stick, kodi krypton, kodi on amazon fire tv, kodi on firestick, kodi tv box, kodi the ultimate guide to kodi, amazon echo, alexa James Ryan, fire hd 8 tablet with alexa, fire hd 8 user manual, fire hd8 manual, fire hd8 tablet, fire hd for dummies, fire hd 8 user guide, new fire hd manual Jake Jacobs, fire hd 8 & 10 user guide Jennifer N. Smith, fire hd 8 & 10 Steve Jacobs, all new fire hd 8 & 10 user guide, Jenna Edwards, fire hd 8 Jordan Pittman, all new fire hd 8 tablet in depth Pharm Ibrahim, the amazon fire & fire hd",,Babson EA,,2018,,,,Book
"Kodi Simplified Manual: A Simplified Manual With Pictures & Simple Language To Help You Download, Install & Restart Kodi (17.6) & The Setup & ... IPTV & SET TV On Your Amazon Fire TV Stick..","DO YOU WANT TO UNLOCK THE TRUE POTENTIALS AND CAPABILITIES OF KODI ON YOUR AMAZON FIRE TV STICK, FIRE TV, XBOX ONE, ANDROID PHONE, TABLET, IPHONE, IPAD, ANDROID TV BOX, AND PC OR WINDOW? Have you ever thought about Kodi as the best open source entertainment app that you need? Have you ever thought about cutting down your cable bills? Have you ever thought about saving more money with Kodi installed on your gadget? Have you ever thought about turning your home into an entertainment center? Do you want to get the best of fictions, non-fictions, romance, comedy, live TV shows, documentary, kid shows, music and a lots more? Have you ever consider streaming online with maximum security, privacy and anonymity with IPVanish installed on your device? Have you ever considered controlling what your kids or people watch on your device? Do you know that you can setup and install Area 51 IPTV on your Amazon Fire TV Stick or Fire TV, Android TV Box and Nvidia? Look no further for Kodi Simplified Manual is here to answer all you questions and lead you into getting the best of Kodi. In this book, the author, Dr. (Mrs.) Magdalene will walk you through the following: How to download and install Kodi on Amazon Fire TV Stick, Amazon Fire TV, Android TV Box, Android Phone or Tablet, PC or Computer, iPad or iPhone and Xbox One How to Turn ON Unknown Source Toggle on iPhone and iPad How to install Genesis Reborn, Covenant, Neptune Rising, Supremacy and Dogs Bollock add-ons on your Kodi How to enable and disenable parental control to manage what people or your children watch from your device that you installed Kodi How to setup IPVanish on Amazon Fire TV Stick or Fire TV and iPhone or iPad How to clear your old add-ons and start all over How to download and install FireDL on your Amazon Fire TV Stick or Fire TV and Android TV Box TV How to download and setup Area 51 IPTV on your Amazon Fire TV Stick, Fire TV, Android TV Box and Nvidia How to setup SET TV on your Amazon Fire TV Stick or Fire TV And many more! Dont hear about it, get it, do it and have the experience! What are you waiting for? Get your copy BY CLICKING THE BUY BUTTON NOW! tags: Box install download iptv, Fire TV stick kodi smart, what update upgrade book, 17.6 latest loaded media, play how dot ipad iphone jailbroken pro kodi guide, python mxq 17.6 18 smart, amazon fire tv Stick box, android manual how book, VPN Add-ons app install ultra python iptv, how what area 51 ip mx3 echo dot box install download hd, raspberry pi krypton what, how upgrade update adds kodi Ray Phillips, installing exodus on new kodi 17 krypton STEVEN MARK, how to install kodi on firestick Alex Silver, how to watch tv without cable Stephen Lovely, how to install kodi on fire stick Steve Wright, exodus on kodi 17 1 krypton KEN ADAMS, kodi, kodi manual, kodi for dummies, kodi book for firestick, kodiak point, kodi for dummies 2017, kodi books, kodi app for fire tv stick, kodi android, kodi for firestick, kodi fire stick, kodi for android, kodi guide, kodi how to book, kodi heart, kodi instructions for fire stick, kodi krypton, kodi on amazon fire tv, kodi on firestick, kodi tv box, kodi the ultimate guide to kodi, amazon echo, alexa James Ryan, fire hd 8 tablet with alexa, fire hd 8 user manual, fire hd8 manual, fire hd8 tablet, fire hd for dummies, fire hd 8 user guide, new fire hd manual Jake Jacobs, fire hd 8 & 10 user guide Jennifer N. Smith, fire hd 8 & 10 Steve Jacobs, all new fire hd 8 & 10 user guide, Jenna Edwards, fire hd 8 Jordan Pittman, all new fire hd 8 tablet in depth Pharm Ibrahim, the amazon fire & fire hd user guide Charles Tulley, amazon fire hd 8 Alan Brown, fire hd user guide manual Jake Jacobs, fire hd 8 Andrew Johansen, fire hd tablet",,Magdalene C,,2018,,,,Book
"KODI: The Missing Guide A Step By Step Guide With Pictures On How To Download & Install Kodi, The Installation & Setting Up Of Mouse Toggle, VPN, ... Stick Or Fire TV, Android Phone, Windows..","LEARN HOW TO DOWNLOAD & INSTALL KODI ON PC OR COMPUTER, IPHONE OR IPAD, AMAZON FIRE TV OR FIRE TV STICK, ANDROID PHONE OR TABLET & ANDROID TV BOX WITH SIMPLE LANGUAGE AND PICTURE Do you know that you can watch limitless movies, TV shows, music, sport, documentary fashion, kid shows and a lots more for free? Do you know that you can turn your PC or computer, iPad or iPhone, Amazon Fire TV Stick or Fire TV, Android TV Box and Android Phone or table into an entertainment center? Do you know that you can clear up you old add-ons that you are having issues with and go back to default and start installing new add-ons that will work perfectly? Do you want to enjoy every single bit of your device? Do you want to explore the full capabilities, functionalities and potentialities of your PC or computer, iPad or iPhone, Amazon Fire TV or Fire TV Stick, Android TV Box and Android Phone or Tablet? Do you know that with Area 51 IPTV or SET TV, you can enjoy a lot of both local and international TV channels on your Kodi? Do you know that with IPVanish installed on your device you can surf online with maximum security, privacy and free from fraudsters? Do you know that with Add-ons installed on your Kodi you will enjoy endless entertainment from sport to music, to kid shows to drama, comedy, fictions, non-fictions, romance, documentary and a lot more for free? Kodi: The Missing Guide is all that you need, to enjoy all the benefits that accompanied Kodi. In this book you will learn: How to download and install Kodi on PC or Windows, iPad or iPhone, Amazon Fire TV or Amazon Fire TV Stick, Android TV and Android Phone or Tablet with pictures and simple language to make it very easier How to set up Area 51 IPTV and SET TV to get the best of both local and international TV channels How to install Add-ons on your installed Kodi How to installed Neptune Rising, Supremacy, Genesis Reborn, Covenant and Dogs Bollock add-ons with pictures and simple language to make it very simple for you to follow the steps and install yours How to install and setup IPVanish on your Amazon Fire TV or Fire TV Stick and iPad or iPhone with a simple language and pictures to simplified it to you How to install and fixed Mouse Toggle on your Amazon Fire TV Stick or Fire TV How to manage what people watch from your Kodi by enabling parental control How to clear your old add-ons that are not working perfectly and start installing new ones that will work perfectly? How to install FireDL on your Amazon Fire TV Stick or Fire TV and Android TV Box And a lot more! Believe me, this book is priceless. Get a copy BY CLICKING THE BUY BUTTON now tags: kodi Ray Phillips, installing exodus on new kodi 17 krypton STEVEN MARK, how to install kodi on firestick Alex Silver, how to watch tv without cable Stephen Lovely, how to install kodi on fire stick Steve Wright, exodus on kodi 17 1 krypton KEN ADAMS, kodi, kodi manual, kodi for dummies, kodi book for firestick, kodiak point, kodi for dummies 2017, kodi books, kodi app for fire tv stick, kodi android, kodi for firestick, kodi fire stick, kodi for android, kodi guide, kodi how to book, kodi heart, kodi instructions for fire stick, kodi krypton, kodi on amazon fire tv, kodi on firestick, kodi tv box, kodi the ultimate guide to kodi, amazon echo, alexa James Ryan, fire hd 8 tablet with alexa, fire hd 8 user manual, fire hd8 manual, fire hd8 tablet, fire hd for dummies, fire hd 8 user guide, new fire hd manual Jake Jacobs, fire hd 8 & 10 user guide Jennifer N. Smith, fire hd 8 & 10 Steve Jacobs, all new fire hd 8 & 10 user guide, Jenna Edwards, fire hd 8 Jordan Pittman, all new fire hd 8 tablet in depth Pharm Ibrahim, the amazon fire & fire hd user guide Charles Tulley, amazon fire hd 8 Alan",,Cabon EA,,2018,,,,Book
"Applications and Techniques in Information Security: 8th International Conference, ATIS 2017, Auckland, New Zealand, July 67, 2017, Proceedings","This book constitutes the refereed proceedings of the 8th International Conference on Applications and Techniques in Information Security, ATIS 2017, held in Auckland, New Zealand, in July 2017. The 14 revised full papers and 4 short papers presented together with two abstracts of invited talks were carefully reviewed and selected from 29 submissions. The papers are organized in topical sections on Keynote Speeches; Crypto Algorithms and Applications; Attacks; Malware and Malicious Events Detection;System and Network Security.",,"Batten L,Kim DS,Zhang X,Li G",,2017,,,,Book
SPSM '12: Proceedings of the Second ACM Workshop on Security and Privacy in Smartphones and Mobile Devices,"It is our great pleasure to welcome you to the Second ACM Workshop on Security and Privacy in Smartphones and Mobile Devices -- SPSM'12, held in association with the 19th ACM Conference on Computer and Communications Security, October 19th, 2012, in Raleigh, NC (USA).The workshop was created last year to organize and foster discussion of security in the emerging area of smartphone and mobile device computing. As organizers of top security venues, we've observed an increasing number of submissions describing novel approaches to solving the challenges of this area. We wanted to provide a dedicated venue to discuss these challenges and promising approaches for future research directions. SPSM'11 was a great success, with an excellent turnout of 80 registered attendees and in-depth discussion. This year, we will continue the 15 minute back-to-back talks followed by 45 minutes of discussion and hope to meet and exceed the high bar that was set.The call for papers attracted 30 submissions from Canada, China, Germany, Greece, India, Iran, Italy, Japan, Lebanon, Nigeria, South Africa, and the United States. The program committee accepted 11 papers that cover a variety of topics, including permission models, user studies, attacks on smartphones, and methods of defense. We are especially pleased to have a keynote speech by Geir Olsen, a Principle Program Manager in the operating systems group on the Windows Phone team, on Windows Phone 8 Security. We hope that these proceedings will serve as a valuable reference for security researchers and developers.",,,,2012,,,,Book
Fast Asymmetric Encryption and Decryption of SimpleMatrix Scheme for Internet of Things,,,Yi H,,2022,145–153,10.1016/j.comcom.2022.04.013,https://doi-org.proxy.bnl.lu/10.1016/j.comcom.2022.04.013;http://dx.doi.org/10.1016/j.comcom.2022.04.013,Journal Article
Laying the Foundations of Climate Justice for Vulnerable States & Peoples : Developing a Human Rights Approach for the South Pacific,"This thesis critically examines the protections available to climate vulnerable Small Island Developing States (SIDS) and their peoples under international law from a climate justice perspective. It explores how key principles, obligations, and mechanisms of international human rights law and state responsibility can be used to secure effective legal remedies for both SIDS governments and individuals. An interdisciplinary approach to the development of a climate justice framework to inform future law and policy making is adopted, applying and building upon the bodies of political theory and international legal scholarship. The findings of the thesis are closely informed by an empirical case study of the South Pacific region, conducted from May-July 2016 through a visiting researcher position at the University of the South Pacific School of Law in Port Vila, Vanuatu. The case study employs qualitative interviewing alongside desk-based doctrinal analysis of the relevant legal frameworks and policy documents. Twenty-eight semi-structured interviews were conducted across two national sites in Port Vila, Vanuatu and Suva, Fiji, with interviewees from UN bodies, Pacific regional organisations, national governments, civil society organisations, and legal practice. A climate justice framework is developed, drawing upon two core tenets of distributive and procedural justice and informed inductively by the findings of the empirical case study. This grounded approach to the conceptualisation of climate justice enables the development of a framework capable of addressing the operational, legal and institutional challenges that are encountered by climate vulnerable SIDS and their peoples in practice. The research is driven by the desire, first and foremost, to construct an empowering climate justice framework capable of strengthening climate change responses at the international level for the benefit of SIDS, and, secondly, to contribute to the growing bodies of political theory and legal literature on climate change from an empirically-grounded, rights-focused perspective.",,Venn AL,,2020,,,,Ph.D. Thesis
"Context-Aware Middleware for Anytime, Anywhere Social Networks","Recent advances in wireless technologies and mobile devices let users form opportunistic social networks of interests with nearby users. However, anytime, anywhere social networks raise several technological issues, including the detection of user location; the modeling, acquisition, and analysis of a user's characterizing properties; and the dynamic extraction of social networks. SAMOA, a semantic context-aware middleware approach, lets you create anytime, anywhere social networks among users in physical proximity. SAMOA separates social-network management from application logic by providing reusable middleware support for various social application scenarios. In addition, SAMOA exploits semantic-based context modeling and matching algorithms for social-network extraction. This article is part of a special issue on social computing.",,"Bottazzi D,Montanari R,Toninelli A",,2007,23–32,,,Journal Article
Home Area Network: A Security Perspective,"This paper relates to establishing and analysing the need for home area network (HAN) security. There always exists the need to balance the security set up against the risk being mitigated. Many of the noncomputing savvy households pay very little attention to home network security. Previously, viruses and other malicious programs were the only threats envisaged. With the advent of broadband, wireless networking, and convergence of different communication technologies being adopted by HANs, these vulnerabilities have further increased. People like to think that private personal information theft, identity theft and credit card frauds will not affect them. This paper begins with a brief background to home area network security, discusses the current and future technology trends likely to impact HAN security. The authors cover why HAN security is crucial and go on to suggest application of security solutions in few of the typical home area network scenarios in New Zealand.",,"Sathu H,Shukla R",,2007,85–90,,,Conference Paper
Analysis of the Essential Factors Affecting of Intention to Use of Mobile Learning Applications: A Comparison between Universities Adopters and Non-Adopters,"Although mobile learning systems offer several benefits for students, academic staff and universities, from easily access and learning anywhere and anytime, the use and acceptance of this new technology in Jordan still very low. However, acceptance of mobile learning by students is crucial to the success of mobile learning. The factors that affect the use and user acceptance of mobile learning are still controversial. Thus, this study mainly proposes an integrated model, with the aim of identifying the most influential factors that may encourage or impede students and universities in Jordan in moving towards acceptance and adoption of mobile learning applications. The proposed model was evaluated empirically with 1200 students from both two groups of universities that already used the mobile learning technology and non-adopters universities in Jordan. The model aims to examine the impact of 11 factors on the adoption of mobile learning applications that were categorised based on four fundamental constructs are: (i) technological factors (security, privacy, compatibility, relative advantage and trust), (ii) organizational factors (resistance to change and technology readiness), (iii) cultural factors and (iv) quality factors (quality of system, quality of content and quality of service). The key findings include: (1) resistance to change, security and privacy concerns still limit mobile learning acceptance and adoption in Jordanian universities; (2) some factors like compatibility, technology readiness, and culture were found to have a negative effect on the intention to use of the mobile learning; (3) five factors (relative advantage, trust, quality of system, quality of content and quality of service) were found to have a positive effect on the intention to use of the mobile learning; and (4) our research also found that the effect of these factors differed in universities that already used the mobile learning and non-adopters. Finally, it is expected that the findings of this research can assist university decision makers, mobile learning application providers and the research community in introducing better strategies for encouraging adoption and acceptance of this technology.",,"Almaiah MA,Al Mulhem A",,2019,1433–1468,10.1007/s10639-018-9840-1,https://doi-org.proxy.bnl.lu/10.1007/s10639-018-9840-1;http://dx.doi.org/10.1007/s10639-018-9840-1,Journal Article
"Heritage Building Information Modelling (HBIM) as a Tool for Heritage Conservation: Observations and Reflections on Data Collection, Management and Use in Research in a Middle Eastern Context","The rich architectural and urban heritage of Jordan is under continuing threat not only through means of physical attack but also physical disaster, increasing urbanization and a diminishing value from multiple stakeholders such as owners and users. This research study explores the potential of digital technologies in documenting and preserving urban architectural heritage in Jordan. Data was collected from diverse stakeholders on heritage conservation in Jordan. The findings evidence that Building Information Modelling (BIM) has the potential to create a classification system for heritage buildings under threat and set forth the application of legislation and regulations about heritage . The study demonstrated that the collection of data information needs to be understood through the context of cultural sensitivity. Lack of awareness in the value of cultural heritage from local communities exacerbates the diminishing efforts in preserving cultural assets. In collecting information for the documentation of this heritage, this study categorizes the challenges of preserving urban heritage as either hierarchical or cultural. The collection, management and storing of data for digital heritage requires an awareness of the issues of time and the power structures that are involved in their collection and upon which they have a profound effect.",,"Aburamadan R,Moustaka A,Trillo C,Makore BC,Udeaja C,Gyau Baffour Awuah K",,2021,3–14,10.1007/978-3-030-77411-0_1,https://doi-org.proxy.bnl.lu/10.1007/978-3-030-77411-0_1;http://dx.doi.org/10.1007/978-3-030-77411-0_1,Conference Paper
A Study on Higher Order Differential Attack of KASUMI,"This paper proposes novel calculuses of linearizing attack that can be applied to higher order differential attack. Higher order differential attack is a powerful and versatile attack on block ciphers. It can be roughly summarized as follows: (1) Derive an attack equation to estimate the key by using the higher order differential properties of the target cipher, (2) Determine the key by solving an attack equation. Linearizing attack is an effective method of solving attack equations. It linearizes an attack equation and determines the key by solving a system of linearized equations using approaches such as the Gauss-Jordan method. We enhance the derivation algorithm of the coefficient matrix for linearizing attack to reduce computational cost (fast calculus 1). Furthermore, we eliminate most of the unknown variables in the linearized equations by making the coefficient column vectors 0 (fast calculus 2). We apply these algorithms to an attack of the five-round variant of KASUMI and show that the attack complexity is equivalent to 228.9 chosen plaintexts and 231.2 KASUMI encryptions.",,"Sugio N,Aono H,Hongo S,Kaneko T",,2007,14–21,10.1093/ietfec/e90-a.1.14,https://doi-org.proxy.bnl.lu/10.1093/ietfec/e90-a.1.14;http://dx.doi.org/10.1093/ietfec/e90-a.1.14,Journal Article
An Empirical Investigation on Acceptance of Mobile Payment System Services in Jordan: Extending UTAUT2 Model with Security and Privacy,"Several developed and developing countries have launched a mobile payment system service, which is known in Jordan as Jordan Mobile Payment (JoMoPay) system to overcome the drawbacks of traditional payment system. The system supports payment transactions by utilising mobile phones applications. However, the acceptance of JoMoPay system in Jordan is still below the level of expectation. This study was undertaken to understand and explain the acceptance of JoMoPay system based on extending the unified theory of acceptance and use of technology (UTAUT2) model in the Jordanian context. The model was extended by considering two additional constructs namely; security and privacy. Utilising a self-reported survey, collected data was analysed using structural equation modelling (SEM) to test the research model. Five constructs were found to be the determinants of behavioural intention to use JoMoPay system, namely performance expectancy, social influence, price value, security and privacy. Together they account for 61.4% of the variance in behavioural intention. However, effort expectancy, facilitating condition and hedonic motivation did not have a significant impact on behavioural intention to use JoMoPay system and hence the related hypotheses were not supported. Lastly, conclusions, limitations and future research directions will be discussed further in the last section of the paper.",,"Al-Okaily M,Rahman MS,Ali A,Abu-Shanab E,Masa'deh R",,2023,123–152,10.1504/ijbis.2023.128306,https://doi-org.proxy.bnl.lu/10.1504/ijbis.2023.128306;http://dx.doi.org/10.1504/ijbis.2023.128306,Journal Article
RETRACTED ARTICLE: Acceptance and Usage of a Mobile Information System Services in University of Jordan,"Along with the significant development of information and communication technologies (ICTSs), an incredible number of mobile applications have become available. Hence, the main purpose of the current study is to investigate the use and acceptance of the ‘Mobile Information System’ developed and implemented by University of Jordan, which Known as (Mobile Student Information System). Data were obtained from 275 undergraduate students of University of Jordan via questionnaire to test the ‘Mobile Services Acceptance Model’ using Structural Equation Model. The results reveal that user acceptance of mobile information system services is largely affected by trust, perceived security, perceived ease of use and perceived usefulness. Findings also show that context of applications is a strong motivational factor of perceived ease of use and perceived usefulness, which then significantly affects user intention to use mobile information system. While, the personal characteristics and features do not have effect on user intentions. Both theoretical and practical implications of the study’s findings are discussed.",,Almaiah MA,,2018,1873–1895,10.1007/s10639-018-9694-6,https://doi-org.proxy.bnl.lu/10.1007/s10639-018-9694-6;http://dx.doi.org/10.1007/s10639-018-9694-6,Journal Article
Understanding the Acceptance of Mobile University Services: An Empirical Analysis,"The use of mobile technologies is increasingly widespread because of the need of mobility. This study aims to explore the utilisation of mobile phone services in the educational environment and investigate students' expectations and attitudes towards mobile university services in Jordan. Data for this study have been collected using a questionnaire containing 34 questions. Out of the 400 questionnaires that were distributed randomly to students at Al-Hussein Bin Talal University in Jordan, 370 were returned (92.5%). As a result of this study it appears that if students have positive attitude towards mobile university, they will use it. This study found that a large majority of the students indicated their willingness to become users of such services if offered; taking into consideration security and privacy, and social influences as factors that could affect the success of using mobile university in Jordan. This study gives quantified indicators about mobile university and a model that might help in understanding the mobile university environment in Jordan.",,Jaradat MI,,2010,407–427,10.1504/IJMLO.2010.037537,https://doi-org.proxy.bnl.lu/10.1504/IJMLO.2010.037537;http://dx.doi.org/10.1504/IJMLO.2010.037537,Journal Article
Exploring Technical Quality Factors That Enhance Mobile Learning Applications Services Using Data Mining Techniques,"Mobile learning (m-learning) has become an increasingly attractive solution for schools and universities that utilize new technologies in their teaching and learning setting. This study investigates the technical factors affecting the development of m-learning applications services from students’ perspectives. It presents a model consisting of twelve technical factors, including content usefulness, scalability, security, functionality, accessibility, interface design, interactivity, reliability, availability, trust, responsiveness, and personalization. To evaluate the model, a questionnaire was designed and distributed to 151 students in Jerash University, Jordan. The results indicate that all technical factors have positive affects on learner satisfaction and overall m-learning applications services, however the data mining analysis revealed that security and scalability factors exert a major impact on student satisfaction with m-learning applications services. This study gives insight for the future of developing and design m-learning applications.",,Abu-Al-Aish A,,2021,1–23,10.4018/IJICTE.20211001.oa14,https://doi-org.proxy.bnl.lu/10.4018/IJICTE.20211001.oa14;http://dx.doi.org/10.4018/IJICTE.20211001.oa14,Journal Article
"Understanding Individuals' Perceptions, Determinants and the Moderating Effects of Age and Gender on the Adoption of Mobile Learning: Developing Country Perspective","In recent years, mobile devices, applications and services have largely spread over the globe and have become a popular commodity. This study is launched to investigate the factors that influence individuals' intention to adopt and use of mobile learning (m-learning) in Jordan. Besides, it provides quantified indicators, designs principles, opportunities, limitations and a conceptual model that might help in understanding m-learning phenomenon in the Jordanian educational environment. The current study is based on a modified Technology Acceptance Model (TAM) by incorporating social influence, security and privacy, price value and service quality factors as external variables. This study examines also the moderating effects of age and gender differences among the study variables. The suggested model was tested with data collected by means of a paper-based questionnaire by using WarpPLS 4.0 software. The results reveal that individuals' adoption and the use of m-learning can be anticipated from individuals' behavioural intention with 39% in variance.",,Jaradat MI,,2014,253–275,10.1504/IJMLO.2014.067028,https://doi-org.proxy.bnl.lu/10.1504/IJMLO.2014.067028;http://dx.doi.org/10.1504/IJMLO.2014.067028,Journal Article
Adoption of Mobile Commerce Technology: An Involvement of Trust and Risk Concerns,"This research extended the Technology Acceptance Model TAM with perceived trust and perceived risks security and privacy concerns constructs to identify the impact of these factors on Jordanian users' intentions to adopt mobile commerce m-commerce. An empirical test was used utilizing 132 responses from students in two public universities in Jordan. Results indicated that perceived trust, perceived usefulness, and perceived ease of use are major influencers of mobile commerce adoption. On the other hand, perceived risk factors security and privacy concerns were not significant in this relation. Discussion, conclusion and future work are stated at the end of this paper.",,"Abu-Shanab E,Ghaleb O",,2012,36–49,10.4018/jtd.2012040104,https://doi-org.proxy.bnl.lu/10.4018/jtd.2012040104;http://dx.doi.org/10.4018/jtd.2012040104,Journal Article
Security and Performance Considerations in Wireless Networks,"The open and shared nature of the wireless medium makes it easy for adversaries to launch simple, yet effective, denial of service attacks (DoS attacks). As an example, jamming attacks, involve the uncoordinated transmission of electromagnetic energy on the medium. In a carrier sensing network (e.g., 802.11), this attack strategy increases the number of collisions at the receiver side and/or blocks the medium access to legitimate nodes at the transmitting side. Both of the above effects degrade the wireless network performance significantly. Frequency hopping (FH) has been traditionally used to overcome jamming attacks. However, we analytically and experimentally show that FH is inadequate to efficiently cope with jamming in today’s networks. Later we propose a suite of systems that aim at coping with jamming attacks at various levels (i.e., detection, localization and prevention). We first identify two intelligent and effective jamming attacks that can be launched in 802.11 WLANs and we provide robust detection systems. In particular, we design and implement (i) CMD, a system to detect active jamming attacks that exploit the carrier sensing functionality of 802.11 networks and (ii) FIJI, a cross-layer system for detecting (and mitigating) jamming attacks that exploit the performance anomaly of 802.11 WLANs. Furthermore, given the importance of locating the jamming device in many deployment scenarios (e.g., battlefield), we propose a lightweight jamming localization scheme. Our system utilizies ideas borrowed from the gradient descent optimization method. The system’s evaluations, show the potentials and applicability of our localization strategy. The final step for coping with jamming attacks is jamming prevention. Based on our initial measurement driven analysis, we do not rely on a FH scheme, that tries to simply avoid the jammer. On the contrary, we design, implement and evaluate a prevention system, called ARES (Anti-jamming REinforcement System), to fight against the saboteur. ARES is applicable to carrier sensing networks and tunes the parameters of rate adaptation and power control to improve the performance under the presence of an attack while ensuring that operations under benign conditions are unaffected. Our extensive evaluations, show that ARES improves the network throughput across all scenarios by up to 150%.",,Pelechrinis K,,2010,,,,Ph.D. Thesis
"Examining the Influence of Mobile Store Features on User E-Satisfaction: Extending UTAUT2 with Personalization, Responsiveness, and Perceived Security and Privacy","Despite the rapid growth in mobile stores (e.g., Apple Store, Google Play), scholarly research in this area is still in the early stages. In particular, there is a need for more empirical analysis of how the main features of these new systems shape the customer experience. This study aims to empirically identify and validate the key factors shaping users’ satisfaction toward mobile stores. The conceptual model was proposed based on a group of the main factors from the extended Unified Theory of Acceptance and Use of Technology (UTAUT2), mobile interactivity, and perceived security and privacy. The empirical analysis was conducted in Jordan by collecting data from a convenience sample of users of mobile stores. Structural equation modelling was applied to test the current study’s model. The results support the significant impact of performance expectancy, price value, hedonic motivation, personalization, responsiveness, and perceived security and privacy on user satisfaction. Discussion of the main limitations and future research directions are also provided.",,"Alalwan AA,Baabdullah AM,Rana NP,Dwivedi YK,Kizgin H",,2019,50–61,10.1007/978-3-030-29374-1_5,https://doi-org.proxy.bnl.lu/10.1007/978-3-030-29374-1_5;http://dx.doi.org/10.1007/978-3-030-29374-1_5,Conference Paper
"Exploring the Influence of Security/Privacy, Trialability, Output Quality and Anxiety on the Adoption of Mobile Decision Support Systems among Nurses: A Developing Country Context","Nursing staff need to be highly mobile in executing their routine work. Therefore, they may need to catch, deliver and/or receive critical information, orders or alerts via mobile devices at any point of care to help them take immediate decisions/actions or orders to accomplish their tasks quickly. This paper investigates the factors that affect the adoption of mobile decision support systems among nurses in Jordan. Experience and voluntariness as moderators in the proposed model were also investigated. The model was analysed and tested using WarpPLS 5.0 software. The findings of this study have demonstrated that perceived usefulness, perceived ease-of-use, security/privacy, trialability, output quality, and anxiety are important constructs in predicting and affecting intentional behaviour to adopt decision support systems among nurses in Jordan. The model has explained 65% of the variance in behavioural intention. Theoretical contributions and practical implications are outlined. Limitations and suggestions for future studies are discussed.",,Jaradat MI,,2021,251–281,10.1504/ijmlo.2021.116508,https://doi-org.proxy.bnl.lu/10.1504/ijmlo.2021.116508;http://dx.doi.org/10.1504/ijmlo.2021.116508,Journal Article
Under Quantum Computer Attack: Is Rainbow a Replacement of RSA and Elliptic Curves on Hardware?,"Among cryptographic systems, multivariate signature is one of the most popular candidates since it has the potential to resist quantum computer attacks. Rainbow belongs to the multivariate signature, which can be viewed as a multilayer unbalanced Oil-Vinegar system. In this paper, we present techniques to exploit Rainbow signature on hardware meeting the requirements of efficient high-performance applications. We propose a general architecture for efficient hardware implementations of Rainbow and enhance our design in three directions. First, we present a fast inversion based on binary trees. Second, we present an efficient multiplication based on compact construction in composite fields. Third, we present a parallel solving system of linear equations based on Gauss-Jordan elimination. Via further other minor optimizations and by integrating the major improvement above, we implement our design in composite fields on standard cell CMOS Application Specific Integrated Circuits (ASICs). The experimental results show that our implementation takes 4.9 us and 242 clock cycles to generate a Rainbow signature with the frequency of 50 MHz. Comparison results show that our design is more efficient than the RSA and ECC implementations.",,"Yi H,Khokhar UM",,2018,,10.1155/2018/2369507,https://doi-org.proxy.bnl.lu/10.1155/2018/2369507;http://dx.doi.org/10.1155/2018/2369507,Journal Article
Development of Secure Protocols for MHIS Using RMPJK-RSA Cryptosystem: Development of Secure Exchange Protocols for Medical Health Insurance System Using RMPJK-RSA Cryptosystem,"One of the essential security services needed to safeguard online transactions is flaxen exchange. In e-Medical transaction protocols two parties can exchange their signatures in a fair manner, so that either each party gain the others signature or no one obtain anything useful. This Thesis examines security solutions for achieving e-Medical transactions among Patient/Doctor/PMHIP/Bank. It proposes new security protocols based on the ""Rebalanced Multi-Prime Jordan-TotientRSA Cryptosystem and Signature Scheme "". This thesis concentrates on security solutions for achieving Threshold e-Medical transactions in e-Medical Health Insurance System applications, Threshold contract signing and Threshold certified delivery of valuable data. A Threshold reasonable contract signing protocol allows two potentially mistrusted parities to exchange their commitments (i.e., digital signatures) to an agreed contract over the Internet in a fair way, so that either each of them obtains the others signature, or neither party does.",,Reddy EM,,2011,,,,Book
CSTST '08: Proceedings of the 5th International Conference on Soft Computing as Transdisciplinary Science and Technology,"Soft Computing (SC) has an evolving collection of methodologies, which is aimed to exploit tolerance for imprecision uncertainty, and partial truth to achieve robustness, tractability, and low cost. SC provides attractive opportunity to represent the ambiguity in human thinking with real life uncertainty. Fuzzy logic (FL), Neural Networks (NN), and Evolutionary Computation (EC) were the core methodologies of soft computing. Later chaos computing, fractal theory, wavelet transformation, cellular automaton, percolation models, and immune network theory were added to enhance soft computing. However, they should not be viewed as competing with each other, but synergistic and complementary, instead. SC was actually the combination or fusion of each methodology which yielded new computational capabilities (hybrid systems). Soft computing is currently causing a paradigm shift (breakthrough) in science and technology.The stage for the Fifth IEEE/ACM International Conference on Soft Computing as Transdisciplinary Science and Technology (CSTST'08) has been set. This edition is dedicated to commemorate the memory of Professor Yasuhiko Dote, Founding Chair of WSTST series of meetings. In essence, CSTST'08 is built on the success of the previous four events held in Muroran, Japan namely the IEEE International Workshop on Neuro Fuzzy Control, in 1993; IEEE International Workshop on Soft Computing in Industry, in 1996, the IEEE International Workshop on Soft Computing in Industry, in 1999 and International Workshop on Soft Computing as Transdisciplinary Science and Technology (WSTST'2005). CSTST'08 is hosted by University of Cergy Pontoise, France and is technically co-sponsored by IEEE Systems Man and Cybernetics Society, ACM SIGAPP (French Chapter), IEEE French Section, World Federation on Soft Computing, European Society for Fuzzy Logic, Technology and International Fuzzy Systems Association, and AFIHM - French Association of Human Computer Interaction. On behalf of the CSTST'08 program committee, we wish to extend a very warm welcome to this edition in Cergy-Pontoise/Paris, France. The conference program committee has organized an exciting and invigorating program comprising presentations from distinguished experts in the field, and important and wide-ranging contributions on state-of-the-art research that provide new insights into current cutting edge results on ""Soft Computing as Transdisciplinary Science and Technology"".This year, we received over 212 regular submissions and we are really gratified by the international diversity of this conference: authors of submitted work hail from no less than 30 countries including Vietnam, Egypt, Bulgaria, Turkey, Russia, Netherlands, Austria, Malaysia, Sweden, Croatia, Kuwait, Cyprus, Belgium, Estonia, Latvia, Lebanon, Macedonia, Singapore, Argentina, United Arab Emirates, Thailand, Ukraine, Hungary, Ireland, Czech, Republic, Spain, Norway, Taiwan, Canada, Libya, Romania, Mexico, Greece, Brazil, Pakistan, Germany, Australia, Tunisia, India, United States of America, Italy, Korea, Poland, Algeria, Japan, United Kingdom, Iran, China, Portugal, and France. The technical program of CSTST'08 conference comprises of 62 papers. The conference program committee had a very challenging task of choosing high quality submissions. Each paper was peer reviewed by at least three or more independent referees of the program committee and the papers were selected based on the referee recommendations. The papers offer stimulating insights into emerging intelligent technologies and their applications in Internet security, chance discovery, humanized computational intelligence, web intelligence, data mining, image processing, swarm intelligence, optimization and so on.",,,,2008,,,,Book
Global Information Society Watch 2014: Communications Surveillance in the Digital Age,"Communications surveillance in the digital age Online surveillance, security and privacy are concerns that have been central to human rights activists for years but with the recent revelations by former National Security Agency (NSA) contractor Edward Snowden of United States (US) government spying on citizens, the issues have reached global attention. This Global Information Society Watch (GISWatch) tracks the state of communications surveillance in 59 countries across the world countries as diverse as Hungary, India, Argentina, The Gambia, Lebanon and the United Kingdom. Each country report approaches the issue from a different perspective. Some analyse legal frameworks that allow surveillance, others the role of businesses in collecting data (including marketing data on children), the potential of biometrics to violate rights, or the privacy challenges when implementing a centralised universal health system. The perspectives from long-time internet activists on surveillance are also recorded. Using the 13 International Principles on the Application of Human Rights to Communications Surveillance as a starting point, eight thematic reports frame the key issues at stake. These include discussions on what we mean by digital surveillance, the implications for a human rights agenda on surveillance, the Five Eyes inter-government surveillance network led by the US, cyber security, and the role of intermediaries. These reports are published at a critical time: they show how rampant government surveillance is across the world, and how business is often complicit in this. They suggest action steps that civil society can take to push for a human rights framework for internet governance and to expose what until now has remained hidden.",,"with Developing Countries (Hivos) HI,for Progressive Communications (APC) A",,2014,,,,Book
An Exploration and Evaluation of the Attitudes and Perceptions of Students toward Use of Mobile Technology in Higher Education in the Kingdom of Jordan,"Despite the growth and use of mobile internet devices in many parts of the world, including the US, UK, and other countries, Jordanian universities have remained resistant regarding adoption of such technology. Many reasons exist behind this specific resistance in implementing mobile learning technology: cost of the devices, distractibility caused by the devices in learning environments, as well as safety, security, and privacy. Since technology has become a pivotal aspect of learning in today's world, it is crucial for students and universities to realize the importance of implementing mobile technology in the field of education, as it will help them in meeting the international standard of education. Mobile learning (m learning) in fact extends the reach of education in many part of the world, not only to all social-economic levels but also in a manner unconstrained of location and time, thus signifying the new opportunity for further development in the education industry. Nonetheless, there is still lack of sufficient understanding regarding the factors affecting acceptance of mobile learning. Based on information systems and mobile commerce acceptance, this research study developed an integrated model to predict the acceptance of mobile learning by the higher education students of Jordan, with the hope this model will provide a framework for future research and serve as a research basis for future surveys and analysis of data gathered by future researchers.",,Allathkani MA,,2013,,,,Ph.D. Thesis
Estimation of Evapotranspiration Using Fused Remote Sensing Image Data and Energy Balance Model for Improving Water Management in Arid Area,"Remote sensing has proved to be very useful in the investigation of vegetation and hydrological monitoring, especially when studying vast areas. In this paper, the complement between two optical remote sensing data (Landsat TM and NOAA- AVHRR) and a Digital Elevation Model (DEM) is used to estimate hydrological parameters based on derived surface reflectance. These parameters which are used in the Modified Soil Energy Balance Algorithm for Land (M-SEBAL) model have been used to estimate net radiation, soil heat flux, sensible heat flux and evapotranspiration (ET) for Sana’a Basin in Yemen. The area is known for arid and semi-arid weather conditions with undulating topography. Image data from AVHRR on-board NOAA satellites with a large areal coverage, good temporal and spectral resolution are found to be very useful in generating some parameters required for the above process. However, the data have poor spatial resolution. On the other hand, image data from the Thematic Mapper on-board the Landsat satellite, with a high spatial and spectral resolution should be able to provide values for the parameters involved, but the area coverage is significantly reduced. This study has been carried out, using a data fusion technique in order to exploit the respective advantages of these two disparate sources of image data. A general framework is then proposed to generate ET maps for arid and semi-arid regions. This is achieved by means of multi-temporal, multi-resolution remote sensing data. Taking into account topographic effects, an attempt has also been made to incorporate DEM information for estimating the net radiation of the areas involved. An application for computing a daily ET map over Sana’a Basin, Yemen is presented. As a result, a daily ET map generated from meteorological observations was compared with estimated ET data simulated from remote sensing data. In conclusion, data from both remote sensing sources give reasonable values with the result from the TM being better than those obtained from the AVHRR. This is attributed to the differences in spatial resolution, in which TM data is higher than AVHRR. The fusion of the two shows improves spatial detail whilst maintaining the spectral signature close to the original.",,"Almhab A,Busu I",,2009,529–533,10.1109/ICCET.2009.228,https://doi-org.proxy.bnl.lu/10.1109/ICCET.2009.228;http://dx.doi.org/10.1109/ICCET.2009.228,Conference Paper
Effectiveness of Mobile Electrocardiogram in Healthcare: From Mobile Application and Development to Community Reaction,"Chronic diseases such as heart and blood vessels are considered among the most common and serious reasons of mortality in the world. In Europe alone, over four million deaths a year (45% of all deaths) are caused by heart diseases [1]. In addition, chronic diseases are responsible for 70 % of United States deaths, and account for more than 75% of annual United States medical care cost [2]. For instance, Cardio Vascular Diseases (CVD) are considered the main cause for around 14.3% of total deaths in Denmark5, and it is the main cause for over 45% of the total death in Lebanon6. It is too costly to keep CVD patients under control locally within the vicinity of a healthcare unit. Thus, researchers recently started to realize the need for automated monitoring in health systems that are expected to reduce the overall death rate and cost associated with monitoring of patients. However, a general monitoring health system will not cover all diseases at once. Therefore, there is a pushing necessity for monitoring health systems which are dedicated to specific health cases. To contribute to the ongoing efforts, this work develops an automated system which could be customized for various chronic diseases. A mobile application based solution is proposed. Further, the work concentrates on CVD by conducting a survey in Lebanon to investigate the acceptance and awareness of ECG for remote monitoring of patients. The results are promising and reflect how specialists are aware of the need to utilize the rapid development in technology combined with the widespread usage of mobile phone which may be used as the main device to guarantee 24/7 communication link for ECG. Adopting ECG in the healthcare system will allow for capturing some valuable data which could guide the development of a recommendation system. This will issue necessary alerts to specialists and guidance to patients and their careers so that specialists could attend to the case on timely basis and patients with their careers could follow the recommendations to keep the case under control until the specialist becomes available. Finally, a secure forum based communication system will be developed to allow patients to share their experience and specialists to provide consultancy and guidance on demand.",,"Kassem A,Yildirim UO,Turğut KA,Wiil UK,Özyer T,Alhajj R",,2017,896–903,10.1145/3110025.3120985,https://doi-org.proxy.bnl.lu/10.1145/3110025.3120985;http://dx.doi.org/10.1145/3110025.3120985,Conference Paper
Unsupervised Recurrent Neural Networks for Grammar Induction,"Grammatical induction using only positive examples is a challenging problem due to the issue of over-generalization. Existing methods attempt to solve this problem by allowing no generalization and instead creating the canonical definite grammar. However, often some level of generalization is needed. An unsupervised recurrent neural network architecture for regular grammar extraction which allows the user to control the level of generalization is presented in this dissertation. One application of grammatical inference is anomaly detection. By using the grammar as a model for normal activity, anomalous activities are those which are not a member of the grammar. Examples of anomalous activities may be difficult or even dangerous to obtain. Grammatical inference using only positive, or normal, examples, then, is the best approach to these types of anomaly detection problems. An example of an anomaly detection problem where anomalous activities cannot be enumerated completely is a facility security system, AMISS, being developed at Los Alamos National Laboratory. In this system, anomalies are not only dangerous and difficult to obtain but encoding specific anomalies into the security system would weaken the system and make it vulnerable to anomalies or security infractions not encoded. By learning normal actions within the facility, and then detecting when deviations occur, the system as a whole is stronger. To address the problem of grammatical inference using only positive examples a new two layer architecture is developed in the research for this dissertation. The first layer is a hybrid of the Fuzzy ART unsupervised neural network with the Jordan recurrent neural network architecture. The hybrid, called Temporal Fuzzy ART, allows sequential patterns to be clustered and a single parameter is used to control the generalization of the grammar which will be extracted. The second layer uses a new algorithm to extract a regular grammar from a trained Temporal Fuzzy ART network. After the grammar is extracted, the grammar can be used to speed up the anomaly detection process. In this dissertation, the two layer architecture and practical methodology for regular grammar inference using only positive examples are presented. The algorithm has been tested in a real-time application at Los Alamos National Laboratory. The anomaly detection system has been shown to be useful in identifying anomalous path data and giving useful feedback in the form of degree of anomaly and an explanation of where the path should have been. Solutions to several fundamental problems in grammatical inference and anomaly detection are presented.",,"Adair KL,Bassett SI",,1998,,,,Ph.D. Thesis
Security Protection Technology in Multi-Attribute Data Transmission Based on Fuzzy Genetic Algorithm,"Because the traditional data transmission security protection methods ignore the process of multi-attribute data detection, resulting in the abnormal data false alarm rate, high missing alarm rate, eliminating accuracy and other problems, a multi-attribute data transmission security protection method based on fuzzy genetic algorithm is proposed. The anomaly detection method based on fuzzy data mining and genetic algorithm is adopted to detect and obtain the abnormal data in the multi-attribute data transmission, and the abnormal data in the multi-attribute data transmission is eliminated through the abnormal data elimination method based on PSO and SVM, so as to realize the security protection of multi-attribute data transmission. It is verified that the recall rate and accuracy rate of abnormal data of the proposed method are higher than 95%, and the removal accuracy of abnormal data is higher. Moreover, this method is far better than the comparison method in positive likelihood ratio and Jordan index, and has higher application value.",,"Lv S,Chen H",,2022,897–917,10.1007/s11277-021-08447-7,https://doi-org.proxy.bnl.lu/10.1007/s11277-021-08447-7;http://dx.doi.org/10.1007/s11277-021-08447-7,Journal Article
"Cybercrime Profiling: Decision-Tree Induction, Examining Perceptions of Internet Risk and Cybercrime Victimisation","The Internet can be a double-edged sword. While offering a range of benefits, it also provides an opportunity for criminals to extend their work to areas previously unimagined. Every country faces the same challenges regarding the fight against cybercrime and how to effectively promote security for its citizens and organisations. The main aim of this study is to introduce and apply a data-mining technique (decision-tree) to cybercrime profiling. This paper also aims to draw attention to the growing number of cybercrime victims, and the relationship between online behaviour and computer victimisation. This study used secondhand data collected for a study was carried out using Jordan a s a case study to investigate whether or not individuals effectively protect themselves against cybercrime, and to examine how perception of law influences actions towards incidents of cybercrime. In Jordan, cybercafe's have become culturally acceptable alternatives for individuals wishing to access the Internet in private, away from the prying eyes of society.",,"Al-Nemrat A,Benzaid C",,2015,1380–1385,,,Conference Paper
People and Pixels: Integrating Remotely-Sensed and Household Survey Data for Food Security and Nutrition,"For several decades now, the study of environmental impacts on human well-being has been informed by what are called ""People and Pixels'' methods: the combining of remotely sensed data about environmental conditions with geolocated data from household surveys about health and nutrition. However, much of this work has been conducted at the scale of individual countries and often relies on only one or two survey waves, which creates substantial issues around spatial autocorrelation and endogeneity. Furthermore, much of this work uses simple linear regression as its analysis technique, which is limited in its ability to describe spatial variation as well as non-linearities in the relationship between the environment and human well-being. Thus, this dissertation uses several insights from the emerging field of data science to advance these methods. First, this analysis draws on large, multinational datasets from dozens of surveys, making it possible to better estimate the non-linear effects of climate extremes on human well-being as well as examine spatial heterogeneities in vulnerability. Secondly, this analysis uses techniques at the boundary between traditional econometric regression models and more complex machine learning models, such as using Generalized Additive Models (GAMs) as well as LASSO estimation. This permits the creation of spatially-varying terms as well as nonlinear effects. Applying these techniques, the dissertation has yielded several insights that could be beneficial to policymakers in governments, non-profits, and multinational organizations. The initial chapters analyze the effects of rainfall anomalies on food security and malnutrition, finding that the effect of an anomaly varies considerably depending on the local socioeconomic and environmental contexts, with low-income, poorly-governed, and arid countries, such as Somalia and Yemen, being the most vulnerable. The latter chapters look at the role of ecosystem services in improving human livelihoods, as well as how land cover is associated with dependence on local provisioning ecosystem services.",,"Cooper MW,Brown M,Sahyoun N,Silva J,Zvoleff A",,2020,,,,Ph.D. Thesis
A Review of Mobile Cloud Computing in Education during the Covid-19 Pandemic in Jordan,"The concept of mobile cloud computing (MCC) becomes a commonly recent trend because of the impact of the covid-19 pandemic on most vital sectors around the world. The educational sector has been affected by the covid-19 pandemic lockdowns. The paradigm shift in education strategies has greatly affected many groups in society related to the educational sector. This research study reviews the perceptions of using MCC on teachers, parents, and students during the covid-19 lockdowns period in Jordan. The objectives of the research are to study the educational environment from the MCC user’s viewpoint, the difficulties that the users face while using MCC in learning, and their concerns about using MCC since it becomes an inevitable solution. Moreover, this research highlights the extent to which users know and accept MCC that they use and its effects on their privacy. Questionnaire results concluded that over 60 % of the users used social media and over 50collaborative applications as a communication method through their smart mobile devices compared to users who relied on specialized e-learning educational platforms, computers, or televisions. About 50 % of the users agreed to learn MCC techniques if it is required especially if they have the opportunity to see someone using these techniques in front of them, although near 35 % of the users thought that MCC threats their privacy. Over 65 % of the users determined the most difficulty or fear is disconnecting communication, especially while performing online exams. In conclusion, this research suggested an MCC online/offline examination framework.",,"Mizher MA,Mazhar AA,Mizher MA",,2022,187–193,10.1145/3489088.3489101,https://doi-org.proxy.bnl.lu/10.1145/3489088.3489101;http://dx.doi.org/10.1145/3489088.3489101,Conference Paper
"Cybercrime Profiling: Decision-Tree Induction, Examining Perceptions of Internet Risk and Cybercrime Victimisation","The Internet can be a double-edged sword. While offering a range of benefits, it also provides an opportunity for criminals to extend their work to areas previously unimagined. Every country faces the same challenges regarding the fight against cybercrime and how to effectively promote security for its citizens and organisations. The main aim of this study is to introduce and apply a data-mining technique (decision-tree) to cybercrime profiling. This paper also aims to draw attention to the growing number of cybercrime victims, and the relationship between online behaviour and computer victimisation. This study used secondhand data collected for a study was carried out using Jordan a s a case study to investigate whether or not individuals effectively protect themselves against cybercrime, and to examine how perception of law influences actions towards incidents of cybercrime. In Jordan, cybercafe's have become culturally acceptable alternatives for individuals wishing to access the Internet in private, away from the prying eyes of society.",,"Al-Nemrat A,Benzaid C",,2015,1380–1385,10.1109/Trustcom.2015.534,https://doi-org.proxy.bnl.lu/10.1109/Trustcom.2015.534;http://dx.doi.org/10.1109/Trustcom.2015.534,Conference Paper
Optimal Design-Space Exploration of Streaming Applications,"Many embedded and scientific applications are pipelined (i.e., streaming) and deployed on application-specific systems. Typically, there are several design parameters in the algorithms and architectures used that impact the tradeoff between different metrics of application performance as well as resource utilization. Efficient automatic exploration of this design space is the goal of our research. We present a global optimization framework comprising a domain-specific variation of branch-and-bound that reduces search complexity by exploiting the topology of the application's pipelining. We exploit the topological information to discover decomposability through the canonical Jordan block form. The reduction in search complexity for four real-world streaming applications (drawn from the literature) is significant, ranging from a million-fold reduction in search space size to a reduction factor of 10 billion. All four optimization problems are thereby solvable in reasonable time.",,"Padmanabhan S,Chen Y,Chamberlain RD",,2011,227–230,10.1109/ASAP.2011.6043274,https://doi-org.proxy.bnl.lu/10.1109/ASAP.2011.6043274;http://dx.doi.org/10.1109/ASAP.2011.6043274,Conference Paper
A High-Efficiency Data Distribution Algorithm in Distributed Storage,"To improve the survivability of distributed storage systems, using the theory of similarity transformation of Jordan standard shape in theory of matrix, combining the method of Lagrange interpolation method, we design a safe and high-efficient data distributing algorithm with threshold scheme. This algorithm has higher efficiency, stronger security and survivability than reference [1]. It has important applications in the intensive data distributed storage system and some storage scenes which have very high expectations for survivability.",,"Yang XY,Liu Z,Zhang W,Guo DT",,2009,627–630,10.1109/IAS.2009.225,https://doi-org.proxy.bnl.lu/10.1109/IAS.2009.225;http://dx.doi.org/10.1109/IAS.2009.225,Conference Paper
The Use of Data Mining to Assist Crop Protection Decisions on Kiwifruit in New Zealand,"A method is developed to predict insecticide spray decisions using machine learning.Spray diary data are used to predict the outcome of spray monitoring decisions.Using a naive Bayes model 70% of no-spray decisions were made to accuracy of 95%.The method provides new insights into factors affecting pest incidence and control.The method has wide application if linked with on-orchard data capture measures. Data mining algorithms were used to develop models to forecast the outcome of leafroller pest monitoring decisions on 'Hayward' kiwifruit crops in New Zealand. Using industry spray diary and pest monitoring data gathered at an orchard block level for compliance purposes, 80 attributes (independent variables) were created in three categories from the spray diary data: (1) individual insecticide applications applied during 2-week time windows, (2) groups of insecticide applications within time periods prior to or after fruit set and (3) orchard management attributes. Five machine learning algorithms (Decision Tree, Naïve Bayes, Random Forest, AdaBoost, Support Vector Machine) and one statistical method (Logistic regression) (classifiers) were used to develop models to forecast insecticide application decisions for leafroller control, by predicting whether pest monitoring results were above or below a spray threshold. Models to forecast 2011 spraying decisions were trained on 2008 and 2009 data and tested on 2010 data. Forecasts were made for spray and no-spray decisions based upon pre-determined acceptable rates of precision (proportion of correct decisions in test results). Orchard blocks in which a forecast could not be made to a prescribed degree of precision were recommended to be monitored, which is the normal practice. Spray decisions could not be forecast to an acceptable degree of precision, but decisions not to spray were successfully forecast for 49% of the blocks to a precision of 98% (AdaBoost) and 70% of the blocks to a precision of 95% (Naïve Bayes). Models with as few as four attributes gave useful forecasts, and orchard management attributes were the most important determinants of model forecasting accuracy. The potential for this methodology to assist with pest spray forecasting using customised data sets is discussed.",,"Hill MG,Connolly PG,Reutemann P,Fletcher D",,2014,250–257,10.1016/j.compag.2014.08.011,https://doi-org.proxy.bnl.lu/10.1016/j.compag.2014.08.011;http://dx.doi.org/10.1016/j.compag.2014.08.011,Journal Article
Recurrent Neural Networks on Duty of Anomaly Detection in Databases,"In the paper we present a new approach based on application of neural networks to detect SQL attacks. SQL attacks are those attacks that take advantage of using SQL statements to be performed. The problem of detection of this class of attacks is transformed to time series prediction problem. SQL queries are used as a source of events in a protected environment. To differentiate between normal SQL queries and those sent by an attacker, we divide SQL statements into tokens and pass them to our detection system, which predicts the next token, taking into account previously seen tokens. In the learning phase tokens are passed to recurrent neural network (RNN) trained by backpropagation through time (BPTT) algorithm. Teaching data are shifted by one token forward in time with relation to input. The purpose of the testing phase is to predict the next token in the sequence. All experiments were conducted on Jordan and Elman networks using data gathered from PHP Nuke portal. Experimental results show that the Jordan network outperforms the Elman network predicting correctly queries of the length up to ten.",,"Skaruz J,Seredynski F",,2007,85–94,10.1007/978-3-540-72395-0_12,https://doi-org.proxy.bnl.lu/10.1007/978-3-540-72395-0_12;http://dx.doi.org/10.1007/978-3-540-72395-0_12,Conference Paper
Tracing SQL Attacks via Neural Networks,"In the paper we present a new approach based on application of neural networks to detect SQL attacks. SQL attacks are those attacks that take the advantage of using SQL statements to be executed. The problem of detection of this class of attacks is transformed into time series prediction problem. SQL queries are used as a source of events in a protected environment. To differentiate between normal SQL queries and those sent by an attacker, we divide SQL statements into tokens and pass them to our detection system, which predicts the next token, taking into account previously seen tokens. In the learning phase tokens are passed to recurrent neural network (RNN) trained by backpropagation through time (BPTT) algorithm. Training data in the output of RNN are shifted by one token forward in time with relation to input. An additional rule is defined to interpret RNNs output. Experiments were conducted on Jordan and Elman networks and the results show that the Jordan network outperforms the Elman network predicting correctly queries with higher efficiency. Moreover, our results lead to the form of the rule, which can be successfuly applied to the subset of SQL statements taken into consideration in this study.",,"Skaruz J,Seredynski F,Bouvry P",,2007,549–558,,,Conference Paper
"CISSP Exam Prep Questions, Answers & Explanations: 1000+ CISSP Practice Questions with Detailed Solutions","-- UPDATED FOR THE 2015 CISSP EXAM -- Countless time and money is spent preparing for the CISSP certification exam. So why aren't students laser-focused on taking practice exams before attempting the real thing?Based on the official CISSP Common Body of Knowledge (CBK) and the ten CBK domains, the practice exams in this book are designed to help students adjust to the pace, subject matter, and difficulty of the real CISSP exam. Geared towards anyone preparing for the exam, all tests include clear solutions to help you understand core CISSP concepts.If you plan on passing the CISSP certification exam, it's time to test your knowledge. It's time for CISSP Exam Prep Questions, Answers, and Explanations.Now packed with Over 1,000 realistic CISSP sample questions to help you pass the exam on your FIRST try.In this book: 1000+ detailed CISSP exam practice questions including 19 condensed CISSP mock exams that can be completed in one hour; 12 Targeted CISSP Domain Area tests, and detailed solution sets for all CISSP questions which include clear explanations and wording, CISSP Domain references, and reasoning based on the CISSP Common Body of Knowledge (CBK) materials (2015).*** Edition Updates ***The latest edition of CISSP Exam Prep Questions, Answers, & Explanations includes content improvements and edits which further assist students in preparing for their CISSP Exam. Improvements include:Content updated to reflect 2015 revisions to CISSP domainsQuestions updated for improved readability and applicability to examExplanations updated to further reinforce CISSP conceptsSpecific responses edited to reflect minor labeling corrections[CISSP is a registered certification mark and (ISC) is a service mark of the International Information Systems Security Certification Consortium, Inc.]Student Testimonials""It is the only product I have seen which utilizes the same question logic as the actual CISSP exam. If you utilize this training course you will have the ability to apply your already learned knowledge to the real exam very well. CISSP exam prep provides an important tool which helped me pass the CISSP on the first try."" Bobby Narasimham, CISSP - Information Security Engineer - Boston, MA""This exam prep delivers as promised. Top notch CISSP questions and explanations are provided that will dramatically expand your insight base. A comprehensive yet affordable augment for your exam preparation."" John Latawiec, CISSP""Great exam prep! Test questions are very similar to what I saw on the actual test. Passed CISSP on first attempt! Thanks."" Steve Toole, CISSP - Buffalo, NY""The thing that made this book stand out is that it helps to train the mind to tackle questions in a more systematic way... and are not just questions that need to be memorized. The questions are real life and I could not find any other set of questions similar."" Ali Jawad, CISSP - HRSmart INC, Lebanon""I passed the CISSP exam which and also have completed my endorsement as a CISSP. The question quality was very good. Keep up the good work. I would recommend this product to anyone who aspires for CISSP."" Ninad Varadkar, CISSP** Visit CISSPExamPractice.com for support and additional materials **",,Logic SSI,,2010,,,,Book
User Behavior and Change: File-Sharers and Copyright Laws,"Though the impact of file-sharing of copyrighted content has been discussed for over a decade, only in the past few years have countries begun to adopt legislation to criminalize this behavior. These laws impose penalties ranging from warnings and monetary fines to disconnecting Internet service. While their supporters are quick to point out trends showing the efficacy of these laws at reducing use of file-sharing sites, their analyses rely on brief snapshots of activity that cannot reveal long- and short-term trends.In this paper, we introduce an approach to model user behavior based on a hidden Markov model and apply it to analyze a two-year-long user-level trace of download activity of over 38k users from around the world. This approach allows us to quantify the true impact of file-sharing laws on user behavior, identifying behavioral trends otherwise difficult to identify. For instance, despite an initial reduction in activity in New Zealand when a three-strikes law took effect, after two months activity had returned to the level observed prior to the law being enacted. Given that punishment seems to, at best, result in short-term compliance, we suggest that incentives-based approaches may be more effective at changing user behavior.",,"Gavaldà-Miralles A,Otto JS,Bustamante FE,Amaral LA,Duch J,Guimerà R",,2014,319–324,10.1145/2674005.2675009,https://doi-org.proxy.bnl.lu/10.1145/2674005.2675009;http://dx.doi.org/10.1145/2674005.2675009,Conference Paper
Blind Signature and Ring Signature Schemes: Rehabilitation and Attack,"Blind signature and ring signature are two signature schemes with privacy concern. Zhang [Jianhong Zhang, Linkability analysis of some blind signature schemes, In International Conference on Computational Intelligence and Security 2006, IEEE, vol. 2, 2006, pp. 1367-1370, (Available at http://dx.doi.org.proxy.bnl.lu/10.1109/ICCIAS.2006.295283.)] analyzed the unlinkability of Zhang and Kim [Fangguo Zhang, Kwangjo Kim, ID-based blind signature and ring signature from pairings, in: Yuliang Zheng (Ed.), Advances in Cryptology - ASIACRYPT 2002, 8th International Conference on the Theory and Application of Cryptology and Information Security, Queenstown, New Zealand, December 1-5, 2002, Proceedings, Lecture Notes in Computer Science, vol. 2501, Springer, 2002, pp. 533-547], Huang et al. [Zhenjie Huang, Kefei Chen, Yumin Wang, Efficient identity-based signatures and blind signatures, in: Yvo Desmedt, Huaxiong Wang, Yi Mu, Yongqing Li (Eds.), Cryptology and Network Security, 4th International Conference, CANS 2005, Xiamen, China, December 14-16, 2005, Proceedings, Lecture Notes in Computer Science, vol. 3810, Springer, 2005, pp. 120-133] and Wu et al. [Qianhong Wu, Willy Susilo, Yi Mu, Fangguo Zhang, Efficient partially blind signatures with provable security, in: Osvaldo Gervasi, Marina L. Gavrilova, (Eds.), Computational Science and Its Applications - ICCSA 2007, International Conference, Kuala Lumpur, Malaysia, August 26-29, 2007. Proceedings. Part III, Lecture Notes in Computer Science, vol. 4707, Springer, 2007, pp. 1096-1105] and claimed that they are indeed linkable. On the other hand, Gamage et al. [Chandana Gamage, Ben Gras, Bruno Crispo, Andrew S. Tanenbaum, An identity-based ring signature scheme with enhanced privacy, Securecomm and Workshops 2006, IEEE, 2006, pp. 1-5, (Available at http://dx.doi.org.proxy.bnl.lu/10.1109/SECCOMW.2006.359554)] claimed that the scheme of Chow et al. [Sherman S.M. Chow, Siu-Ming Yiu, Lucas Chi Kwong Hui, Efficient identity based ring signature, in: John Ioannidis, Angelos D. Keromytis, Moti Yung (Eds.), Applied Cryptography and Network Security, Third International Conference, ACNS 2005, New York, NY, USA, June 7-10, 2005, Proceedings, Lecture Notes in Computer Science, vol. 3531, 2005, pp. 499-512] is vulnerable to key exposure attack. This paper shows that all these claims are incorrect. Furthermore, we show that the scheme proposed by Gamage et al. [Chandana Gamage, Ben Gras, Bruno Crispo, Andrew S. Tanenbaum, An identity-based ring signature scheme with enhanced privacy, Securecomm and Workshops 2006, IEEE, 2006, pp. 1-5, (Available at http://dx.doi.org.proxy.bnl.lu/10.1109/SECCOMW.2006.359554)] which aimed to provide enhanced privacy actually has privacy level reduced. We hope this work can pinpoint the standard one should use when analyzing the unlinkability of blind signatures and the anonymity of ring signatures.",,Chow SS,,2009,707–712,10.1016/j.csi.2008.09.002,https://doi-org.proxy.bnl.lu/10.1016/j.csi.2008.09.002;http://dx.doi.org/10.1016/j.csi.2008.09.002,Journal Article
"Nature as Diplomat : Scale, Agency, and Purpose in Environmental Peacebuilding","Environmental peacebuilding, the idea that the environment can be integratedin peacebuilding practices, is a growing trend in international relations and public diplomacy. Advocates for environmental peacebuilding believe that natural resources and environmental concerns hold positive potential for peace. Environmental peacebuilding is a fairly new idea, but it is growing in prominence, frequently connected to discourses of environmental security, conflict, and cooperation. United Nations programmes around the world, as well as a variety of other international and local organisations, are beginning to use and apply the rationale in many of their projects. However, the bulk of these initiatives encounter and reproduce a divorce between formal negotiations over international armed conflicts and interpersonal relationship- and community building efforts in broader settings. Additional gaps emerge between fields of practice, with distinct bodies of scholarship and practice emerging around different resources and conflict types. These siloes mean that research, policy, and practice miss out on possible benefits of sharing lessons and strategies. Building on participatory methods with practitioners in Morocco, Lebanon, and Kuwait, the study contributes to emerging scholarship by focusing on the environment's unique role in peacebuilding. Scale, agency, and purpose are employed as interrogative concepts to examine practitioner viewpoints on the nascent field. Primary research questions interrogate: 1. How are the relationships between the environment, peace, and conflict conceptualised in scholarship and practice? 2. How is the scope of environmental peacebuilding approached differently by theoretical scholarship and empirical practice? 3. How do marginalised stakeholders and non-traditional actors hold and enact power in environmental peacebuilding? 4. How are various objectives for environmental peacebuilding work contradictory, complementary, or mutually reinforcing? In addressing these themes, the study makes an original contribution through a definition for this emerging field that highlights its duality as study and practice, emphasises the relational agency of nature, and encourages attention to complex power dynamics yielding different outcomes for diverse stakeholders: Environmental peacebuilding is the critical study of the environment-conflict cooperation nexus and the applied practice of integrating the environment in action towards positive peace. Effective environmental peacebuilding recognises the potential of the environment to be a bystander to, object of, tool for, and actor in peacebuilding, engaging with nature as a partner for peace and minimising negative impacts on marginalised stakeholders by using environmental justice frameworks to inform the approach.",,Farnum B,,2020,,,,Ph.D. Thesis
Square-Free Decomposition in Finite Characteristic: An Application to Jordon Form Computation,"In ([GT]) has been addressed the problem of the computation of the square-free decomposition for univariate polynomials with coefficients in arbitrary fields. The complete square-free decomposition can be computed over arbitrary fields of finite characteristic solely assuming that the field satisfies the Condition P of Seidenberg ([Se]), which has been proven equivalent to the ability computing such decompositions (see also [MRR]). If we assume that the field is only an effective field (i.e. of a field K where there are constructive procedures for performing rational operations in K and for deciding whether or not two elements in K are equal), it is possible to obtain a weaker decomposition into powers of relatively prime factors, not necessarily square-free, but such that within each factor the roots have constant multiplicity. Although this is a partial decomposition, much useful information can be gathered from this result. As an application we present an algorithm to compute the Jordan form of a matrix over an arbitrary effective field. In particular we show how to handle problems of inseparability while splitting invariant factors and constructing symbolic Jordan form.The computation of normal forms of a matrix, in particular of the Jordan form, is a very important task and has many useful applications, so it has been widely studied for many years and many efficient algorithms, sequential and parallel ([O], [L], [Gi1], [Gi2], [Ol], [KKS], [RV]), are already available for its computation. There are already algorithms which compute the Jordan form of a matrix over general fields ([GD], [RV]), but they are based on dynamic evaluation ([D5]) and we want to avoid the use of such a scheme, that requires a special computational environment. Storjohann ([St]) has given a new algorithm for computing the rational canonical form which has a deterministic complexity of O(n3) but he does not compute the transition matrix with the same complexity. Steel's ([S]) algorithm for computing generalized Jordan form has a complexity O(n4) but requires factoring polynomials into irreducibles. Kaltofen et. al. ([KKS]) give fast parallel algorithms for canonical forms and make the observation that one could compute a symbolic Jordan form from a rational canonical form by splitting the invariant factors using gcd's and square-free decompositions. They require the computation of complete square-free decompositions and thus also require that K be a perfect field with the ability to compute pth roots. They also don't compute the transition matrix. Ozello ([O]) presents an algorithm for computing the rational canonical form which is deterministic with complexity O(n4), and leaves the question of faster probabilitic approaches for future work. Giesbrecht ([Gi2]) gives a probabilistic algorithm whose complexity is essentially the same as matrix multiplication but requires choosing n ""good"" random vectors simultaneously thus giving only a probability of 1/4 of making a successful choice.Our aim is to obtain a general sequential algorithm, of a complexity comparable with most of the existing algorithms, that works in the widest possible setting, without requiring particular computing resources and hence of easy and straightforward implementation. Because of our hypothesis, in general, our algorithm will produce a symbolic Jordan form ([K], [RV]), but the main difference with the other available algorithms based on dynamic evaluation is that our algorithm is a rational algorithm, since all the computations take place in the given field, except for the output and eventually the computation of the inverse of the transition matrix. To obtain all the information on the symbolic roots of the characteristic polynomial (multiplicities and recognition) we, at first, transform the given matrix A into a pseudo-rational form, i.e. a block diagonal matrix, similar to A, with companion matrices on the diagonal without requiring any kind of divisibility of the associated polynomials. Then we refine the factorization of the characteristic polynomial, given by the polynomials whose companion matrices are on the diagonal of the pseudo-rational form, using partial square-free decomposition and gcd computations, so that we can identify the same roots in different blocks and also we reduce, as much as possible without factorization, the degree of the defining polynomials for the eigenvalues.The pseudo-rational form is computed with a probabilistic algorithm of complexity O(n3) such that each independent random choice is verifiable with probability better than 1 - 1/n of success. We derive this probabilistic algorithm from one for the computation of the rational form, which has a complexity of O(n4), and is obtained via a straightforward analysis of the properties of the minimal polynomial that leads to a natural way to construct invariant subspaces.",,"Fortuna E,Gianni P",,1999,14–32,10.1145/500457.500460,https://doi-org.proxy.bnl.lu/10.1145/500457.500460;http://dx.doi.org/10.1145/500457.500460,Journal Article
Wave Information Studies (WIS) Pacific Regional Hindcast,"Coastal wave information is invaluable to coastal engineering projects, designs, maintenance of structures, erosion studies and storm climatology analyses. The mission of the Wave Information Studies (WIS) program in the Coastal and Hydraulics Laboratory (CHL), Engineer Research and Development Center (ERDC) in Vicksburg, MS, is to provide a database of wave information for all the United States coastlines. This information is useful in both civil and military applications. WIS populates this wave information database with wave parameter results from a wave hindcast, a process that uses input wind fields over a gridded area (grid points are identified as water with a specific depth or land) as input to a numerical wave hindcast computer code that models all the physical processes produced from ocean wind wave generation for past events. These wave hindcasts produce wave spectral energy information for every grid point and provide a continuous record of wave information. WIS hindcast results are compared with measurements from in-situ buoys for quality control. The WIS website currently contains at least 20 years of recent wave information for stations near the Atlantic, Gulf of Mexico, Pacific Basin, and Great Lakes coastlines. The Pacific Basin hindcast presented special challenges because of the vast area of the basin, the scarcity of measured information, the necessity for accurate propagation of swell energy from North Pacific and southern hemisphere storms over the Pacific basin, and accurate obstruction definition of small islands in the grid. Research and testing of several wave hindcast models resulted in the choice of the WAVEWATCH III (version 2.22 developed at NOAA/NCEP) numerical wave hindcast model for the Pacific Basin hindcast. This numerical hindcast model using state-of-the-art input wind fields from Oceanweather, Inc., produced 23 years (1981–2004) of wave hindcast information for the Pacific Basin. This information has proved to be invaluable for projects in the Hawaiian Islands and American Samoa. The next frontier for WIS is to produce wave information close to the Pacific mainland coastline. Regional Pacific wind fields for five years at a spacing of 0.25 degrees were secured from Oceanweather, Inc., to use with existing basin wind fields for the same time period at 0.5 degree spacing. The WIS CHL staff has collaborated with NOAA/NCEP to use the new multi-grid WAVEWATCH III numerical wave hindcast model to produce a wave hindcast that computes results from three nested grids in one run. This new technology developed and released by NOAA/NCEP late in 2007 allows WIS to run the basin 0.5 degree grid along with two regional grids (0.25 deg and 1/12 deg) covering the Pacific mainland west coast. Energy can move freely in and out of the boundaries of the three nested grids. This complex multi-grid WAVEWATCH III MPI parallel application would not be possible without the parallel computing resources available at the ERDC MSRC. This paper will show the initial results of the hindcast, comparisons of results with measured information, and will give an overview of the computing process for the Pacific Regional hindcasts.",,"Tracy B,Spindler D",,2008,299–304,10.1109/DoD.HPCMP.UGC.2008.84,https://doi-org.proxy.bnl.lu/10.1109/DoD.HPCMP.UGC.2008.84;http://dx.doi.org/10.1109/DoD.HPCMP.UGC.2008.84,Conference Paper
Use of Jordan Forms for Convection-Pressure Split Euler Solvers,,,"Garg NK,Maruthi NH,Rao SV,Sekhar M",,2020,,10.1016/j.jcp.2020.109258,https://doi-org.proxy.bnl.lu/10.1016/j.jcp.2020.109258;http://dx.doi.org/10.1016/j.jcp.2020.109258,Journal Article
A Framework for Regional Scale Quantitative Landslide Risk Analysis,"Landslides are among the most common and damaging natural hazards on earth. Policy makers, land use planners, and community members need to know not only when and where landslides are likely to occur (hazard) but also the consequences on the human-built environment (risk). Existing methods for quantitative landslide hazard analysis contain several critical limitations, including oversimplification of the diverse range of landslide phenomena, computationally expensive models which prohibit application on local or regional scales, dependence on costly and rare landslide inventories, and deterministic methods which do not account for uncertainty in environmental and human inputs. Landslide hazard is rarely translated into landslide risk due to a lack of quantitative data necessary to model landslide runout and estimate the vulnerability of people, buildings, and infrastructure.I address these challenges through the development and validation of a multimodal, regional scale framework for coseismic and precipitation-induced landslide risk analysis which implements physically-based models in a probabilistic system. I develop new tools to characterize landslide runout and the vulnerability of elements at risk.By applying the multimodal landslide risk analysis framework at two study regions, I explore questions at the intersection of natural hazards, human ecology, and policy-making. In the country of Lebanon, geologic risk has soared due to the influx of 1.5 million refugees fleeing the civil war in neighboring Syria. I examine the temporal and spatial patterns of landslide risk within Lebanon, noting the impact of refugee resettlement policies and illustrating the utility of real-time risk analyses for immediate refugee crisis response.Seattle, Washington, is one of the most landslide-prone urban areas in the United States. However, up until now, no estimates of landslide risk have been available for land use decision-making in the region. I perform a probabilistic landslide hazard and risk analysis for the city of Seattle, providing quantitative, spatially explicit estimates of landslide-related losses in future precipitation and earthquake events. By disaggregating the unique consequences of Seattle's various types of landslides, this work informs targeted risk mitigation strategies to protect individuals and the built environment from preventable landslide losses.",,"Pollock W,Duvall A,Massey C",,2020,,,,Ph.D. Thesis
Carnivalesque Politics and Popular Resistance : A Bakhtinian Reading of Contemporary Jordanian Political Humour,"This thesis examines contemporary Jordanian political humour in the context of the political history of Jordan and the 2011 Arab Spring revolutions. It applies Mikhail Bakhtin's mid-20th century theory of carnival and the carnivalesque (folk humour) as a framework for thinking about Jordanian politics and political humour in social media spaces following the Arab Spring. The Bakhtinian approach to humour has predominantly focused on the role of humour as a revolutionary impulse that aims to attack and expose the shortcomings of established political power, as well as to highlight public attitudes towards that power. The analysis undertaken here of Jordanian politics and political humour in Jordanian social media spaces after the Arab Spring found that Bakhtin's 'marketplace' is no longer the streets and material public spaces, but rather the social media spaces. The nature of the carnivals in social media spaces is in many ways just as carnivalesque as the 'marketplace' of Bakhtin's Medieval France, characterised by polyphony, the overturning of social hierarchies and the presence of dialogism (and monologism) and the grotesque. To more fully address the relevance – and some of the limitations – of application of Bakhtin's ideas about carnival to the Jordanian socio-political context after the Arab Spring, this thesis analyses key political cartoons, satirical articles, comedy sketches, politically satirical videos and internet memes produced by Jordanians from the start of the Arab Spring to early 2019. The analysis reveals five salient qualities of carnivalesque political humour in Jordanian social media spaces following the Arab Spring: praising the government (intentionally satirical), parodying the government, mocking the government, scatalogising the government and, finally, dethroning the government (the temporarily and metaphorically comic death of the government). These five qualities collectively and individually provide us with a useful framework to think of contemporary Jordanian political humour as a time and place for socio-political 'flattening' and cathartic hedonism (but not revolution) that have led to changes in Jordanian society where people are more willing to criticise and mock the government. Such humour has allowed ridicule of the government but not of the monarch and allowed individuals (carnival-goers) in social media spaces to cope with socio-economic inequalities and the absurdities of political power.",,Barahmeh Y,,2020,,,,Ph.D. Thesis
Computing Enclosures for the Matrix Mittag–Leffler Function,"We propose two algorithms for numerically calculating interval matrices including two-parameter matrix Mittag–Leffler (ML) functions. We first present an algorithm for computing enclosures for scalar ML functions. Then, the two proposed algorithms are developed by exploiting the scalar algorithm and verified block diagonalization. The first algorithm relies on a numerical spectral decomposition. The cost of this algorithm is only cubic plus that of the scalar algorithm if the second parameter is not too small. The second algorithm is based on a numerical Jordan decomposition, and can also be applied to defective matrices. The cost of this algorithm is quartic plus that of the scalar algorithm. A numerical experiment illustrates an application to a fractional differential equation.",,Miyajima S,,2021,,10.1007/s10915-021-01447-6,https://doi-org.proxy.bnl.lu/10.1007/s10915-021-01447-6;http://dx.doi.org/10.1007/s10915-021-01447-6,Journal Article
Some Issues on Intrusion Detection in Web Applications,"In the paper we present a new approach based on application of neural networks to detect SQL attacks. SQL attacks are those attacks that take the advantage of using SQL statements to be performed. The problem of detection of this class of attacks is transformed to time series prediction problem. SQL queries are used as a source of events in a protected environment. To differentiate between normal SQL queries and those sent by an attacker, we divide SQL statements into tokens and pass them to our detection system, which predicts the next token, taking into account previously seen tokens. In the learning phase tokens are passed to a recurrent neural network (RNN) trained by backpropagation through time (BPTT) algorithm. Then, two coefficients of the rule are evaluated. The rule is used to interpret RNN output. In the testing phase RNN with the rule is examined against attacks and legal data to find out how evaluated rule affects efficiency of detecting attacks. All experiments were conducted on Jordan network. Experimental results show the relationship between the rule and a length of SQL queries.",,"Skaruz J,Seredynski F",,2006,164–174,10.1007/978-3-540-69731-2_17,https://doi-org.proxy.bnl.lu/10.1007/978-3-540-69731-2_17;http://dx.doi.org/10.1007/978-3-540-69731-2_17,Conference Paper
Some Geometric Measures of Spheres in Banach Spaces,"In this paper, we first give relations between Pythagorean parameters and other well-known parameters: the coefficient of weak orthogonality, James and von Neumann-Jordan constants. Consequently, some known results in [J. Gao, On the generalized Pythagorean parameters and the applications in Banach spaces, Discrete Contin. Dyn. Syst. Ser. B, 8 (3) (2007) 557-567; J. Gao, On some geometric parameters in Banach spaces, J. Math. Anal. Appl. 344 (2007) 114-122; A. Jimenez-Melado, E. Llorens-Fuster, S. Saejung, The von Neumann-Jordan constant, weak orthogonality and normal structure in Banach spaces, Proc. Am. Math. Soc. 134 (2006) 355-364] are deduced and strengthened. Secondly we present several sufficient conditions for a Banach space and its dual to have normal structure. Finally, some open questions posed at the end of Gao (2007) are answered in the negative.",,"Gao J,Saejung S",,2009,102–107,10.1016/j.amc.2009.03.060,https://doi-org.proxy.bnl.lu/10.1016/j.amc.2009.03.060;http://dx.doi.org/10.1016/j.amc.2009.03.060,Journal Article
What’s with the Attitude? Policymaker Attitudes Towards Intelligence and National Security,"What determines why policymakers react to intelligence with elation or anger? There are countless examples of decision-makers blustering at their intelligence professionals, and there are equally as many instances of these same individuals later patting them on the back in triumph. This dissertation seeks to understand why. Specifically, it investigates what determines national security decision-makers' attitudes towards intelligence. This research applies attitude theory to our understanding of intelligence utilization and represents a departure from previous intelligence research that employs cognitive psychology. The Cognitive-Affective Theory of Intelligence (CATI) contends that policymakers develop predictable attitudes towards intelligence. These attitudes are the result of three variables: 1) the type of intelligence, 2) the specificity of the intelligence, and 3) the level of decision-maker commitment to policy.Chapters One through Three contain an introduction to the puzzle, literature review, theoretical development, and methodological review. Chapter One provides context for the dissertation; here I demonstrate what attitude theory adds to Intelligence Studies. Chapter Two delves into the existing literature. Current research does not specifically examine the determinants of policymaker attitudes towards intelligence, and psychological and situational variables provides further analytical leverage. In Chapter Three, there is an explication of variable measurement, case selection, and methodology.Chapter Four examines President Lyndon B. Johnson's unusually favorable attitude towards the intelligence preceding the Six-Day War. While uncommitted to his policy on Israel, he received confirmatory and specific intelligence that fit his preconceived notions about the situation that led to a favorable attitude. Not only does this case demonstrate the value-added of studying attitudes towards intelligence, but it also shows how demeanor can ultimately affect a policymaker's relationship with his intelligence producer. Chapter Five focuses on the intelligence President Reagan received leading up to, and throughout the Lebanon debacle in the early 1980s. While data is limited due to classification and other intelligence usage issues, it is clear that the President displays a very unfavorable attitude towards the intelligence that is specific and disconfirming when he was highly committed to his peace plan. As the evidence in this case is more circumstantial, I conduct textual analysis and counterfactual analysis to bolster my claims. Chapter Six analyzes President Carter's attitude towards the intelligence he received on the Korean Peninsula. Jimmy Carter ran on the importance of human rights and linked this policy to the withdrawal of American troops from South Korea as a result of Seoul's poor record on the subject. While highly committed to this plan, the President received a significant amount of unspecific and disconfirming intelligence that stood in stark contrast to his stated political and moral objectives. These situations resulted in his very unfavorable attitude towards the intelligence he received. Chapter Seven explores Eisenhower's attitude toward intelligence in five mini-cases. Each case addresses the remaining five of eight cells in the CATI's two-by-two matrices to provide evidence for the robustness of the theory. While focusing primarily on the correlational nature of the CATI, this chapter provides more breadth and additional support for the generalizability of my theory. In the conclusion I provide a summary of the CATI and its implications for the theory and practice of intelligence. Overall, attitudes are a significant but understudied concept in both International Relations and intelligence research. This study demonstrates the value-added by examining the social-psychological dimensions of intelligence usage and opens a new stream of research and inquiry in Intelligence Studies.",,"Halman AM,Williams P,Morgan F,Rovner J",,2020,,,,Ph.D. Thesis
Information Spreading Forensics via Sequential Dependent Snapshots,"Mining the characteristics of information spreading in networks is crucial in communication studies, network security management, epidemic investigations, etc. Previous works are restrictive because they mainly focused on the information source detection using either a single observation, or multiple but independent observations of the underlying network while assuming a homogeneous information spreading rate. We conduct a theoretical and experimental study on information spreading, and propose a new and novel estimation framework to estimate 1 information spreading rates, 2 start time of the information source, and 3 the location of information source by utilizing multiple sequential and dependent snapshots where information can spread at heterogeneous rates. Our framework generalizes the current state-of-the-art rumor centrality [1] and the union rumor centrality [2]. Furthermore, we allow heterogeneous information spreading rates at different branches of a network. Our framework provides conditional maximum likelihood estimators for the above three metrics and is more accurate than rumor centrality and Jordan center in both synthetic networks and real-world networks. Applying our framework to the Twitter’s retweet networks, we can accurately determine who made the initial tweet and at what time the tweet was sent. Furthermore, we also validate that the rates of information spreading are indeed heterogeneous among different parts of a retweet network.",,"Cai K,Xie H,Lui JC",,2018,478–491,10.1109/TNET.2018.2791412,https://doi-org.proxy.bnl.lu/10.1109/TNET.2018.2791412;http://dx.doi.org/10.1109/TNET.2018.2791412,Journal Article
Towards a Photogrammetry and Virtual Reality Based Heritage Information System: A Case Study of Shawbak Castle in Jordan,"The paper presents an interdisciplinary project which is the first step towards a 3D Geographical Information System (GIS) dedicated to Cultural Heritage with a specific focus application on the Castle of Shawbak, also known as the ""Crac de Montréal"" in Jordan.Current 3D GIS already provide support for urban models on a city scale. Our project however focuses on a building scale encompassing its atomic elements such as ashlars blocks, cement, stratigraphic unit and architectonic elements. At this scale we need a full 3D interface in order to manage accurate measurements and a mainly heterogeneous archaeological documentation.The project is conducted by four laboratories: the MAP-GAMSAU located in the school of Architecture of Marseilles, France in charge of the photogrammetric survey phase; The LSIS laboratory, France, will be in charge of the knowledge based approach; SimVis from The Department of Computer Science, University of Hull, UK, for the virtual reality aspect and of course the ""Dipartimento di Studi storici e Geografici"" from the University of Florence, Italy, in charge of the archaeological part.To manage these archaeological data the project is divided into three phases: The survey phase: using a knowledge based photogrammetric tool, Arpenteur (http://www.arpenteur.net), the photogrammetric campaign ensures a survey founded on archaeological knowledge and directly linked with a database built by archaeologists. The objective here is to link an already existing archaeological database with a photogrammetric tool in order to simplify the photogrammetric process. Our goal here is to offer to the archaeology community a new tool for surveying where technical photogrammetric aspects are more or less hidden from the surveyor. The second phase is the use of the knowledge base to ensure data consistency through a complex and multi-user survey phase. Based on data fusion coming from different sources, this phase will ensure a reversible way to merge several partial surveys exploiting the complementarities between sources, solving different existing conflicts and reducing the possible redundancies. This fusion process deals with archaeological information as well as spatial information. Finally we need a high resolution interface between the final geometry and the archaeological database. Virtual reality using interactive immersive devices and specially designed software tools is an efficient method for revisiting the site and for analysing, updating and revising knowledge.This project described in this paper is work in progress. After three photogrammetric campaigns in Jordan the first results are available on the project web site: http://www.shawbak.net",,"Drap P,Durand A,Nedir M,Seinturier J,Papini O,Boucault F,Chapman P,Viant W,Vannini G,Nuccioti M",,2006,67–74,,,Conference Paper
Unraveling Humanitarian Narratives : Syrian Gender Norms in Contestation,"The war in Syria has displaced millions, leading to many Syrians seeking refuge in countries like Jordan, where they access humanitarian assistance, including interventions designed to promote 'gender equality and women's empowerment'. Many international humanitarian agencies assert that Syrian refugees experience gendered changes during displacement. This includes increased early marriage and gender-based violence, transformed gender roles as women shoulder economic responsibilities, and altered mobility for women and girls. In these narratives, conclusions are often made about the role of forced migration in disrupting gender norms. Based on ethnographic research with Syrian women and men in Jordan, and interviews with humanitarian workers who work on gender issues, this thesis seeks to unravel dominant humanitarian narratives about gender norms and 'change'. It contributes knowledge to the study of gender norms and power, focusing on (im)mobility, family relationships and resistance to gender norms. These contributions are situated within three periods: before displacement, during the conflict in Syria and during displacement in Jordan. The findings challenge assumptions about gendered (im)mobility and vulnerability, offer insights on how older women exercise power over younger women, and contribute to thinking on how Syrian women and men resist prevailing gender norms in sometimes unexpected ways. These findings suggest that the humanitarian agency fixation on 'changes' in gender norms during displacement hides complexity and can overstate the role of displacement as an intervening force. It builds on existing literature on gender and forced migration, highlighting the complexities in finding a singular narrative around 'change' and pointing to the importance of understanding intersecting power hierarchies both in the lives and experiences of Syrians, and in the humanitarian structures that serve displaced populations.",,Lokot M,,2019,,,,Ph.D. Thesis
Generalized Jordan Sets in the Theory of Singular Partial Differential-Operator Equations,"We apply the generalized Jordan sets techniques to reduce partial differential-operator equations with the Fredholm operator in the main expression to regular problems. In addition this techniques has been exploited to prove a theorem of existence and uniqueness of a singular initial problem, as well as to construct the left and right regularizators of singular operators in Banach spaces and to construct fundamental operators in the theory of generalized solutions of singular equations.",,"Falaleev MV,Romanova OA,Sidorov NA",,2003,523–532,,,Conference Paper
A Polygonal Approximation for General 4-Contours Corresponding to Weakly Simple Curves,"The paper proposes a polygonal approximation for closed 4-paths obtained from standard contour following under 4-connectivity. Those 4-contours generate weakly simple polygons in the Euclidean plane; in general, they are not digital Jordan curves. The proposed polygon is easy to calculate and useful for shape representation and data reduction. The paper presents a linear algorithm for determining the ordered list of polygon vertices which are pixels determined from the point list of the given 4-contour. The algorithm relies on local extremity and semi-local shortest path requirements, and it uses only simple operations of integer calculus. The resulting polygon approximates what would be a generalization of the minimal perimeter polygon. The latter is known from the literature for 4-contours only for restricted cases such as digital Jordan curves related to simple grid continua, or certain types of subsets of rectangular mosaics, where it has been applied to perimeter estimation and for convexity and concavity analysis. We apply the polygon proposed to approximate 4-contours corresponding to weakly simple curves of known perimeter and report on experiments of perimeter estimation.",,"Villafuerte M,Wiederhold P",,2022,161–193,10.1007/s10851-021-01060-0,https://doi-org.proxy.bnl.lu/10.1007/s10851-021-01060-0;http://dx.doi.org/10.1007/s10851-021-01060-0,Journal Article
"Depth-Integrated, Non-Hydrostatic Model with Grid Nesting for Tsunami Generation, Propagation, and Runup","This dissertation describes the formulation, verification, and validation of a dispersive wave model with a shock-capturing scheme, and its implementation for basin-wide evolution and coastal runup of tsunamis using two-way nested computational grids. The depth-integrated formulation builds on the nonlinear shallow-water equations and utilizes a non-hydrostatic pressure term to describe weakly dispersive waves. The semi-implicit, finite difference solution captures flow discontinuities associated with bores or hydraulic jumps through a momentum conservation scheme, which also accounts for energy dissipation in the wave breaking process without the use of an empirical model. An upwind scheme extrapolates the free surface elevation instead of the flow depth to provide the flux in the momentum and continuity equations. This eliminates depth extrapolation errors and greatly improves the model stability, which is essential for computation of energetic breaking waves and runup. The vertical velocity term associated with non-hydrostatic pressure also describes tsunami generation and transfer of kinetic energy due to dynamic seafloor deformation. A depth-dependent Gaussian function smooths bathymetric features smaller than the water depth to improve convergence of the implicit, non-hydrostatic solution. A two-way grid-nesting scheme utilizes the Dirichlet condition of the non-hydrostatic pressure and both the velocity and surface elevation at the grid interface to ensure propagation of dispersive waves and discontinuities through computational grids of different resolution. The inter-grid boundary can adapt to topographic features to model wave transformation processes at optimal resolution and computational efficiency. The computed results show very good agreement with data from previous laboratory experiments for wave propagation, transformation, breaking, and runup over a wide range of conditions. The present model is applied to the 2009 Samoa Tsunami for demonstration and validation. These case studies confirm the validity and effectiveness of the present modeling approach for tsunami research and impact assessment. Since the numerical scheme to the momentum and continuity equations remains explicit, the implicit non-hydrostatic solution is directly applicable to existing nonlinear shallow-water models.",,Yamazaki Y,,2010,,,,Ph.D. Thesis
Batched Gauss-Jordan Elimination for Block-Jacobi Preconditioner Generation on GPUs,"In this paper, we design and evaluate a routine for the efficient generation of block-Jacobi preconditioners on graphics processing units (GPUs). Concretely, to exploit the architecture of the graphics accelerator, we develop a batched Gauss-Jordan elimination CUDA kernel for matrix inversion that embeds an implicit pivoting technique and handles the entire inversion process in the GPU registers. In addition, we integrate extraction and insertion CUDA kernels to rapidly set up the block-Jacobi preconditioner.Our experiments compare the performance of our implementation against a sequence of batched routines from the MAGMA library realizing the inversion via the LU factorization with partial pivoting. Furthermore, we evaluate the costs of different strategies for the block-Jacobi extraction and insertion steps, using a variety of sparse matrices from the SuiteSparse matrix collection. Finally, we assess the efficiency of the complete block-Jacobi preconditioner generation in the context of an iterative solver applied to a set of computational science problems, and quantify its benefits over a scalar Jacobi preconditioner.",,"Anzt H,Dongarra J,Flegar G,Quintana-Ortí ES",,2017,1–10,10.1145/3026937.3026940,https://doi-org.proxy.bnl.lu/10.1145/3026937.3026940;http://dx.doi.org/10.1145/3026937.3026940,Conference Paper
"Semimodularity and the Jordan–HöLder Theorem in Posets, with Applications to Partial Partitions","Lattice-theoretical generalizations of the Jordan–Hölder theorem of group theory give isomorphisms between finite maximal chains with same endpoints. The best one has been given by Czédli and Schmidt (after Grätzer and Nation), and it applies to semimodular lattices and gives a chain isomorphism by iterating up and down the perspectivity relation between intervals [x∧y,x] and [y,x∨y] where x covers x∧y and x∨y covers y. In this paper, we extend to arbitrary (and possibly infinite) posets the definitions of standard semimodularity and of the slightly weaker “Birkhoff condition”, following the approach of Ore (Bull Amer Math Soc 49(8):567–568, 1943). Instead of perspectivity, we associate tags to the covering relation, a more flexible approach. We study the finiteness and length constancy of maximal chains under both conditions and obtain Jordan–Hölder theorems. Our theory is easily applied to groups, to closure ranges of an arbitrary poset, and also to five new order relations on the set of partial partitions of a set (i.e. partitions of its subsets), which do not constitute lattices.",,Ronse C,,2019,255–280,10.1007/s10801-018-0852-0,https://doi-org.proxy.bnl.lu/10.1007/s10801-018-0852-0;http://dx.doi.org/10.1007/s10801-018-0852-0,Journal Article
The 2-Dimensional Rigidity of Certain Families of Graphs,"Laman's characterization of minimally rigid 2-dimensional generic frameworks gives a matroid structure on the edge set of the underlying graph, as was first pointed out and exploited by L. Lovász and Y. Yemini. Global rigidity has only recently been characterized by a combination of two results due to T. Jordán and the first named author, and R. Connelly, respectively. We use these characterizations to investigate how graph theoretic properties such as transitivity, connectivity and regularity influence (2-dimensional generic) rigidity and global rigidity and apply some of these results to reveal rigidity properties of random graphs. In particular, we characterize the globally rigid vertex transitive graphs, and show that a random d-regular graph is asymptotically almost surely globally rigid for all d ≥ 4. © 2006 Wiley Periodicals, Inc. J Graph Theory 54: 154–166, 2007",,"Jackson B,Servatius B,Servatius H",,2007,154–166,,,Journal Article
Sampling Strategies for Mining in Data-Scarce Domains,"Data mining has traditionally focused on the task of drawing inferences from large data sets. However, many scientific and engineering domains, such as fluid dynamics and aircraft design, are characterized by scarce data, due to the expense and complexity of associated experiments and simulations. In such data-scarce domains, it is advantageous to focus the data collection effort on only those regions deemed most important to support a particular data mining objective. This article describes a mechanism that interleaves bottom-up data mining, to uncover multilevel structures in spatial data, with top-down sampling, to clarify difficult decisions in the mining process. The mechanism exploits relevant physical properties, such as continuity, correspondence, and locality, in a unified framework. This leads to effective mining and sampling decisions that are explainable in terms of domain knowledge and data characteristics. This approach is demonstrated in two diverse applications-mining pockets in spatial data, and qualitative determination of Jordan forms of matrices.",,"Ramakrishnan N,Bailey-Kellogg C",,2002,31–43,10.1109/MCISE.2002.1014978,https://doi-org.proxy.bnl.lu/10.1109/MCISE.2002.1014978;http://dx.doi.org/10.1109/MCISE.2002.1014978,Journal Article
"Software Testing: High-Impact Strategies - What You Need to Know Definitions, Adoptions, Impact, Benefits, Maturity, Vendors","Software testing is an investigation conducted to provide stakeholders with information about the quality of the product or service under test. Software testing also provides an objective, independent view of the software to allow the business to appreciate and understand the risks of software implementation. Test techniques include, but are not limited to, the process of executing a program or application with the intent of finding software bugs (errors or other defects). This book is your ultimate resource for Software Testing. Here you will find the most up-to-date information, analysis, background and everything you need to know. In easy to read chapters, with extensive references and links to get you to know all there is to know about Software Testing right away, covering: Software testing, Acceptance testing, Ad hoc testing, Agile testing, All-pairs testing, American Software Testing Qualifications Board, Api Sanity Autotest, Association for Software Testing, Attack patterns, Augmented Reality-based testing, Australian and New Zealand Testing Board, Automated Testing Framework, Avalanche (dynamic analysis tool), Bebugging, Behavior Driven Development, Black-box testing, Block design, Boundary case, Boundary testing, Boundary-value analysis, Browser speed test, Bs 7925-1, Bs 7925-2, Bug bash, Build verification test, Ca/Eztest, Cause-effect graph, Centercode, Characterization test, Cloud testing, Code coverage, Code integrity, Codenomicon, Compatibility testing, Component-Based Usability Testing, Conference Room Pilot, Conformance testing, Core Security, Corner case, Daikon (system), Data-driven testing, Decision table, Decision-to-decision path, Design predicates, Development, testing, acceptance and production, DeviceAnywhere, Dry run (testing), Dynamic program analysis, Dynamic testing, Edge case, Endeavour Software Project Management,",,Roebuck K,,2011,,,,Book
Exploring the Computational Capabilities of Recurrent Neural Networks,"This dissertation addresses the issues surrounding the computational capabilities of recurrent neural networks. My results apply not only to simple recurrent networks, Jordan networks, and higher order recurrent networks, but many other networks implemented as input-parameterized iterated functions. The following reasons have driven my efforts to understand the computational capabilities of recurrent networks. First, the question of knowledge content arises whenever we attempt to understand how a given network produces its behavior. Second, knowing the range of what is computable by a recurrent network can guide us in their intelligent application. Finally, this knowledge may also help us to develop new training strategies that bias the network toward desirable solutions.Recurrent networks can clearly perform complex computation by simulating machine tapes and stacks. Such simulations are always the product of design. We know the functional decomposition of the network with respect to the computation it implements because the designer can identify the intended roles of each part. Unfortunately, weak learning methods, like back-propagation, which discover operable network weights cannot explain the internal functionality of the final product. Thus, missing functional specifications force us to externally determine the recurrent network's computation process by observing its structure and behavior.To this end, I identify three facets of recurrent networks that directly affect their emergent computational descriptions: system dynamics, input modulation of state dynamics, and output generation. System dynamics, the mapping of current state to next state, have been traditionally considered the source of complex behavior. Input modulation occurs as a finite set of input vectors induces behavior in the networks like that of iterated function systems. This selection creates state space representations for information processing states that display recursive structure. I show that the mechanism producing discrete outputs dramatically affects the apparent system complexity by imposing information processing regularities in the output stream strong enough to manipulate both complexion (number of states) and generative class of the observed computation.As for new training methods, I outline a method of network training called entrainment learning that offers a novel explanation of the transmission of grammatical behavior structures between agents.",,Kolen JF,,1995,,,,Ph.D. Thesis
Low Latency and Division Free GaussJordan Solver in Floating Point Arithmetic,"In many applications, the solution of a linear system is computed with Gaussian elimination followed by back-substitution, or GaussJordan elimination. The latter is intrinsically more parallel, enabling smaller computing latencies at the price of more complex hardware. However both methods require the division operator, which leads to a time-consuming resource in the critical path of the algorithms and impacts the global processings latency. Jordan was already aware of a division free algorithm. However, its implementation involves multiplications at each step and the size of the numbers rapidly becomes too big for an efficient implementation of large systems. In this work, we present a small modification to the division free algorithm in order to keep the size of the numbers in a reasonable range for standard floating point numbers. This is possible thanks to the special format of floating point numbers, which enables error free and hardware efficient divisions by powers of two. We also propose a parallel and pipelined architecture that best exploits the proposed algorithm, including partial pivoting. We specially focus on the global latency of the system as a function of its size, the latency of the floating point operators, and the number of operators that are available. Results demonstrate that current FPGAs can solve linear systems larger than hundred equations within ten microseconds. This represents a two order of magnitude improvement over previous implementations for relatively small systems. Low latency solvers are necessary for real time applications (simulation/control).The divider circuits used in most previous works induce long latencies.We propose a division free parallel architecture adapted to floating point arithmetic.We obtain two orders of magnitude gains compared to previous works.100-equation systems can be solved under 10 microseconds.",,David JP,,2017,185–193,10.1016/j.jpdc.2016.12.013,https://doi-org.proxy.bnl.lu/10.1016/j.jpdc.2016.12.013;http://dx.doi.org/10.1016/j.jpdc.2016.12.013,Journal Article
The Cyber Espionage Crimes in the Jordanian Law,"The current article aims to show the position of the Arab laws in general and the Jordanian legislator in particular against cyber spying. First, the article pointed out the concept and classification of state secrets. Then, it approached the extent of applicability of the traditional provisions on the espionage committed via electronic means. Furthermore, an analysis was done on Article 12 of the Jordanian Electronic Crimes Law by going deep into stating the elements of the crime set therein and arriving at the penalties that resulted from breaching it. Finally, the article concluded the possibility of applying Articles 15 and 16 of the Jordanian Law on the Protection of State Secrets and Documents (1970) criminalising obtaining state secrets and disclosing secrets obtained by the office without legal justification.",,"Issa HA,Alkhseilat A",,2022,111–123,10.1504/ijesdf.2022.121203,https://doi-org.proxy.bnl.lu/10.1504/ijesdf.2022.121203;http://dx.doi.org/10.1504/ijesdf.2022.121203,Journal Article
Efficient Architecture for Controlled Accurate Computation Using AVX,"Several applications have problems with the representation of the real numbers because of its drawbacks like the propagation and the accumulation of errors. These numbers have a fixed length format representation that provides a large dynamic range, but on the other hand it causes truncation of some parts of the numbers in case of a number that needs to be represented by a long stream of bits. Researchers suggested many solutions for these errors, one of these solutions is the Multi-Number (MN) system. MN system represents the real number as a vector of floating-point numbers with controlled accuracy by adjusting the length of the vector to accumulate the non-overlapping real number sequences. MN system main drawback is the MN computations that are iterative and time consuming, making it unsuitable for real time applications. In this work, the Single Instruction Multiple Data (SIMD) model supported in modern CPUs is exploited to accelerate the MN Computations. The basic arithmetic operation algorithms had been adjusted to make use of the SIMD architecture and support both single and double precision operations. The new architecture maintains the same accuracy of the original one, when was implemented for both single and double precision. Also, in this paper the normal Gaussian Jordan Elimination algorithm was proposed and used to get the inverse of the Hilbert Matrix, as an example of ill-conditioned matrices, instead of using iterative and time-consuming methods. The accuracy of the operations was proved by getting the inverse of the Hilbert Matrix and verify that the multiplication of the inverse and the original matrix producing the unity matrix. Hilbert Matrix inverse execution time was accelerated and achieved a speedup 3x, compared to the original NM operations. In addition to the previous, the accelerated MN system version was used to solve the polynomial regression problem.",,"Osman DM,Sobh MA,Bahaa-Eldin AM,Zaki AM",,2018,121–125,10.1145/3220267.3220292,https://doi-org.proxy.bnl.lu/10.1145/3220267.3220292;http://dx.doi.org/10.1145/3220267.3220292,Conference Paper
Demand for Higher Education and the Role of Starting Earnings Expectations : The Case of Final-Year Secondary Education Students in Jordan,"This thesis examines the determinants of student demand for higher education (HE) in Jordan with special attention devoted to the role of expected rates of return (ERRs) to HE. In the context of economic theory, mainly Human Capital Theory (HCT), earnings expectations lie at the heart of students' post-secondary education decisions. Therefore, the study is primarily based on final-year secondary school students' HE decisions and starting earnings expectations. The starting earnings expectations are critically analysed and used to construct short-cut ERRs to HE. Also, comparisons between expected starting earnings and actual public sector starting wage rates are carried out. The findings lend support to the economic explanations of demand and, in particular, the HCT predictions. Through applying logistic models, the study finds that ERRs influence students' post-secondary education decisions to enrol in HE positively. Students from low-income and large families are found to be less likely to consider continuing into HE. Consistent with the empirical literature, student academic ability is also reported to impact positively the likelihood to consider enrolment. Other variables such as parents' level of education and student's area of residence are proved not to be significantly associated with demand. Consumption value of education is greatly perceived, however, among both groups of the participants (i.e. those intending to undertake HE and those not). This indicates a weak prediction role of consumption motives in student demand for HE. Overall, the analysis also indicates a strong role of education in determining students' starting earnings expectations. Furthermore, the calculated ERRs show females to expect higher return from HE than males, a pattern matching with the most recent Jordanian study of rates of return (RORs) to education (Talafeh, 2003). However, students appear to be overoptimistic regarding starting earnings for both secondary education and HE. In this regard, the analysis suggests that students do not base their starting earnings expectations on the current actual wage rates, a finding to consider in future RORs and ERRs and their link with demand for HE studies in the context of Jordan. devoting more resources towards poor students, particularly those characterised with high-academic ability. Making students better-informed about HE and labour market return and conditions may also enhance the efficiency of individual decisions on HE and contribute to alleviating the mismatch between HE and the labour market in the country.",,Alhawarin I,,2006,,,,Ph.D. Thesis
Performance Prediction of Parallel Programs,"Two important paradigms for programming parallel computers are data parallelism and task parallelism. Data parallel programs have a single locus of control and task parallel programs have multiple locii of control. Multicomputers of today are essentially MIMD computers, and hence are more amenable to the task parallel programming paradigm. Consequently, a data parallel program must be translated to a task parallel program and then executed on multicomputers. In order to maintain a single locus of control, explicit global synchronizations must be inserted in the translated task parallel program. This results in performance degradation, since global synchronizations are very expensive. The first part of this dissertation presents techniques to reduce and replace such global synchronizations. In particular, an adaptive synchronization technique is presented which eliminates global synchronizations that cannot be eliminated the widely known inspector-executor strategy. Simulation studies are presented which estimate the potential performance improvement due to this technique. The work in the second and major part of this dissertation was motivated by the need for a fast parallel simulation engine of data parallel programs. Existing simulation engines are either sequential and slow, or parallel and not portable. We first present the design and implementation of DPSIM, a parallel simulation engine for data parallel programs written in UC. DPSIM exploits the deterministic nature of data parallel programs to optimize its underlying conservative simulation protocol. These optimizations are specific to the data parallel programming paradigm and hence not restricted to a particular architecture or programming language. Experimental results on the performance of DPSIM for a set of scientific applications including Gauss Jordan elimination and matrix multiplication show speedup factors as high as 11 when using 16 processors.We then present the design and implementation of MPISIM, a simulation library for the parallel simulation of MPI programs. MPISIM demonstrates concepts that can be used in the simulation of any general message passing language. It uses a conservative simulation protocol that integrates the conditional event protocol and the null message protocol. The null message protocol provides fast progress when good lookaheads are available. Lookaheads are extracted dynamically, and may be extracted at compile time as well, using program analysis. When good lookaheads are not available, the conditional event protocol provides fast progress. Deterministic sections of code are detected at runtime, and the same optimizations used for DPSIM are automatically applied resulting in extremely fast simulation. We present results for the validation and performance of MPISIM for a subset of the NAS Parallel Benchmarks (NPB2). As with DPSIM, excellent speedups are obtained on increasing the number of processors used by MPISIM.",,"Prakash S,Bagrodia R",,1996,,,,Ph.D. Thesis
Weakly Homogeneous Variational Inequalities and Solvability of Nonlinear Equations over Cones,,,"Gowda MS,Sossa D",,2019,149–171,10.1007/s10107-018-1263-7,https://doi-org.proxy.bnl.lu/10.1007/s10107-018-1263-7;http://dx.doi.org/10.1007/s10107-018-1263-7,Journal Article
Front Matter,"In the summer of 1956, John McCarthy organized the famous Dartmouth Conference which is now commonly viewed as the founding event for the field of Artificial Intelligence. During the last 50 years, AI has seen a tremendous development and is now a well-established scientific discipline all over the world. Also in Europe AI is in excellent shape, as witnessed by the large number of high quality submissions we received. About 600 papers and posters were registered for ECAI-06, out of which 550 were actually reviewed (501 paper submissions and 49 poster submissions). The program committee decided to accept 131 full papers, which amounts to an acceptance rate of 26.1%, and 75 posters (authors of full papers had the possibility to opt for acceptance as poster in case of rejectance as full paper).We received submissions from 43 different countries, and accepted papers from 25 countries. The following table shows the number of submissions and accepted papers per country, based on the contact author affiliation:Algeria 5 0Australia 19 9Austria 9 5Belgium 4 2Brazil 12 0Bulgaria 1 0Canada 13 4China 4 0Cyprus 1 0Czechia 4 0Egypt 1 0Estonia 2 0France 111 46Finland 3 2Germany 47 19Greece 15 4Hungary 1 0India 1 0Iran 5 1Ireland 14 9Israel 8 2Italy 83 36Japan 9 1Luxemburg 5 3Mexico 2 0Netherlands 26 12New Zealand 3 0Pakistan 1 0Poland 3 0Portugal 1 0Romania 4 1Russia 3 0Singapore 4 4Spain 32 5Slovenia 2 2Slovakia 3 3Sweden 5 5Switzerland 5 3Thailand 2 0Turkey 3 0UK 49 22USA 22 5Venezuela 1 1It is also interesting to look at the areas of the submitted and accepted papers/posters. We show both absolute numbers and percentage. The area information is based on the first two key words chosen by the authors:# submitted % # accepted %Case-Based Reasoning 10.5 1.9 0.5 0.2Cognitive Modelling 33.5 6.1 13 6.3Constraints & Search 54.5 10.0 26 12.6Distributed AI/Agents 107 19.6 36.5 17.7KR & Reasoning 141.5 25.9 55.5 26.9Machine Learning 83 15.2 29 14.1Model-Based Reasoning 32 5.9 8 3.9Natural Language 21.5 3.9 7.5 3.6Perception/Vision 6 1.1 2 1.0Planning and Scheduling 27 4.9 12 5.8Robotics 12.5 2.3 5 2.4PAIS 18 3.3 11 5.3In comparison with ECAI 2004, we see a strong increase in the relative number of submissions from Distributed AI/Agents and Cognitive Modelling. Knowledge Representation & Reasoning is traditionally strong in Europe and remains the biggest area of ECAI-06. One reason the figures for Case-Based Reasoning are rather low is that much of the high quality work in this area has found its way into prestigious applications and is thus represented under the heading of PAIS.The ECAI-06 best paper award, sponsored by Elsevier, goes to a machine learning paper:A Real Generalization of Discrete AdaBoost, by Richard Nock and Frank NielsenCongratulations! The best poster award, sponsored by IOS Press, will be decided after the poster sessions in Riva. The 10 best papers are invited to a fast track of the Artificial Intelligence Journal.A conference of the size of ECAI needs a lot of support from many people. At this point we would like to thank all those who helped to make ECAI-06 a tremendous success: the poster, workshop, PAIS and area chairs faced a heavy workload and they all did an excellent job. The PC members and additional reviewers came up with timely, high quality reviews and made the life of the PC chair as easy as it can be. Thanks also to Alex Nittka who ran the conference management software, to the PC chair's great relief. We also want to thank the many people involved in the local organization of the conference: there would be no conference without you.Our very special thanks go to a person who is no longer with us. Rob Milne, chair of ECAI-06 until he died of a heart attack close to the summit of Mount Everest on June 5th, 2005. He shaped this conference from the beginning, and we did our best to organize ECAI-06 in his spirit.June 2006, Gerhard Brewka, Silvia Coradeschi, Anna Perini, Paolo Traverso",,,,2006,i–xxvi,,,Conference Paper
The Horofunction Boundary and Denjoy-Wolff Type Theorems,"In this thesis we will study the horofunction boundary of metric spaces, in particular the Funk, reverse-Funk and Hilbert's metrics, and one of its applications, Denjoy-Wolff type theorems. In a Denjoy-Wolff type setting we will show that Beardon points are star points of the union of the ω-limit sets. We will also show that Beardon and Karlsson points are not unique in R2. In fact, we will show one can have a continuum of Karlsson points. We will establish two Denjoy-Wolff type theorem that confirm the Karlsson-Nussbaum conjecture for classes of non-expanding maps on Hilbert' metric spaces. For unital Euclidean Jordan algebras we will give a description of the intersection of closed horoballs with the boundary of the cone as the radius tends to minus infinity. We will expand on results by Walsh by establishing a general form for the Funk and reverse Funk horofunction boundaries of order-unit spaces. We will also give a full classification of the horofunctions of JH-algebras and the horofunctions and Busemann points of the spin factors for the Funk, reverse Funk and Hilbert metrics. Finally we will show that there exists a reverse-Funk non-Busemann horofunction for the cone of positive bounded self-adjoint operators on an infinite dimensional Hilbert space, the infinite dimensional spin factors and a space in which the pure states are weak* closed, answering a question raised by Walsh in [65].",,Claassens F,,2020,,,,Ph.D. Thesis
Recognizing Weak Embeddings of Graphs,"We present an efficient algorithm for a problem in the interface between clustering and graph embeddings. An embedding φ : G → M of a graph G into a 2-manifold M maps the vertices in V(G) to distinct points and the edges in E(G) to interior-disjoint Jordan arcs between the corresponding vertices. In applications in clustering, cartography, and visualization, nearby vertices and edges are often bundled to the same point or overlapping arcs due to data compression or low resolution. This raises the computational problem of deciding whether a given map φ : G → M comes from an embedding. A map φ : G → M is a weak embedding if it can be perturbed into an embedding ψ ε : G → M with ‖ φ − ψ ε ‖ 0, where ‖.‖ is the unform norm.A polynomial-time algorithm for recognizing weak embeddings has recently been found by Fulek and Kynčl. It reduces the problem to solving a system of linear equations over Z2. It runs in O(n2ω)≤ O(n4.75) time, where ω ∈ [2,2.373) is the matrix multiplication exponent and n is the number of vertices and edges of G. We improve the running time to O(n log n). Our algorithm is also conceptually simpler: We perform a sequence of local operations that gradually “untangles” the image φ(G) into an embedding ψ(G) or reports that φ is not a weak embedding. It combines local constraints on the orientation of subgraphs directly, thereby eliminating the need for solving large systems of linear equations.",,"Akitaya HA,Fulek R,Tóth CD",,2019,,10.1145/3344549,https://doi-org.proxy.bnl.lu/10.1145/3344549;http://dx.doi.org/10.1145/3344549,Journal Article
"A New Method in Volcano-Morphology to Investigate the Tectonic Constraints on the Volcanism, Case Study of Harrat Al Sham Volcanic Field, Arabia Plate: The Interest of GIS and Relational Database","The volcanic activity of Arabian plate offers an attractive example of intraplate volcanism constrained by a complex tectonic setting. Harrat Ash Shaam volcanic field (HASV) is a basaltic province, extends widely at Arabian plate (over 50 000 km2), covers south of Syria, northeast of Jordan, north of Saudi Arabia, and contains hundreds of well- preserved monogenic volcanic cones.Our method aims to identify those cones volcanic, calculate its morphological parameters (heights, slopes, surfaces, volumes...etc.), and study their correlation. The farther intention of this study is to investigate the consequence of the tectonic events on the volcanic activity by testing the relations between the volcano-morphological parameters and the structure of the lithosphere (basement and moho surfaces).The realization of these objectives is problematic and even impossible by using the traditional exploration methods. Thanks to the computing technology which offers a vast opportunity to develop a new digital method in order to achieve such complex geospatial study.We suggest the integration and the exploitation of following primary data (so called to be distinguished from the extracted data): 1) the remote sensing (RS) technology provides several satellite scenes (landsat7, ETM+), 2) digital elevation models (SRTM data), 3) Digital earth application (by Cornell University), latter source offers internet based open access system of interpretation of geophysical data as estimation of crustal thickness and depth to Moho, 4) several geological maps of the study area, 5) K-Ar ages.The main challenges of this work are: a) building up geospatial database, geo-referencing and managing the primary data of different sources at same platform, b) treating the primary data to extract advanced levels of data, c) sustain the principle of ""data independence"" in order to protect the root of data from confusion after the process of extraction, which is imperative during the results interpretation (e.g. it must be specified if the volume parameter has been mathematically extracted based on the surface or independently).This paper spotlights the role of GIS in our geospatial investigation, and explains techniques employed in our method: 1) processing the satellite images with the purpose of distinguish the cones volcanic at HASV (more than 800 cones have been detected), 2) using spatial analysis tools in order to obtain automatically the periphery and the virtual bases of each cones volcanic from the digital elevation model, and 3) using GIS platform as tools to manage multi sources- data efficiently.In addition to, we highlight the integration between GIS and Relational Database, that we redesigned the geospatial database and restructured the data tables according to the new defined objects (i.g. the volcanic cones). In addition we have been used SQL widely in order to extract the advanced data levels (e.g. the morphological parameters), and to develop an approach in order to protect data roots.On other hand, the facility of exporting and importing the geospatial data tables between GIS and Microsoft application permitted us to reproduce easily and quickly the results of our research in form of maps (by using mapping tools based on geo-statistical methods of GIS). Consequently we propose an example of data streaming characterized by a geo-database system growing up in two directions (rows and columns). Our suggestion based on an observation; that in this case study, the geospatial database grows throughout a ""looping process"" between input and output data (i.g. the output data turn out to be a new batch of input data inserts again to the system). The probability of this looping process seems to be high and relates to the number of the scientific questions under investigation using the geospatial database system.Our prospective of the future challenge is to enhance the update links between the primary and the extracted data levels with the aim of bridging the gap created by high rate data streaming, as well as we proposed to develop the approach of ""data independence"".This case study shows that the role of GIS and Relational Database in geospatial investigation is not only indispensable as tools to manage and manipulate multi sources data, but is also significant to answer composite scientific questions.That our results demonstrate different spatial density zones contain clusters of hundreds of cones volcanic, in addition to considerable spatial differentiations of the morphological parameters. Consequently, we linked those variations to the lithosphere structures.On other hand our results obtained by applying digital based-method to HASV are in agreement with our dating results (K-Ar ages) of the volcanic activity, and the petrologic evidences at the northern parts of HASV. That enhanced our understanding of the volcano-tectonic evolution of the study area and played key role to suggest a spatial-temporal model characterizes the tectonic alteration between the compression and the extension during the last 26 Ma.We expect the mentioned harmony between results (i.e. direct method and digital based- methods) could reflect an important advantage in comparing the volcanism of HASV and other terrestrial planets.Moreover our results suggest a huge tectonic constrain on the volumes of the volcanic activity; that is fundamental to manage the volcanic risks at active analog zones. Therefore, we concluded that the digital based- method is helpful for monitoring and predication the natural hazards at regional scales.In the light of the previous facts building up the digital earth and developing its management tools are the future duties.",,"Kwatli MA,Gillot PY",,2010,,10.1145/1823854.1823908,https://doi-org.proxy.bnl.lu/10.1145/1823854.1823908;http://dx.doi.org/10.1145/1823854.1823908,Conference Paper
Recognizing Weak Embeddings of Graphs,"We present an efficient algorithm for a problem in the interface between clustering and graph embeddings. An embedding ϕ : G → M of a graph G into a 2-manifold M maps the vertices in V(G) to distinct points and the edges in E(G) to interior-disjoint Jordan arcs between the corresponding vertices. In applications in clustering, cartography, and visualization, nearby vertices and edges are often bundled to a common node or arc, due to data compression or low resolution. This raises the computational problem of deciding whether a given map ϕ : G → M comes from an embedding. A map ϕ : G → M is a weak embedding if it can be perturbed into an embedding φε : G → M with [Equation] for every ε > 0.A polynomial-time algorithm for recognizing weak embeddings was recently found by Fulek and Kynčl [14], which reduces to solving a system of linear equations over Z2. It runs in O(n2ω) ≤ O(n4.75) time, where ω ≈ 2.373 is the matrix multiplication exponent and n is the number of vertices and edges of G. We improve the running time to O(n log n). Our algorithm is also conceptually simpler than [14]: We perform a sequence of local operations that gradually ""untangles"" the image ψ(G) into an embedding φ(G), or reports that φ is not a weak embedding. It generalizes a recent technique developed for the case that G is a cycle and the embedding is a simple polygon [1], and combines local constraints on the orientation of subgraphs directly, thereby eliminating the need for solving large systems of linear equations.",,"Akitaya HA,Fulek R,Tóth CD",,2018,274–292,,,Conference Paper
FMSE '05: Proceedings of the 2005 ACM Workshop on Formal Methods in Security Engineering,"It is our great pleasure to welcome you to the Third ACM Workshop on Formal Methods in Security Engineering (FMSE 2005) held in conjunction with the 12th ACM Conference on Computer and Communications Security.The purpose of FMSE is to bring together researchers and practitioners from both the security and the software engineering communities, from academia and industry, who are working on applying formal methods to designing and validating large-scale security-critical systems. The scope of the workshop covers security and formal-methods related aspects of security specification techniques, formal trust models, combination of formal techniques with semi-formal techniques like UML, formal analyses of specific security properties relevant to software development, security-preserving composition and refinement of processes, faithful abstractions of cryptographic primitives and protocols in process abstractions, integration of formal security specifications, as well as refinement and validation techniques in development methods and tools.The paper selection process was very competitive this year. The call for papers attracted 22 submissions from Australia, Asia, Europe, New Zealand, and the United States. The program committee accepted 8 papers for presentation at the workshop, which means that many high-quality papers had to be rejected. In addition, the program includes invited talks by Virgil Gligor and Andrew Myers.",,,,2005,,,,Book
ACSC '08: Proceedings of the Thirty-First Australasian Conference on Computer Science - Volume 74,"The Australasian Computer Science Conference (ACSC) series is an annual forum, bringing together research sub-disciplines in Computer Science. The meeting allows academics and researchers to discuss research topics as well as progress in the field, and policies to stimulate its growth. This volume contains papers presented at the Thirty First ACSC in Wollongong, NSW, Australia. ACSC 2008 is part of the Australasian Computer Science Week which ran from Jan 22nd to 25th, 2008.The ACSC 2008 call for papers solicited contributions in all areas of computer science research. This years conference received submissions from Australia, New Zealand, China, France, India, Iran, Jamaica, Jordon, Malaysia, Pakistan, South Africa, Turkey, UK, and Taiwan. The topics addressed by the submitted papers illustrate the broadness of the discipline. The authors categorised their submissions into one or more of the following topics:- Algorithms (9 papers)- Artificial Intelligence (7 papers)- Communications and Networks (4 papers)- Computer Architecture (2 paper)- Computer Vision (4 papers)- Databases (5 papers)- Distributed Systems (6 papers)- E-Commerce (4 papers)- Formal Methods (6 papers)- Graphics (6 papers)- High Performance Computing (7 papers)- Human-Computer Interaction (8 papers)- Mobile Computing (6 papers)- Multimedia (1 paper)- Object Oriented Systems (3 papers)- Ontologies (1 paper)- Operating Systems (5 papers)- Programming Languages (4 papers)- Robotics (1 paper)- Scientific Computing (5 papers)- Security and Trusted Systems (5 papers)- Simulation (6 papers)- Software Engineering (5 papers)- Speech (1 paper)- Theory (3 papers)- Visualization (6 papers)- Web Services (3 papers)The programme committee consisted of 28 highly regarded academics from around the globe, including Australia, Brazil, Canada, Japan, New Zealand, Singapore and USA. All papers were sent to at least three programme committee members for review and every effort was made to obtain at least three reviews. Of the 47 papers submitted, 16 were selected for presentation at the conference.The programme committee invited Professor Joxan Jaffar, to give a keynote on Constraint Logic Programming for Program Analysis. Professor Jaffar has recently completed a stint as Dean of the School of Computing from 2001-2007 at the National University of Singapore. His interests are in programming languages and applications, with emphasis on the logic and constraint programming paradigms. Amongst his main contributions are the principles of constraint logic programming, and the widely-used CLP(R) system. The committee also invited Dr Benjamin Burton and Associate Professor Ewan Tempero to give invited talks. Dr Burtons talk was entitled Informatics Olympiads:Challenges in Programming and Algorithm Design. Associate Professor Temperos talk is entitled On Measuring Java Software.",,,,2008,,,,Book
"BCC '19: Proceedings of the Third ACM Workshop on Blockchains, Cryptocurrencies and Contracts","It is our pleasure to welcome you to The Third ACM Workshop on Blockchains, Cryptocurrencies, and Contracts (BCC'19), held in conjunction with The 14th ACM Asia Conference on Computer and Communications Security (AsiaCCS'19) during July 7-12, 2019, at Auckland, New Zealand. Blockchain is an emerging technology currently being used for a variety of applications such as cryptocurrencies and other financial transactions, smart properties, credential management, Internet-of-Things, supply chain management and many more. The theme of this workshop is to understand the foundations of blockchain technology and the design of new blockchain applications using smart contracts.The call for papers attracted 8 submissions from around the world. The program committee accepted 2 full papers and 1 work in progress based on their overall quality and novelty. We are grateful to the authors of all the papers for submitting their work to this workshop. We thank the members of the program committee, external reviewers and the web chairs for their valuable service. We hope these proceedings will serve as a valuable reference for researchers and practitioners in the field of blockchain technologies.",,,,2019,,,,Book
Front Matter,"In the great digital era, we are witnessing many rapid scientific and technological developments in human-centered, seamless computing environments, interfaces, devices, and systems with applications ranging from business and communication to entertainment and learning. These developments are collectively best characterized as Active Media Technology (AMT), a new area of intelligent information technology and computer science that emphasizes the proactive, seamless roles of interfaces and systems as well as new media in all aspects of digital life. An AMT based computer system offers services that enable the rapid design, implementation, deploying and support of customized solutions.The first International Conference on Active Media Technology (AMT01) was held in Hong Kong in 2001, the second International Conference on Active Media Technology (AMT03) was held in Chongqing, China in May 29--31 of 2004, and the third International Conference on Active Media Technology (AMT05) was held in Kagawa, Japan in May 2005. The 4th International Conference on Active Media Technology (AMT06) follows the success of AMT01, AMT03 and AMT05.AMT06 is the leading International Conference focusing on Active Media Technology. It aims to bring together researchers from diverse areas, such as Web intelligence, data mining, intelligent agents, smart information use, networking and intelligent interface. It also encourages collaborative research in these areas to provide best services for enabling the rapid design, implementation, deploying and support of customized solutions.The conference includes the following topics:• Active Computer Systems and Intelligent Interfaces• Adaptive Web Systems and Information Foraging Agents• Web mining, Wisdom Web and Web Intelligence• E-Commerce and Web Services• Data Mining, Ontology Mining and Data Reasoning• Network, Mobile and Wireless Security• Entertainment and Social Applications of Active Media• Agent-Based Software Engineering and Multi-Agent Systems• Digital City and Digital Interactivity• Machine Learning and Human-Centred Robotics• Multi-Modal Processing, Detection, Recognition, and Expression Analysis• Personalized, Pervasive, and Ubiquitous Systems and their Interfaces• Smart Digital Media• Evaluation of Active Media and AMT Based SystemsAMT06 is sponsored by the IEEE Systems, Man, and Cybernetics Society and Queensland University of Technology. It attracted 123 submissions from 19 countries and regions: Algeria, Australia, China, Canada, England, Finland, France, Hong Kong, India, Japan, Korea, New Zealand, Pakistan, Poland, Republic of Korea, Taiwan, United Arab Emirates, United Kingdom, and United States of America. The review process was rigorous. Each paper was reviewed by two reviewers at least, and most of them reviewed by three reviewers.The Program Committee accepted 39 regular papers (the approximate acceptance rate is 32%), 33 short papers (the approximate acceptance rate is 39%) and 9 industry/demonstration papers.We would like to thank the members of Program Committee and Organization Committee and reviewers who contributed to the success of this conference.Yuefeng Li, Mark Looi and Ning Zhong, 17 March 2006",,,,2006,i–xvi,,,Conference Paper
A Survey of Web Engineering Practice in Small Jordanian Web Development Firms,"Web based-applications became increasingly important to all aspects of life, and most of these web applications projects run over time and budget. So there is a need to encourage practitioners to adopt best practices so as to improve the quality of the processes in use, and therefore achieve targets relating to time, budget and quality. The web development industry worldwide is dominated by a myriad of small firms. This presents a challenge in terms of determining the current practices of industry participants, and in devising improvement initiatives which are feasible for small firms. Currently, the level of adoption of best practice among web developers is unknown. To help improve the web industry, it is necessary to determine the current status of use of practices and techniques. The objective of this research is to understand the extent of web development practices currently in use. To achieve this objective, a survey of web engineering practice in small Jordanian firms was conducted. A detailed description of the survey procedures is provided in this paper. The results showed that there is a weakness in applying web engineering practices in small Jordanian web development firm.",,"El Sheikh A,Tarawneh H",,2007,481–490,10.1145/1287624.1287692,https://doi-org.proxy.bnl.lu/10.1145/1287624.1287692;http://dx.doi.org/10.1145/1287624.1287692,Conference Paper
A Survey of Web Engineering Practice in Small Jordanian Web Development Firms,"Web based-applications became increasingly important to all aspects of life, and most of these web applications projects run over time and budget. So there is a need to encourage practitioners to adopt best practices so as to improve the quality of the processes in use, and therefore achieve targets relating to time, budget and quality. The web development industry worldwide is dominated by a myriad of small firms. This presents a challenge in terms of determining the current practices of industry participants, and in devising improvement initiatives which are feasible for small firms. Currently, the level of adoption of best practice among web developers is unknown. To help improve the web industry, it is necessary to determine the current status of use of practices and techniques. The objective of this research is to understand the extent of web development practices currently in use. To achieve this objective, a survey of web engineering practice in small Jordanian firms was conducted. A detailed description of the survey procedures is provided in this paper. The results showed that there is a weakness in applying web engineering practices in small Jordanian web development firms.",,"El Sheikh A,Tarawneh H",,2007,481–489,10.1145/1295014.1295023,https://doi-org.proxy.bnl.lu/10.1145/1295014.1295023;http://dx.doi.org/10.1145/1295014.1295023,Conference Paper
The State-of-the-Art Technology of Currency Identification: A Comparative Study,"The security issue of currency has attracted awareness from the public. De-spite the development of applying various anti-counterfeit methods on currency notes, cheaters are able to produce illegal copies and circulate them in market without being detected. By reviewing related work in currency security, the focus of this paper is on conducting a comparative study of feature extraction and classification algorithms of currency notes authentication. We extract various computational features from the dataset consisting of US dollar USD, Chinese Yuan CNY and New Zealand Dollar NZD and apply the classification algorithms to currency identification. Our contributions are to find and implement various algorithms from the existing literatures and choose the best approaches for use.",,"Yan W,Wang G,Wu X",,2017,58–72,10.4018/IJDCF.2017070106,https://doi-org.proxy.bnl.lu/10.4018/IJDCF.2017070106;http://dx.doi.org/10.4018/IJDCF.2017070106,Journal Article
Shape-Constrained Estimation for Modern Statistical Problems,"Shape constraints encode a relatively weak form of prior information specifying the direction of certain relationships in an unknown signal. Classical examples include estimation of a convex function or a monotone density. Shape constraints are often strong enough to dramatically reduce statistical complexity while still yielding flexible, nonparametric estimators. This thesis brings shape constraints to bear on several recent research areas in statistics—distribution-free inference, high-dimensional covariance estimation, empirical Bayes, and multiple hypothesis testing.Chapter 2 discusses my joint work with Professor Aditya Guntuboyina and Professor Jim Pitman on distribution-free properties of isotonic regression. In this work, we establish a distributional result for the components of the isotonic least squares estimator using its characterization as the derivative of the greatest convex minorant of a random walk. Provided the walk has exchangeable increments, we prove that the slopes of the greatest convex minorant are distributed as order statistics of the running averages. This result implies an exact formula for the squared error risk of least squares in homoscedastic isotonic regression when the true sequence is constant that holds for every exchangeable error distribution.Chapter 3 discusses my joint work with Professor Aditya Guntuboyina and Professor Michael I. Jordan on sign-constrained precision matrix estimation. We investigate the problem of high-dimensional covariance estimation under the constraint that the partial correlations are nonnegative. The sign constraints dramatically simplify estimation: the Gaussian maximum likelihood estimator is well defined with only two observations regardless of the number of variables. We analyze its performance in the setting where the dimension may be much larger than the sample size. We establish that the estimator is both high-dimensionally consistent and minimax optimal in the symmetrized Stein loss. We also prove a negative result which shows that the sign-constraints can introduce substantial bias for estimating the top eigenvalue of the covariance matrix.Chapter 4 discusses my joint work with Professor Aditya Guntuboyina and Professor Bodhisattva Sen on nonparametric empirical Bayes with multivariate, heteroscedastic Gaussian errors. Multivariate, heteroscedastic errors complicate statistical inference in many large- scale denoising problems. Empirical Bayes is attractive in such settings, but standard para- metric approaches rest on assumptions about the form of the prior distribution which can be hard to justify and which introduce unnecessary tuning parameters. We extend the nonparametric maximum likelihood estimator (NPMLE) for Gaussian location mixture densities to allow for multivariate, heteroscedastic errors. NPMLEs estimate an arbitrary prior by solving an infinite-dimensional, convex optimization problem; we show that this convex optimization problem can be tractably approximated by a finite-dimensional version. We introduce a dual mixture density whose modes contain the atoms of every NPMLE, and we leverage the dual both to establish non-uniqueness in multivariate settings as well as to construct explicit bounds on the support of the NPMLE.The empirical Bayes posterior means based on an NPMLE have low regret, meaning they closely target the oracle posterior means one would compute with the true prior in hand. We prove an oracle inequality implying that the empirical Bayes estimator performs at nearly the optimal level (up to logarithmic factors) for denoising without prior knowledge. We provide finite-sample bounds on the average Hellinger accuracy of an NPMLE for estimating the marginal densities of the observations. We also demonstrate the adaptive and nearly- optimal properties of NPMLEs for deconvolution. We apply the method to two astronomy datasets, constructing a fully data-driven color-magnitude diagram of 1.4 million stars in the Milky Way and investigating the distribution of chemical abundance ratios for 27 thousand stars in the red clump.Chapter 5 discusses my joint work with Daniel Xiang and Professor William Fithian on finite-sample control of the maximum local false discovery rate in multiple hypothesis testing. Despite the popularity of the false discovery rate (FDR) as an error control metric for large- scale multiple testing, its close Bayesian counterpart the local false discovery rate (lfdr), defined as the posterior probability that a particular null hypothesis is false, is a more directly relevant standard for justifying and interpreting individual rejections. However, the lfdr is difficult to work with in small samples, as the prior distribution is typically unknown. We propose a simple multiple testing procedure and prove that it controls the expectation of the maximum lfdr across all rejections; equivalently, it controls the probability that the rejection with the largest p-value is a false discovery. Our method operates without knowledge of the prior, assuming only that the p-value density is uniform under the null and decreasing under the alternative. We also show that our method asymptotically implements the oracle Bayes procedure for a weighted classification risk, optimally trading off between false positives and false negatives. We derive the limiting distribution of the attained maximum lfdr over the rejections, and the limiting empirical Bayes regret relative to the oracle procedure.",,"Soloff J,J Wainwright M",,2022,,,,Ph.D. Thesis
Privacy Laws and Privacy by Design Schemes for the Internet of Things: A Developer’s Perspective,"Internet of Things applications have the potential to derive sensitive information about individuals. Therefore, developers must exercise due diligence to make sure that data are managed according to the privacy regulations and data protection laws. However, doing so can be a difficult and challenging task. Recent research has revealed that developers typically face difficulties when complying with regulations. One key reason is that, at times, regulations are vague and could be challenging to extract and enact such legal requirements. In this article, we have conducted a systematic analysis of the privacy and data protection laws that are used across different continents, namely (i) General Data Protection Regulations, (ii) the Personal Information Protection and Electronic Documents Act, (iii) the California Consumer Privacy Act, (iv) Australian Privacy Principles, and (v) New Zealand’s Privacy Act 1993. Then, we used framework analysis method to attain a comprehensive view of different privacy and data protection laws and highlighted the disparities to assist developers in adhering to the regulations across different regions, along with creating a Combined Privacy Law Framework (CPLF). After that, the key principles and individuals’ rights of the CPLF were mapped with Privacy by Design (PbD) schemes (e.g., privacy principles, strategies, guidelines, and patterns) developed previously by different researchers to investigate the gaps in existing schemes. Subsequently, we have demonstrated how to apply and map privacy patterns into IoT architectures at the design stage and have also highlighted the complexity of doing such mapping. Finally, we have identified the major challenges that should be addressed and potential research directions to take the burden off software developers when applying privacy-preserving techniques that comply with privacy and data protection laws. We have released a companion technical report [3] that comprises all definitions, detailed steps on how we developed the CPLF, and detailed mappings between CPLF and PbD schemes.",,"Aljeraisy A,Barati M,Rana O,Perera C",,2021,,10.1145/3450965,https://doi-org.proxy.bnl.lu/10.1145/3450965;http://dx.doi.org/10.1145/3450965,Journal Article
Understanding Sustainable Mobility: The Potential of Electric Vehicles,"Rising awareness of the environmental impacts of dominant mobility practices lead to the development of the sustainable mobility paradigm. This paradigm advocates three features of a mobility system: 1. A reduced need to travel, 2. Modal shift towards more sustainable options, and 3. Reduced vehicle kilometres travelled. In this paper, two sets of data are presented to explore the potential of electric vehicles to contribute to a more sustainable mobility system. First, data from an international Delphi of transport experts shows how a sustainable future can be characterised by different features: efficient internal combustion engine vehicles, electric vehicles, and reduced personal car ownership. Thus electric vehicles are presented as both an opportunity and a threat in relation to sustainable mobility. A second body of empirical material is drawn from interviews with electric vehicle owners, and discusses the drivers and barriers to ownership. Interestingly, participants suggest changing mobility practices associated with electric vehicle ownership, evidenced by decreasing kilometres travelled. The paper concludes by suggesting that there may be potential for electric vehicles to contribute to a sustainable mobility future through modified mobility practices and renewable energy sources in New Zealand.",,"Scott M,Hopkins D,Stephenson J",,2014,27–30,10.1109/MDM.2014.63,https://doi-org.proxy.bnl.lu/10.1109/MDM.2014.63;http://dx.doi.org/10.1109/MDM.2014.63,Conference Paper
"Information Assurance and Security Education and Training: 8th IFIP WG 11.8 World Conference on Information Security Education, WISE 8, Auckland, New","This book constitutes the refereed proceedings of the 8th IFIP WG 11.8 World Conference on Security Education, WISE 8, held in Auckland, New Zealand, in July 2013. It also includes papers from WISE 6, held in Bento Gonalves, Brazil, in July 2009 and WISE 7, held in Lucerne, Switzerland in June 2011. The 34 revised papers presented were carefully reviewed and selected for inclusion in this volume. They represent a cross section of applicable research as well as case studies in security education.",,"Dodge RC,Futcher L",,2013,,,,Book
CrowdEstimator: Approximating Crowd Sizes with Multi-Modal Data for Internet-of-Things Services,"Crowd mobility has been paid attention for the Internet-of-things (IoT) applications. This paper addresses the crowd estimation problem and builds an IoT service to share the crowd estimation results across different systems. The crowd estimation problem is to approximate the crowd size in a targeted area using the observed information (e.g., Wi-Fi data). This paper exploits Wi-Fi probe request packets (""Wi-Fi probes"" for short) broadcasted by mobile devices to solve this problem. However, using only Wi-Fi probes to estimate the crowd size may result in inaccurate results due to various environmental uncertainties which may lead to crowd overestimation or underestimation. Moreover, the ground-truth is unavailable because the coverage of Wi-Fi signals is time-varying and invisible. This paper introduces auxiliary sensors, stereoscopic cameras, to collect the near ground-truth at a specified calibration choke point. Two calibration algorithms are proposed to solve the crowd estimation problem. The key idea is to calibrate the Wi-Fi-only crowd estimation based on the correlations between the two types of data modalities. Then, to share the calibrated results across systems required by different stakeholders, our system is integrated with the FIWARE-based IoT platform. To verify the proposed system, we have launched an indoor pilot study in the Wellington Railway Station and an outdoor pilot study in the Christchurch Re:START Mall in New Zealand. The large-scale pilot studies show that stereoscopic cameras can reach minimum accuracy of 85% and high precision detection for providing the near ground-truth. The proposed calibration algorithms reduce estimation errors by 43.68% on average compared to the Wi-Fi-only approach.",,"Wu FJ,Solmaz G",,2018,337–349,10.1145/3210240.3210320,https://doi-org.proxy.bnl.lu/10.1145/3210240.3210320;http://dx.doi.org/10.1145/3210240.3210320,Conference Paper
Hamlet on the Holodeck: The Future of Narrative in Cyberspace,"From the Book: Introduction to Hamlet on the Holodeck by Janet H. Murray All media as extensions of ourselves serve to provide new transforming vision and awareness. --Marshall McLuhan Our various improvements not only mark a diminution of the function improved upon...but they also work to dissolve some of the fundamental authority of the human itself. We are experiencing the gradual but steady erosion...of the species itself. --Sven Birkerts The birth of a new medium of communication is both exhilarating and frightening. Any industrial technology that dramatically extends our capabilities also makes us uneasy by challenging our concept of humanity itself. (Are people meant to move across the ocean like the fish Are peoples words supposed to be transmitted by dead paper or cold wires ) The boat, car, and airplane are seemingly magical extensions of our arms and legs; the telephone extends our voices; and the book extends our memory. The computer of the 1990s, with its ability to transport us to virtual places, to connect us with people at the other end of the earth, and to retrieve vast quantities of information, combines aspects of all of these. And as if that were not amazing enough, it also runs our warplanes and plays a masterly game of chess. It is not surprising, then, that half of the people I know seem to look upon the computer as an omnipotent, playful genie while the other half see it as Frankensteins monster. To me -- a teacher of humanities for the past twenty five years in the world-class electronic toy shop of MIT, a Victorian scholar, and educational software designer -- the computer looks more each day like the movie camera of the 1890s: a truly revolutionary invention humankind is just on the verge of putting to use as a spellbinding storyteller. It is somewhat surprising to me to find myself on the optimistic side of this pervasive new cultural device. When I first trained as a systems programmer, as an IBM employee in the late 1960s, I was only biding my time and saving up money for graduate school in English literature. I found the clean logic of computer programming satisfying, and I enjoyed deciphering the mysterious 0s and 1s of a core dump to reveal what the machine was up to when a program crashed (as they so often did). But there seemed no deeper purpose in this work than there had been in the intriguing geometry proofs I had enjoyed in high school and then promptly forgotten. For me at the age of twenty, the only activity worthy of serious human effort was reading novels. Only once during my time at IBM did I catch a glimpse of a more inspired use of the computer. Although we did not use the terms at that time, the corporate world was clearly divided into suits and hackers. The suits were running the company (better than they would in later years), but the hackers were running the secret playground within the company, the world of the machines. Computer systems in those days were mammoth arrays of cumbersome appliances kept isolated in ice-cold rooms. The tape drives alone (the equivalent of todays floppy disks) were the size of refrigerators. The noisiest component was the card reader, which jangled and thumped like a subway train full of bowling balls as it processed stacks of the punch cards that were the main form of human to computer communication in that era. Dealing with this machine was an unpleasant daily necessity. But one day the icy, clamorous card printer room was turned into a whimsical cabaret: a clever young hacker had created a set of punch cards that worked like a player piano roll and caused the card reader to chug out a recognizable version of the Marine Corps Hymn: bam-bam-THUMP bam-THUMP team THUMP THUMP THUMP. All day long, programmers sneaked away from the work to hear this thunderously awful but mesmerizing concert. The data it was processing was of course meaningless, but the song was a work of true virtuosity. When programming was fun, it was a lot like that performance. Creating a successful machine code program made me feel as if I had communicated with some recalcitrant, stupid beast deep inside the refrigerator cabinet and taught it a new little tune. But my real work was waiting for me somewhere else, in the form of a long, thoughtful walk down an endless shelf of books. When I was offered a fellowship for graduate school at Harvard, I did not hesitate to accept it. My IBM manager wanted me to take just a temporary leave of absence. He gave me an article about how computers were being used to study English literature (someone was putting all of WAR AND PEACE -- to me the pinnacle of human wisdom -- into electronic form in order to count the number of words in each of Tolstoys sentences). The article ended by referring to literature as mans greatest output. I told my manager to write me up as a permanent resignation. I began reading my own way down that long shelf of books. I agreed with D. H. Lawrence that the novel was the one bright book of life, the measure of all things, although I much preferred the work of Jane Austen and the Victorians. My favorite critic was Northrop Frye, who combined detailed analyses of the structure of stories with a profound appreciation of their mythic power. Reading Frye it was possible to believe that the formal beauty of literary art is an expression of its deeper truth. Yet the more I read, the clearer it became that stories did not tell the whole truth about the world. As I researched the lives of women in the Victorian era, I (like others of my generation) was struck by the fact that much of what I was learning had been left out of the great novels of the era. Although my faith in the deeper powers of literature was unshaken, I learned from the feminist movement that some truths about the world are beyond the reach of a particular art form at a particular moment in time. Before the novel could tell the stories of women who did not wind up either happily married or dead, it would have to change in form as well as in content. For the stories I wanted to hear, I looked in other formats, in feminist magazines and maverick novels. I compiled an anthology documenting the experiences of Victorian women -- prostitutes, medical students, circles of women friends -- who had not found a place in classic fiction. But the anthology format was as limiting in its way as the marriage plot. Frustrated by the constraint of producing a single book with a single pattern of organization, I filled my collection with multiple cross-references, encouraging the reader to jump from one topic to another. I simply wanted the reader to understand Mary Taylors exhilaration in opening a dry goods store in New Zealand in the context of her friendship with Charlotte Bronte as well as in relation to the range of Victorian opinion on womens work. I did not think of this cross-referencing as hypertext because I had not yet heard the term. Though I had been teaching at MIT since 1971, I was not drawn to computers again until the early 1980s. While I had been exploring social history and raising my two children, literature and academic feminism itself seemed somehow to have fallen into the hands of the suits. The new theoreticians no longer saw the novel as the bright book of life but as an infinite regression of words about words about words. Joining in this conversation involved learning a discourse as arcane as machine code, and even farther from experience. Truth and beauty were nowhere in sight. But at the same time that literary theorists were denouncing meaning as something to be Reconstructed into absurdity, theorists of learning methods were embracing meaning as the key to successful pedagogy. One conference paper after another celebrated the fact that students wrote better papers and learned to speak foreign languages with greater fluency when they actually had something they wanted to communicate to one another. The new research in cognition and sociolinguistics seemed to define what those processes of communication entailed. Thinking about teaching was much more satisfying to my earnest Victorian temperament than thinking about literary criticism. And the more I thought about it, the more I wondered if these practical and process-oriented methodologies could be transported into the world of the computer. I was at that time the humanities faculty member in the Experimental Study Group (ESG), in which conventional courses were taught in an individualized manner. ESG attracted some of the most creative and self-directed students at MIT, many of whom were also ingenious computer hackers. They wrote their papers on-line, explored imaginary dungeons filled with trolls, exchanged wisecracks with computer-based imaginary characters, and engaged in a perpetual telnet tour of the globe by playfully breaking into other peoples computers. They believed the particular programming language they were reaming was both the brains own secret code and a magical method for creating anything on earth out of ordinary English words. They saw themselves as wizards and alchemists, and the computer as a land of enchantment. MIT was paradise for these hackers, who were largely engaged in navigating through an elaborate fictional universe. With such students as my guides, I got myself a network account and renewed my acquaintance with the digital world. I had left computing in the age of punch cards and came back to it in the age of video display terminals and microcomputers. Nevertheless, educational computing had not advanced very far beyond the days of quantifying Tolstoys output. The computer was mostly seen as a drudge, a workhorse for word frequency analysis and for drill and practice teaching. However, to my students and my MIT colleagues, it was clearly something considerably more nimble. Seymour Papert had developed the LOGO programming language that allowed children to learn mathematical concepts by choreographing the actions of magic sprites that raced across the screen. A follower of Piaget, Papert believed that computers are tools for thinking and should be used to create microworlds where inquisitive students can learn through a process of exploration and discovery. Nicholas Negropontes group had created a suite of dazzling demonstration projects (the seed work for the Media Lab) that included a movie map of Aspen, Colorado, and a movie manual for car repair. The combination of text, video, and navigable space suggested that a computer-based microworld need not be mathematical but could be shaped as a dynamic fictional universe with characters and events. My interest in creating narrative microworlds coincided with the interests of foreign language teachers in creating immersive learning environments. Together we designed multimedia applications for learning Spanish and French, which motivated students by giving them a role in an unfolding story and allowing them to move through authentically photographed environments as if they were on a visit to Bogota or Paris. These projects and others that I have worked on in the past fifteen years -- including a Shakespeare archive and a film art digital textbook -- as well as many kindred efforts pursued by others elsewhere, have confirmed my view of the computer as offering a thrilling extension of human powers. I say this despite the often agonizing uncertainties of software development and the continual frustration over the gap between what designers want the hardware and software to do and what they actually support. For my experience in humanities computing has convinced me that some kinds of knowledge can be better represented in digital formats than they have been in print. The knowledge of a foreign language, for instance, can be better conveyed with examples from multiple speakers in authentic environments than with lists of words on a page. The dramatic power of Hamlets soliloquies is better illustrated by multiple performance examples in juxtaposition with the text than by the printed version alone. Discussions of film art make more sense when they are grounded by excerpted scenes from the movies being discussed. Computers can present the text, images, and moving pictures valued by humanistic disciplines with a new precision of reference; they can show us all the different ways a French person says hello in a single day or all the passages Zeffirelli chose to leave out of his production of ROMEO AND JULIET. By giving us greater control over different kinds of information, they invite us to tackle more complex tasks and to ask new kinds of questions. Although the computer is often accused of fragmenting information and overwhelming us, I believe this view is a function of its current undomesticated state. The more we cultivate it as a tool for serious inquiry, the more it will offer itself as both an analytical and a synthetic medium. My experiences in educational computing have also offered me evidence of how frightening the new technologies can be. Several years ago I was invited to talk with the committee that was then overseeing the production of a variorum of Shakespeare, a set of editions of individual plays with extensive annotation covering all known textual variants as well as notes on the significant critical commentary on the plays. The variorum format dates back to the nineteenth century and was still an endearingly Victorian endeavor. The pace of production was glacial, with many of the editors collecting their notes in stacks of index cards and filling hundreds of shoe boxes with twenty years worth of investigation before publishing. The night before my appearance I was invited for a drink in a high-rise New York hotel room by two of the most computer-friendly committee members. I had already received an irate note from another member of the committee, and my hosts, an English woman and a Southern man, were anxious to prepare me for the kind of opposition that others might offer. My scrupulously polite colleagues displayed a courtly commitment to moving the variorum into the digital age while avoiding offending anyone. With the naivete of someone who had spent much of the past twenty years in the company of engineers, I told them that my remarks would be limited to the obvious practicalities of their work. Clearly, the pages of a book were a poor match for the task at hand. Often the text of the play took up only a single line at the top, with the rest of the page covered with footnotes in several numbering schemes, many of which were condensed to cryptic abbreviations that conveyed no information to the uninitiated. Thus, commentary for a line of text often appeared a dozen pages away from the line it referred to. The effort of compiling a variorum edition was clearly heroic, but the arbitrary limitation of the printed page was a disservice to the depth of information and expertise involved. At this point in my preview of the next days presentation, my genteel hostess started to shake in her chair. I love the book! she cried. If you are coming to talk against the book tomorrow, I will throw you out the window. And though she was considerably smaller than I, she looked quite prepared to do so. Why should the prospect of a scholarly CD-ROM bring a mild-mannered Shakespearean editor to such paroxysms of violence To my mind it was because she could not separate the activities of research from the particular form they had historically assumed. Her love of books (which I share) momentarily blinded her to the true object of reverence: the creation of a superb reference work. Her reaction was a sign that the new technologies are extending our powers faster than we can assimilate the change. Even when we are already engaged in enterprises that cry out for the help of a computer, many of us still see the machine as a threat rather than an ally. We cling to books as if we believed that coherent human thought is only possible on bound, numbered pages. I am not among those who are eager for the death of the book, as I hope the present volume demonstrates. Nor do I fear it as an imminent event. The computer is not the enemy of the book. It is the child of print culture, a result of the five centuries of organized, collective inquiry and invention that the printing press made possible. My work as a software developer has made me painfully aware of the primitive nature of the current digital medium and of the difficulty of predicting what it can or cannot do in any given time scheme. Nevertheless, I find myself longing for a computer-based literary form even more passionately than I have longed for computer-based educational environments, in part because my heart belongs to the hackers. I am hooked on the charm of making the dumb machines sing. Since 1992 I have been teaching a course on how to write electronic fiction. My students include freshmen, writing majors, and Media Lab graduate students. Some of them are virtuoso programmers. Some of them do not program at all. All of them are drawn to the medium because they want to write stories that cannot be told in other ways. These stories cover every range and style, from oral histories to adventure tales, from the exploits of comic book heroes to domestic dramas. The only constant in the course is that every year what is written is even more inventive than what was written the year before. Every year my students arrive in class feeling more at home with electronic environments and are more prepared to elicit something with the tone of a human voice out of the silent circuitry of the machine. As I watch the yearly growth in ingenuity among my students, I find myself anticipating a new kind of storyteller, one who is half hacker, half bard. The spirit of the hacker is one of the great creative wellsprings of our time, causing the inanimate circuits to sing with ever more individualized and quirky voices; the spirit of the bard is eternal and irreplaceable, telling us what we are doing here and what we mean to one another. I am drawn to imagining a cyberdrama of the future by the same fascination that draws me to the Victorian novel. I see glimmers of a medium that is capacious and broadly expressive, a medium capable of capturing both the hairbreadth movements of individual human consciousness and the colossal crosscurrents of global society. Just as the computer promises to reshape knowledge in ways that sometimes complement and sometimes supersede the work of the book and the lecture hall, so too does it promise to reshape the spectrum of narrative expression, not by replacing the novel or the movie but by continuing their timeless bardic work within another framework. This book is an effort to imagine what kinds of pleasures such a cyberliterature will bring us and what sorts of stories it might tell. I believe that we are living through a historic transition, as important to literary history as it is to the history of information processing. My sixteen-year-old son will no doubt look back upon the moment at which we (finally!) got our home computer hooked up to the World Wide Web with the same delight with which my father recalled plucking voices out of the air with his home-made crystal radio set. My paternal grandmother, who started life in a Russian shtetl, jumped in terror when she heard that disembodied speech, thinking it must be a dybbuk or ghost. Yet only a few decades later, I sat in my crib, as my mother fondly reports, calmly enraptured by the voice of Arthur Godfrey. Today, my husband collects tapes of old Bob and Ray programs, which we listen to on long car trips, savoring the intimacy of what now seems like a touchingly low-tech format. Those of us who have spent our lives in love with books may always approach the computer with something of my grandmothers terror before the crystal radio, but our children are already at home with the joystick, mouse, and keyboard. They take the powerful sensory presence and participatory formats of digital media for granted. They are impatient to see what is next. This book is an attempt to imagine a future digital medium, shaped by the hackers spirit and the enduring power of the imagination and worthy of the rapture our children are bringing to it. Excerpt from Hamlet on the Holodeck copyright © 1997 by Janet Horowitz Murray. Reprinted by permission of the publisher, Simon & Schuster.",,Murray JH,,1997,,,,Book
Hamlet on the Holodeck: The Future of Narrative in Cyberspace,"From the Book: Introduction to Hamlet on the Holodeck by Janet H. Murray All media as extensions of ourselves serve to provide new transforming vision and awareness. --Marshall McLuhan Our various improvements not only mark a diminution of the function improved upon...but they also work to dissolve some of the fundamental authority of the human itself. We are experiencing the gradual but steady erosion...of the species itself. --Sven Birkerts The birth of a new medium of communication is both exhilarating and frightening. Any industrial technology that dramatically extends our capabilities also makes us uneasy by challenging our concept of humanity itself. (Are people meant to move across the ocean like the fish Are peoples words supposed to be transmitted by dead paper or cold wires ) The boat, car, and airplane are seemingly magical extensions of our arms and legs; the telephone extends our voices; and the book extends our memory. The computer of the 1990s, with its ability to transport us to virtual places, to connect us with people at the other end of the earth, and to retrieve vast quantities of information, combines aspects of all of these. And as if that were not amazing enough, it also runs our warplanes and plays a masterly game of chess. It is not surprising, then, that half of the people I know seem to look upon the computer as an omnipotent, playful genie while the other half see it as Frankensteins monster. To me -- a teacher of humanities for the past twenty five years in the world-class electronic toy shop of MIT, a Victorian scholar, and educational software designer -- the computer looks more each day like the movie camera of the 1890s: a truly revolutionary invention humankind is just on the verge of putting to use as a spellbinding storyteller. It is somewhat surprising to me to find myself on the optimistic side of this pervasive new cultural device. When I first trained as a systems programmer, as an IBM employee in the late 1960s, I was only biding my time and saving up money for graduate school in English literature. I found the clean logic of computer programming satisfying, and I enjoyed deciphering the mysterious 0s and 1s of a core dump to reveal what the machine was up to when a program crashed (as they so often did). But there seemed no deeper purpose in this work than there had been in the intriguing geometry proofs I had enjoyed in high school and then promptly forgotten. For me at the age of twenty, the only activity worthy of serious human effort was reading novels. Only once during my time at IBM did I catch a glimpse of a more inspired use of the computer. Although we did not use the terms at that time, the corporate world was clearly divided into suits and hackers. The suits were running the company (better than they would in later years), but the hackers were running the secret playground within the company, the world of the machines. Computer systems in those days were mammoth arrays of cumbersome appliances kept isolated in ice-cold rooms. The tape drives alone (the equivalent of todays floppy disks) were the size of refrigerators. The noisiest component was the card reader, which jangled and thumped like a subway train full of bowling balls as it processed stacks of the punch cards that were the main form of human to computer communication in that era. Dealing with this machine was an unpleasant daily necessity. But one day the icy, clamorous card printer room was turned into a whimsical cabaret: a clever young hacker had created a set of punch cards that worked like a player piano roll and caused the card reader to chug out a recognizable version of the Marine Corps Hymn: bam-bam-THUMP bam-THUMP team THUMP THUMP THUMP. All day long, programmers sneaked away from the work to hear this thunderously awful but mesmerizing concert. The data it was processing was of course meaningless, but the song was a work of true virtuosity. When programming was fun, it was a lot like that performance. Creating a successful machine code program made me feel as if I had communicated with some recalcitrant, stupid beast deep inside the refrigerator cabinet and taught it a new little tune. But my real work was waiting for me somewhere else, in the form of a long, thoughtful walk down an endless shelf of books. When I was offered a fellowship for graduate school at Harvard, I did not hesitate to accept it. My IBM manager wanted me to take just a temporary leave of absence. He gave me an article about how computers were being used to study English literature (someone was putting all of WAR AND PEACE -- to me the pinnacle of human wisdom -- into electronic form in order to count the number of words in each of Tolstoys sentences). The article ended by referring to literature as mans greatest output. I told my manager to write me up as a permanent resignation. I began reading my own way down that long shelf of books. I agreed with D. H. Lawrence that the novel was the one bright book of life, the measure of all things, although I much preferred the work of Jane Austen and the Victorians. My favorite critic was Northrop Frye, who combined detailed analyses of the structure of stories with a profound appreciation of their mythic power. Reading Frye it was possible to believe that the formal beauty of literary art is an expression of its deeper truth. Yet the more I read, the clearer it became that stories did not tell the whole truth about the world. As I researched the lives of women in the Victorian era, I (like others of my generation) was struck by the fact that much of what I was learning had been left out of the great novels of the era. Although my faith in the deeper powers of literature was unshaken, I learned from the feminist movement that some truths about the world are beyond the reach of a particular art form at a particular moment in time. Before the novel could tell the stories of women who did not wind up either happily married or dead, it would have to change in form as well as in content. For the stories I wanted to hear, I looked in other formats, in feminist magazines and maverick novels. I compiled an anthology documenting the experiences of Victorian women -- prostitutes, medical students, circles of women friends -- who had not found a place in classic fiction. But the anthology format was as limiting in its way as the marriage plot. Frustrated by the constraint of producing a single book with a single pattern of organization, I filled my collection with multiple cross-references, encouraging the reader to jump from one topic to another. I simply wanted the reader to understand Mary Taylors exhilaration in opening a dry goods store in New Zealand in the context of her friendship with Charlotte Bronte as well as in relation to the range of Victorian opinion on womens work. I did not think of this cross-referencing as hypertext because I had not yet heard the term. Though I had been teaching at MIT since 1971, I was not drawn to computers again until the early 1980s. While I had been exploring social history and raising my two children, literature and academic feminism itself seemed somehow to have fallen into the hands of the suits. The new theoreticians no longer saw the novel as the bright book of life but as an infinite regression of words about words about words. Joining in this conversation involved learning a discourse as arcane as machine code, and even farther from experience. Truth and beauty were nowhere in sight. But at the same time that literary theorists were denouncing meaning as something to be Reconstructed into absurdity, theorists of learning methods were embracing meaning as the key to successful pedagogy. One conference paper after another celebrated the fact that students wrote better papers and learned to speak foreign languages with greater fluency when they actually had something they wanted to communicate to one another. The new research in cognition and sociolinguistics seemed to define what those processes of communication entailed. Thinking about teaching was much more satisfying to my earnest Victorian temperament than thinking about literary criticism. And the more I thought about it, the more I wondered if these practical and process-oriented methodologies could be transported into the world of the computer. I was at that time the humanities faculty member in the Experimental Study Group (ESG), in which conventional courses were taught in an individualized manner. ESG attracted some of the most creative and self-directed students at MIT, many of whom were also ingenious computer hackers. They wrote their papers on-line, explored imaginary dungeons filled with trolls, exchanged wisecracks with computer-based imaginary characters, and engaged in a perpetual telnet tour of the globe by playfully breaking into other peoples computers. They believed the particular programming language they were reaming was both the brains own secret code and a magical method for creating anything on earth out of ordinary English words. They saw themselves as wizards and alchemists, and the computer as a land of enchantment. MIT was paradise for these hackers, who were largely engaged in navigating through an elaborate fictional universe. With such students as my guides, I got myself a network account and renewed my acquaintance with the digital world. I had left computing in the age of punch cards and came back to it in the age of video display terminals and microcomputers. Nevertheless, educational computing had not advanced very far beyond the days of quantifying Tolstoys output. The computer was mostly seen as a drudge, a workhorse for word frequency analysis and for drill and practice teaching. However, to my students and my MIT colleagues, it was clearly something considerably more nimble. Seymour Papert had developed the LOGO programming language that allowed children to learn mathematical concepts by choreographing the actions of magic sprites that raced across the screen. A follower of Piaget, Papert believed that computers are tools for thinking and should be used to create microworlds where inquisitive students can learn through a process of exploration and discovery. Nicholas Negropontes group had created a suite of dazzling demonstration projects (the seed work for the Media Lab) that included a movie map of Aspen, Colorado, and a movie manual for car repair. The combination of text, video, and navigable space suggested that a computer-based microworld need not be mathematical but could be shaped as a dynamic fictional universe with characters and events. My interest in creating narrative microworlds coincided with the interests of foreign language teachers in creating immersive learning environments. Together we designed multimedia applications for learning Spanish and French, which motivated students by giving them a role in an unfolding story and allowing them to move through authentically photographed environments as if they were on a visit to Bogota or Paris. These projects and others that I have worked on in the past fifteen years -- including a Shakespeare archive and a film art digital textbook -- as well as many kindred efforts pursued by others elsewhere, have confirmed my view of the computer as offering a thrilling extension of human powers. I say this despite the often agonizing uncertainties of software development and the continual frustration over the gap between what designers want the hardware and software to do and what they actually support. For my experience in humanities computing has convinced me that some kinds of knowledge can be better represented in digital formats than they have been in print. The knowledge of a foreign language, for instance, can be better conveyed with examples from multiple speakers in authentic environments than with lists of words on a page. The dramatic power of Hamlets soliloquies is better illustrated by multiple performance examples in juxtaposition with the text than by the printed version alone. Discussions of film art make more sense when they are grounded by excerpted scenes from the movies being discussed. Computers can present the text, images, and moving pictures valued by humanistic disciplines with a new precision of reference; they can show us all the different ways a French person says hello in a single day or all the passages Zeffirelli chose to leave out of his production of ROMEO AND JULIET. By giving us greater control over different kinds of information, they invite us to tackle more complex tasks and to ask new kinds of questions. Although the computer is often accused of fragmenting information and overwhelming us, I believe this view is a function of its current undomesticated state. The more we cultivate it as a tool for serious inquiry, the more it will offer itself as both an analytical and a synthetic medium. My experiences in educational computing have also offered me evidence of how frightening the new technologies can be. Several years ago I was invited to talk with the committee that was then overseeing the production of a variorum of Shakespeare, a set of editions of individual plays with extensive annotation covering all known textual variants as well as notes on the significant critical commentary on the plays. The variorum format dates back to the nineteenth century and was still an endearingly Victorian endeavor. The pace of production was glacial, with many of the editors collecting their notes in stacks of index cards and filling hundreds of shoe boxes with twenty years worth of investigation before publishing. The night before my appearance I was invited for a drink in a high-rise New York hotel room by two of the most computer-friendly committee members. I had already received an irate note from another member of the committee, and my hosts, an English woman and a Southern man, were anxious to prepare me for the kind of opposition that others might offer. My scrupulously polite colleagues displayed a courtly commitment to moving the variorum into the digital age while avoiding offending anyone. With the naivete of someone who had spent much of the past twenty years in the company of engineers, I told them that my remarks would be limited to the obvious practicalities of their work. Clearly, the pages of a book were a poor match for the task at hand. Often the text of the play took up only a single line at the top, with the rest of the page covered with footnotes in several numbering schemes, many of which were condensed to cryptic abbreviations that conveyed no information to the uninitiated. Thus, commentary for a line of text often appeared a dozen pages away from the line it referred to. The effort of compiling a variorum edition was clearly heroic, but the arbitrary limitation of the printed page was a disservice to the depth of information and expertise involved. At this point in my preview of the next days presentation, my genteel hostess started to shake in her chair. I love the book! she cried. If you are coming to talk against the book tomorrow, I will throw you out the window. And though she was considerably smaller than I, she looked quite prepared to do so. Why should the prospect of a scholarly CD-ROM bring a mild-mannered Shakespearean editor to such paroxysms of violence To my mind it was because she could not separate the activities of research from the particular form they had historically assumed. Her love of books (which I share) momentarily blinded her to the true object of reverence: the creation of a superb reference work. Her reaction was a sign that the new technologies are extending our powers faster than we can assimilate the change. Even when we are already engaged in enterprises that cry out for the help of a computer, many of us still see the machine as a threat rather than an ally. We cling to books as if we believed that coherent human thought is only possible on bound, numbered pages. I am not among those who are eager for the death of the book, as I hope the present volume demonstrates. Nor do I fear it as an imminent event. The computer is not the enemy of the book. It is the child of print culture, a result of the five centuries of organized, collective inquiry and invention that the printing press made possible. My work as a software developer has made me painfully aware of the primitive nature of the current digital medium and of the difficulty of predicting what it can or cannot do in any given time scheme. Nevertheless, I find myself longing for a computer-based literary form even more passionately than I have longed for computer-based educational environments, in part because my heart belongs to the hackers. I am hooked on the charm of making the dumb machines sing. Since 1992 I have been teaching a course on how to write electronic fiction. My students include freshmen, writing majors, and Media Lab graduate students. Some of them are virtuoso programmers. Some of them do not program at all. All of them are drawn to the medium because they want to write stories that cannot be told in other ways. These stories cover every range and style, from oral histories to adventure tales, from the exploits of comic book heroes to domestic dramas. The only constant in the course is that every year what is written is even more inventive than what was written the year before. Every year my students arrive in class feeling more at home with electronic environments and are more prepared to elicit something with the tone of a human voice out of the silent circuitry of the machine. As I watch the yearly growth in ingenuity among my students, I find myself anticipating a new kind of storyteller, one who is half hacker, half bard. The spirit of the hacker is one of the great creative wellsprings of our time, causing the inanimate circuits to sing with ever more individualized and quirky voices; the spirit of the bard is eternal and irreplaceable, telling us what we are doing here and what we mean to one another. I am drawn to imagining a cyberdrama of the future by the same fascination that draws me to the Victorian novel. I see glimmers of a medium that is capacious and broadly expressive, a medium capable of capturing both the hairbreadth movements of individual human consciousness and the colossal crosscurrents of global society. Just as the computer promises to reshape knowledge in ways that sometimes complement and sometimes supersede the work of the book and the lecture hall, so too does it promise to reshape the spectrum of narrative expression, not by replacing the novel or the movie but by continuing their timeless bardic work within another framework. This book is an effort to imagine what kinds of pleasures such a cyberliterature will bring us and what sorts of stories it might tell. I believe that we are living through a historic transition, as important to literary history as it is to the history of information processing. My sixteen-year-old son will no doubt look back upon the moment at which we (finally!) got our home computer hooked up to the World Wide Web with the same delight with which my father recalled plucking voices out of the air with his home-made crystal radio set. My paternal grandmother, who started life in a Russian shtetl, jumped in terror when she heard that disembodied speech, thinking it must be a dybbuk or ghost. Yet only a few decades later, I sat in my crib, as my mother fondly reports, calmly enraptured by the voice of Arthur Godfrey. Today, my husband collects tapes of old Bob and Ray programs, which we listen to on long car trips, savoring the intimacy of what now seems like a touchingly low-tech format. Those of us who have spent our lives in love with books may always approach the computer with something of my grandmothers terror before the crystal radio, but our children are already at home with the joystick, mouse, and keyboard. They take the powerful sensory presence and participatory formats of digital media for granted. They are impatient to see what is next. This book is an attempt to imagine a future digital medium, shaped by the hackers spirit and the enduring power of the imagination and worthy of the rapture our children are bringing to it. Excerpt from Hamlet on the Holodeck copyright © 1997 by Janet Horowitz Murray. Reprinted by permission of the publisher, Simon & Schuster.",,Murray JH,,1998,,,,Book
"Efficient Mining of the Multidimensional Traffic Cluster Hierarchy for Digesting, Visualization, and Anomaly Identification","Mining traffic to identify the dominant flows sent over a given link, over a specified time interval, is a valuable capability with applications to traffic auditing, simulation, visualization, as well as anomaly detection. Recently, Estan advanced a comprehensive data mining structure tailored for networking data-a parsimonious, multidimensional flow hierarchy, along with an algorithm for its construction. While they primarily targeted offline auditing, use in interactive traffic visualization and anomaly/attack detection will require real-time data mining. We suggest several improvements to Estan's algorithm that substantially reduce the computational complexity of multidimensional flow mining. We also propose computational and memory-efficient approaches for unidimensional clustering of the IP address spaces. For baseline implementations, evaluated on the New Zealand (NZIX) trace data, our method reduced CPU execution times of the Estan method by a factor of more than eight. We also develop a methodology for anomaly/attack detection based on flow mining, demonstrating the usefulness of this approach on traces from the Slammer and Code Red worms and the MIT Lincoln Laboratories DDoS data",,"Wang J,Miller DJ,Kesidis G",,2006,1929–1941,10.1109/JSAC.2006.877216,https://doi-org.proxy.bnl.lu/10.1109/JSAC.2006.877216;http://dx.doi.org/10.1109/JSAC.2006.877216,Journal Article
Estimating Performance Efficiency of Mining and Extracting Sectors Using DEA Models: The Case of Jordan,"In this study, we estimated the performance efficiency of the Jordanian mining and extracting sector based on Data Envelopment Analysis (DEA). The utilized dataset includes 6 out of 15 corporations that reflect around 90% of the total market capitalization under the mining and extracting sector in the Amman Stock Exchange (ASE). The sample consists of 126 observations from 2000 to 2020. It should be noted that estimating the efficiency of the sector based on time series for each company is not mentioned in the literature review. Therefore, we applied BCC (Banker–Charnes–Cooper) models to estimate performance efficiency and compared between input and output models under DEA. We also estimated the average performance efficiency of the sector to detect weaknesses/strengths among companies. The market capitalization and the operating revenue are used to evaluate the companies’ performance. In addition to the performance variables as output to the DEA models, the current assets, non-current assets, operating expenses, and general administrative expenses are also used as input variables under the DEA models. This study also examined the effect of Gross Domestic Product (GDP) growth and Return on Assets (ROA) on performance efficiency scores for BCC models. In the results, we found that there are differences in performance efficiency across time series in each company based on dynamic BCC models. It is observed that the performance efficiency of NAST Company is better than the other companies based on BCC (Input/output). The GDP growth and ROA reveal the effect on efficiency performance under BCC models. The proposed model can be used to improve the performance efficiency of companies in stock exchange markets.",,"Jaber JJ,Beldjilali F,Shehadeh AA,Hamadneh NN,Saleh M,Tahir M,Al Wadi S,Zhou Y",,2022,,10.1155/2022/3688381,https://doi-org.proxy.bnl.lu/10.1155/2022/3688381;http://dx.doi.org/10.1155/2022/3688381,Journal Article
Measuring the Impact of the Copyright Amendment Act on New Zealand Residential DSL Users,"The Copyright (Infringing File Sharing) Amendment Act 2011 (CAA) is a New Zealand law that aims to provide copyright holders with legal recourse when content is illegally shared over the Internet. This paper presents a study of residential DSL user behaviour using packet traces captured at a New Zealand ISP before, shortly after and several months after the CAA coming into effect. We use libprotoident to classify the observed traffic based on the application protocol being used to identify and examine any changes in traffic patterns that may be a result of the new law. We find that the use of peer-to-peer applications declined significantly once the CAA was in effect, suggesting a strong correlation. We also found that there were increases in tunneling, secure file transfer and remote access traffic amongst a small segment of the user population, which may indicate an increased uptake in the use of foreign seedboxes to bypass the jurisdiction of the CAA.",,"Alcock S,Nelson R",,2012,551–558,10.1145/2398776.2398833,https://doi-org.proxy.bnl.lu/10.1145/2398776.2398833;http://dx.doi.org/10.1145/2398776.2398833,Conference Paper
The Internet of Money,"The Internet of Money Volume Two: a collection of talks is the spectacular sequel to the cult classic and best seller The Internet of Money Volume One: a collection of talks by Andreas M. Antonopoulos. Volume Two contains 11 of his most inspiring and thought-provoking talks, including: Introduction to Bitcoin; Blockchain vs Bullshit; Fake News, Fake Money; Currency Wars; Bubble Boy and the Sewer Rat; Rocket Science and Ethereum's Killer App; and many more. Volume Two also includes an all new frequently asked questions section! In 2013, Andreas M. Antonopoulos started publicly speaking about bitcoin and quickly became one of the world's most sought-after speakers in the industry. To date, he has delivered more than 75, TED-style talks in venues ranging from the Henry Ford Museum in the United States to packed-out Bitcoin Meetups around the world including Brazil, the Czech Republic, and New Zealand, and every talk is completely different. In these performances, Antonopoulos walks onto the stage and delivers a live, unscripted talk. Without a deck in sight, he unleashes his latest insights into the lightning-fast changes surrounding bitcoin. Combining the knowledge of one of the world's leading blockchain technologists, with cultural context, comedy, and the flair of a performance artist, Antonopoulos conveys an up-to-the-second understanding of bitcoin to live audiences worldwide. Many of these talks were so visionary, their content so educational, that they were curated and refined into a book form. On 7 September 2016, The Internet of Money Volume One was launched on The Joe Rogan Experience podcast (the interview has since been viewed more than 300,000 times). With its genesis in the lived, human experience, The Internet of Money offered something that was desperately needed: an explanation of the philosophy, economics, politics, poetics, and technologies of bitcoin and open blockchains set within a broad historical context and using clear, simple language that delighted general audiences and bitcoin enthusiasts alike. During its first year, Volume One quickly became a hit in the global crypto-currency community-appealing to audiences from fields as diverse as the arts, sciences, and humanities. As one reader wrote: ""It provides a uniquely accessible take on a mind-bendingly abstract system."" The Internet of Money Volume Two: a collection of talks builds on that momentum and offers readers an opportunity to experience more these inspiring and thought-provoking talks in print. It also includes a bonus question and answer section, where Andreas answers some of the most frequently asked questions from audience members during his worldwide tour. Volume Two is a sequel that rivals, even exceeds, the first, in content, scope, and vision. These talks are intellectual fire-starters you won't want to miss. Make this book part of your collection and see why Andreas M. Antonopoulos is considered the most powerful and engaging voice in crypto-currency and blockchain.",,Antonopoulos AM,,2017,,,,Book
A Game to Teach Network Communication Reliability Problems and Solutions,"After the recent introduction of Computer Science into New Zealand High Schools, a lack of coherent resources for the new topics has created challenges for teachers and students working with the new qualifications. In response to this, a ""Computer Science Field Guide"" has been developed, which provides an on-line open-source ""text book"" that contains chapters with rich content that covers each topic required, each including videos, games and interactive applications. The Network Communication Protocols topic has been particularly difficult to cover because it is focuses on the key concepts in protocols, whereas most teaching material available is about using protocols rather than creating them. Also, the field guide takes a constructivist approach, and there is little material available that approaches the topic this way. This paper presents the research and development of a new interactive game for teaching Network Communication Protocols. Packet Attack is designed to teach key concepts around communication reliability problems and their solutions with respect to the transmission control protocol. The game has a novel approach of letting the user become the problem (unreliability of transmission) to try to prevent messages getting through, rather than them trying to solve the issues.",,"Jarman S,Bell T",,2014,43–49,10.1145/2670757.2670773,https://doi-org.proxy.bnl.lu/10.1145/2670757.2670773;http://dx.doi.org/10.1145/2670757.2670773,Conference Paper
Scene Collaging: Analysis and Synthesis of Natural Images with Semantic Layers,"To quickly synthesize complex scenes, digital artists often collage together visual elements from multiple sources: for example, mountains from New Zealand behind a Scottish castle with wisps of Saharan sand in front. In this paper, we propose to use a similar process in order to parse a scene. We model a scene as a collage of warped, layered objects sampled from labeled, reference images. Each object is related to the rest by a set of support constraints. Scene parsing is achieved through analysis-by-synthesis. Starting with a dataset of labeled exemplar scenes, we retrieve a dictionary of candidate object segments that match a query image. We then combine elements of this set into a ""scene collage"" that explains the query image. Beyond just assigning object labels to pixels, scene collaging produces a lot more information such as the number of each type of object in the scene, how they support one another, the ordinal depth of each object, and, to some degree, occluded content. We exploit this representation for several applications: image editing, random scene synthesis, and image-to-anaglyph.",,"Isola P,Liu C",,2013,3048–3055,10.1109/ICCV.2013.457,https://doi-org.proxy.bnl.lu/10.1109/ICCV.2013.457;http://dx.doi.org/10.1109/ICCV.2013.457,Conference Paper
Digital Literacy and Knowledge Societies,"With a structurally entrenched digital divide on the one hand, and increasing ubiquity of the Internet in a techno-centric world on the other, the imperative to exploit information and knowledge for development remains a significant driver for equitable growth. It is posited that the silver-bullet for reducing this gap lies in increasing digital literacies within a society in order integrate segments who may be marginalized into the inclusive mainstream. In enabling greater and wider participation of digital citizens in their countries' socio-economic activities, the opportunities of a sustainable economy arise. This article is a study of ICT policies, applications and the resulting transformations in five mature economies committed to the vision of knowledge-based development with high levels of digital participation among their citizens. Specifically, using a multi-dimensional scorecard derived from prior work, we conduct a grounded theory investigation of how the five societies have applied digital literacies in knowledge-intensive public services such as education, healthcare and e-government, to derive best practices as well as lessons learned. This study investigates the significance of digital literacy programmes on sustainable development in a knowledge society.Specifically, the notion of digital entitlements that promote the inclusion and participation of the community is seen as an effective motivation for exploiting the opportunities that are available over the Internet and new media.A Constructivist Grounded Theory approach was taken to examine the track-record of five successful knowledge societies - Finland, Hong Kong, Qatar, New Zealand and Singapore - in the areas of providing education, health and government services over digital platforms.While our analysis and findings reveal some interesting ""best practices"" as well as caveats, it is clear that digital literacy initiatives are not ends by themselves but means to an end.If the desired outcome of digital policies in knowledge societies is sustainable growth and development; we theorise that policies must focus on effective digital entitlements such as infrastructure, governance, human development and innovation.",,"Sharma R,Fantin AR,Prabhu N,Guan C,Dattakumar A",,2016,628–643,10.1016/j.telpol.2016.05.003,https://doi-org.proxy.bnl.lu/10.1016/j.telpol.2016.05.003;http://dx.doi.org/10.1016/j.telpol.2016.05.003,Journal Article
Continuous Transition in Outsourcing: A Case Study,"Outsourcing is typically considered to occur in three phases: decision, transition and operation. As outsourcing is now well established the switching of vendors and transitioning from one system to another is common. However, most of the research to date on outsourcing has focused on the decision and operation phases, leaving a gap between theory and practice concerning the transition phase. Transition in outsourcing entails the changing of systems, business processes and/or vendors. If a suitable transition approach is not applied pressures for another transition can immediately build. This paper presents results from a case study carried out on the 'Novopay Project' in which the Ministry of Education in New Zealand changed their vendor from an onshore to a near-shore provider. This project resulted in a sequence of three transitions, with each following a different approach as a direct result of the experiences encountered in the previous transition. In this research we made use of the rich 'data dump' of evidence provided by the Ministry of Education (MoE). Our analysis describes how a client organization can become trapped in a continuous transition cycle if a suitable approach is not applied. Transition1 involved the client - MoE - moving from complete outsourcing to selective insourcing. After realizing that they did not have the capabilities to manage insourcing, Transition2 was initiated. In Transition2 the sourcing approach reverted back to complete outsourcing. When it was realized that the new vendor in Transition2 could not in fact deliver a new service model or support end-users in following new business processes, Transition3 was initiated. In Transition3, the client established an internal company to insource service operations to support end-users. Transition can be a sound business strategy initiated for a range of reasons. However, if a flawed sourcing approach is chosen it can result in 'continuous transition'.",,"Raza B,Clear T,MacDonell SG",,2017,41–50,10.1109/ICGSE.2017.6,https://doi-org.proxy.bnl.lu/10.1109/ICGSE.2017.6;http://dx.doi.org/10.1109/ICGSE.2017.6,Conference Paper
Multivariate Sequential Analytics for Treatment Trajectory Forecasting,"Chronic conditions, especially cardiovascular disease account for a large burden on modern healthcare systems. These conditions are by their nature ones that unfold over a long period of time, typically involving many healthcare events, treatments and changes of patient status. The gold standard in public health informatics for risk assessment is regression-based. While these techniques are effective in identifying factors contributing to risk, they produce reductive scores (e.g. probability of a specific class of event, like a heart attack) or binary prediction results, and moreover, they are sequence agnostic. In the area of long-term chronic disease management, multivariate sequential modeling offers an opportunity to forecast disease progression and treatment trajectory in a fine-grained manner in order to aid clinical decision making. This paper investigates the suitability of Long short-term memory, a type of recurrent neural network, in conducting multivariate sequential modeling in the healthcare domain, specifically in the task of forecasting. The eventual goal is to apply this technique to linked New Zealand health data through the Vascular Informatics using Epidemiology and the Web (VIEW) research project. This paper presents initial experiments and results for modeling patients' treatment trajectories during hospitalization using the Medical Information Mart for Intensive Care (MIMIC-III) data set.",,"Hsu W,Warren J,Riddle P",,2019,,10.1145/3290688.3290724,https://doi-org.proxy.bnl.lu/10.1145/3290688.3290724;http://dx.doi.org/10.1145/3290688.3290724,Conference Paper
Mixed Methods Action Research: Intervention Strategies for Employee Turnover in Ethnic Asian Enterprises in New Zealand,"Excessive employee turnover can pose a threat to a firm's growth and survival. This is particularly true for small ethnic Asian businesses that rely heavily on human labour input with cultural and language challenges. This paper sets out to develop effective intervening strategies for the high labour turnover found in ABC (pseudonym), a small ethnic Asian company in New Zealand that provides commercial cleaning and shopping trolley collections services. This study used a multistrand mixed method action research (MMAR) approach that leverages discussions, a survey and interviews for data collection in the cycle of action research (AR) proposed by Coghlan and Brannick: 'constructing' (Phase 1), 'planning action' (Phase 2), 'taking action' (Phase 3), and 'evaluating action' (Phase 4). This design helps cross-validate the gathered data and enhance the rigour and credibility of the research outcomes.In Phase 1, having identified excessively high employee turnover as the research problem, the subsequent literature review revealed three candidate intervening variables: leadership styles, job satisfaction and level of ethnic entrepreneurship (co-ethnic community involvement). In Phase 2, data were collected and analyzed using a mixed method to understand the impact of the intervening variables on turnover and identify the areas for improvement when applying the found-to-be effective variables in ABC. The quantitative data was collected from employees of ethnic Asian companies including ABC. The statistical analyses on 222 usable questionnaires suggested that two variables (leadership styles – supportive and participative, and job satisfaction) were found to be the strong predictors of employee withdrawal intention. Interestingly, it was not possible from the data to claim a moderating effect of ethnic entrepreneurship on the relationships between leadership and turnover propensity. The succeeding qualitative study gathered the data from twelve ABC employees via phone. The interview results were largely aligned with the quantitative findings. They confirmed the beneficial effect of supportive and participative leadership styles on job satisfaction, and highlighted the detrimental effect of the directive style. In phase 3, the meta-inferences gained from merging the outcomes of Phase 2 were validated in ABC's context through the discussions with ABC executives. These yielded three feasible action plans with six strategies to tackle employee turnover under leadership styles and job satisfaction categories: taking leadership training, facilitating effective communication systems (changing the frequencies and mode of the communications), and providing non-monetary rewards (free snacks, job titles, and celebrating personal and work milestones). In phase 4, the suggested action plans are evaluated and consideration is given for future research.Overall, this MMAR study fulfilled its objective of producing context-specific outcomes to my real work context. At the same time, it has contributed to the body of knowledge by extending the Western and large organisation oriented turnover study, to the small ethnic Asian companies in New Zealand. However, the suggested strategies are not the final solutions to the problem, and measuring their effect remains a task for future research as the second cycle of action research (AR).",,Kim RS,,2017,,,,Ph.D. Thesis
Fundamental Issues in Defense Training and Simulation,"Defense forces have always invested a great deal of their resources in training. In recent times, changes in the complexity and intensity of operations have reaffirmed the importance of ensuring that warfighters are adequately prepared for the environments in which they are required to work. The emergence of new operational drivers such as asymmetric threats, urban operations, joint and coalition operations and the widespread use of military communications and information technology networks has highlighted the importance of providing warfighters with the competencies required to act in a coordinated, adaptable fashion, and to make effective decisions in environments characterized by large amounts of sometimes ambiguous information. While investment in new technologies can make available new opportunities for action, it is only through effective training that personnel can be made ready to apply their tools in the most decisive and discriminating fashion. There are many factors which can have an impact on the efficacy of training and many issues to consider when designing and implementing training strategies. These issues are often complex and nuanced, and in order to grasp them fully a significant investment of time and energy is required. However, the requirement to respond quickly to ever-changing technology, a high operational tempo and minimal staffing may preclude many in today's defense forces from seeking out all such resources on their own. This edited collection provides brief, easy-to-understand summaries of the key issues in defense training and simulation, as well as guidance for further reading. It consists of a collection of short essays, each of which addresses a fundamental issue in defense training and simulation, and features an up-to-date reference list to enable the reader to undertake further investigation of the issues addressed. In essence, this book provides the optimum starting point, or first resource, for readers to come to terms with the important issues associated with defense training and simulation. The contributions are written by leading scholars from military research institutions in the US, UK, Canada, Australia and New Zealand, as well as selected researchers from academic and private sector research institutions.",,"Galanis G,Sottilare R,Best C",,2017,,,,Book
Fundamental Issues in Defense Training and Simulation,"Defence forces have always invested a great deal of their resources in training. In recent times, changes in the complexity and intensity of operations have reaffirmed the importance of ensuring that warfighters are adequately prepared for the environments in which they are required to work. The emergence of new operational drivers such as asymmetric threats, urban operations, joint and coalition operations and the widespread use of military communications and information technology networks has highlighted the importance of providing warfighters with the competencies required to act in a coordinated, adaptable fashion, and to make effective decisions in environments characterised by large amounts of sometimes ambiguous information. While investment in new technologies can make available new opportunities for action, it is only through effective training that personnel can be made ready to apply their tools in the most decisive and discriminating fashion, and by doing so transform military technology into defence capability.There are many factors which can have an impact on the efficacy of training, and there are therefore many issues to consider when designing and implementing training strategies. These issues are often complex and nuanced, and in order to grasp them fully a significant investment of time and energy is required. However, the requirement to respond quickly to ever-changing technology, a high operational tempo and minimal staffing may preclude many in today's defence forces from seeking out all such resources on their own.This edited collection provides brief, easy-to-understand summaries of the key issues in defence training and simulation, as well as guidance for further reading. It consists of a collection of short essays and frequently asked questions, each of which addresses a fundamental issue in defence training and simulation, and features an up-to-date reference list to enable the reader to undertake further investigation of the issues that are addressed. In essence, this book provides the optimum starting point, or first resource, for readers to come to terms with the important issues associated with defence training and simulation. The contributions are written by leading scholars from military research institutions in the US, UK, Canada, Australia and New Zealand as well as selected researchers from academic and private sector research institutions.",,"Best C,Galanis G,Kerry J,Sottilare R",,2013,,,,Book
Measuring the Organizational Resilience of Critical Infrastructure Providers,"Modern societies are becoming increasingly dependent on critical infrastructure services. This dependence is not only on the technology used in infrastructures, but also on the organizations that manage the infrastructures. Initiatives that assess infrastructure resilience often concentrate on strengthening the physical infrastructure through robustness and redundancy. Few studies recognize the important role of critical infrastructure providers. This study presents a method for assessing the organizational resilience of critical infrastructure providers. The method is demonstrated using data from a group of critical infrastructure providers in New Zealand. The application of the Benchmark Resilience Tool developed by Resilient Organisations reveals that the surveyed organizations are strong in effective partnerships, but are weak in breaking silos and in conducting stress testing plans. The results also indicate that senior managers have much more positive views of the resilience of their organizations compared with other staff members.",,"Brown C,Seville E,Vargo J",,2017,37–49,10.1016/j.ijcip.2017.05.002,https://doi-org.proxy.bnl.lu/10.1016/j.ijcip.2017.05.002;http://dx.doi.org/10.1016/j.ijcip.2017.05.002,Journal Article
DocEng '16: Proceedings of the 2016 ACM Symposium on Document Engineering,"It is both an honor and a pleasure to hold the 16th ACM Symposium on Document Engineering, DocEng 2016, at the TU Wien, Austria, organized by the Computer Vision Lab (CVL). DocEng is the leading international ACM symposium for researchers, practitioners, developers, and users to explore cutting-edge ideas and to exchange techniques, tools, and experiences in the domain of document engineering. It aims at bringing together researchers in the fields of computer vision, multimedia technologies, image processing, image analysis, information and systems analysis, electronic publishing, business process analysis, and business informatics. The symposium is intended as convention of renowned experts in all areas of document engineering of both academia and industry to present and discuss recent progress and advances in the fields of: document models and structures, document representation and standards, distributed documents, collaborative documents and the sharing economy, document internationalization, multilingual representations, document authoring tools and systems, document presentation (typography, formatting, layout), automatically generated documents, content customization, variable printing, documents for mobile devices, web document processing and interaction, document repositories, massive collections of documents, digital libraries and archives, secure document workflows, collaborative authoring and editing, culture-dependent layouts, and many more.Our call for papers attracted submissions from 27 countries (Australia, Austria, Brazil, Canada, China, France, Germany, Greece, India, Indonesia, Iran, Italy, Japan, Korea, Macao, Malta, Netherlands, New Zealand, Romania, Russian Federation, Slovakia, Spain, Switzerland, Tunisia, United Kingdom, United States, Vietnam). All papers were carefully reviewed by a minimum of three Program Committee members, upon which decisions for acceptance were based on correctness, presentation, technical depth, scientific significance and originality. The Program Committee accepted 11 of 35 reviewed full paper submissions (31%) and 12 of 36 reviewed short paper submissions (33%) for oral presentation, for a combined acceptance rate of 32%. A further 10 short paper submissions were accepted for poster presentation.This year's program includes a Doctoral Consortium as a special session for the fourth time, where doctoral students in their second year or later present their dissertation project and get feedback from a panel of senior researchers as well as from the general audience. This session, called ProDoc@DocEng, is intended to provide students with constructive criticism and helps them in formulating their research question, deciding about methods and approaches to use, and creating further ideas. This is one of the key ways in which we support the future generation of researchers in Document Engineering.A true highlight of this year's DocEng are the valuable and insightful keynote talks: Design Is Not What You Think It Is, Peter Bi?ak, founder of the Typotheque design studio and Lecturer at the Royal Academy of Arts in The Hague, NetherlandsResearch Infrastructures, or How Document Engineering, Cultural Heritage and Digital Humanities Can Go Together, Günter Mühlberger, from the University of Innsbruck, AustriaThe Proceedings of DocEng 2016 contain the papers in the same order as they were presented at the conference, grouped by their corresponding thematic session. In putting these Proceedings together, many people played a significant role which we would like to acknowledge: First of all, our thanks are due to the authors who contributed their work to the symposium. Secondly, we are grateful for the dedicated work of the 60 members of the Program Committee for their effort in evaluating the submitted papers and in providing the necessary decision support information and the valuable feedback for the authors. We also thank Sonja Schimmler for organizing the first day with two tutorials and two workshops, Cerstin Mahlow for coordinating ProDoc@DocEng, Charles Nicholas for chairing the Birds of a Feather session, and Ethan Munson for his support regarding the Student Travel Awards. We also thank the Steering Committee and in particular Steven Simske for their support.",,,,2016,,,,Book
The Study of Expanded Polytetrafluoroethylene New Material in Dural Repair,"Dural defect is a common problem in neurosurgery, and the emergence of expanded polytetrafluoroethylene material provides an effective solution for the rehabilitation of artificial blood vessels, heart patches, and other fields. However, studies on the repair of expanded polytetrafluoroethylene in the dura have reported the occurrence of adverse events of cerebrospinal fluid leakage. Therefore, the task of improving expanded polytetrafluoroethylene materials cannot be delayed. In this study, a new composite dural repair material based on expanded polytetrafluoroethylene and polylactic acid-glycolic acid (expanded polytetrafluoroethylene/polylactic acid-glycolic acid) was designed and synthesized. The results of in vivo experiments confirmed that the material can fully meet the requirements of repairing the integrity of the dura mater, providing protection for the intracranial structure and rebuilding the extracellular matrix. More importantly, the new composite dural repair material can greatly reduce the incidence of cerebrospinal fluid leakage and inhibit inflammation. Therefore, the application data of this study on New Zealand rabbit species will lay an important foundation for the development of dural repair technology.",,"Xu YQ,Gao WB,Xing MY,Gao Y,Zhang HT,Chen W,Liu PF,Velmurugan P",,2022,,10.1155/2022/4143413,https://doi-org.proxy.bnl.lu/10.1155/2022/4143413;http://dx.doi.org/10.1155/2022/4143413,Journal Article
The Analysis of Gold in Plants and Soils by Inductively Coupled Plasma Mass Spectrometry,"Inductively coupled plasma mass spectrometry (ICP-MS) was employed for the determination of gold in plant materials. Optimum conditions for maximum gold (197Au) signal were found to be a nebuliser flow rate of 1.02 l min-1 at a forward power of 1.2 kW. Dissolution of plant materials was achieved by investigating two procedures. Excellent recovery (102±3.6%) and reproducibility were obtained for procedure 2 (i.e. a simple aqua regia-hydrofluoric acid attack). Wash out with an 8% aqua regia solution eliminated gold 'memory effects'. Internal standardisation was achieved by iridium (193Ir). Quality control assurance for gold analyses was insured by NIST 1571 Orchard Leaves and two 'in-house' reference materials. The methodologies developed were applied to three separate studies. Under experimental conditions bryophytes were treated with solutions containing gold and multi-elements. All species exhibited a high degree of gold accumulation, as high as 6.90 μg g-1 (dry wt.)-producing a percentage uptake from solution of 69%. Significant relationships were noted between gold levels and elements reported in the literature to be associated with gold (i.e. Ag, As, Fe). Secondly, a region of known gold mineralisation in Scotland was investigated. Three distinct zones of gold mineralisation were noted, with gold levels of 620 and 302 ng g-1 (dry wt.), in contrast to background levels of 3.9 and 4.2 ng g-1 (dry wt.) in plant and soil materials, respectively. Finally, plant and soil materials were removed from the vicinity of a gold mine in New Zealand. The highest gold levels were observed for bryophytes with a mean gold concentration of 4791 ng g-1 (dry wt.). This work highlights the analytical difficulties in determining gold in plant materials by ICP-MS and demonstrates the efficacious use of bryophytes as a plant medium for the uptake of gold.",,Williams CA,,1996,,,,Ph.D. Thesis
A Digital Rock Density Map of New Zealand,"Digital geological maps of New Zealand (QMAP) are combined with 9256 samples with rock density measurements from the national rock catalogue PETLAB and supplementary geological sources to generate a first digital density model of New Zealand. This digital density model will be used to compile a new geoid model for New Zealand. The geological map GIS dataset contains 123 unique main rock types spread over more than 1800 mapping units. Through these main rock types, rock densities from measurements in the PETLAB database and other sources have been assigned to geological mapping units. A mean surface rock density of 2440kg/m^3 for New Zealand is obtained from the analysis of the derived digital density model. The lower North Island mean of 2336kg/m^3 reflects the predominance of relatively young, weakly consolidated sedimentary rock, tephra, and ignimbrite compared to the South Island's 2514kg/m^3 mean where igneous intrusions and metamorphosed sedimentary rocks including schist and gneiss are more common. All of these values are significantly lower than the mean density of the upper continental crust that is commonly adopted in geological, geophysical, and geodetic applications (2670kg/m^3) and typically attributed to the crystalline and granitic rock formations. The lighter density has implications for the calculation of the geoid surface and gravimetric reductions through New Zealand.",,"Tenzer R,Sirguey P,Rattenbury M,Nicolson J",,2011,1181–1191,10.1016/j.cageo.2010.07.010,https://doi-org.proxy.bnl.lu/10.1016/j.cageo.2010.07.010;http://dx.doi.org/10.1016/j.cageo.2010.07.010,Journal Article
The Adoption of ECommerce Communications and Applications Technologies in Small Businesses in New Zealand,"This research investigates the impact of 10 factors, extended from the technological innovation literature, on the adoption of different eCommerce communications and applications technologies (EC) in small businesses (SMEs) in New Zealand (NZ). The research results showed that the CEO's innovativeness was the only determinant of external-email adoption. CEO's involvement was found to be the only determinant of Intranet adoption. Relative advantage and competition were found to influence Extranet/VPN adoption significantly and positively. However, support from technology vendors appeared to violate its hypothesised effect on Extranet/VPN adoption. Regression analysis found that pressure from suppliers was the only determinant of Internet-EDI adoption. The adoption of Web sites was influenced by the information intensity of products and the CEO's innovativeness. The significant factors suggested the uniqueness of the adoption phenomenon in SMEs in NZ. However, the factors that appeared to be significant and the ones that appeared to be insignificant factors and the implications arising from these factors led to a conclusion which suggested the weakness of the EC adoption phenomenon in SMEs in NZ. The research discusses theoretical implications emerging from the research factors and portrays a path for future research.",,Al-Qirim N,,2007,462–473,10.1016/j.elerap.2007.02.012,https://doi-org.proxy.bnl.lu/10.1016/j.elerap.2007.02.012;http://dx.doi.org/10.1016/j.elerap.2007.02.012,Journal Article
Detecting Multiple Mean Breaks at Unknown Points in Official Time Series,"In this paper, we propose a computationally effective approach to detect multiple structural breaks in the mean occurring at unknown dates. We present a non-parametric approach that exploits, in the framework of least squares regression trees, the contiguity property of data generating processes in time series data. The proposed approach is applied first to simulated data and then to the Quarterly Gross Domestic Product in New Zealand to assess some of anomalous observations indicated by the seasonal adjustment procedure implemented in X12-ARIMA are actually structural breaks.",,"Cappelli C,Penny RN,Rea WS,Reale M",,2008,351–356,10.1016/j.matcom.2008.01.041,https://doi-org.proxy.bnl.lu/10.1016/j.matcom.2008.01.041;http://dx.doi.org/10.1016/j.matcom.2008.01.041,Journal Article
The Digital Estate,"The Digital Estate provides an analysis of the rights and liabilities associated with digital information passing from, to and through computing and other devices owned and controlled by fiduciaries, including trustees, personal representatives, lasting and other attorneys. Key features include: A practical guide to the administration of digital assets The book provides an analysis of the rights and liabilities associated with digital information passing from, to and through computing and other devices owned and controlled by fiduciaries, including trustees, personal representatives, lasting and other attorneys, partners and company directors Provides practical solutions to the problems that the individual and his personal representatives may face in securing succession to assets and safe transmission of information that may otherwise be deleted, locked or lost. Guides the practitioner through pre-death preventative measures relating to wills. Considers the issues that arise when an individual who owns assets or stores information on-line, dies Covers steps to be taken during probate for the administration of digital assets Covers claims and recovery, including costs and proceedings Covers specific roles in administration including personal representatives, trustees and agents and attorneys Includes clear practical guidance on the actions that should be taken or considered in the administration of digital information or assets including a useful section on drafting for the digital estate featuring precedents for will drafting and Lasting Powers of Attorney Examines the principles of English law that define the proprietary nature of information, taking into account the approach to this issue in other jurisdictions, such as the United States, Australia and New Zealand Identifies the property rights that are associated with information and examine their operation. These include intellectual property rights, contractual rights, and other rights, such as bitcoin and the associated blockchain technology. Investigates the principles applicable to the use of digital information that has no proprietary status. Analyses the regulatory consequences of the control or use of digital information, including data protection, financial regulation and computer misuse.",,Sagar L,,2017,,,,Book
An Insider's Perspective: Governance of Large ICT Software Projects in the Australian and New Zealand Public Sectors,"For many decades, world-wide, and across sectors, large ICT software projects have experienced ongoing poor outcomes with industry research indicating that almost all will fail to deliver to original expectations, some spectacularly so. There is much existing research on the causes of both public and private sector project failure, such as poor project management. Despite all this past learning and research, the problems continue.To address an identified gap in literature this research differentiates itself from other research by a number of factors. Firstly, it will focus on the collective Australian and New Zealand public sectors, where it is argued there is a dearth of targeted research. Secondly, these large projects all operate within institutional frameworks that provide the rules, guidelines, and controls for these projects. These collectively form the institutional governance of large ICT software projects. Given that the Australian and New Zealand public sectors also continue to have poor outcomes, yet they have historically developed institutional frameworks, there is something amiss. Therefore, the research puzzle is, how effective are these institutional frameworks in providing the governance for large ICT software projects in these sectors?To address this puzzle the research further differentiates itself from existing literature. The thesis applies an institutionalist's lens. To obtain the data a qualitative, interpretive, and comparative research design was applied. Seventy-five elite interviews were conducted, stakeholders who have had and continue to have direct involvement in these large projects and therefore have a very personal perspective on the institutional frameworks. This in effect is a collaborative exercise to discover the perspectives of the institutional governance from those most impacted.The narrative to emerge is that the institutional frameworks are in a state of inertia. They are failing to adapt due to a number of institutional factors. Change is costly, and politically and organisationally not prioritised. The frameworks 'stick' to a path historically implemented. Governance is imposing structure over agency. Leadership in governance is failing to collaborate. Finally, there is a culture of forgetting, from one project to the next. All have public policy implications.There is a perception that the inertia will continue. Therefore, the dominant perspective was to reduce the complexity. Stop undertaking large projects as traditionally planned, where a 'superhuman' capability is required, break them down into a series of smaller component-based projects. Actors with agency and entrepreneurial skills have done so successfully. However, they succeeded by circumventing the institutional frameworks to address their weaknesses. These entrepreneurs are also rare.To address the rather sad perspective that nothing much is likely to change, and that success will remain dependent upon chance, a more practical proposal was identified. Undertake a brutal independent assessment at the initiation stage of the likelihood of the project to deliver as planned. The assumption is that given the likelihood is you will have poor outcomes, that the forecast is just a guess, the agency/project need to explain how they will address this. If you have planned as a single large project, you cannot start. If you do not have a skilled, trained, and committed sponsor you cannot start. If you do not have the project management capability and capacity required, you cannot start. The logic is simple, if you do not have the ability to enable successful delivery, it is better to stop the project at the initiation stage and work on a revised plan until you determine how you can. Project funding also needs to change to support this approach, to be iterative and progressive based on results, delivery, and revised forecasts for the next stage.",,Douglas G,,2021,,,,Ph.D. Thesis
